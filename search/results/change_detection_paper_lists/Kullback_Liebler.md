count=2
* Informative Object Annotations: Tell Me Something I Don't Know
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Bracha_Informative_Object_Annotations_Tell_Me_Something_I_Dont_Know_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Bracha_Informative_Object_Annotations_Tell_Me_Something_I_Dont_Know_CVPR_2019_paper.pdf)]
    * Title: Informative Object Annotations: Tell Me Something I Don't Know
    * Year: `2019`
    * Authors: Lior Bracha,  Gal Chechik
    * Abstract: Capturing the interesting components of an image is a key aspect of image understanding. When a speaker annotates an image, selecting labels that are informative greatly depends on the prior knowledge of a prospective listener. Motivated by cognitive theories of categorization and communication, we present a new unsupervised approach to model this prior knowledge and quantify the informativeness of a description. Specifically, we compute how knowledge of a label reduces uncertainty over the space of labels and use this uncertainty reduction to rank candidate labels for describing an image. While the full estimation problem is intractable, we describe an efficient algorithm to approximate entropy reduction using a tree-structured graphical model. We evaluate our approach on the open-images dataset using a new evaluation set of 10K ground-truth ratings and find that it achieves over 65% agreement with human raters, close to the upper bound of inter-rater agreement and largely outperforming other unsupervised baseline approaches.
count=2
* Cross-Modal Deep Variational Hashing
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Liong_Cross-Modal_Deep_Variational_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Liong_Cross-Modal_Deep_Variational_ICCV_2017_paper.pdf)]
    * Title: Cross-Modal Deep Variational Hashing
    * Year: `2017`
    * Authors: Venice Erin Liong, Jiwen Lu, Yap-Peng Tan, Jie Zhou
    * Abstract: In this paper, we propose a cross-modal deep variational hashing (CMDVH) method to learn compact binary codes for cross-modality multimedia retrieval. Unlike most existing cross-modal hashing methods which learn a single pair of projections to map each example into a binary vector, we design a deep fusion neural network to learn non-linear transformations from image-text input pairs, such that a unified binary code is achieved in a discrete and discriminative manner using a classification-based hinge-loss criterion. We then design modality-specific neural networks in a probabilistic manner such that we model a latent variable to be close as possible from the inferred binary codes, at the same time approximated by a posterior distribution regularized by a known prior, which is suitable for out-of-sample extension. Experimental results on three benchmark datasets show the efficacy of the proposed approach.
count=2
* Provable convergence guarantees for black-box variational inference
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d0bcff6425bbf850ec87d5327a965db9-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d0bcff6425bbf850ec87d5327a965db9-Paper-Conference.pdf)]
    * Title: Provable convergence guarantees for black-box variational inference
    * Year: `2023`
    * Authors: Justin Domke, Robert Gower, Guillaume Garrigos
    * Abstract: Black-box variational inference is widely used in situations where there is no proof that its stochastic optimization succeeds. We suggest this is due to a theoretical gap in existing stochastic optimization proofs—namely the challenge of gradient estimators with unusual noise bounds, and a composite non-smooth objective. For dense Gaussian variational families, we observe that existing gradient estimators based on reparameterization satisfy a quadratic noise bound and give novel convergence guarantees for proximal and projected stochastic gradient descent using this bound. This provides rigorous guarantees that methods similar to those used in practice converge on realistic inference problems.
count=1
* Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Zero-TPrune_Zero-Shot_Token_Pruning_through_Leveraging_of_the_Attention_Graph_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Zero-TPrune_Zero-Shot_Token_Pruning_through_Leveraging_of_the_Attention_Graph_CVPR_2024_paper.pdf)]
    * Title: Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers
    * Year: `2024`
    * Authors: Hongjie Wang, Bhishma Dedhia, Niraj K. Jha
    * Abstract: Deployment of Transformer models on edge devices is becoming increasingly challenging due to the exponentially growing inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However most token pruning methods require computationally expensive fine-tuning which is undesirable in many edge deployment cases. In this work we propose Zero-TPrune the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. It leverages the attention graph of pre-trained Transformer models to produce an importance distribution for tokens via our proposed Weighted Page Rank (WPR) algorithm. This distribution further guides token partitioning for efficient similarity-based pruning. Due to the elimination of the fine-tuning overhead Zero-TPrune can prune large models at negligible computational cost switch between different pruning configurations at no computational cost and perform hyperparameter tuning efficiently. We evaluate the performance of Zero-TPrune on vision tasks by applying it to various vision Transformer backbones and testing them on ImageNet. Without any fine-tuning Zero-TPrune reduces the FLOPs cost of DeiT-S by 34.7% and improves its throughput by 45.3% with only 0.4% accuracy loss. Compared with state-of-the-art pruning methods that require fine-tuning Zero-TPrune not only eliminates the need for fine-tuning after pruning but also does so with only 0.1% accuracy loss. Compared with state-of-the-art fine-tuning-free pruning methods Zero-TPrune reduces accuracy loss by up to 49% with the same or higher throughput.
count=1
* Fair Feature Distillation for Visual Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Jung_Fair_Feature_Distillation_for_Visual_Recognition_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Jung_Fair_Feature_Distillation_for_Visual_Recognition_CVPR_2021_paper.pdf)]
    * Title: Fair Feature Distillation for Visual Recognition
    * Year: `2021`
    * Authors: Sangwon Jung, Donggyu Lee, Taeeon Park, Taesup Moon
    * Abstract: Fairness is becoming an increasingly crucial issue for computer vision, especially in the human-related decision systems. However, achieving algorithmic fairness, which makes a model produce indiscriminative outcomes against protected groups, is still an unresolved problem. In this paper, we devise a systematic approach which reduces algorithmic biases via feature distillation for visual recognition tasks, dubbed as MMD-based Fair Distillation (MFD). While the distillation technique has been widely used in general to improve the prediction accuracy, to the best of our knowledge, there has been no explicit work that also tries to improve fairness via distillation. Furthermore, We give a theoretical justification of our MFD on the effect of knowledge distillation and fairness. Throughout the extensive experiments, we show our MFD significantly mitigates the bias against specific minorities without any loss of the accuracy on both synthetic and real-world face datasets.
count=1
* VDSM: Unsupervised Video Disentanglement With State-Space Modeling and Deep Mixtures of Experts
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Vowels_VDSM_Unsupervised_Video_Disentanglement_With_State-Space_Modeling_and_Deep_Mixtures_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Vowels_VDSM_Unsupervised_Video_Disentanglement_With_State-Space_Modeling_and_Deep_Mixtures_CVPR_2021_paper.pdf)]
    * Title: VDSM: Unsupervised Video Disentanglement With State-Space Modeling and Deep Mixtures of Experts
    * Year: `2021`
    * Authors: Matthew J. Vowels, Necati Cihan Camgoz, Richard Bowden
    * Abstract: Disentangled representations support a range of downstream tasks including causal reasoning, generative modeling, and fair machine learning. Unfortunately, disentanglement has been shown to be impossible without the incorporation of supervision or inductive bias. Given that supervision is often expensive or infeasible to acquire, we choose to incorporate structural inductive bias and present an unsupervised, deep State-Space-Model for Video Disentanglement (VDSM). The model disentangles latent time-varying and dynamic factors via the incorporation of hierarchical structure with a dynamic prior and a Mixture of Experts decoder. VDSM learns separate disentangled representations for the identity of the object or person in the video, and for the action being performed. We evaluate VDSM across a range of qualitative and quantitative tasks including identity and dynamics transfer, sequence generation, Frechet Inception Distance, and factor classification. VDSM achieves state-of-the-art performance and exceeds adversarial methods, even when the methods use additional supervision.
count=1
* Minimally Invasive Surgery for Sparse Neural Networks in Contrastive Manner
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Minimally_Invasive_Surgery_for_Sparse_Neural_Networks_in_Contrastive_Manner_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Yu_Minimally_Invasive_Surgery_for_Sparse_Neural_Networks_in_Contrastive_Manner_CVPR_2021_paper.pdf)]
    * Title: Minimally Invasive Surgery for Sparse Neural Networks in Contrastive Manner
    * Year: `2021`
    * Authors: Chong Yu
    * Abstract: With the development of deep learning, neural networks tend to be deeper and larger to achieve good performance. Trained models are more compute-intensive and memory-intensive, which lead to the big challenges on memory bandwidth, storage, latency, and throughput. In this paper, we propose the neural network compression method named minimally invasive surgery. Different from traditional model compression and knowledge distillation methods, the proposed method refers to the minimally invasive surgery principle. It learns the principal features from a pair of dense and compressed models in a contrastive manner. It also optimizes the neural networks to meet the specific hardware acceleration requirements. Through qualitative, quantitative, and ablation experiments, the proposed method shows a compelling performance, acceleration, and generalization in various tasks.
count=1
* End-to-End Optimization of Scene Layout
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Luo_End-to-End_Optimization_of_Scene_Layout_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_End-to-End_Optimization_of_Scene_Layout_CVPR_2020_paper.pdf)]
    * Title: End-to-End Optimization of Scene Layout
    * Year: `2020`
    * Authors: Andrew Luo,  Zhoutong Zhang,  Jiajun Wu,  Joshua B. Tenenbaum
    * Abstract: We propose an end-to-end variational generative model for scene layout synthesis conditioned on scene graphs. Unlike unconditional scene layout generation, we use scene graphs as an abstract but general representation to guide the synthesis of diverse scene layouts that satisfy relationships included in the scene graph. This gives rise to more flexible control over the synthesis process, allowing various forms of inputs such as scene layouts extracted from sentences or inferred from a single color image. Using our conditional layout synthesizer, we can generate various layouts that share the same structure of the input example. In addition to this conditional generation design, we also integrate a differentiable rendering module that enables layout refinement using only 2D projections of the scene. Given a depth and a semantics map, the differentiable rendering module enables optimizing over the synthesized layout to fit the given input in an analysis-by-synthesis fashion. Experiments suggest that our model achieves higher accuracy and diversity in conditional scene synthesis and allows exemplar-based scene generation from various input forms.
count=1
* NestedVAE: Isolating Common Factors via Weak Supervision
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Vowels_NestedVAE_Isolating_Common_Factors_via_Weak_Supervision_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Vowels_NestedVAE_Isolating_Common_Factors_via_Weak_Supervision_CVPR_2020_paper.pdf)]
    * Title: NestedVAE: Isolating Common Factors via Weak Supervision
    * Year: `2020`
    * Authors: Matthew J. Vowels,  Necati Cihan Camgoz,  Richard Bowden
    * Abstract: Fair and unbiased machine learning is an important and active field of research, as decision processes are increasingly driven by models that learn from data. Unfortunately, any biases present in the data may be learned by the model, thereby inappropriately transferring that bias into the decision making process. We identify the connection between the task of bias reduction and that of isolating factors common between domains whilst encouraging domain specific invariance. To isolate the common factors we combine the theory of deep latent variable models with information bottleneck theory for scenarios whereby data may be naturally paired across domains and no additional supervision is required. The result is the Nested Variational AutoEncoder (NestedVAE). Two outer VAEs with shared weights attempt to reconstruct the input and infer a latent space, whilst a nested VAE attempts to reconstruct the latent representation of one image, from the latent representation of its paired image. In so doing, the nested VAE isolates the common latent factors/causes and becomes invariant to unwanted factors that are not shared between paired images. We also propose a new metric to provide a balanced method of evaluating consistency and classifier performance across domains which we refer to as the Adjusted Parity metric. An evaluation of NestedVAE on both domain and attribute invariance, change detection, and learning common factors for the prediction of biological sex demonstrates that NestedVAE significantly outperforms alternative methods.
count=1
* Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_Painting_Many_Pasts_Synthesizing_Time_Lapse_Videos_of_Paintings_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Painting_Many_Pasts_Synthesizing_Time_Lapse_Videos_of_Paintings_CVPR_2020_paper.pdf)]
    * Title: Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings
    * Year: `2020`
    * Authors: Amy Zhao,  Guha Balakrishnan,  Kathleen M. Lewis,  Fredo Durand,  John V. Guttag,  Adrian V. Dalca
    * Abstract: We introduce a new video synthesis task: synthesizing time lapse videos depicting how a given painting might have been created. Artists paint using unique combinations of brushes, strokes, and colors. There are often many possible ways to create a given painting. Our goal is to learn to capture this rich range of possibilities. Creating distributions of long-term videos is a challenge for learning-based video synthesis methods. We present a probabilistic model that, given a single image of a completed painting, recurrently synthesizes steps of the painting process. We implement this model as a convolutional neural network, and introduce a novel training scheme to enable learning from a limited dataset of painting time lapses. We demonstrate that this model can be used to sample many time steps, enabling long-term stochastic video synthesis. We evaluate our method on digital and watercolor paintings collected from video websites, and show that human raters find our synthetic videos to be similar to time lapse videos produced by real artists.
count=1
* Spatially Binned ROC: A Comprehensive Saliency Metric
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Wloka_Spatially_Binned_ROC_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Wloka_Spatially_Binned_ROC_CVPR_2016_paper.pdf)]
    * Title: Spatially Binned ROC: A Comprehensive Saliency Metric
    * Year: `2016`
    * Authors: Calden Wloka, John Tsotsos
    * Abstract: A recent trend in saliency algorithm development is large-scale benchmarking and algorithm ranking with ground truth provided by datasets of human fixations. In order to accommodate the strong bias humans have toward central fixations, it is common to replace traditional ROC metrics with a shuffled ROC metric which uses randomly sampled fixations from other images in the database as the negative set. However, the shuffled ROC introduces a number of problematic elements, including a fundamental assumption that it is possible to separate visual salience and image spatial arrangement. We argue that it is more informative to directly measure the effect of spatial bias on algorithm performance rather than try to correct for it. To capture and quantify these known sources of bias, we propose a novel metric for measuring saliency algorithm performance: the spatially binned ROC (spROC). This metric provides direct insight into the spatial biases of a saliency algorithm without sacrificing the intuitive raw performance evaluation of traditional ROC measurements. By quantitatively measuring the bias in saliency algorithms, researchers will be better equipped to select and optimize the most appropriate algorithm for a given task. We use a baseline measure of inherent algorithm bias to show that Adaptive Whitening Saliency (AWS) [14], Attention by Information Maximization (AIM) [8], and Dynamic Visual Attention (DVA) [20] provide the least spatially biased results, suiting them for tasks in which there is no information about the underlying spatial bias of the stimuli, whereas algorithms such as Graph Based Visual Saliency (GBVS) [18] and Context-Aware Saliency (CAS) [15] have a significant inherent central bias.
count=1
* Recursive Estimation of the Stein Center of SPD Matrices and Its Applications
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Salehian_Recursive_Estimation_of_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Salehian_Recursive_Estimation_of_2013_ICCV_paper.pdf)]
    * Title: Recursive Estimation of the Stein Center of SPD Matrices and Its Applications
    * Year: `2013`
    * Authors: Hesamoddin Salehian, Guang Cheng, Baba C. Vemuri, Jeffrey Ho
    * Abstract: Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negativelycurved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However,the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance – which is the square root of the LogDet divergence – that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable comparisons to other competing methods.
count=1
* Attentive Transfer Entropy to Exploit Transient Emergence of Coupling Effect
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/00bb4e415ef117f2dee2fc3b778d806d-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/00bb4e415ef117f2dee2fc3b778d806d-Paper-Conference.pdf)]
    * Title: Attentive Transfer Entropy to Exploit Transient Emergence of Coupling Effect
    * Year: `2023`
    * Authors: Xiaolei Ru, XINYA ZHANG, Zijia Liu, Jack Murdoch Moore, Gang Yan
    * Abstract: We consider the problem of reconstructing coupled networks (e.g., biological neural networks) connecting large numbers of variables (e.g.,nerve cells), of which state evolution is governed by dissipative dynamics consisting of strong self-drive (dominants the evolution) and weak coupling-drive. The core difficulty is sparseness of coupling effect that emerges (the coupling force is significant) only momentarily and otherwise remains quiescent in time series (e.g., neuronal activity sequence). Here we learn the idea from attention mechanism to guide the classifier to make inference focusing on the critical regions of time series data where coupling effect may manifest. Specifically, attention coefficients are assigned autonomously by artificial neural networks trained to maximise the Attentive Transfer Entropy (ATEn), which is a novel generalization of the iconic transfer entropy metric. Our results show that, without any prior knowledge of dynamics, ATEn explicitly identifies areas where the strength of coupling-drive is distinctly greater than zero. This innovation substantially improves reconstruction performance for both synthetic and real directed coupling networks using data generated by neuronal models widely used in neuroscience.
count=1
* Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/1e5f58d98523298cba093f658cfdf2d6-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/1e5f58d98523298cba093f658cfdf2d6-Paper-Conference.pdf)]
    * Title: Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions
    * Year: `2023`
    * Authors: Samantha Chen, Yusu Wang
    * Abstract: Learning distance functions between complex objects, such as the Wasserstein distance to compare point sets, is a common goal in machine learning applications. However, functions on such complex objects (e.g., point sets and graphs) are often required to be invariant to a wide variety of group actions e.g. permutation or rigid transformation. Therefore, continuous and symmetric *product* functions (such as distance functions) on such complex objects must also be invariant to the *product* of such group actions. We call these functions symmetric and factor-wise group invariant functions (or SGFI functions} in short).In this paper, we first present a general neural network architecture for approximating SFGI functions. The main contribution of this paper combines this general NN with a sketching idea in order to develop a specific and efficient neural network which can approximate the $p$-th Wasserstein distance between point sets.Very importantly, the required model complexity is *independent* of the sizes of input point sets. On the theoretical front, to the best of our knowledge, this is the first result showing that there exists a neural network with the capacity to approximate Wasserstein distance with bounded model complexity. Our work provides an interesting integration of sketching ideas for geometric problems with universal approximation of symmetric functions. On the empirical front, we present a range of results showing that our newly proposed neural network architecture performs comparatively or better than other models (including a SOTA Siamese Autoencoder based approach). In particular, our NN generalizes significantly better and trains much faster than the SOTA Siamese AE.Finally, this line of investigation could be useful in exploring effective neural network design for solving a broad range of geometric optimization problems (e.g., $k$-means in a metric space).
count=1
* CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graph Diffusion
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/5fba70900a84a8fb755c48ba99420c95-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/5fba70900a84a8fb755c48ba99420c95-Paper-Conference.pdf)]
    * Title: CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graph Diffusion
    * Year: `2023`
    * Authors: Guangyao Zhai, Evin Pınar Örnek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, Benjamin Busam
    * Abstract: Controllable scene synthesis aims to create interactive environments for numerous industrial use cases. Scene graphs provide a highly suitable interface to facilitate these applications by abstracting the scene context in a compact manner. Existing methods, reliant on retrieval from extensive databases or pre-trained shape embeddings, often overlook scene-object and object-object relationships, leading to inconsistent results due to their limited generation capacity. To address this issue, we present CommonScenes, a fully generative model that converts scene graphs into corresponding controllable 3D scenes, which are semantically realistic and conform to commonsense. Our pipeline consists of two branches, one predicting the overall scene layout via a variational auto-encoder and the other generating compatible shapes via latent diffusion, capturing global scene-object and local inter-object relationships in the scene graph while preserving shape diversity. The generated scenes can be manipulated by editing the input scene graph and sampling the noise in the diffusion model. Due to the lack of a scene graph dataset offering high-quality object-level meshes with relations, we also construct SG-FRONT, enriching the off-the-shelf indoor dataset 3D-FRONT with additional scene graph labels. Extensive experiments are conducted on SG-FRONT, where CommonScenes shows clear advantages over other methods regarding generation consistency, quality, and diversity. Codes and the dataset are available on the website.
count=1
* On the Statistical Consistency of Risk-Sensitive Bayesian Decision-Making
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/a6df53f082619d02b9fad64a022e5de3-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/a6df53f082619d02b9fad64a022e5de3-Paper-Conference.pdf)]
    * Title: On the Statistical Consistency of Risk-Sensitive Bayesian Decision-Making
    * Year: `2023`
    * Authors: Prateek Jaiswal, Harsha Honnappa, Vinayak Rao
    * Abstract: We study data-driven decision-making problems in the Bayesian framework, where the expectation in the Bayes risk is replaced by a risk-sensitive entropic risk measure with respect to the posterior distribution. We focus on problems where calculating the posterior distribution is intractable, a typical situation in modern applications with large datasets and complex data generating models. We leverage a dual representation of the entropic risk measure to introduce a novel risk-sensitive variational Bayesian (RSVB) framework for jointly computing a risk-sensitive posterior approximation and the corresponding decision rule. Our general framework includes \textit{loss-calibrated} VB (Lacoste-Julien et al. [2011] ) as a special case. We also study the impact of these computational approximations on the predictive performance of the inferred decision rules. We compute the convergence rates of the RSVB approximate posterior and the corresponding optimal value. We illustrate our theoretical findings in parametric and nonparametric settings with the help of three examples.
count=1
* MMD-Fuse: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/edd00cead3425393baf13004de993017-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/edd00cead3425393baf13004de993017-Paper-Conference.pdf)]
    * Title: MMD-Fuse: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting
    * Year: `2023`
    * Authors: Felix Biggs, Antonin Schrab, Arthur Gretton
    * Abstract: We propose novel statistics which maximise the power of a two-sample test based on the Maximum Mean Discrepancy (MMD), byadapting over the set of kernels used in defining it.For finite sets, this reduces to combining (normalised) MMD values under each of these kernels via a weighted soft maximum.Exponential concentration bounds are proved for our proposed statistics under the null and alternative.We further show how these kernels can be chosen in a data-dependent but permutation-independent way, in a well-calibrated test, avoiding data splitting.This technique applies more broadly to general permutation-based MMD testing, and includes the use of deep kernels with features learnt using unsupervised models such as auto-encoders.We highlight the applicability of our MMD-Fuse tests on both synthetic low-dimensional and real-world high-dimensional data, and compare its performance in terms of power against current state-of-the-art kernel tests.
count=1
* Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/ff521f7570d6ed23217ba5780753a1f7-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/ff521f7570d6ed23217ba5780753a1f7-Paper-Conference.pdf)]
    * Title: Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning
    * Year: `2023`
    * Authors: Van Cuong Pham, Cuong Nguyen, Trung Le, Dinh Phung, Gustavo Carneiro, Thanh-Toan Do
    * Abstract: Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.
count=1
* Truly Deterministic Policy Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/3819dd04c2c87bf0d1deea1740ef0ad5-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/3819dd04c2c87bf0d1deea1740ef0ad5-Paper-Conference.pdf)]
    * Title: Truly Deterministic Policy Optimization
    * Year: `2022`
    * Authors: Ehsan Saleh, Saba Ghaffari, Tim Bretl, Matthew West
    * Abstract: In this paper, we present a policy gradient method that avoids exploratory noise injection and performs policy search over the deterministic landscape, with the goal of improving learning with long horizons and non-local rewards. By avoiding noise injection all sources of estimation variance can be eliminated in systems with deterministic dynamics (up to the initial state distribution). Since deterministic policy regularization is impossible using traditional non-metric measures such as the KL divergence, we derive a Wasserstein-based quadratic model for our purposes. We state conditions on the system model under which it is possible to establish a monotonic policy improvement guarantee, propose a surrogate function for policy gradient estimation, and show that it is possible to compute exact advantage estimates if both the state transition model and the policy are deterministic. Finally, we describe two novel robotic control environments---one with non-local rewards in the frequency domain and the other with a long horizon (8000 time-steps)---for which our policy gradient method (TDPO) significantly outperforms existing methods (PPO, TRPO, DDPG, and TD3). Our implementation with all the experimental settings and a video of the physical hardware test is available at https://github.com/ehsansaleh/tdpo .
count=1
* Chroma-VAE: Mitigating Shortcut Learning with Generative Classifiers
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/80098914b3b3bad79b80377751a85430-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/80098914b3b3bad79b80377751a85430-Paper-Conference.pdf)]
    * Title: Chroma-VAE: Mitigating Shortcut Learning with Generative Classifiers
    * Year: `2022`
    * Authors: Wanqian Yang, Polina Kirichenko, Micah Goldblum, Andrew G. Wilson
    * Abstract: Deep neural networks are susceptible to shortcut learning, using simple features to achieve low training loss without discovering essential semantic structure. Contrary to prior belief, we show that generative models alone are not sufficient to prevent shortcut learning, despite an incentive to recover a more comprehensive representation of the data than discriminative approaches. However, we observe that shortcuts are preferentially encoded with minimal information, a fact that generative models can exploit to mitigate shortcut learning. In particular, we propose Chroma-VAE, a two-pronged approach where a VAE classifier is initially trained to isolate the shortcut in a small latent subspace, allowing a secondary classifier to be trained on the complementary, shortcut-free latent subspace. In addition to demonstrating the efficacy of Chroma-VAE on benchmark and real-world shortcut learning tasks, our work highlights the potential for manipulating the latent space of generative classifiers to isolate or interpret specific correlations.
count=1
* Imitation with Neural Density Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/2b0aa0d9e30ea3a55fc271ced8364536-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/2b0aa0d9e30ea3a55fc271ced8364536-Paper.pdf)]
    * Title: Imitation with Neural Density Models
    * Year: `2021`
    * Authors: Kuno Kim, Akshat Jindal, Yang Song, Jiaming Song, Yanan Sui, Stefano Ermon
    * Abstract: We propose a new framework for Imitation Learning (IL) via density estimation of the expert's occupancy measure followed by Maximum Occupancy Entropy Reinforcement Learning (RL) using the density as a reward. Our approach maximizes a non-adversarial model-free RL objective that provably lower bounds reverse Kullback–Leibler divergence between occupancy measures of the expert and imitator. We present a practical IL algorithm, Neural Density Imitation (NDI), which obtains state-of-the-art demonstration efficiency on benchmark control tasks.
count=1
* Conservative Data Sharing for Multi-Task Offline Reinforcement Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/5fd2c06f558321eff612bbbe455f6fbd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/5fd2c06f558321eff612bbbe455f6fbd-Paper.pdf)]
    * Title: Conservative Data Sharing for Multi-Task Offline Reinforcement Learning
    * Year: `2021`
    * Authors: Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey Levine, Chelsea Finn
    * Abstract: Offline reinforcement learning (RL) algorithms have shown promising results in domains where abundant pre-collected data is available. However, prior methods focus on solving individual problems from scratch with an offline dataset without considering how an offline RL agent can acquire multiple skills. We argue that a natural use case of offline RL is in settings where we can pool large amounts of data collected in various scenarios for solving different tasks, and utilize all of this data to learn behaviors for all the tasks more effectively rather than training each one in isolation. However, sharing data across all tasks in multi-task offline RL performs surprisingly poorly in practice. Thorough empirical analysis, we find that sharing data can actually exacerbate the distributional shift between the learned policy and the dataset, which in turn can lead to divergence of the learned policy and poor performance. To address this challenge, we develop a simple technique for data- sharing in multi-task offline RL that routes data based on the improvement over the task-specific data. We call this approach conservative data sharing (CDS), and it can be applied with multiple single-task offline RL methods. On a range of challenging multi-task locomotion, navigation, and vision-based robotic manipulation problems, CDS achieves the best or comparable performance compared to prior offline multi- task RL methods and previous data sharing approaches.
count=1
* Learning Nonparametric Volterra Kernels with Gaussian Processes
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/ca5fbbbddd0c0ff6c01f782c60c9d1b5-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/ca5fbbbddd0c0ff6c01f782c60c9d1b5-Paper.pdf)]
    * Title: Learning Nonparametric Volterra Kernels with Gaussian Processes
    * Year: `2021`
    * Authors: Magnus Ross, Michael T Smith, Mauricio Álvarez
    * Abstract: This paper introduces a method for the nonparametric Bayesian learning of nonlinear operators, through the use of the Volterra series with kernels represented using Gaussian processes (GPs), which we term the nonparametric Volterra kernels model (NVKM). When the input function to the operator is unobserved and has a GP prior, the NVKM constitutes a powerful method for both single and multiple output regression, and can be viewed as a nonlinear and nonparametric latent force model. When the input function is observed, the NVKM can be used to perform Bayesian system identification. We use recent advances in efficient sampling of explicit functions from GPs to map process realisations through the Volterra series without resorting to numerical integration, allowing scalability through doubly stochastic variational inference, and avoiding the need for Gaussian approximations of the output processes. We demonstrate the performance of the model for both multiple output regression and system identification using standard benchmarks.
count=1
* Variational Policy Gradient Method for Reinforcement Learning with General Utilities
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/30ee748d38e21392de740e2f9dc686b6-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/30ee748d38e21392de740e2f9dc686b6-Paper.pdf)]
    * Title: Variational Policy Gradient Method for Reinforcement Learning with General Utilities
    * Year: `2020`
    * Authors: Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, Mengdi Wang
    * Abstract: In recent years, reinforcement learning systems with general goals beyond a cumulative sum of rewards have gained traction, such as in constrained problems, exploration, and acting upon prior experiences. In this paper, we consider policy optimization in Markov Decision Problems, where the objective is a general utility function of the state-action occupancy measure, which subsumes several of the aforementioned examples as special cases. Such generality invalidates the Bellman equation. As this means that dynamic programming no longer works, we focus on direct policy search. Analogously to the Policy Gradient Theorem \cite{sutton2000policy} available for RL with cumulative rewards, we derive a new Variational Policy Gradient Theorem for RL with general utilities, which establishes that the gradient may be obtained as the solution of a stochastic saddle point problem involving the Fenchel dual of the utility function. We develop a variational Monte Carlo gradient estimation algorithm to compute the policy gradient based on sample paths. Further, we prove that the variational policy gradient scheme converges globally to the optimal policy for the general objective, and we also establish its rate of convergence that matches or improves the convergence rate available in the case of RL with cumulative rewards.
count=1
* A Spectral Energy Distance for Parallel Speech Synthesis
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/9873eaad153c6c960616c89e54fe155a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/9873eaad153c6c960616c89e54fe155a-Paper.pdf)]
    * Title: A Spectral Energy Distance for Parallel Speech Synthesis
    * Year: `2020`
    * Authors: Alexey Gritsenko, Tim Salimans, Rianne van den Berg, Jasper Snoek, Nal Kalchbrenner
    * Abstract: Speech synthesis is an important practical generative modeling problem that has seen great progress over the last few years, with likelihood-based autoregressive neural models now outperforming traditional concatenative systems. A downside of such autoregressive models is that they require executing tens of thousands of sequential operations per second of generated audio, making them ill-suited for deployment on specialized deep learning hardware. Here, we propose a new learning method that allows us to train highly parallel models of speech, without requiring access to an analytical likelihood function. Our approach is based on a generalized energy distance between the distributions of the generated and real audio. This spectral energy distance is a proper scoring rule with respect to the distribution over magnitude-spectrograms of the generated waveform audio and offers statistical consistency guarantees. The distance can be calculated from minibatches without bias, and does not involve adversarial learning, yielding a stable and consistent method for training implicit generative models. Empirically, we achieve state-of-the-art generation quality among implicit generative models, as judged by the recently-proposed cFDSD metric. When combining our method with adversarial techniques, we also improve upon the recently-proposed GAN-TTS model in terms of Mean Opinion Score as judged by trained human evaluators.
count=1
* A convex optimization formulation for multivariate regression
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/ccd2d123f4ec4d777fc6ef757d0fb642-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/ccd2d123f4ec4d777fc6ef757d0fb642-Paper.pdf)]
    * Title: A convex optimization formulation for multivariate regression
    * Year: `2020`
    * Authors: Yunzhang Zhu
    * Abstract: Multivariate regression (or multi-task learning) concerns the task of predicting the value of multiple responses from a set of covariates. In this article, we propose a convex optimization formulation for high-dimensional multivariate linear regression under a general error covariance structure. The main difficulty with simultaneous estimation of the regression coefficients and the error covariance matrix lies in the fact that the negative log-likelihood function is not convex. To overcome this difficulty, a new parameterization is proposed, under which the negative log-likelihood function is proved to be convex. For faster computation, two other alternative loss functions are also considered, and proved to be convex under the proposed parameterization. This new parameterization is also useful for covariate-adjusted Gaussian graphical modeling in which the inverse of the error covariance matrix is of interest. A joint non-asymptotic analysis of the regression coefficients and the error covariance matrix is carried out under the new parameterization. In particular, we show that the proposed method recovers the oracle estimator under sharp scaling conditions, and rates of convergence in terms of vector $\ell_\infty$ norm are also established. Empirically, the proposed methods outperform existing high-dimensional multivariate linear regression methods that are based on either minimizing certain non-convex criteria or certain two-step procedures.
count=1
* The All-or-Nothing Phenomenon in Sparse Tensor PCA
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/cd0b43eac0392accf3624b7372dec36e-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/cd0b43eac0392accf3624b7372dec36e-Paper.pdf)]
    * Title: The All-or-Nothing Phenomenon in Sparse Tensor PCA
    * Year: `2020`
    * Authors: Jonathan Niles-Weed, Ilias Zadik
    * Abstract: We study the statistical problem of estimating a rank-one sparse tensor corrupted by additive gaussian noise, a Gaussian additive model also known as sparse tensor PCA. We show that for Bernoulli and Bernoulli-Rademacher distributed signals and \emph{for all} sparsity levels which are sublinear in the dimension of the signal, the sparse tensor PCA model exhibits a phase transition called the \emph{all-or-nothing phenomenon}. This is the property that for some signal-to-noise ratio (SNR) $\mathrm{SNR_c}$ and any fixed $\epsilon>0$, if the SNR of the model is below $\left(1-\epsilon\right)\mathrm{SNR_c}$, then it is impossible to achieve any arbitrarily small constant correlation with the hidden signal, while if the SNR is above $\left(1+\epsilon \right)\mathrm{SNR_c}$, then it is possible to achieve almost perfect correlation with the hidden signal. The all-or-nothing phenomenon was initially established in the context of sparse linear regression, and over the last year also in the context of sparse 2-tensor (matrix) PCA and Bernoulli group testing. Our results follow from a more general result showing that for any Gaussian additive model with a discrete uniform prior, the all-or-nothing phenomenon follows as a direct outcome of an appropriately defined ``near-orthogonality" property of the support of the prior distribution.
count=1
* Improving Sparse Vector Technique with Renyi Differential Privacy
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/e9bf14a419d77534105016f5ec122d62-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/e9bf14a419d77534105016f5ec122d62-Paper.pdf)]
    * Title: Improving Sparse Vector Technique with Renyi Differential Privacy
    * Year: `2020`
    * Authors: Yuqing Zhu, Yu-Xiang Wang
    * Abstract: The Sparse Vector Technique (SVT) is one of the most fundamental algorithmic tools in differential privacy (DP). It also plays a central role in the state-of-the-art algorithms for adaptive data analysis and model-agnostic private learning. In this paper, we revisit SVT from the lens of Renyi differential privacy, which results in new privacy bounds, new theoretical insight and new variants of SVT algorithms. A notable example is a Gaussian mechanism version of SVT, which provides better utility over the standard (Laplace-mechanism-based) version thanks to its more concentrated noise and tighter composition. Extensive empirical evaluation demonstrates the merits of Gaussian SVT over the Laplace SVT and other alternatives, which encouragingly suggests that using Gaussian SVT as a drop-in replacement could make SVT-based algorithms practical in downstream tasks.
count=1
* Capacity Bounded Differential Privacy
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf)]
    * Title: Capacity Bounded Differential Privacy
    * Year: `2019`
    * Authors: Kamalika Chaudhuri, Jacob Imola, Ashwin Machanavajjhala
    * Abstract: Differential privacy, a notion of algorithmic stability, is a gold standard for measuring the additional risk an algorithm's output poses to the privacy of a single record in the dataset. Differential privacy is defined as the distance between the output distribution of an algorithm on neighboring datasets that differ in one entry. In this work, we present a novel relaxation of differential privacy, capacity bounded differential privacy, where the adversary that distinguishes output distributions is assumed to be capacity-bounded -- i.e. bounded not in computational power, but in terms of the function class from which their attack algorithm is drawn. We model adversaries in terms of restricted f-divergences between probability distributions, and study properties of the definition and algorithms that satisfy them.
count=1
* Beyond Online Balanced Descent: An Optimal Algorithm for Smoothed Online Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/9f36407ead0629fc166f14dde7970f68-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/9f36407ead0629fc166f14dde7970f68-Paper.pdf)]
    * Title: Beyond Online Balanced Descent: An Optimal Algorithm for Smoothed Online Optimization
    * Year: `2019`
    * Authors: Gautam Goel, Yiheng Lin, Haoyuan Sun, Adam Wierman
    * Abstract: We study online convex optimization in a setting where the learner seeks to minimize the sum of a per-round hitting cost and a movement cost which is incurred when changing decisions between rounds. We prove a new lower bound on the competitive ratio of any online algorithm in the setting where the costs are $m$-strongly convex and the movement costs are the squared $\ell_2$ norm. This lower bound shows that no algorithm can achieve a competitive ratio that is $o(m^{-1/2})$ as $m$ tends to zero. No existing algorithms have competitive ratios matching this bound, and we show that the state-of-the-art algorithm, Online Balanced Decent (OBD), has a competitive ratio that is $\Omega(m^{-2/3})$. We additionally propose two new algorithms, Greedy OBD (G-OBD) and Regularized OBD (R-OBD) and prove that both algorithms have an $O(m^{-1/2})$ competitive ratio. The result for G-OBD holds when the hitting costs are quasiconvex and the movement costs are the squared $\ell_2$ norm, while the result for R-OBD holds when the hitting costs are $m$-strongly convex and the movement costs are Bregman Divergences. Further, we show that R-OBD simultaneously achieves constant, dimension-free competitive ratio and sublinear regret when hitting costs are strongly convex.
count=1
* Maximum-Entropy Fine Grained Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf)]
    * Title: Maximum-Entropy Fine Grained Classification
    * Year: `2018`
    * Authors: Abhimanyu Dubey, Otkrist Gupta, Ramesh Raskar, Nikhil Naik
    * Abstract: Fine-Grained Visual Classification (FGVC) is an important computer vision problem that involves small diversity within the different classes, and often requires expert annotators to collect data. Utilizing this notion of small visual diversity, we revisit Maximum-Entropy learning in the context of fine-grained classification, and provide a training routine that maximizes the entropy of the output probability distribution for training convolutional neural networks on FGVC tasks. We provide a theoretical as well as empirical justification of our approach, and achieve state-of-the-art performance across a variety of classification tasks in FGVC, that can potentially be extended to any fine-tuning task. Our method is robust to different hyperparameter values, amount of training data and amount of training label noise and can hence be a valuable tool in many similar problems.
count=1
* Online Robust Policy Learning in the Presence of Unknown Adversaries
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/8a36dfc67ebfbbea9bd01cd8a4c8ad32-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/8a36dfc67ebfbbea9bd01cd8a4c8ad32-Paper.pdf)]
    * Title: Online Robust Policy Learning in the Presence of Unknown Adversaries
    * Year: `2018`
    * Authors: Aaron Havens, Zhanhong Jiang, Soumik Sarkar
    * Abstract: The growing prospect of deep reinforcement learning (DRL) being used in cyber-physical systems has raised concerns around safety and robustness of autonomous agents. Recent work on generating adversarial attacks have shown that it is computationally feasible for a bad actor to fool a DRL policy into behaving sub optimally. Although certain adversarial attacks with specific attack models have been addressed, most studies are only interested in off-line optimization in the data space (e.g., example fitting, distillation). This paper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that is attack model-agnostic and more suited to reinforcement learning, via handling the attacks in the decision space (as opposed to data space) and directly mitigating learned bias introduced by the adversary. In MLAH, we learn separate sub-policies (nominal and adversarial) in an online manner, as guided by a supervisory master agent that detects the presence of the adversary by leveraging the advantage function for the sub-policies. We demonstrate that the proposed algorithm enables policy learning with significantly lower bias as compared to the state-of-the-art policy learning approaches even in the presence of heavy state information attacks. We present algorithm analysis and simulation results using popular OpenAI Gym environments.
count=1
* Collapsed variational Bayes for Markov jump processes
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/e0a209539d1e74ab9fe46b9e01a19a97-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/e0a209539d1e74ab9fe46b9e01a19a97-Paper.pdf)]
    * Title: Collapsed variational Bayes for Markov jump processes
    * Year: `2017`
    * Authors: Boqian Zhang, Jiangwei Pan, Vinayak A. Rao
    * Abstract: Markov jump processes are continuous-time stochastic processes widely used in statistical applications in the natural sciences, and more recently in machine learning. Inference for these models typically proceeds via Markov chain Monte Carlo, and can suffer from various computational challenges. In this work, we propose a novel collapsed variational inference algorithm to address this issue. Our work leverages ideas from discrete-time Markov chains, and exploits a connection between these two through an idea called uniformization. Our algorithm proceeds by marginalizing out the parameters of the Markov jump process, and then approximating the distribution over the trajectory with a factored distribution over segments of a piecewise-constant function. Unlike MCMC schemes that marginalize out transition times of a piecewise-constant process, our scheme optimizes the discretization of time, resulting in significant computational savings. We apply our ideas to synthetic data as well as a dataset of check-in recordings, where we demonstrate superior performance over state-of-the-art MCMC methods.
count=1
* Optimistic Gittins Indices
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/452bf208bf901322968557227b8f6efe-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/452bf208bf901322968557227b8f6efe-Paper.pdf)]
    * Title: Optimistic Gittins Indices
    * Year: `2016`
    * Authors: Eli Gutin, Vivek Farias
    * Abstract: Starting with the Thomspon sampling algorithm, recent years have seen a resurgence of interest in Bayesian algorithms for the Multi-armed Bandit (MAB) problem. These algorithms seek to exploit prior information on arm biases and while several have been shown to be regret optimal, their design has not emerged from a principled approach. In contrast, if one cared about Bayesian regret discounted over an infinite horizon at a fixed, pre-specified rate, the celebrated Gittins index theorem offers an optimal algorithm. Unfortunately, the Gittins analysis does not appear to carry over to minimizing Bayesian regret over all sufficiently large horizons and computing a Gittins index is onerous relative to essentially any incumbent index scheme for the Bayesian MAB problem. The present paper proposes a sequence of 'optimistic' approximations to the Gittins index. We show that the use of these approximations in concert with the use of an increasing discount factor appears to offer a compelling alternative to a variety of index schemes proposed for the Bayesian MAB problem in recent years. In addition, we show that the simplest of these approximations yields regret that matches the Lai-Robbins lower bound, including achieving matching constants.
count=1
* Object based Scene Representations using Fisher Scores of Local Subspace Projections
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/856fc81623da2150ba2210ba1b51d241-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/856fc81623da2150ba2210ba1b51d241-Paper.pdf)]
    * Title: Object based Scene Representations using Fisher Scores of Local Subspace Projections
    * Year: `2016`
    * Authors: Mandar D. Dixit, Nuno Vasconcelos
    * Abstract: Several works have shown that deep CNN classifiers can be easily transferred across datasets, e.g. the transfer of a CNN trained to recognize objects on ImageNET to an object detector on Pascal VOC. Less clear, however, is the ability of CNNs to transfer knowledge across tasks. A common example of such transfer is the problem of scene classification that should leverage localized object detections to recognize holistic visual concepts. While this problem is currently addressed with Fisher vector representations, these are now shown ineffective for the high-dimensional and highly non-linear features extracted by modern CNNs. It is argued that this is mostly due to the reliance on a model, the Gaussian mixture of diagonal covariances, which has a very limited ability to capture the second order statistics of CNN features. This problem is addressed by the adoption of a better model, the mixture of factor analyzers (MFA), which approximates the non-linear data manifold by a collection of local subspaces. The Fisher score with respect to the MFA (MFA-FS) is derived and proposed as an image representation for holistic image classifiers. Extensive experiments show that the MFA-FS has state of the art performance for object-to-scene transfer and this transfer actually outperforms the training of a scene CNN from a large scene dataset. The two representations are also shown to be complementary, in the sense that their combination outperforms each of the representations by itself. When combined, they produce a state of the art scene classifier.
count=1
* Linear Response Methods for Accurate Covariance Estimates from Mean Field Variational Bayes
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/4b0a59ddf11c58e7446c9df0da541a84-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/4b0a59ddf11c58e7446c9df0da541a84-Paper.pdf)]
    * Title: Linear Response Methods for Accurate Covariance Estimates from Mean Field Variational Bayes
    * Year: `2015`
    * Authors: Ryan J. Giordano, Tamara Broderick, Michael I. Jordan
    * Abstract: Mean field variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, a well known failing of MFVB is that it underestimates the uncertainty of model variables (sometimes severely) and provides no information about model variable covariance. We generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables---both for individual variables and coherently across variables. We call our method linear response variational Bayes (LRVB). When the MFVB posterior approximation is in the exponential family, LRVB has a simple, analytic form, even for non-conjugate models. Indeed, we make no assumptions about the form of the true posterior. We demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data.
count=1
* Sample Complexity Bounds for Iterative Stochastic Policy Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/97d98119037c5b8a9663cb21fb8ebf47-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/97d98119037c5b8a9663cb21fb8ebf47-Paper.pdf)]
    * Title: Sample Complexity Bounds for Iterative Stochastic Policy Optimization
    * Year: `2015`
    * Authors: Marin Kobilarov
    * Abstract: This paper is concerned with robustness analysis of decision making under uncertainty. We consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration. In particular, we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs. A novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation. The bound serves as a high-confidence certificate for providing future performance or safety guarantees. The approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented.
count=1
* On the Information Theoretic Limits of Learning Ising Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/2715518c875999308842e3455eda2fe3-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/2715518c875999308842e3455eda2fe3-Paper.pdf)]
    * Title: On the Information Theoretic Limits of Learning Ising Models
    * Year: `2014`
    * Authors: Rashish Tandon, Karthikeyan Shanmugam, Pradeep K. Ravikumar, Alexandros G. Dimakis
    * Abstract: We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i.i.d. samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erdos-Renyi graphs in a certain dense setting.
count=1
* PAC-Bayesian AUC classification and scoring
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/7fe1f8abaad094e0b5cb1b01d712f708-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/7fe1f8abaad094e0b5cb1b01d712f708-Paper.pdf)]
    * Title: PAC-Bayesian AUC classification and scoring
    * Year: `2014`
    * Authors: James Ridgway, Pierre Alquier, Nicolas Chopin, Feng Liang
    * Abstract: We develop a scoring and classification procedure based on the PAC-Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially on the class of linear score functions. We derive PAC-Bayesian non-asymptotic bounds for two types of prior for the score parameters: a Gaussian prior, and a spike-and-slab prior; the latter makes it possible to perform feature selection. One important advantage of our approach is that it is amenable to powerful Bayesian computational tools. We derive in particular a Sequential Monte Carlo algorithm, as an efficient method which may be used as a gold standard, and an Expectation-Propagation algorithm, as a much faster but approximate method. We also extend our method to a class of non-linear score functions, essentially leading to a nonparametric procedure, by considering a Gaussian process prior.
