count=29
* Effective Data Fusion With Generalized Vegetation Index: Evidence From Land Cover Segmentation in Agriculture
    [[abs-CVF](https://openaccess.thecvf.com/content_CVPRW_2020/html/w5/Sheng_Effective_Data_Fusion_With_Generalized_Vegetation_Index_Evidence_From_Land_CVPRW_2020_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w5/Sheng_Effective_Data_Fusion_With_Generalized_Vegetation_Index_Evidence_From_Land_CVPRW_2020_paper.pdf)]
    * Title: Effective Data Fusion With Generalized Vegetation Index: Evidence From Land Cover Segmentation in Agriculture
    * Publisher: CVPR
    * Publication Date: `2020`
    * Authors: Hao Sheng, Xiao Chen, Jingyi Su, Ram Rajagopal, Andrew Ng
    * Abstract: How can we effectively leverage the domain knowledge from remote sensing to better segment agriculture land cover from satellite images? In this paper, we propose a novel, model-agnostic, data-fusion approach for vegetation-related computer vision tasks. Motivated by the various Vegetation Indices (VIs), which are introduced by domain experts, we systematically reviewed the VIs that are widely used in remote sensing and their feasibility to be incorporated in deep neural networks. To fully leverage the Near-Infrared channel, the traditional Red-Green-Blue channels, and Vegetation Index or its variants, we propose a Generalized Vegetation Index (GVI), a lightweight module that can be easily plugged into many neural network architectures to serve as an additional information input. To smoothly train models with our GVI, we developed an Additive Group Normalization (AGN) module that does not require extra parameters of the prescribed neural networks. Our approach has improved the IoUs of vegetation-related classes by 0.9-1.3 percent and consistently improves the overall mIoU by 2 percent on our baseline.

count=26
* Image Vegetation Index Through a Cycle Generative Adversarial Network
    [[abs-CVF](https://openaccess.thecvf.com/content_CVPRW_2019/html/PBVS/Suarez_Image_Vegetation_Index_Through_a_Cycle_Generative_Adversarial_Network_CVPRW_2019_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_CVPRW_2019/papers/PBVS/Suarez_Image_Vegetation_Index_Through_a_Cycle_Generative_Adversarial_Network_CVPRW_2019_paper.pdf)]
    * Title: Image Vegetation Index Through a Cycle Generative Adversarial Network
    * Publisher: CVPR
    * Publication Date: `2019`
    * Authors: Patricia L. Suarez,  Angel D. Sappa,  Boris X. Vintimilla,  Riad I. Hammoud
    * Abstract: This paper proposes a novel approach to estimate the Normalized Difference Vegetation Index (NDVI) just from an RGB image. The NDVI values are obtained by using images from the visible spectral band together with a synthetic near infrared image obtained by a cycled GAN. The cycled GAN network is able to obtain a NIR image from a given gray scale image. It is trained by using unpaired set of gray scale and NIR images by using a U-net architecture and a multiple loss function (gray scale images are obtained from the provided RGB images). Then, the NIR image estimated with the proposed cycle generative adversarial network is used to compute the NDVI index. Experimental results are provided showing the validity of the proposed approach. Additionally, comparisons with previous approaches are also provided.

count=7
* The 1st Agriculture-Vision Challenge: Methods and Results
    [[abs-CVF](https://openaccess.thecvf.com/content_CVPRW_2020/html/w5/Chiu_The_1st_Agriculture-Vision_Challenge_Methods_and_Results_CVPRW_2020_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w5/Chiu_The_1st_Agriculture-Vision_Challenge_Methods_and_Results_CVPRW_2020_paper.pdf)]
    * Title: The 1st Agriculture-Vision Challenge: Methods and Results
    * Publisher: CVPR
    * Publication Date: `2020`
    * Authors: Mang Tik Chiu, Xingqian Xu, Kai Wang, Jennifer Hobbs, Naira Hovakimyan, Thomas S. Huang, Honghui Shi
    * Abstract: The first Agriculture-Vision Challenge aims to encourage research in developing novel and effective algorithms for agricultural pattern recognition from aerial images, especially for the semantic segmentation task associated with our challenge dataset. Around 57 participating teams from various countries compete to achieve state-of-the-art in aerial agriculture semantic segmentation. The Agriculture-Vision Challenge Dataset was employed, which comprises of 21,061 aerial and multi-spectral farmland images. This paper provides a summary of notable methods and results in the challenge. Our submission server and leaderboard will continue to open for researchers that are interested in this challenge dataset and task; the link can be found here.

count=5
* Estimating Soil Organic Carbon from Multispectral Images using Physics-Informed Neural Networks
    [[abs-CVF](https://openaccess.thecvf.com/content/ACCV2024/html/Sargeant_Estimating_Soil_Organic_Carbon_from_Multispectral_Images_using_Physics-Informed_Neural_ACCV_2024_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/ACCV2024/papers/Sargeant_Estimating_Soil_Organic_Carbon_from_Multispectral_Images_using_Physics-Informed_Neural_ACCV_2024_paper.pdf)]
    * Title: Estimating Soil Organic Carbon from Multispectral Images using Physics-Informed Neural Networks
    * Publisher: ACCV
    * Publication Date: `2024`
    * Authors: James Sargeant, Shyh Wei Teng, Manzur Murshed, Manoranjan Paul, David Brennan
    * Abstract: Understanding the amount of Soil Organic Carbon (SOC) at farm and field scale is a necessary precursor to effective management, important for both agricultural productivity and to reduce CO2 emissions. To avoid the prohibitive cost of measurement, SOC can be estimated by using multispectral images. In this study, we propose a novel Physics-Informed Convolutional Neural Network (CNN) to model well-known but noisy relationship between a soil index and SOC using the networks loss function. This study is also conducted by resampling the European Land Use/Classification Area Survey (LUCAS) dataset to Sentinel-2 bands. Our experimental results show that our proposed network converges more quickly, has a lower root mean squared error (RMSE) and is more robust (as measured by the standard deviation of RMSE over multiple trials) than a compatible standard CNN. The operation of the novel Physics-Informed CNN is explained in terms of the components of the loss function.

count=5
* Progressive Unsupervised Deep Transfer Learning for Forest Mapping in Satellite Image
    [[abs-CVF](https://openaccess.thecvf.com/content/ICCV2021W/LUAI/html/Ahmed_Progressive_Unsupervised_Deep_Transfer_Learning_for_Forest_Mapping_in_Satellite_ICCVW_2021_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/ICCV2021W/LUAI/papers/Ahmed_Progressive_Unsupervised_Deep_Transfer_Learning_for_Forest_Mapping_in_Satellite_ICCVW_2021_paper.pdf)]
    * Title: Progressive Unsupervised Deep Transfer Learning for Forest Mapping in Satellite Image
    * Publisher: ICCV
    * Publication Date: `2021`
    * Authors: Nouman Ahmed, Sudipan Saha, Muhammad Shahzad, Muhammad Moazam Fraz, Xiao Xiang Zhu
    * Abstract: Automated forest mapping is important to understand our forests that play a key role in ecological system. However, efforts towards forest mapping is impeded by difficulty to collect labeled forest images that show large intraclass variation. Recently unsupervised learning has shown promising capability when exploiting limited labeled data. Motivated by this, we propose a progressive unsupervised deep transfer learning method for forest mapping. The proposed method exploits a pre-trained model that is subsequently fine-tuned over the target forest domain. We propose two different fine-tuning mechanism, one works in a totally unsupervised setting by jointly learning the parameters of CNN and the k-means based cluster assignments of the resulting features and the other one works in a semi-supervised setting by exploiting the extracted knearest neighbor based pseudo labels. The proposed progressive scheme is evaluated on publicly available EuroSAT dataset using the relevant base model trained on BigEarthNet labels. The results show that the proposed method greatly improves the forest regions classification accuracy as compared to the unsupervised baseline, nearly approaching the supervised classification approach.

count=4
* EarthNet2021: A Large-Scale Dataset and Challenge for Earth Surface Forecasting as a Guided Video Prediction Task.
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Requena-Mesa_EarthNet2021_A_Large-Scale_Dataset_and_Challenge_for_Earth_Surface_Forecasting_CVPRW_2021_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Requena-Mesa_EarthNet2021_A_Large-Scale_Dataset_and_Challenge_for_Earth_Surface_Forecasting_CVPRW_2021_paper.pdf)]
    * Title: EarthNet2021: A Large-Scale Dataset and Challenge for Earth Surface Forecasting as a Guided Video Prediction Task.
    * Publisher: CVPR
    * Publication Date: `2021`
    * Authors: Christian Requena-Mesa, Vitus Benson, Markus Reichstein, Jakob Runge, Joachim Denzler
    * Abstract: Satellite images are snapshots of the Earth surface. We propose to forecast them. We frame Earth surface forecasting as the task of predicting satellite imagery conditioned on future weather. EarthNet2021 is a large dataset suitable for training deep neural networks on the task. It contains Sentinel 2 satellite imagery at 20m resolution, matching topography and mesoscale (1.28km) meteorological variables packaged into 32000 samples. Additionally we frame EarthNet2021 as a challenge allowing for model intercomparison. Resulting forecasts will greatly improve (>x50) over the spatial resolution found in numerical models. This allows localized impacts from extreme weather to be predicted, thus supporting downstream applications such as crop yield prediction, forest health assessments or biodiversity monitoring. Find data, code, and how to participate at www.earthnet.tech

count=4
* From RGB to NIR: Predicting of Near Infrared Reflectance From Visible Spectrum Aerial Images of Crops
    [[abs-CVF](https://openaccess.thecvf.com/content/ICCV2021W/CVPPA/html/Aslahishahri_From_RGB_to_NIR_Predicting_of_Near_Infrared_Reflectance_From_ICCVW_2021_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/ICCV2021W/CVPPA/papers/Aslahishahri_From_RGB_to_NIR_Predicting_of_Near_Infrared_Reflectance_From_ICCVW_2021_paper.pdf)]
    * Title: From RGB to NIR: Predicting of Near Infrared Reflectance From Visible Spectrum Aerial Images of Crops
    * Publisher: ICCV
    * Publication Date: `2021`
    * Authors: Masoomeh Aslahishahri, Kevin G. Stanley, Hema Duddu, Steve Shirtliffe, Sally Vail, Kirstin Bett, Curtis Pozniak, Ian Stavness
    * Abstract: Near infrared spectroscopy (NIR) provides rich information in agricultural operations and experiments to determine crop parameters which are not visible to the human eye. Collecting the NIR spectral band requires a multispectral camera which is typically more expensive and has lower resolution than a comparable RGB camera. We investigate image-to-image translation as a means to generate an NIR spectral band from an RGB image alone in aerial crop imagery. Aerial images were captured via a multispectral sensor mounted on an unmanned aerial vehicle (UAV) flown over canola, lentil, dry bean, and wheat breeding trials. A software workflow was created to preprocess raw aerial images creating a dataset suitable for training and evaluating deep learning based band inferencing algorithms. Two different experiments over different crop types in our dataset were conducted to evaluate efficacy in an agricultural context.

count=3
* ArcticNet: A Deep Learning Solution to Classify the Arctic Area
    [[abs-CVF](https://openaccess.thecvf.com/content_CVPRW_2019/html/DOAI/Jiang_ArcticNet_A_Deep_Learning_Solution_to_Classify_the_Arctic_Area_CVPRW_2019_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_CVPRW_2019/papers/DOAI/Jiang_ArcticNet_A_Deep_Learning_Solution_to_Classify_the_Arctic_Area_CVPRW_2019_paper.pdf)]
    * Title: ArcticNet: A Deep Learning Solution to Classify the Arctic Area
    * Publisher: CVPR
    * Publication Date: `2019`
    * Authors: Ziyu Jiang,  Kate Von Ness,  Julie Loisel,  Zhangyang Wang
    * Abstract: Arctic environments are rapidly changing under the warming climate. Of particular interest are wetlands, a type of ecosystem that constitutes the most effective terrestrial long-term carbon store. As permafrost thaws, the carbon that was locked in these wetland soils for millennia becomes available for aerobic and anaerobic decomposition, which releases carbon dioxide CO2 and methane CH4, respectively, back to the atmosphere. As CO2 and CH4 are potent greenhouse gases, this transfer of carbon from the land to the atmosphere further contributes to global warming, thereby increasing the rate of permafrost degradation in a positive feedback loop. Therefore, monitoring Arctic wetland health and dynamics is a key scientific task that is also of importance for policy. However, the identification and delineation of these important wetland ecosystems, remain incomplete and often inaccurate. Mapping the extent of Arctic wetlands remains a challenge for the scientific community. Conventional, coarser remote sensing methods are inadequate at distinguishing the diverse and micro-topographically complex non-vascular vegetation that characterize Arctic wetlands, presenting the need for better identification methods. To tackle this challenging problem, we constructed and annotated the first-of-its-kind Arctic Wetland Dataset (AWD). Based on that, we present ArcticNet, a deep neural network that exploits the multi-spectral, high-resolution imagery captured from nanosatellites (Planet Dove CubeSats) with additional Digital Elevation Model (DEM) from the ArcticDEM project, to semantically label a Arctic study area into six types, in which three Arctic wetland functional types are included. We present multi-fold efforts to handle the arising challenges, including class imbalance, and the choice of fusion strategies. Preliminary results endorse the high promise of ArcticNet, achieving 93.12% in labelling a hold-out set of regions in our Arctic study area.

count=2
* Monitoring Ethiopian Wheat Fungus With Satellite Imagery and Deep Feature Learning
    [[abs-CVF](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w18/html/Pryzant_Monitoring_Ethiopian_Wheat_CVPR_2017_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w18/papers/Pryzant_Monitoring_Ethiopian_Wheat_CVPR_2017_paper.pdf)]
    * Title: Monitoring Ethiopian Wheat Fungus With Satellite Imagery and Deep Feature Learning
    * Publisher: CVPR
    * Publication Date: `2017`
    * Authors: Reid Pryzant, Stefano Ermon, David Lobell
    * Abstract: Wheat is the most important Ethiopian crop, and rust one of its greatest antagonists. There is a need for cheap and scalable rust monitoring in the developing world, but existing methods employ costly data collection techniques. We introduce a scalable, accurate, and inexpensive method for tracking outbreaks with publicly available remote sensing data. Our approach improves existing techniques in two ways. First, we forgo the spectral features employed by the remote sensing community in favor of automatically learned features generated by Convolutional and Long Short-Term Memory Networks. Second, we aggregate data into larger geospatial regions. We evaluate our approach on nine years of agricultural outcomes, show that it outperforms competing techniques, and demonstrate its predictive foresight. This is a promising new direction in crop disease monitoring, one that has the potential to grow more powerful with time.

count=2
* Deep Density Estimation Based on Multi-Spectral Remote Sensing Data for In-Field Crop Yield Forecasting
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2022W/WiCV/html/Baghdasaryan_Deep_Density_Estimation_Based_on_Multi-Spectral_Remote_Sensing_Data_for_CVPRW_2022_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2022W/WiCV/papers/Baghdasaryan_Deep_Density_Estimation_Based_on_Multi-Spectral_Remote_Sensing_Data_for_CVPRW_2022_paper.pdf)]
    * Title: Deep Density Estimation Based on Multi-Spectral Remote Sensing Data for In-Field Crop Yield Forecasting
    * Publisher: CVPR
    * Publication Date: `2022`
    * Authors: Liana Baghdasaryan, Razmik Melikbekyan, Arthur Dolmajain, Jennifer Hobbs
    * Abstract: Yield forecasting has been a central task in computational agriculture because of its impact on agricultural management from the individual farmer to the government level. With advances in remote sensing technology, computational processing power, and machine learning, the ability to forecast yield has improved substantially over the past years. However, most previous work has been done leveraging low-resolution satellite imagery and forecasting yield at the region, county, or occasionally farm-level. In this work, we use high-resolution aerial imagery and output from high-precision harvesters to predict in-field harvest values for corn-raising farms in the US Midwest. By using the harvester information, we are able to cast the problem of yield-forecasting as a density estimation problem and predict a harvest rate, in bushels/acre, at every pixel in the field image. This approach provides the farmer with a detailed view of which areas of the farm may be performing poorly so he can make the appropriate management decisions in addition to providing an improved prediction of total yield. We evaluate both traditional machine learning approaches with hand-crafted features alongside deep learning methods. We demonstrate the superiority of our pixel-level approach based on an encoder-decoder framework which produces a 5.41% MAPE at the field-level.

count=2
* MethaneMapper: Spectral Absorption Aware Hyperspectral Transformer for Methane Detection
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2023/html/Kumar_MethaneMapper_Spectral_Absorption_Aware_Hyperspectral_Transformer_for_Methane_Detection_CVPR_2023_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumar_MethaneMapper_Spectral_Absorption_Aware_Hyperspectral_Transformer_for_Methane_Detection_CVPR_2023_paper.pdf)]
    * Title: MethaneMapper: Spectral Absorption Aware Hyperspectral Transformer for Methane Detection
    * Publisher: CVPR
    * Publication Date: `2023`
    * Authors: Satish Kumar, Ivan Arevalo, ASM Iftekhar, B S Manjunath
    * Abstract: Methane (CH 4 ) is the chief contributor to global climate change. Recent Airborne Visible-Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) has been very useful in quantitative mapping of methane emissions. Existing methods for analyzing this data are sensitive to local terrain conditions, often require manual inspection from domain experts, prone to significant error and hence are not scalable. To address these challenges, we propose a novel end-to-end spectral absorption wavelength aware transformer network, MethaneMapper, to detect and quantify the emissions. MethaneMapper introduces two novel modules that help to locate the most relevant methane plume regions in the spectral domain and uses them to localize these accurately. Thorough evaluation shows that MethaneMapper achieves 0.63 mAP in detection and reduces the model size (by 5x) compared to the current state of the art. In addition, we also introduce a large-scale dataset of methane plume segmentation mask for over 1200 AVIRIS-NG flightlines from 2015-2022. It contains over 4000 methane plume sites. Our dataset will provide researchers the opportunity to develop and advance new methods for tackling this challenging green-house gas detection problem with significant broader social impact. Dataset and source code link.

count=2
* Deep Landscape Features for Improving Vector-borne Disease Prediction
    [[abs-CVF](https://openaccess.thecvf.com/content_CVPRW_2019/html/cv4gc/Rehman_Deep_Landscape_Features_for_Improving_Vector-borne_Disease_Prediction_CVPRW_2019_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/Rehman_Deep_Landscape_Features_for_Improving_Vector-borne_Disease_Prediction_CVPRW_2019_paper.pdf)]
    * Title: Deep Landscape Features for Improving Vector-borne Disease Prediction
    * Publisher: CVPR
    * Publication Date: `2019`
    * Authors: Nabeel Abdur Rehman,  Umar Saif,  Rumi Chunara
    * Abstract: The global population at risk of mosquito-borne diseases such as dengue, yellow fever, chikungunya and Zika is expanding. Infectious disease models commonly incorporate environmental measures like temperature and precipitation. Given increasing availability of high-resolution satellite imagery, here we consider including landscape features from satellite imagery into infectious disease prediction models. To do so, we implement a Convolutional Neural Network (CNN) model trained on Imagenet data and labelled landscape features in satellite data from London. We then incorporate landscape features from satellite image data from Pakistan, labelled using the CNN, in a well-known Susceptible-Infectious-Recovered epidemic model, alongside dengue case data from 2012-2016 in Pakistan. We study improvement of the prediction model for each of the individual landscape features, and assess the feasibility of using image labels from a different place. We find that incorporating satellite-derived landscape features can improve prediction of outbreaks, which is important for proactive and strategic surveillance and control programmes.

count=2
* Urban Semantic 3D Reconstruction From Multiview Satellite Imagery
    [[abs-CVF](https://openaccess.thecvf.com/content_CVPRW_2019/html/EarthVision/Leotta_Urban_Semantic_3D_Reconstruction_From_Multiview_Satellite_Imagery_CVPRW_2019_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_CVPRW_2019/papers/EarthVision/Leotta_Urban_Semantic_3D_Reconstruction_From_Multiview_Satellite_Imagery_CVPRW_2019_paper.pdf)]
    * Title: Urban Semantic 3D Reconstruction From Multiview Satellite Imagery
    * Publisher: CVPR
    * Publication Date: `2019`
    * Authors: Matthew J. Leotta,  Chengjiang Long,  Bastien Jacquet,  Matthieu Zins,  Dan Lipsa,  Jie Shan,  Bo Xu,  Zhixin Li,  Xu Zhang,  Shih-Fu Chang,  Matthew Purri,  Jia Xue,  Kristin Dana
    * Abstract: Methods for automated 3D urban modeling typically result in very dense point clouds or surface meshes derived from either overhead lidar or imagery (multiview stereo). Such models are very large and have no semantic separation of individual structures (i.e. buildings, bridges) from the terrain. Furthermore, such dense models often appear "melted" and do not capture sharp edges. This paper demonstrates an end-to-end system for segmenting buildings and bridges from terrain and estimating simple, low polygon, textured mesh models of these structures. The approach uses multiview-stereo satellite imagery as a starting point, but this work focuses on segmentation methods and regularized 3D surface extraction. Our work is evaluated on the IARPA CORE3D public data set using the associated ground truth and metrics. A web-based application deployed on AWS runs the algorithms and provides visualization of the results. Both the algorithms and web application are provided as open source software as a resource for further research or product development.

count=2
* Weed Mapping with Convolutional Neural Networks on High Resolution Whole-Field Images
    [[abs-CVF](https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Wang_Weed_Mapping_with_Convolutional_Neural_Networks_on_High_Resolution_Whole-Field_ICCVW_2023_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/papers/Wang_Weed_Mapping_with_Convolutional_Neural_Networks_on_High_Resolution_Whole-Field_ICCVW_2023_paper.pdf)]
    * Title: Weed Mapping with Convolutional Neural Networks on High Resolution Whole-Field Images
    * Publisher: ICCV
    * Publication Date: `2023`
    * Authors: Yuemin Wang, Thuan Ha, Kathryn Aldridge, Hema Duddu, Steve Shirtliffe, Ian Stavness
    * Abstract: Weed mapping is a technique used to identify and locate harmful weed plants in farm fields. Accurate weed mapping enables targeted herbicide application and helps plant scientists to estimate the effectiveness of field experiments. In this paper we discuss a highly practical and effective working pipeline to weed map a wheat field combining GIS and deep learning technology. This pipeline is an end-to-end process including using an unoccupied aerial vehicle (UAV) to collect ultra-high definition whole-field images, labelling and training deep learning models and an efficient evaluation process for the resulting weed map. We show that our method can generate accurate pixel-wise weed maps by only training on small regions of the field, and can generalize well when making predictions back on the larger whole-field orthomosaic image.

count=1
* Real-Time Vehicle Tracking in Aerial Video Using Hyperspectral Features
    [[abs-CVF](https://openaccess.thecvf.com/content_cvpr_2016_workshops/w29/html/Uzkent_Real-Time_Vehicle_Tracking_CVPR_2016_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_cvpr_2016_workshops/w29/papers/Uzkent_Real-Time_Vehicle_Tracking_CVPR_2016_paper.pdf)]
    * Title: Real-Time Vehicle Tracking in Aerial Video Using Hyperspectral Features
    * Publisher: CVPR
    * Publication Date: `2016`
    * Authors: Burak Uzkent, Matthew J. Hoffman, Anthony Vodacek
    * Abstract: Vehicle tracking from a moving aerial platform poses a number of unique challenges including the small number of pixels representing a vehicle, large camera motion, and parallax error. This paper considers a multi-modal sensor to design a real-time persistent aerial tracking system. Wide field of view (FOV) panchromatic imagery is used to remove global camera motion whereas narrow FOV hyperspectral image is used to detect the target of interest (TOI). Hyperspectral features provide distinctive information to reject objects with different reflectance characteristics from the TOI. This way the density of detected vehicles is reduced, which increases tracking consistency. Finally, we use a spatial data based classifier to remove spurious detections. With such framework, parallax effect in non-planar scenes is avoided. The proposed tracking system is evaluated in a dense, synthetic scene and outperforms other state-of-the-art traditional and aerial object trackers.

count=1
* Dense Semantic Labeling of Very-High-Resolution Aerial Imagery and LiDAR With Fully-Convolutional Neural Networks and Higher-Order CRFs
    [[abs-CVF](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w18/html/Liu_Dense_Semantic_Labeling_CVPR_2017_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w18/papers/Liu_Dense_Semantic_Labeling_CVPR_2017_paper.pdf)]
    * Title: Dense Semantic Labeling of Very-High-Resolution Aerial Imagery and LiDAR With Fully-Convolutional Neural Networks and Higher-Order CRFs
    * Publisher: CVPR
    * Publication Date: `2017`
    * Authors: Yansong Liu, Sankaranarayanan Piramanayagam, Sildomar T. Monteiro, Eli Saber
    * Abstract: Efficient and effective multisensor fusion techniques are demanded in order to fully exploit two complementary data modalities, e.g aerial optical imagery, and the LiDAR data. Recent efforts have been mostly devoted to exploring how to properly combine both sensor data using pre-trained deep convolutional neural networks (DCNNs) at the feature level. In this paper, we propose a decision-level fusion approach with a simpler architecture for the task of dense semantic labeling. Our proposed method first obtains two initial probabilistic labeling results from a fully-convolutional neural network and a simple classifier, e.g. logistic regression exploiting spectral channels and LiDAR data, respectively. These two outcomes are then combined within a higher-order conditional random field (CRF). The CRF inference will estimate the final dense semantic labeling results. The proposed method generates the state-of-the-art semantic labeling results.

count=1
* Temporal Vegetation Modelling Using Long Short-Term Memory Networks for Crop Identification From Medium-Resolution Multi-Spectral Satellite Images
    [[abs-CVF](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w18/html/Russwurm_Temporal_Vegetation_Modelling_CVPR_2017_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w18/papers/Russwurm_Temporal_Vegetation_Modelling_CVPR_2017_paper.pdf)]
    * Title: Temporal Vegetation Modelling Using Long Short-Term Memory Networks for Crop Identification From Medium-Resolution Multi-Spectral Satellite Images
    * Publisher: CVPR
    * Publication Date: `2017`
    * Authors: Marc Russwurm, Marco Korner
    * Abstract: Land-cover classification is one of the key problems in earth observation and extensively investigated over the recent decades. Usually, approaches concentrate on single-time and multi- or hyperspectral reflectance space- or airborne sensor measurements observed. However, land-cover classes, e.g., crops, change their reflective characteristics over time complicating classification at one observation time. Contrary, these features change in a systematic and predictive manner, which can be utilized in a multi-temporal approach. We use long short-term memory (LSTM) networks to extract temporal characteristics from a sequence of Sentinel-2 observations. We compare the performance of LSTM and other network architectures and a SVM baseline to show the effectiveness of dynamic temporal feature extraction. A large test area combined with rich ground truth labels was used for training and evaluation. Our LSTM variant achieves state-of-the art performance opening potential for further research.

count=1
* The First Automatic Method for Mapping the Pothole in Seagrass
    [[abs-CVF](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w3/html/Rahnemoonfar_The_First_Automatic_CVPR_2017_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w3/papers/Rahnemoonfar_The_First_Automatic_CVPR_2017_paper.pdf)]
    * Title: The First Automatic Method for Mapping the Pothole in Seagrass
    * Publisher: CVPR
    * Publication Date: `2017`
    * Authors: Maryam Rahnemoonfar, Masoud Yari, Abdullah Rahman, Richard Kline
    * Abstract: There is a vital need to map seagrass ecosystems in order to determine worldwide abundance and distribution. Currently there is no established method for mapping the pothole or scars in seagrass. Detection of seagrass with optical remote sensing is challenged by the fact that light is attenuated as it passes through the water column and reflects back from the benthos. Optical remote sensing of seagrass is only possible if the water is shallow and relatively clear. In reality, coastal waters are commonly turbid, and seagrasses can grow under 10 meters of water or even deeper. One of the most precise sensors to map the seagrass disturbance is side scan sonar. Underwater acoustics mapping produces a high definition, two-dimensional sonar image of seagrass ecosystems. This paper proposes a methodology which detects seagrass potholes in sonar images. Side scan sonar images usually contain speckle noise and uneven illumination across the image. Moreover, disturbance presents complex patterns where most segmentation techniques will fail. In this paper, the quality of image is improved in the first stage using adaptive thresholding and wavelet denoising techniques. In the next step, a novel level set technique is applied to identify the pothole patterns. Our method is robust to noise and uneven illumination. Moreover it can detect the complex pothole patterns. We tested our proposed approach on a collection of underwater sonar images taken from Laguna Madre in Texas. Experimental results in comparison with the ground-truth show the efficiency of the proposed method.

count=1
* Aerial Vehicle Tracking by Adaptive Fusion of Hyperspectral Likelihood Maps
    [[abs-CVF](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w3/html/Uzkent_Aerial_Vehicle_Tracking_CVPR_2017_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w3/papers/Uzkent_Aerial_Vehicle_Tracking_CVPR_2017_paper.pdf)]
    * Title: Aerial Vehicle Tracking by Adaptive Fusion of Hyperspectral Likelihood Maps
    * Publisher: CVPR
    * Publication Date: `2017`
    * Authors: Burak Uzkent, Aneesh Rangnekar, Matthew Hoffman
    * Abstract: Hyperspectral cameras provide unique spectral signatures that can be used to solve surveillance tasks. This paper proposes a novel real-time hyperspectral likelihood maps-aided tracking method (HLT) inspired by an adaptive hyperspectral sensor. We focus on the target detection part of a tracking system and remove the necessity to build any offline classifiers and tune large amount of hyper-parameters, instead learning a generative target model in an online manner for hyperspectral channels ranging from visible to infrared wavelengths. The key idea is that our adaptive fusion method can combine likelihood maps from multiple bands of hyperspectral imagery into one single more distinctive representation increasing the margin between mean value of foreground and background pixels in the fused map. Experimental results show that the HLT not only outperforms all established fusion methods but is on par with the current state-of-the-art hyperspectral target tracking frameworks.

count=1
* Large Scale High-Resolution Land Cover Mapping With Multi-Resolution Data
    [[abs-CVF](https://openaccess.thecvf.com/content_CVPR_2019/html/Robinson_Large_Scale_High-Resolution_Land_Cover_Mapping_With_Multi-Resolution_Data_CVPR_2019_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_CVPR_2019/papers/Robinson_Large_Scale_High-Resolution_Land_Cover_Mapping_With_Multi-Resolution_Data_CVPR_2019_paper.pdf)]
    * Title: Large Scale High-Resolution Land Cover Mapping With Multi-Resolution Data
    * Publisher: CVPR
    * Publication Date: `2019`
    * Authors: Caleb Robinson,  Le Hou,  Kolya Malkin,  Rachel Soobitsky,  Jacob Czawlytko,  Bistra Dilkina,  Nebojsa Jojic
    * Abstract: In this paper we propose multi-resolution data fusion methods for deep learning-based high-resolution land cover mapping from aerial imagery. The land cover mapping problem, at country-level scales, is challenging for common deep learning methods due to the scarcity of high-resolution labels, as well as variation in geography and quality of input images. On the other hand, multiple satellite imagery and low-resolution ground truth label sources are widely available, and can be used to improve model training efforts. Our methods include: introducing low-resolution satellite data to smooth quality differences in high-resolution input, exploiting low-resolution labels with a dual loss function, and pairing scarce high-resolution labels with inputs from several points in time. We train models that are able to generalize from a portion of the Northeast United States, where we have high-resolution land cover labels, to the rest of the US. With these models, we produce the first high-resolution (1-meter) land cover map of the contiguous US, consisting of over 8 trillion pixels. We demonstrate the robustness and potential applications of this data in a case study with domain experts and develop a web application to share our results. This work is practically useful, and can be applied to other locations over the earth as high-resolution imagery becomes more widely available even as high-resolution labeled land cover data remains sparse.

count=1
* Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis
    [[abs-CVF](https://openaccess.thecvf.com/content_CVPR_2020/html/Chiu_Agriculture-Vision_A_Large_Aerial_Image_Database_for_Agricultural_Pattern_Analysis_CVPR_2020_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chiu_Agriculture-Vision_A_Large_Aerial_Image_Database_for_Agricultural_Pattern_Analysis_CVPR_2020_paper.pdf)]
    * Title: Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis
    * Publisher: CVPR
    * Publication Date: `2020`
    * Authors: Mang Tik Chiu,  Xingqian Xu,  Yunchao Wei,  Zilong Huang,  Alexander G. Schwing,  Robert Brunner,  Hrant Khachatrian,  Hovnatan Karapetyan,  Ivan Dozier,  Greg Rose,  David Wilson,  Adrian Tudor,  Naira Hovakimyan,  Thomas S. Huang,  Honghui Shi
    * Abstract: The success of deep learning in visual recognition tasks has driven advancements in multiple fields of research. Particularly, increasing attention has been drawn towards its application in agriculture. Nevertheless, while visual pattern recognition on farmlands carries enormous economic values, little progress has been made to merge computer vision and crop sciences due to the lack of suitable agricultural image datasets. Meanwhile, problems in agriculture also pose new challenges in computer vision. For example, semantic segmentation of aerial farmland images requires inference over extremely large-size images with extreme annotation sparsity. These challenges are not present in most of the common object datasets, and we show that they are more challenging than many other aerial image datasets. To encourage research in computer vision for agriculture, we present Agriculture-Vision: a large-scale aerial farmland image dataset for semantic segmentation of agricultural patterns. We collected 94,986 high-quality aerial images from 3,432 farmlands across the US, where each image consists of RGB and Near-infrared (NIR) channels with resolution as high as 10 cm per pixel. We annotate nine types of field anomaly patterns that are most important to farmers. As a pilot study of aerial agricultural semantic segmentation, we perform comprehensive experiments using popular semantic segmentation models; we also propose an effective model designed for aerial agricultural pattern recognition. Our experiments demonstrate several challenges Agriculture-Vision poses to both the computer vision and agriculture communities. Future versions of this dataset will include even more aerial images, anomaly patterns and image channels.

count=1
* Satellite Image Time Series Classification With Pixel-Set Encoders and Temporal Self-Attention
    [[abs-CVF](https://openaccess.thecvf.com/content_CVPR_2020/html/Garnot_Satellite_Image_Time_Series_Classification_With_Pixel-Set_Encoders_and_Temporal_CVPR_2020_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_CVPR_2020/papers/Garnot_Satellite_Image_Time_Series_Classification_With_Pixel-Set_Encoders_and_Temporal_CVPR_2020_paper.pdf)]
    * Title: Satellite Image Time Series Classification With Pixel-Set Encoders and Temporal Self-Attention
    * Publisher: CVPR
    * Publication Date: `2020`
    * Authors: Vivien Sainte Fare Garnot,  Loic Landrieu,  Sebastien Giordano,  Nesrine Chehata
    * Abstract: Satellite image time series, bolstered by their growing availability, are at the forefront of an extensive effort towards automated Earth monitoring by international institutions. In particular, large-scale control of agricultural parcels is an issue of major political and economic importance. In this regard, hybrid convolutional-recurrent neural architectures have shown promising results for the automated classification of satellite image time series. We propose an alternative approach in which the convolutional layers are advantageously replaced with encoders operating on unordered sets of pixels to exploit the typically coarse resolution of publicly available satellite images. We also propose to extract temporal features using a bespoke neural architecture based on self-attention instead of recurrent networks. We demonstrate experimentally that our method not only outperforms previous state-of-the-art approaches in terms of precision, but also significantly decreases processing time and memory requirements. Lastly, we release a large open-access annotated dataset as a benchmark for future work on satellite image time series.

count=1
* Learning To Predict Crop Type From Heterogeneous Sparse Labels Using Meta-Learning
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Tseng_Learning_To_Predict_Crop_Type_From_Heterogeneous_Sparse_Labels_Using_CVPRW_2021_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2021W/EarthVision/papers/Tseng_Learning_To_Predict_Crop_Type_From_Heterogeneous_Sparse_Labels_Using_CVPRW_2021_paper.pdf)]
    * Title: Learning To Predict Crop Type From Heterogeneous Sparse Labels Using Meta-Learning
    * Publisher: CVPR
    * Publication Date: `2021`
    * Authors: Gabriel Tseng, Hannah Kerner, Catherine Nakalembe, Inbal Becker-Reshef
    * Abstract: There are many labelled datasets relating to land cover and crop type mapping that cover diverse geographies, agroecologies and land uses. However, these labels are often extremely sparse, particularly in low- and middle-income regions, with as few as tens of examples for certain crop types. This makes it challenging to train supervised machine learning models to detect specific crops in satellite observations of these regions. We investigate the utility of model-agnostic meta-learning (MAML) to learn from diverse global datasets and improve performance in data-sparse regions. We find that in a variety of countries (Togo, Kenya and Brazil) and across a variety of tasks (crop type mapping, crop vs. non-crop mapping), MAML improves performance compared to pretrained and random initial weights. We also investigate the utility of MAML for different target data-size regimes. We find MAML outperforms other methods for a wide range of training set sizes and positive to negative label ratios, indicating its general suitability for land use and crop type mapping.

count=1
* Augmentation Invariance and Adaptive Sampling in Semantic Segmentation of Agricultural Aerial Images
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2022W/AgriVision/html/Tavera_Augmentation_Invariance_and_Adaptive_Sampling_in_Semantic_Segmentation_of_Agricultural_CVPRW_2022_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2022W/AgriVision/papers/Tavera_Augmentation_Invariance_and_Adaptive_Sampling_in_Semantic_Segmentation_of_Agricultural_CVPRW_2022_paper.pdf)]
    * Title: Augmentation Invariance and Adaptive Sampling in Semantic Segmentation of Agricultural Aerial Images
    * Publisher: CVPR
    * Publication Date: `2022`
    * Authors: Antonio Tavera, Edoardo Arnaudo, Carlo Masone, Barbara Caputo
    * Abstract: In this paper, we investigate the problem of Semantic Segmentation for agricultural aerial imagery. We observe that the existing methods used for this task are designed without considering two characteristics of the aerial data: (i) the top-down perspective implies that the model cannot rely on a fixed semantic structure of the scene, because the same scene may be experienced with different rotations of the sensor; (ii) there can be a strong imbalance in the distribution of semantic classes because the relevant objects of the scene may appear at extremely different scales (e.g., a field of crops and a small vehicle). We propose a solution to these problems based on two ideas: (i) we use together a set of suitable augmentation and a consistency loss to guide the model to learn semantic representations that are invariant to the photometric and geometric shifts typical of the top-down perspective (Augmentation Invariance); (ii) we use a sampling method (Adaptive Sampling) that selects the training images based on a measure of pixel-wise distribution of classes and actual network confidence. With an extensive set of experiments conducted on the Agriculture-Vision dataset, we demonstrate that our proposed strategies improve the performance of the current state-of-the-art method.

count=1
* Semi-Supervised Hyperspectral Object Detection Challenge Results - PBVS 2022
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Rangnekar_Semi-Supervised_Hyperspectral_Object_Detection_Challenge_Results_-_PBVS_2022_CVPRW_2022_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Rangnekar_Semi-Supervised_Hyperspectral_Object_Detection_Challenge_Results_-_PBVS_2022_CVPRW_2022_paper.pdf)]
    * Title: Semi-Supervised Hyperspectral Object Detection Challenge Results - PBVS 2022
    * Publisher: CVPR
    * Publication Date: `2022`
    * Authors: Aneesh Rangnekar, Zachary Mulhollan, Anthony Vodacek, Matthew Hoffman, Angel D. Sappa, Erik Blasch, Jun Yu, Liwen Zhang, Shenshen Du, Hao Chang, Keda Lu, Zhong Zhang, Fang Gao, Ye Yu, Feng Shuang, Lei Wang, Qiang Ling, Pranjay Shyam, Kuk-Jin Yoon, Kyung-Soo Kim
    * Abstract: This paper summarizes the top contributions to the first semi-supervised hyperspectral object detection (SSHOD) challenge, which was organized as a part of the Perception Beyond the Visible Spectrum (PBVS) 2022 workshop at the Computer Vision and Pattern Recognition (CVPR) conference. The SSHOD challenge is a first-of-its-kind hyperspectral dataset with temporally contiguous frames collected from a university rooftop observing a 4-way vehicle intersection over a period of three days. The dataset contains a total of 2890 frames, captured at an average resolution of 1600 x 192 pixels, with 51 hyperspectral bands from 400nm to 900nm. SSHOD challenge uses 989 images as the training set, 605 images as validation set and 1296 images as the evaluation (test) set. Each set was acquired on a different day to maximize the variance in weather conditions. Labels are provided for 10% of the annotated data, hence formulating a semi-supervised learning task for the participants which is evaluated in terms of average precision over the entire set of classes, as well as individual moving object classes: namely vehicle, bus and bike. The challenge received participation registration from 38 individuals, with 8 participating in the validation phase and 3 participating in the test phase. This paper describes the dataset acquisition, with challenge formulation, proposed methods and qualitative and quantitative results.

count=1
* CIPPSRNet: A Camera Internal Parameters Perception Network Based Contrastive Learning for Thermal Image Super-Resolution
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Wang_CIPPSRNet_A_Camera_Internal_Parameters_Perception_Network_Based_Contrastive_Learning_CVPRW_2022_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Wang_CIPPSRNet_A_Camera_Internal_Parameters_Perception_Network_Based_Contrastive_Learning_CVPRW_2022_paper.pdf)]
    * Title: CIPPSRNet: A Camera Internal Parameters Perception Network Based Contrastive Learning for Thermal Image Super-Resolution
    * Publisher: CVPR
    * Publication Date: `2022`
    * Authors: Kai Wang, Qigong Sun, Yicheng Wang, Huiyuan Wei, Chonghua Lv, Xiaolin Tian, Xu Liu
    * Abstract: Thermal Image Super-Resolution (TISR) is a technique for converting Low-Resolution (LR) thermal images to High-Resolution (HR) thermal images. This technique has recently become a research hotspot due to its ability to reduce sensor costs and improve visual perception. However, current research does not provide an effective solution for multi-sensor data training, possibly driven by pixel mismatch and simple degradation setting issues. In this paper, we proposed a Camera Internal Parameters Perception Network (CIPPSRNet) for LR thermal image enhancement. The camera internal parameters (CIP) were explicitly modeled as a feature representation, the LR features were transformed into the intermediate domain containing the internal parameters information by perceiving CIP representation. The mapping between the intermediate domain and the spatial domain of the HR features was learned by CIPPSRNet. In addition, we introduced contrastive learning to optimize the pretrained Camera Internal Parameters Representation Network and the feature encoders. Our proposed network is capable of achieving a more efficient transformation from the LR to the HR domains. Additionally, the use of contrastive learning can improve the network's adaptability to misalignment data with insufficient pixel matching and its robustness. Experiments on PBVS2022 TISR Dataset show that our network has achieved state-of-the-art performance for the Thermal SR task.

count=1
* Tree Instance Segmentation With Temporal Contour Graph
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2023/html/Firoze_Tree_Instance_Segmentation_With_Temporal_Contour_Graph_CVPR_2023_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2023/papers/Firoze_Tree_Instance_Segmentation_With_Temporal_Contour_Graph_CVPR_2023_paper.pdf)]
    * Title: Tree Instance Segmentation With Temporal Contour Graph
    * Publisher: CVPR
    * Publication Date: `2023`
    * Authors: Adnan Firoze, Cameron Wingren, Raymond A. Yeh, Bedrich Benes, Daniel Aliaga
    * Abstract: We present a novel approach to perform instance segmentation, and counting, for densely packed self-similar trees using a top-view RGB image sequence. We propose a solution that leverages pixel content, shape, and self-occlusion. First, we perform an initial over-segmentation of the image sequence and aggregate structural characteristics into a contour graph with temporal information incorporated. Second, using a graph convolutional network and its inherent local messaging passing abilities, we merge adjacent tree crown patches into a final set of tree crowns. Per various studies and comparisons, our method is superior to all prior methods and results in high-accuracy instance segmentation and counting, despite the trees being tightly packed. Finally, we provide various forest image sequence datasets suitable for subsequent benchmarking and evaluation captured at different altitudes and leaf conditions.

count=1
* Comprehensive Quality Assessment of Optical Satellite Imagery Using Weakly Supervised Video Learning
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Pasquarella_Comprehensive_Quality_Assessment_of_Optical_Satellite_Imagery_Using_Weakly_Supervised_CVPRW_2023_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Pasquarella_Comprehensive_Quality_Assessment_of_Optical_Satellite_Imagery_Using_Weakly_Supervised_CVPRW_2023_paper.pdf)]
    * Title: Comprehensive Quality Assessment of Optical Satellite Imagery Using Weakly Supervised Video Learning
    * Publisher: CVPR
    * Publication Date: `2023`
    * Authors: Valerie J. Pasquarella, Christopher F. Brown, Wanda Czerwinski, William J. Rucklidge
    * Abstract: Identifying high-quality (i.e., relatively clear) measurements of surface conditions is a near-universal first step in working with optical satellite imagery. Many cloud masking algorithms have been developed to characterize the likelihood that reflectance measurements for individual pixels were influenced by clouds, cloud shadows, and other atmospheric effects. However, due to the continuous density of the atmospheric volume, we argue that quantification of occlusion and corruption effects is better treated as a regression problem rather than a discretized classification problem as done in prior work. We propose a space-time context network trained using a bootstrapping procedure that leverages millions of automatically-mined video sequences informed by a weakly supervised measure of atmospheric similarity. We find that our approach outperforms existing machine learning and physical basis cloud and cloud shadow detection algorithms, producing state-of-the-art results for Sentinel-2 imagery on two different out-of-distribution reference datasets. The resulting product offers a flexible quality assessment (QA) solution that enables both standard cloud and cloud shadow masking via thresholding and more complex image grading for compositing or downstream models. By way of generality, minimal supervision, and scale of our training data, our approach has the potential to significantly improve the utility and usability of optical remote sensing imagery.

count=1
* C-PLES: Contextual Progressive Layer Expansion With Self-Attention for Multi-Class Landslide Segmentation on Mars Using Multimodal Satellite Imagery
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Reyes_C-PLES_Contextual_Progressive_Layer_Expansion_With_Self-Attention_for_Multi-Class_Landslide_CVPRW_2023_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2023W/PBVS/papers/Reyes_C-PLES_Contextual_Progressive_Layer_Expansion_With_Self-Attention_for_Multi-Class_Landslide_CVPRW_2023_paper.pdf)]
    * Title: C-PLES: Contextual Progressive Layer Expansion With Self-Attention for Multi-Class Landslide Segmentation on Mars Using Multimodal Satellite Imagery
    * Publisher: CVPR
    * Publication Date: `2023`
    * Authors: Abel A. Reyes, Sidike Paheding, A. Rajaneesh, K.S. Sajinkumar, Thomas Oommen
    * Abstract: Landslide segmentation on Earth has been a challenging computer vision task, in which the lack of annotated data or limitation on computational resources has been a major obstacle in the development of accurate and scalable artificial intelligence-based models. However, the accelerated progress in deep learning techniques and the availability of data-sharing initiatives have enabled significant achievements in landslide segmentation on Earth. With the current capabilities in technology and data availability, replicating a similar task on other planets, such as Mars, does not seem an impossible task anymore. In this research, we present C-PLES (Contextual Progressive Layer Expansion with Self-attention), a deep learning architecture for multi-class landslide segmentation in the Valles Marineris (VM) on Mars. Even though the challenges could be different from on-Earth landslide segmentation, due to the nature of the environment and data characteristics, the outcomes of this research lead to a better understanding of the geology and terrain of the planet, in addition, to providing valuable insights regarding the importance of image modality for this task. The proposed architecture combines the merits of the progressive neuron expansion with attention mechanisms in an encoder-decoder-based framework, delivering competitive performance in comparison with state-of-the-art deep learning architectures for landslide segmentation. In addition to the new multi-class segmentation architecture, we introduce a new multi-modal multi-class Martian landslide segmentation dataset for the first time.

count=1
* Multi-modal Learning for Geospatial Vegetation Forecasting
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2024/html/Benson_Multi-modal_Learning_for_Geospatial_Vegetation_Forecasting_CVPR_2024_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2024/papers/Benson_Multi-modal_Learning_for_Geospatial_Vegetation_Forecasting_CVPR_2024_paper.pdf)]
    * Title: Multi-modal Learning for Geospatial Vegetation Forecasting
    * Publisher: CVPR
    * Publication Date: `2024`
    * Authors: Vitus Benson, Claire Robin, Christian Requena-Mesa, Lazaro Alonso, Nuno Carvalhais, José Cortés, Zhihan Gao, Nora Linscheid, Mélanie Weynants, Markus Reichstein
    * Abstract: Precise geospatial vegetation forecasting holds potential across diverse sectors including agriculture forestry humanitarian aid and carbon accounting. To leverage the vast availability of satellite imagery for this task various works have applied deep neural networks for predicting multispectral images in photorealistic quality. However the important area of vegetation dynamics has not been thoroughly explored. Our study introduces GreenEarthNet the first dataset specifically designed for high-resolution vegetation forecasting and Contextformer a novel deep learning approach for predicting vegetation greenness from Sentinel 2 satellite images with fine resolution across Europe. Our multi-modal transformer model Contextformer leverages spatial context through a vision backbone and predicts the temporal dynamics on local context patches incorporating meteorological time series in a parameter-efficient manner. The GreenEarthNet dataset features a learned cloud mask and an appropriate evaluation scheme for vegetation modeling. It also maintains compatibility with the existing satellite imagery forecasting dataset EarthNet2021 enabling cross-dataset model comparisons. Our extensive qualitative and quantitative analyses reveal that our methods outperform a broad range of baseline techniques. This includes surpassing previous state-of-the-art models on EarthNet2021 as well as adapted models from time series forecasting and video prediction. To the best of our knowledge this work presents the first models for continental-scale vegetation modeling at fine resolution able to capture anomalies beyond the seasonal cycle thereby paving the way for predicting vegetation health and behaviour in response to climate variability and extremes. We provide open source code and pre-trained weights to reproduce our experimental results under https://github.com/vitusbenson/greenearthnet.

count=1
* SwinFuSR: An Image Fusion-inspired Model for RGB-guided Thermal Image Super-resolution
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2024W/PBVS/html/Arnold_SwinFuSR_An_Image_Fusion-inspired_Model_for_RGB-guided_Thermal_Image_Super-resolution_CVPRW_2024_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2024W/PBVS/papers/Arnold_SwinFuSR_An_Image_Fusion-inspired_Model_for_RGB-guided_Thermal_Image_Super-resolution_CVPRW_2024_paper.pdf)]
    * Title: SwinFuSR: An Image Fusion-inspired Model for RGB-guided Thermal Image Super-resolution
    * Publisher: CVPR
    * Publication Date: `2024`
    * Authors: Cyprien Arnold, Philippe Jouvet, Lama Seoud
    * Abstract: ?

count=1
* HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing
    [[abs-CVF](https://openaccess.thecvf.com/content/CVPR2024W/Vision4Ag/html/Xu_HarvestNet_A_Dataset_for_Detecting_Smallholder_Farming_Activity_Using_Harvest_CVPRW_2024_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/CVPR2024W/Vision4Ag/papers/Xu_HarvestNet_A_Dataset_for_Detecting_Smallholder_Farming_Activity_Using_Harvest_CVPRW_2024_paper.pdf)]
    * Title: HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing
    * Publisher: CVPR
    * Publication Date: `2024`
    * Authors: Jonathan Xu, Amna Elmustafa, Liya Weldegebriel, Emnet Negash, Richard Lee, Chenlin Meng, Stefano Ermon, David Lobell
    * Abstract: Small farms contribute to a large share of the productive land in developing countries. In regions such as sub-Saharan Africa where 80% of farms are small (under 2 ha in size) the task of mapping smallholder cropland is an important part of tracking sustainability measures such as crop productivity. However the visually diverse and nuanced appearance of small farms has limited the effectiveness of traditional approaches to cropland mapping. Here we introduce a new approach based on the detection of harvest piles characteristic of many smallholder systems throughout the world. We present HarvestNet a dataset for mapping the presence of farms in the Ethiopian regions of Tigray and Amhara during 2020-2023 collected using expert knowledge and satellite images totaling 7k hand-labeled images and 2k ground-collected labels. We also benchmark a set of baselines including SOTA models in remote sensing with our best models having around 80% classification performance on hand labelled data and 90% and 98% accuracy on ground truth data for Tigray and Amhara respectively. We also perform a visual comparison with a widely used pre-existing coverage map and show that our model detects an extra 56621 hectares of cropland in Tigray. We conclude that remote sensing of harvest piles can contribute to more timely and accurate cropland assessments in food insecure regions. The dataset can be accessed through https://figshare.com/s/45a7b45556b90a9a11d2 while the code for the dataset and benchmarks is publicly available at https://github.com/jonxuxu/harvest-piles.

count=1
* ProTractor: A Lightweight Ground Imaging and Analysis System for Early-Season Field Phenotyping
    [[abs-CVF](https://openaccess.thecvf.com/content_CVPRW_2019/html/CVPPP/Higgs_ProTractor_A_Lightweight_Ground_Imaging_and_Analysis_System_for_Early-Season_CVPRW_2019_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_CVPRW_2019/papers/CVPPP/Higgs_ProTractor_A_Lightweight_Ground_Imaging_and_Analysis_System_for_Early-Season_CVPRW_2019_paper.pdf)]
    * Title: ProTractor: A Lightweight Ground Imaging and Analysis System for Early-Season Field Phenotyping
    * Publisher: CVPR
    * Publication Date: `2019`
    * Authors: Nico Higgs,  Blanche Leyeza,  Jordan Ubbens,  Josh Kocur,  William van der Kamp,  Theron Cory,  Christina Eynck,  Sally Vail,  Mark Eramian,  Ian Stavness
    * Abstract: Acquiring high-resolution images in the field for image-based crop phenotyping is typically performed by complicated, custom built "pheno-mobiles." In this paper, we demonstrate that large datasets of crop row images can be easily acquired with consumer cameras attached to a regular tractor. Localization and labeling of individual rows of plants are performed by a computer vision approach, rather than sophisticated real-time geo-location hardware on the tractor. We evaluate our approach for cropping rows of early-season plants from a Brassica carinata field trial where we achieve 100% recall and 99% precision. We also demonstrate a proof-of-concept plant counting method for our ProTractor system using an object detection network that achieves a mean average precision of 0.82 when detecting plants, and an R2 of 0.89 when counting plants. The ProTractor design and software are open source to advance the collection of large outdoor plant phenotyping datasets with inexpensive and easy to use acquisition systems.

count=1
* Sen1Floods11: A Georeferenced Dataset to Train and Test Deep Learning Flood Algorithms for Sentinel-1
    [[abs-CVF](https://openaccess.thecvf.com/content_CVPRW_2020/html/w11/Bonafilia_Sen1Floods11_A_Georeferenced_Dataset_to_Train_and_Test_Deep_Learning_CVPRW_2020_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Bonafilia_Sen1Floods11_A_Georeferenced_Dataset_to_Train_and_Test_Deep_Learning_CVPRW_2020_paper.pdf)]
    * Title: Sen1Floods11: A Georeferenced Dataset to Train and Test Deep Learning Flood Algorithms for Sentinel-1
    * Publisher: CVPR
    * Publication Date: `2020`
    * Authors: Derrick Bonafilia, Beth Tellman, Tyler Anderson, Erica Issenberg
    * Abstract: Accurate flood mapping at global scales can support disaster relief and recovery efforts. Improving flood relief with more accurate data is of great importance due to expected increases in the frequency and magnitude of flood events with climatic and demographic changes. To assist efforts to operationalize deep learning algorithms for flood mapping at global scales, we introduce Sen1Floods11, a surface water data set including classified permanent water, flood water, and raw Sentinel-1 imagery. This dataset consists of 4,831 512x512 chips covering 120,406 km\textsuperscript 2 and spans all 14 biomes, 357 ecoregions, and 6 continents of the world across 11 flood events. We used Sen1Floods11 to train, validate, and test fully convolutional neural networks (FCNN) to segment permanent and flood water. We compare results of classifying permanent, flood, and total surface water from training four FCNN models: i) 446 hand labeled chips of surface water from flood events; ii) 814 chips of publicly available permanent water data labels from Landsat (JRC surface water dataset); iii) 4385 chips of surface water classified from Sentinel-2 images from flood events and iv) 4385 chips of surface water classified from Sentinel-1 imagery from flood events. We compare these four models to a common remote sensing approach of thresholding radar backscatter to identify surface water. Future research to operationalize computer vision approaches to mapping flood and surface water could build new models from Sen1Floods11 and expand this dataset to include additional sensors and flood events. We provide Sen1Floods11, as well as our training and evaluation code at: https://github.com/cloudtostreet/Sen1Floods11

count=1
* Improving Superpixel Boundaries Using Information Beyond the Visual Spectrum
    [[abs-CVF](https://openaccess.thecvf.com/content_cvpr_workshops_2015/W05/html/Sullivan_Improving_Superpixel_Boundaries_2015_CVPR_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_cvpr_workshops_2015/W05/papers/Sullivan_Improving_Superpixel_Boundaries_2015_CVPR_paper.pdf)]
    * Title: Improving Superpixel Boundaries Using Information Beyond the Visual Spectrum
    * Publisher: CVPR
    * Publication Date: `2015`
    * Authors: Keith Sullivan, Wallace Lawson, Donald Sofge
    * Abstract: Superpixels enable a scene to be analyzed on a larger scale, by examining regions that have a high level of similarity. These regions can change depending on how similarity is measured. Color is a simple and effective measure, but it is adversely affected in environments where the boundary between objects and the surrounding environment are difficult to detect due to similar colors and/or shadows. We extend a common superpixel algorithm (SLIC) to include near-infrared intensity information and measured distance information to help oversegmentation in complex environments. We demonstrate the efficacy of our approach on two problems: object segmentation and scene segmentation.

count=1
* Semantic Segmentation of Urban Scenes by Learning Local Class Interactions
    [[abs-CVF](https://openaccess.thecvf.com/content_cvpr_workshops_2015/W13/html/Volpi_Semantic_Segmentation_of_2015_CVPR_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_cvpr_workshops_2015/W13/papers/Volpi_Semantic_Segmentation_of_2015_CVPR_paper.pdf)]
    * Title: Semantic Segmentation of Urban Scenes by Learning Local Class Interactions
    * Publisher: CVPR
    * Publication Date: `2015`
    * Authors: Michele Volpi, Vittorio Ferrari
    * Abstract: Traditionally, land-cover mapping from remote sensing images is performed by classifying each atomic region in the image in isolation and by enforcing simple smoothing priors via random fields models as two independent steps. In this paper, we propose to model the segmentation problem by a discriminatively trained Conditional Random Field (CRF). To this end, we employ Structured Support Vector Machines (SSVM) to learn the weights of an informative set of appearance descriptors jointly with local class interactions. We propose a principled strategy to learn pairwise potentials encoding local class preferences from sparsely annotated ground truth. We show that this approach outperform standard baselines and more expressive CRF models, improving by 4-6 points the average class accuracy on a challenging dataset involving urban high resolution satellite imagery.

count=1
* HyKo: A Spectral Dataset for Scene Understanding
    [[abs-CVF](https://openaccess.thecvf.com/content_ICCV_2017_workshops/w3/html/Winkens_HyKo_A_Spectral_ICCV_2017_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Winkens_HyKo_A_Spectral_ICCV_2017_paper.pdf)]
    * Title: HyKo: A Spectral Dataset for Scene Understanding
    * Publisher: ICCV
    * Publication Date: `2017`
    * Authors: Christian Winkens, Florian Sattler, Veronika Adams, Dietrich Paulus
    * Abstract: We present a novel dataset captured with compact, low-cost, snapshot mosaic (SSM) imaging cameras, which are able to capture a whole spectral cube in one shot. For the best of our knowledge its the first dataset in which hyperspectral data was recorded from a moving vehicle enabling hyperspectral scene analysis for road scene understanding. In total, we recorded several hours of traffic scenarios using a variety of sensor modalities such as hyperspectral cameras and 3D laser scanners. We captured and hand labeled diverse scenarios ranging from real-world traffic situations in city scenes to suburban areas. Our data is synchronized and annotated, containing semantic and material labels which allows training classifiers for scene understanding and autonomous driving. The data covers wavelengths from 400 to 1000 nm spanning the visible and near infrared spectral ranges. In this work we describe our recording platforms, the data format and needed utilities to work with the data.

count=1
* TeleViT: Teleconnection-Driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting
    [[abs-CVF](https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/html/Prapas_TeleViT_Teleconnection-Driven_Transformers_Improve_Subseasonal_to_Seasonal_Wildfire_Forecasting_ICCVW_2023_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/ICCV2023W/AIHADR/papers/Prapas_TeleViT_Teleconnection-Driven_Transformers_Improve_Subseasonal_to_Seasonal_Wildfire_Forecasting_ICCVW_2023_paper.pdf)]
    * Title: TeleViT: Teleconnection-Driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting
    * Publisher: ICCV
    * Publication Date: `2023`
    * Authors: Ioannis Prapas, Nikolaos-Ioannis Bountos, Spyros Kondylatos, Dimitrios Michail, Gustau Camps-Valls, Ioannis Papoutsis
    * Abstract: Wildfires are increasingly exacerbated as a result of climate change, necessitating advanced proactive measures for effective mitigation. It is important to forecast wildfires weeks and months in advance to plan forest fuel management, resource procurement and allocation. To achieve such accurate long-term forecasts at a global scale, it is crucial to employ models that account for the Earth system's inherent spatio-temporal interactions, such as memory effects and teleconnections. We propose a teleconnection-driven vision transformer (TeleViT), capable of treating the Earth as one interconnected system, integrating fine-grained local-scale inputs with global-scale inputs, such as climate indices and coarse-grained global variables. Through comprehensive experimentation, we demonstrate the superiority of TeleViT in accurately predicting global burned area patterns for various forecasting windows, up to four months in advance. The gain is especially pronounced in larger forecasting windows, demonstrating the improved ability of deep learning models that exploit teleconnections to capture Earth system dynamics. Code available at https://github.com/Orion-Ai-Lab/TeleViT.

count=1
* Semantic Segmentation of Crops and Weeds with Probabilistic Modeling and Uncertainty Quantification
    [[abs-CVF](https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Celikkan_Semantic_Segmentation_of_Crops_andWeeds_with_Probabilistic_Modeling_and_Uncertainty_ICCVW_2023_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/papers/Celikkan_Semantic_Segmentation_of_Crops_andWeeds_with_Probabilistic_Modeling_and_Uncertainty_ICCVW_2023_paper.pdf)]
    * Title: Semantic Segmentation of Crops and Weeds with Probabilistic Modeling and Uncertainty Quantification
    * Publisher: ICCV
    * Publication Date: `2023`
    * Authors: Ekin Celikkan, Mohammadmehdi Saberioon, Martin Herold, Nadja Klein
    * Abstract: We propose a Bayesian approach for semantic segmentation of crops and weeds. Farmers often manage weeds by applying herbicides to the entire field, which has negative environmental and financial impacts. Site-specific weed management (SSWM) considers the variability in the field and localizes the treatment. The prerequisite for automated SSWM is accurate detection of weeds. Moreover, to integrate a method into a real-world setting, the model should be able to make informed decisions to avoid potential mistakes and consequent losses. Existing methods are deterministic and they cannot go beyond assigning a class label to the unseen input based on the data they were trained with. The main idea of our approach is to quantify prediction uncertainty, while making class predictions. Our method achieves competitive performance in an established dataset for weed segmentation. Moreover, through accurate uncertainty quantification, our method is able to detect cases and areas which it is the most uncertain about. This information is beneficial, if not necessary, while making decisions with real-world implications to avoid unwanted consequences. In this work, we show that an end-to-end trainable Bayesian segmentation network can be successfully deployed for the weed segmentation task. In the future it could be integrated into real weeding systems to contribute to better informed decisions and more reliable automated systems.

count=1
* Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-Wise Regression
    [[abs-CVF](https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Baumann_Probabilistic_MIMO_U-Net_Efficient_and_Accurate_Uncertainty_Estimation_for_Pixel-Wise_ICCVW_2023_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/ICCV2023W/UnCV/papers/Baumann_Probabilistic_MIMO_U-Net_Efficient_and_Accurate_Uncertainty_Estimation_for_Pixel-Wise_ICCVW_2023_paper.pdf)]
    * Title: Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-Wise Regression
    * Publisher: ICCV
    * Publication Date: `2023`
    * Authors: Anton Baumann, Thomas Roßberg, Michael Schmitt
    * Abstract: Uncertainty estimation in machine learning is paramount for enhancing the reliability and interpretability of predictive models, especially in high-stakes real-world scenarios. Despite the availability of numerous methods, they often pose a trade-off between the quality of uncertainty estimation and computational efficiency. Addressing this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework - an approach exploiting the overparameterization of deep neural networks - for pixel-wise regression tasks. Our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. For that purpose, we adapted the U-Net architecture to train multiple subnetworks within a single model, harnessing the overparameterization in deep neural networks. Additionally, we introduce a novel procedure for synchronizing subnetwork performance within the MIMO framework. Our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and considerable improvements in parameter size and inference time.

count=1
* Multi-Task Hypergraphs for Semi-Supervised Learning Using Earth Observations
    [[abs-CVF](https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Pirvu_Multi-Task_Hypergraphs_for_Semi-Supervised_Learning_Using_Earth_Observations_ICCVW_2023_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/ICCV2023W/VCL/papers/Pirvu_Multi-Task_Hypergraphs_for_Semi-Supervised_Learning_Using_Earth_Observations_ICCVW_2023_paper.pdf)]
    * Title: Multi-Task Hypergraphs for Semi-Supervised Learning Using Earth Observations
    * Publisher: ICCV
    * Publication Date: `2023`
    * Authors: Mihai Pirvu, Alina Marcu, Maria Alexandra Dobrescu, Ahmed Nabil Belbachir, Marius Leordeanu
    * Abstract: There are many ways of interpreting the world and they are highly interdependent. We exploit such complex dependencies and introduce a powerful multi-task hypergraph, in which every node is a task and different paths through the hypergraph reaching a given task become unsupervised teachers, by forming ensembles that learn to generate reliable pseudolabels for that task. Each hyperedge is part of an ensemble teacher for a given task and it is also a student of the self-supervised hypergraph system. We apply our model to one of the most important problems of our times, that of Earth Observation, which is highly multi-task and it often suffers from missing ground-truth data. By performing extensive experiments on the NASA NEO Dataset, spanning a period of 22 years, we demonstrate the value of our multi-task semi-supervised approach, by consistent improvements over strong baselines and recent work. We also show that the hypergraph can adapt unsupervised to gradual data distribution shifts and reliably recover, through its multi-task self-supervision process, the missing data for several observational layers for up to seven years

count=1
* MarsLS-Net: Martian Landslides Segmentation Network and Benchmark Dataset
    [[abs-CVF](https://openaccess.thecvf.com/content/WACV2024/html/Paheding_MarsLS-Net_Martian_Landslides_Segmentation_Network_and_Benchmark_Dataset_WACV_2024_paper.html)]
    [[pdf-CVF](https://openaccess.thecvf.com/content/WACV2024/papers/Paheding_MarsLS-Net_Martian_Landslides_Segmentation_Network_and_Benchmark_Dataset_WACV_2024_paper.pdf)]
    * Title: MarsLS-Net: Martian Landslides Segmentation Network and Benchmark Dataset
    * Publisher: WACV
    * Publication Date: `2024`
    * Authors: Sidike Paheding, Abel A. Reyes, A. Rajaneesh, K.S. Sajinkumar, Thomas Oommen
    * Abstract: Martian landslide segmentation is a challenging task compared to the same task on Earth. One of the reasons is that vegetation is typically lost or significantly less compared to its surroundings in the regions of landslide on Earth. In contrast, Mars is a desert planet, and there is no vegetation to aid landslide detection and segmentation. Recent work has demonstrated the strength of vision transformer (ViT) based deep learning models for various computer vision tasks. Inspired by the multi-head attention mechanism in ViT, which can model the global long-range spatial correlation between local regions in the input image, we hypothesize self-attention mechanism can effectively capture pertinent contextual information for the Martian landslide segmentation task. Furthermore, considering parameter efficiency or model size is another important factor for deep learning algorithms, we construct a new feature representation block, namely Progressively Expanded Neuron Attention (PEN-Attention), to extract more relevant features with significantly fewer trainable parameters. Overall, we refer to our deep learning architecture as the Martian landslide segmentation network (MarsLS-Net). In addition to the new architecture, we introduce a new multi-modal Martian landslide segmentation dataset for the first time, which will be made publicly available at https://github.com/MAIN-Lab/Multimodal-Martian-Landslides-Dataset

count=1
* Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean
    [[abs-NIPS](https://papers.nips.cc/paper_files/paper/2023/hash/9ee3ed2dd656402f954ef9dc37e39f48-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NIPS](https://papers.nips.cc/paper_files/paper/2023/file/9ee3ed2dd656402f954ef9dc37e39f48-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean
    * Publisher: NeurIPS
    * Publication Date: `2023`
    * Authors: Spyridon Kondylatos, Ioannis Prapas, Gustau Camps-Valls, Ioannis Papoutsis
    * Abstract: We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire modeling in the Mediterranean. Mesogeos integrates variables representing wildfire drivers (meteorology, vegetation, human activity) and historical records of wildfire ignitions and burned areas for 17 years (2006-2022). It is designed as a cloud-friendly spatio-temporal dataset, namely a datacube, harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The datacube structure offers opportunities to assess machine learning (ML) usage in various wildfire modeling tasks. We extract two ML-ready datasets that establish distinct tracks to demonstrate this potential: (1) short-term wildfire danger forecasting and (2) final burned area estimation given the point of ignition. We define appropriate metrics and baselines to evaluate the performance of models in each track. By publishing the datacube, along with the code to create the ML datasets and models, we encourage the community to foster the implementation of additional tracks for mitigating the increasing threat of wildfires in the Mediterranean.

count=1
* WildfireSpreadTS: A dataset of multi-modal time series for wildfire spread prediction
    [[abs-NIPS](https://papers.nips.cc/paper_files/paper/2023/hash/ebd545176bdaa9cd5d45954947bd74b7-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NIPS](https://papers.nips.cc/paper_files/paper/2023/file/ebd545176bdaa9cd5d45954947bd74b7-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: WildfireSpreadTS: A dataset of multi-modal time series for wildfire spread prediction
    * Publisher: NeurIPS
    * Publication Date: `2023`
    * Authors: Sebastian Gerard, Yu Zhao, Josephine Sullivan
    * Abstract: We present a multi-temporal, multi-modal remote-sensing dataset for predicting how active wildfires will spread at a resolution of 24 hours. The dataset consists of 13607 images across 607 fire events in the United States from January 2018 to October 2021. For each fire event, the dataset contains a full time series of daily observations, containing detected active fires and variables related to fuel, topography and weather conditions. The dataset is challenging due to: a) its inputs being multi-temporal, b) the high number of 23 multi-modal input channels, c) highly imbalanced labels and d) noisy labels, due to smoke, clouds, and inaccuracies in the active fire detection. The underlying complexity of the physical processes adds to these challenges. Compared to existing public datasets in this area, WildfireSpreadTS allows for multi-temporal modeling of spreading wildfires, due to its time series structure. Furthermore, we provide additional input modalities and a high spatial resolution of 375m for the active fire maps. We publish this dataset to encourage further research on this important task with multi-temporal, noise-resistant or generative methods, uncertainty estimation or advanced optimization techniques that deal with the high-dimensional input space.

