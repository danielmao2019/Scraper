count=141
* Towards Convergence Rate Analysis of Random Forests for Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/6925f2a16026e36e4fc112f82dd79406-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/6925f2a16026e36e4fc112f82dd79406-Paper.pdf)]
    * Title: Towards Convergence Rate Analysis of Random Forests for Classification
    * Year: `2020`
    * Authors: Wei Gao, Zhi-Hua Zhou
    * Abstract: Random forests have been one of the successful ensemble algorithms in machine learning. The basic idea is to construct a large number of random trees individually and make prediction based on an average of their predictions. The great successes have attracted much attention on the consistency of random forests, mostly focusing on regression. This work takes one step towards convergence rates of random forests for classification. We present the first finite-sample rate O(n^{-1/(8d+2)}) on the convergence of pure random forests for classification, which can be improved to be of O(n^{-1/(3.87d+2)}) by considering the midpoint splitting mechanism. We introduce another variant of random forests, which follow Breiman's original random forests but with different mechanisms on splitting dimensions and positions. We get a convergence rate O(n^{-{1}/(d+2)}(\ln n)^{{1}/(d+2)}) for the variant of random forests, which reaches the minimax rate, except for a factor (\ln n)^{{1}/(d+2)}, of the optimal plug-in classifier under the L-Lipschitz assumption. We achieve tighter convergence rate O(\sqrt{\ln n/n}) under proper assumptions over structural data.
count=94
* On the Gini-impurity Preservation For Privacy Random Forests
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/8d6b1d775014eff18256abeb207202ad-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/8d6b1d775014eff18256abeb207202ad-Paper-Conference.pdf)]
    * Title: On the Gini-impurity Preservation For Privacy Random Forests
    * Year: `2023`
    * Authors: XinRan Xie, Man-Jie Yuan, Xuetong Bai, Wei Gao, Zhi-Hua Zhou
    * Abstract: Random forests have been one successful ensemble algorithms in machine learning. Various techniques have been utilized to preserve the privacy of random forests from anonymization, differential privacy, homomorphic encryption, etc., whereas it rarely takes into account some crucial ingredients of learning algorithm. This work presents a new encryption to preserve data's Gini impurity, which plays a crucial role during the construction of random forests. Our basic idea is to modify the structure of binary search tree to store several examples in each node, and encrypt data features by incorporating label and order information. Theoretically, we prove that our scheme preserves the minimum Gini impurity in ciphertexts without decrypting, and present the security guarantee for encryption. For random forests, we encrypt data features based on our Gini-impurity-preserving scheme, and take the homomorphic encryption scheme CKKS to encrypt data labels due to their importance and privacy. We conduct extensive experiments to show the effectiveness, efficiency and security of our proposed method.
count=79
* When do random forests fail?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/204da255aea2cd4a75ace6018fad6b4d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/204da255aea2cd4a75ace6018fad6b4d-Paper.pdf)]
    * Title: When do random forests fail?
    * Year: `2018`
    * Authors: Cheng Tang, Damien Garreau, Ulrike von Luxburg
    * Abstract: Random forests are learning algorithms that build large collections of random trees and make predictions by averaging the individual tree predictions. In this paper, we consider various tree constructions and examine how the choice of parameters affects the generalization error of the resulting random forests as the sample size goes to infinity. We show that subsampling of data points during the tree construction phase is important: Forests can become inconsistent with either no subsampling or too severe subsampling. As a consequence, even highly randomized trees can lead to inconsistent forests if no subsampling is used, which implies that some of the commonly used setups for random forests can be inconsistent. As a second consequence we can show that trees that have good performance in nearest-neighbor search can be a poor choice for random forests.
count=76
* Global Refinement of Random Forest
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Ren_Global_Refinement_of_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Ren_Global_Refinement_of_2015_CVPR_paper.pdf)]
    * Title: Global Refinement of Random Forest
    * Year: `2015`
    * Authors: Shaoqing Ren, Xudong Cao, Yichen Wei, Jian Sun
    * Abstract: Random forest is well known as one of the best learning methods. In spite of its great success, it also has certain drawbacks: the heuristic learning rule does not effectively minimize the global training loss; the model size is usually too large for many real applications. To address the issues, we propose two techniques, global refinement and global pruning, to improve a pre-trained random forest. The proposed global refinement jointly relearns the leaf nodes of all trees under a global objective function so that the complementary information between multiple trees is well exploited. In this way, the fitting power of the forest is significantly enhanced. The global pruning is developed to reduce the model size as well as the over-fitting risk. The refined model has better performance and smaller storage cost, as verified in extensive experiments.
count=60
* Depth is More Powerful than Width with Prediction Concatenation in Deep Forest
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/c017e92288b5056c578bb6b0b69d9e76-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/c017e92288b5056c578bb6b0b69d9e76-Paper-Conference.pdf)]
    * Title: Depth is More Powerful than Width with Prediction Concatenation in Deep Forest
    * Year: `2022`
    * Authors: Shen-Huan Lyu, Yi-Xiao He, Zhi-Hua Zhou
    * Abstract: Random Forest (RF) is an ensemble learning algorithm proposed by \citet{breiman2001random} that constructs a large number of randomized decision trees individually and aggregates their predictions by naive averaging. \citet{zhou2019deep} further propose Deep Forest (DF) algorithm with multi-layer feature transformation, which significantly outperforms random forest in various application fields. The prediction concatenation (PreConc) operation is crucial for the multi-layer feature transformation in deep forest, though little has been known about its theoretical property. In this paper, we analyze the influence of Preconc on the consistency of deep forest. Especially when the individual tree is inconsistent (as in practice, the individual tree is often set to be fully grown, i.e., there is only one sample at each leaf node), we find that the convergence rate of two-layer DF \textit{w.r.t.} the number of trees $M$ can reach $\mathcal{O}(1/M^2)$ under some mild conditions, while the convergence rate of RF is $\mathcal{O}(1/M)$. Therefore, with the help of PreConc, DF with deeper layer will be more powerful than the shallower layer. Experiments confirm theoretical advantages.
count=43
* Universal consistency and minimax rates for online Mondrian Forests
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/f80ff32e08a25270b5f252ce39522f72-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/f80ff32e08a25270b5f252ce39522f72-Paper.pdf)]
    * Title: Universal consistency and minimax rates for online Mondrian Forests
    * Year: `2017`
    * Authors: Jaouad Mourtada, Stéphane Gaïffas, Erwan Scornet
    * Abstract: We establish the consistency of an algorithm of Mondrian Forests~\cite{lakshminarayanan2014mondrianforests,lakshminarayanan2016mondrianuncertainty}, a randomized classification algorithm that can be implemented online. First, we amend the original Mondrian Forest algorithm proposed in~\cite{lakshminarayanan2014mondrianforests}, that considers a \emph{fixed} lifetime parameter. Indeed, the fact that this parameter is fixed actually hinders statistical consistency of the original procedure. Our modified Mondrian Forest algorithm grows trees with increasing lifetime parameters $\lambda_n$, and uses an alternative updating rule, allowing to work also in an online fashion. Second, we provide a theoretical analysis establishing simple conditions for consistency. Our theoretical analysis also exhibits a surprising fact: our algorithm achieves the minimax rate (optimal rate) for the estimation of a Lipschitz regression function, which is a strong extension of previous results~\cite{arlot2014purf_bias} to an \emph{arbitrary dimension}.
count=42
* Robust Visual Tracking Using Oblique Random Forests
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Robust_Visual_Tracking_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Robust_Visual_Tracking_CVPR_2017_paper.pdf)]
    * Title: Robust Visual Tracking Using Oblique Random Forests
    * Year: `2017`
    * Authors: Le Zhang, Jagannadan Varadarajan, Ponnuthurai Nagaratnam Suganthan, Narendra Ahuja, Pierre Moulin
    * Abstract: Random forest has emerged as a powerful classification technique with promising results in various vision tasks including image classification, pose estimation and object detection. However, current techniques have shown little improvements in visual tracking as they mostly rely on piece wise orthogonal hyperplanes to create decision nodes and lack a robust incremental learning mechanism that is much needed for online tracking. In this paper, we propose a discriminative tracker based on a novel incremental oblique random forest. Unlike conventional orthogonal decision trees that use a single feature and heuristic measures to obtain a split at each node, we propose to use a more powerful proximal SVM to obtain oblique hyperplanes to capture the geometric structure of the data better. The resulting decision surface is not restricted to be axis aligned and hence has the ability to represent and classify the input data better. Furthermore, in order to generalize to online tracking scenarios, we derive incremental update steps that enable the hyperplanes in each node to be updated recursively, efficiently and in a closed-form fashion. We demonstrate the effectiveness of our method using two large scale benchmark datasets (OTB-51 and OTB-100) and show that our method gives competitive results on several challenging cases by relying on simple HOG features as well as in combination with more sophisticated deep neural network based models. The implementations of the proposed random forest are available at https://github.com/ZhangLeUestc/ Incremental-Oblique-Random-Forest.
count=42
* Unsupervised Random Forest Manifold Alignment for Lipreading
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Pei_Unsupervised_Random_Forest_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Pei_Unsupervised_Random_Forest_2013_ICCV_paper.pdf)]
    * Title: Unsupervised Random Forest Manifold Alignment for Lipreading
    * Year: `2013`
    * Authors: Yuru Pei, Tae-Kyun Kim, Hongbin Zha
    * Abstract: Lipreading from visual channels remains a challenging topic considering the various speaking characteristics. In this paper, we address an efficient lipreading approach by investigating the unsupervised random forest manifold alignment (RFMA). The density random forest is employed to estimate affinity of patch trajectories in speaking facial videos. We propose novel criteria for node splitting to avoid the rank-deficiency in learning density forests. By virtue of the hierarchical structure of random forests, the trajectory affinities are measured efficiently, which are used to find embeddings of the speaking video clips by a graph-based algorithm. Lipreading is formulated as matching between manifolds of query and reference video clips. We employ the manifold alignment technique for matching, where the L ? norm-based manifold-to-manifold distance is proposed to find the matching pairs. We apply this random forest manifold alignment technique to various video data sets captured by consumer cameras. The experiments demonstrate that lipreading can be performed effectively, and outperform state-of-the-arts.
count=39
* Stability of Random Forests and Coverage of Random-Forest Prediction Intervals
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/6452474601429509f3035dc81c233226-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/6452474601429509f3035dc81c233226-Paper-Conference.pdf)]
    * Title: Stability of Random Forests and Coverage of Random-Forest Prediction Intervals
    * Year: `2023`
    * Authors: Yan Wang, Huaiqing Wu, Dan Nettleton
    * Abstract: We establish stability of random forests under the mild condition that the squared response ($Y^2$) does not have a heavy tail. In particular, our analysis holds for the practical version of random forests that is implemented in popular packages like \texttt{randomForest} in \texttt{R}. Empirical results show that stability may persist even beyond our assumption and hold for heavy-tailed $Y^2$. Using the stability property, we prove a non-asymptotic lower bound for the coverage probability of prediction intervals constructed from the out-of-bag error of random forests. With another mild condition that is typically satisfied when $Y$ is continuous, we also establish a complementary upper bound, which can be similarly established for the jackknife prediction interval constructed from an arbitrary stable algorithm. We also discuss the asymptotic coverage probability under assumptions weaker than those considered in previous literature. Our work implies that random forests, with its stability property, is an effective machine learning method that can provide not only satisfactory point prediction but also justified interval prediction at almost no extra computational cost.
count=34
* Fast and Accurate Image Upscaling With Super-Resolution Forests
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Schulter_Fast_and_Accurate_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Schulter_Fast_and_Accurate_2015_CVPR_paper.pdf)]
    * Title: Fast and Accurate Image Upscaling With Super-Resolution Forests
    * Year: `2015`
    * Authors: Samuel Schulter, Christian Leistner, Horst Bischof
    * Abstract: The aim of single image super-resolution is to reconstruct a high-resolution image from a single low-resolution input. Although the task is ill-posed it can be seen as finding a non-linear mapping from a low to high-dimensional space. Recent methods that rely on both neighborhood embedding and sparse-coding have led to tremendous quality improvements. Yet, many of the previous approaches are hard to apply in practice because they are either too slow or demand tedious parameter tweaks. In this paper, we propose to directly map from low to high-resolution patches using random forests. We show the close relation of previous work on single image super-resolution to locally linear regression and demonstrate how random forests nicely fit into this framework. During training the trees, we optimize a novel and effective regularized objective that not only operates on the output space but also on the input space, which especially suits the regression task. During inference, our method comprises the same well-known computational efficiency that has made random forests popular for many computer vision problems. In the experimental part, we demonstrate on standard benchmarks for single image super-resolution that our approach yields highly accurate state-of-the-art results, while being fast in both training and evaluation.
count=34
* Mondrian Forests: Efficient Online Random Forests
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/d1dc3a8270a6f9394f88847d7f0050cf-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/d1dc3a8270a6f9394f88847d7f0050cf-Paper.pdf)]
    * Title: Mondrian Forests: Efficient Online Random Forests
    * Year: `2014`
    * Authors: Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh
    * Abstract: Ensembles of randomized decision trees, usually referred to as random forests, are widely used for classification and regression tasks in machine learning and statistics. Random forests achieve competitive predictive performance and are computationally efficient to train and test, making them excellent candidates for real-world prediction tasks. The most popular random forest variants (such as Breiman's random forest and extremely randomized trees) operate on batches of training data. Online methods are now in greater demand. Existing online random forests, however, require more training data than their batch counterpart to achieve comparable predictive performance. In this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of random decision trees we call Mondrian forests. Mondrian forests can be grown in an incremental/online fashion and remarkably, the distribution of online Mondrian forests is the same as that of batch Mondrian forests. Mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically re-trained batch random forests, while being more than an order of magnitude faster, thus representing a better computation vs accuracy tradeoff.
count=32
* Modern Neural Networks Generalize on Small Data Sets
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/fface8385abbf94b4593a0ed53a0c70f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/fface8385abbf94b4593a0ed53a0c70f-Paper.pdf)]
    * Title: Modern Neural Networks Generalize on Small Data Sets
    * Year: `2018`
    * Authors: Matthew Olson, Abraham Wyner, Richard Berk
    * Abstract: In this paper, we use a linear program to empirically decompose fitted neural networks into ensembles of low-bias sub-networks. We show that these sub-networks are relatively uncorrelated which leads to an internal regularization process, very much like a random forest, which can explain why a neural network is surprisingly resistant to overfitting. We then demonstrate this in practice by applying large neural networks, with hundreds of parameters per training observation, to a collection of 116 real-world data sets from the UCI Machine Learning Repository. This collection of data sets contains a much smaller number of training examples than the types of image classification tasks generally studied in the deep learning literature, as well as non-trivial label noise. We show that even in this setting deep neural nets are capable of achieving superior classification accuracy without overfitting.
count=29
* Neural Architecture Search With Representation Mutual Information
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Zheng_Neural_Architecture_Search_With_Representation_Mutual_Information_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Neural_Architecture_Search_With_Representation_Mutual_Information_CVPR_2022_paper.pdf)]
    * Title: Neural Architecture Search With Representation Mutual Information
    * Year: `2022`
    * Authors: Xiawu Zheng, Xiang Fei, Lei Zhang, Chenglin Wu, Fei Chao, Jianzhuang Liu, Wei Zeng, Yonghong Tian, Rongrong Ji
    * Abstract: Performance evaluation strategy is one of the most important factors that determine the effectiveness and efficiency in Neural Architecture Search (NAS). Existing strategies, such as employing standard training or performance predictor, often suffer from high computational complexity and low generality. To address this issue, we propose to rank architectures by Representation Mutual Information (RMI). Specifically, given an arbitrary architecture that has decent accuracy, architectures that have high RMI with it always yield good accuracies. As an accurate performance indicator to facilitate NAS, RMI not only generalizes well to different search spaces, but is also efficient enough to evaluate architectures using only one batch of data. Building upon RMI, we further propose a new search algorithm termed RMI-NAS, facilitating with a theorem to guarantee the global optimal of the searched architecture. In particular, RMI-NAS first randomly samples architectures from the search space, which are then effectively classified as positive or negative samples by RMI. We then use these samples to train a random forest to explore new regions, while keeping track of the distribution of positive architectures. When the sample size is sufficient, the architecture with the largest probability from the aforementioned distribution is selected, which is theoretically proved to be the optimal solution. The architectures searched by our method achieve remarkable top-1 accuracies with the magnitude times faster search process. Besides, RMI-NAS also generalizes to different datasets and search spaces. Our code has been made available at https://git.openi.org.cn/PCL_AutoML/XNAS.
count=26
* Learning an Image-based Motion Context for Multiple People Tracking
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Leal-Taixe_Learning_an_Image-based_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Leal-Taixe_Learning_an_Image-based_2014_CVPR_paper.pdf)]
    * Title: Learning an Image-based Motion Context for Multiple People Tracking
    * Year: `2014`
    * Authors: Laura Leal-Taixe, Michele Fenzi, Alina Kuznetsova, Bodo Rosenhahn, Silvio Savarese
    * Abstract: We present a novel method for multiple people tracking that leverages a generalized model for capturing interactions among individuals. At the core of our model lies a learned dictionary of interaction feature strings which capture relationships between the motions of targets. These feature strings, created from low-level image features, lead to a much richer representation of the physical interactions between targets compared to hand-specified social force models that previous works have introduced for tracking. One disadvantage of using social forces is that all pedestrians must be detected in order for the forces to be applied, while our method is able to encode the effect of undetected targets, making the tracker more robust to partial occlusions. The interaction feature strings are used in a Random Forest framework to track targets according to the features surrounding them. Results on six publicly available sequences show that our method outperforms state-of-the-art approaches in multiple people tracking.
count=26
* Accurate Object Detection with Joint Classification-Regression Random Forests
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Schulter_Accurate_Object_Detection_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Schulter_Accurate_Object_Detection_2014_CVPR_paper.pdf)]
    * Title: Accurate Object Detection with Joint Classification-Regression Random Forests
    * Year: `2014`
    * Authors: Samuel Schulter, Christian Leistner, Paul Wohlhart, Peter M. Roth, Horst Bischof
    * Abstract: In this paper, we present a novel object detection approach that is capable of regressing the aspect ratio of objects. This results in accurately predicted bounding boxes having high overlap with the ground truth. In contrast to most recent works, we employ a Random Forest for learning a template-based model but exploit the nature of this learning algorithm to predict arbitrary output spaces. In this way, we can simultaneously predict the object probability of a window in a sliding window approach as well as regress its aspect ratio with a single model. Furthermore, we also exploit the additional information of the aspect ratio during the training of the Joint Classification-Regression Random Forest, resulting in better detection models. Our experiments demonstrate several benefits: (i) Our approach gives competitive results on standard detection benchmarks. (ii) The additional aspect ratio regression delivers more accurate bounding boxes than standard object detection approaches in terms of overlap with ground truth, especially when tightening the evaluation criterion. (iii) The detector itself becomes better by only including the aspect ratio information during training.
count=26
* Consistent Sufficient Explanations and Minimal Local Rules for explaining the decision of any classifier or regressor
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/34b13425b5ba8ad05d97b0043df52ed3-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/34b13425b5ba8ad05d97b0043df52ed3-Paper-Conference.pdf)]
    * Title: Consistent Sufficient Explanations and Minimal Local Rules for explaining the decision of any classifier or regressor
    * Year: `2022`
    * Authors: Salim I. Amoukou, Nicolas Brunel
    * Abstract: To explain the decision of any regression and classification model, we extend the notion of probabilistic sufficient explanations (P-SE). For each instance, this approach selects the minimal subset of features that is sufficient to yield the same prediction with high probability, while removing other features. The crux of P-SE is to compute the conditional probability of maintaining the same prediction. Therefore, we introduce an accurate and fast estimator of this probability via random Forests for any data $(\boldsymbol{X}, Y)$ and show its efficiency through a theoretical analysis of its consistency. As a consequence, we extend the P-SE to regression problems. In addition, we deal with non-discrete features, without learning the distribution of $\boldsymbol{X}$ nor having the model for making predictions. Finally, we introduce local rule-based explanations for regression/classification based on the P-SE and compare our approaches w.r.t other explainable AI methods. These methods are available as a Python Package.
count=25
* On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/a134eaebd55b7406ff29cd75d5f1a622-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/a134eaebd55b7406ff29cd75d5f1a622-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets
    * Year: `2023`
    * Authors: Jiashuo Liu, Tianyu Wang, Peng Cui, Hongseok Namkoong
    * Abstract: Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they \emph{implicitly} focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build ``WhyShift``, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we \emph{identify covariate regions} that suffer the biggest $Y|X$-shifts and discuss implications for algorithmic and data-based interventions. Our testbed highlights the importance of future research that builds an understanding of why distributions differ.
count=25
* Identifying Learning Rules From Neural Network Observables
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/1ba922ac006a8e5f2b123684c2f4d65f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/1ba922ac006a8e5f2b123684c2f4d65f-Paper.pdf)]
    * Title: Identifying Learning Rules From Neural Network Observables
    * Year: `2020`
    * Authors: Aran Nayebi, Sanjana Srivastava, Surya Ganguli, Daniel L. Yamins
    * Abstract: The brain modifies its synaptic strengths during learning in order to better adapt to its environment. However, the underlying plasticity rules that govern learning are unknown. Many proposals have been suggested, including Hebbian mechanisms, explicit error backpropagation, and a variety of alternatives. It is an open question as to what specific experimental measurements would need to be made to determine whether any given learning rule is operative in a real biological system. In this work, we take a "virtual experimental" approach to this problem. Simulating idealized neuroscience experiments with artificial neural networks, we generate a large-scale dataset of learning trajectories of aggregate statistics measured in a variety of neural network architectures, loss functions, learning rule hyperparameters, and parameter initializations. We then take a discriminative approach, training linear and simple non-linear classifiers to identify learning rules from features based on these observables. We show that different classes of learning rules can be separated solely on the basis of aggregate statistics of the weights, activations, or instantaneous layer-wise activity changes, and that these results generalize to limited access to the trajectory and held-out architectures and learning curricula. We identify the statistics of each observable that are most relevant for rule identification, finding that statistics from network activities across training are more robust to unit undersampling and measurement noise than those obtained from the synaptic strengths. Our results suggest that activation patterns, available from electrophysiological recordings of post-synaptic activities on the order of several hundred units, frequently measured at wider intervals over the course of learning, may provide a good basis on which to identify learning rules.
count=24
* A Refined Margin Distribution Analysis for Forest Representation Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/db5cea26ca37aa09e5365f3e7f5dd9eb-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/db5cea26ca37aa09e5365f3e7f5dd9eb-Paper.pdf)]
    * Title: A Refined Margin Distribution Analysis for Forest Representation Learning
    * Year: `2019`
    * Authors: Shen-Huan Lyu, Liang Yang, Zhi-Hua Zhou
    * Abstract: In this paper, we formulate the forest representation learning approach called \textsc{CasDF} as an additive model which boosts the augmented feature instead of the prediction. We substantially improve the upper bound of the generalization gap from $\mathcal{O}(\sqrt{\ln m/m})$ to $\mathcal{O}(\ln m/m)$, while the margin ratio of the margin standard deviation to the margin mean is sufficiently small. This tighter upper bound inspires us to optimize the ratio. Therefore, we design a margin distribution reweighting approach for deep forest to achieve a small margin ratio by boosting the augmented feature. Experiments confirm the correlation between the margin distribution and generalization performance. We remark that this study offers a novel understanding of \textsc{CasDF} from the perspective of the margin theory and further guides the layer-by-layer forest representation learning.
count=24
* Model Agnostic Supervised Local Explanations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/b495ce63ede0f4efc9eec62cb947c162-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf)]
    * Title: Model Agnostic Supervised Local Explanations
    * Year: `2018`
    * Authors: Gregory Plumb, Denali Molitor, Ameet S. Talwalkar
    * Abstract: Model interpretability is an increasingly important component of practical machine learning. Some of the most common forms of interpretability systems are example-based, local, and global explanations. One of the main challenges in interpretability is designing explanation systems that can capture aspects of each of these explanation types, in order to develop a more thorough understanding of the model. We address this challenge in a novel model called MAPLE that uses local linear modeling techniques along with a dual interpretation of random forests (both as a supervised neighborhood approach and as a feature selection method). MAPLE has two fundamental advantages over existing interpretability systems. First, while it is effective as a black-box explanation system, MAPLE itself is a highly accurate predictive model that provides faithful self explanations, and thus sidesteps the typical accuracy-interpretability trade-off. Specifically, we demonstrate, on several UCI datasets, that MAPLE is at least as accurate as random forests and that it produces more faithful local explanations than LIME, a popular interpretability system. Second, MAPLE provides both example-based and local explanations and can detect global patterns, which allows it to diagnose limitations in its local explanations.
count=23
* Learning Compact Binary Codes for Visual Tracking
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Li_Learning_Compact_Binary_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Li_Learning_Compact_Binary_2013_CVPR_paper.pdf)]
    * Title: Learning Compact Binary Codes for Visual Tracking
    * Year: `2013`
    * Authors: Xi Li, Chunhua Shen, Anthony Dick, Anton van den Hengel
    * Abstract: A key problem in visual tracking is to represent the appearance of an object in a way that is robust to visual changes. To attain this robustness, increasingly complex models are used to capture appearance variations. However, such models can be difficult to maintain accurately and efficiently. In this paper, we propose a visual tracker in which objects are represented by compact and discriminative binary codes. This representation can be processed very efficiently, and is capable of effectively fusing information from multiple cues. An incremental discriminative learner is then used to construct an appearance model that optimally separates the object from its surrounds. Furthermore, we design a hypergraph propagation method to capture the contextual information on samples, which further improves the tracking accuracy. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.
count=23
* Alternating Decision Forests
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Schulter_Alternating_Decision_Forests_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Schulter_Alternating_Decision_Forests_2013_CVPR_paper.pdf)]
    * Title: Alternating Decision Forests
    * Year: `2013`
    * Authors: Samuel Schulter, Paul Wohlhart, Christian Leistner, Amir Saffari, Peter M. Roth, Horst Bischof
    * Abstract: This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. During training, the losses are minimized via keeping an adaptive weight distribution over the training samples, similar to Boosting methods. In order to keep the method as flexible and general as possible, we adopt the principle of employing gradient descent in function space, which allows to minimize arbitrary losses. Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits of common Random Forests, such as, parallel processing. We derive the new classifier and give a discussion and evaluation on standard machine learning data sets. Furthermore, we show how ADFs can be easily integrated into an object detection application. Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth.
count=23
* Statistically Valid Variable Importance Assessment through Conditional Permutations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d60e14c19cd6e0fc38556ad29ac8fbc9-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d60e14c19cd6e0fc38556ad29ac8fbc9-Paper-Conference.pdf)]
    * Title: Statistically Valid Variable Importance Assessment through Conditional Permutations
    * Year: `2023`
    * Authors: Ahmad CHAMMA, Denis A. Engemann, Bertrand Thirion
    * Abstract: Variable importance assessment has become a crucial step in machine-learning applications when using complex learners, such as deep neural networks, on large-scale data. Removal-based importance assessment is currently the reference approach, particularly when statistical guarantees are sought to justify variable inclusion. It is often implemented with variable permutation schemes. On the flip side, these approaches risk misidentifying unimportant variables as important in the presence of correlations among covariates. Here we develop a systematic approach for studying Conditional Permutation Importance (CPI) that is model agnostic and computationally lean, as well as reusable benchmarks of state-of-the-art variable importance estimators. We show theoretically and empirically that \textit{CPI} overcomes the limitations of standard permutation importance by providing accurate type-I error control. When used with a deep neural network, \textit{CPI} consistently showed top accuracy across benchmarks. An experiment on real-world data analysis in a large-scale medical dataset showed that \textit{CPI} provides a more parsimonious selection of statistically significant variables. Our results suggest that \textit{CPI} can be readily used as drop-in replacement for permutation-based methods.
count=23
* Zero-shot recognition with unreliable attributes
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf)]
    * Title: Zero-shot recognition with unreliable attributes
    * Year: `2014`
    * Authors: Dinesh Jayaraman, Kristen Grauman
    * Abstract: In principle, zero-shot learning makes it possible to train an object recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like striped and four-legged, one can construct a classifier for the zebra category by enumerating which properties it possesses --- even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute’s error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.
count=22
* Incremental Learning of NCM Forests for Large-Scale Image Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Ristin_Incremental_Learning_of_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Ristin_Incremental_Learning_of_2014_CVPR_paper.pdf)]
    * Title: Incremental Learning of NCM Forests for Large-Scale Image Classification
    * Year: `2014`
    * Authors: Marko Ristin, Matthieu Guillaumin, Juergen Gall, Luc Van Gool
    * Abstract: In recent years, large image data sets such as "ImageNet", "TinyImages" or ever-growing social networks like "Flickr" have emerged, posing new challenges to image classification that were not apparent in smaller image sets. In particular, the efficient handling of dynamically growing data sets, where not only the amount of training images, but also the number of classes increases over time, is a relatively unexplored problem. To remedy this, we introduce Nearest Class Mean Forests (NCMF), a variant of Random Forests where the decision nodes are based on nearest class mean (NCM) classification. NCMFs not only outperform conventional random forests, but are also well suited for integrating new classes. To this end, we propose and compare several approaches to incorporate data from new classes, so as to seamlessly extend the previously trained forest instead of re-training them from scratch. In our experiments, we show that NCMFs trained on small data sets with 10 classes can be extended to large data sets with 1000 classes without significant loss of accuracy compared to training from scratch on the full data.
count=22
* MABSplit: Faster Forest Training Using Multi-Armed Bandits
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/08857467641ad82f635023d530605b4c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/08857467641ad82f635023d530605b4c-Paper-Conference.pdf)]
    * Title: MABSplit: Faster Forest Training Using Multi-Armed Bandits
    * Year: `2022`
    * Authors: Mo Tiwari, Ryan Kang, Jaeyong Lee, Chris Piech, Ilan Shomorony, Sebastian Thrun, Martin J. Zhang
    * Abstract: Random forests are some of the most widely used machine learning models today, especially in domains that necessitate interpretability. We present an algorithm that accelerates the training of random forests and other popular tree-based learning methods. At the core of our algorithm is a novel node-splitting subroutine, dubbed MABSplit, used to efficiently find split points when constructing decision trees. Our algorithm borrows techniques from the multi-armed bandit literature to judiciously determine how to allocate samples and computational power across candidate split points. We provide theoretical guarantees that MABSplit improves the sample complexity of each node split from linear to logarithmic in the number of data points. In some settings, MABSplit leads to 100x faster training (an 99% reduction in training time) without any decrease in generalization performance. We demonstrate similar speedups when MABSplit is used across a variety of forest-based variants, such as Extremely Random Forests and Random Patches. We also show our algorithm can be used in both classification and regression tasks. Finally, we show that MABSplit outperforms existing methods in generalization performance and feature importance calculations under a fixed computational budget. All of our experimental results are reproducible via a one-line script at https://github.com/ThrunGroup/FastForest.
count=22
* Fast and Flexible Monotonic Functions with Ensembles of Lattices
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/c913303f392ffc643f7240b180602652-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/c913303f392ffc643f7240b180602652-Paper.pdf)]
    * Title: Fast and Flexible Monotonic Functions with Ensembles of Lattices
    * Year: `2016`
    * Authors: Mahdi Milani Fard, Kevin Canini, Andrew Cotter, Jan Pfeifer, Maya Gupta
    * Abstract: For many machine learning problems, there are some inputs that are known to be positively (or negatively) related to the output, and in such cases training the model to respect that monotonic relationship can provide regularization, and makes the model more interpretable. However, flexible monotonic functions are computationally challenging to learn beyond a few features. We break through this barrier by learning ensembles of monotonic calibrated interpolated look-up tables (lattices). A key contribution is an automated algorithm for selecting feature subsets for the ensemble base models. We demonstrate that compared to random forests, these ensembles produce similar or better accuracy, while providing guaranteed monotonicity consistent with prior knowledge, smaller model size and faster evaluation.
count=21
* Learning to Group Objects
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Yanulevskaya_Learning_to_Group_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Yanulevskaya_Learning_to_Group_2014_CVPR_paper.pdf)]
    * Title: Learning to Group Objects
    * Year: `2014`
    * Authors: Victoria Yanulevskaya, Jasper Uijlings, Nicu Sebe
    * Abstract: This paper presents a novel method to generate a hypothesis set of class-independent object regions. It has been shown that such object regions can be used to focus computer vision techniques on the parts of an image that matter most leading to significant improvements in both object localisation and semantic segmentation in recent years. Of course, the higher quality of class-independent object regions, the better subsequent computer vision algorithms can perform. In this paper we focus on generating higher quality object hypotheses. We start from an oversegmentation for which we propose to extract a wide variety of region-features. We group regions together in a hierarchical fashion, for which we train a Random Forest which predicts at each stage of the hierarchy the best possible merge. Hence unlike other approaches, we use relatively powerful features and classifiers at an early stage of the generation of likely object regions. Finally, we identify and combine stable regions in order to capture objects which consist of dissimilar parts. We show on the PASCAL 2007 and 2012 datasets that our method yields higher quality regions than competing approaches while it is at the same time more computationally efficient.
count=21
* Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Bergamo_Leveraging_Structure_from_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Bergamo_Leveraging_Structure_from_2013_CVPR_paper.pdf)]
    * Title: Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification
    * Year: `2013`
    * Authors: Alessandro Bergamo, Sudipta N. Sinha, Lorenzo Torresani
    * Abstract: In this paper we propose a new technique for learning a discriminative codebook for local feature descriptors, specifically designed for scalable landmark classification. The key contribution lies in exploiting the knowledge of correspondences within sets of feature descriptors during codebook learning. Feature correspondences are obtained using structure from motion (SfM) computation on Internet photo collections which serve as the training data. Our codebook is defined by a random forest that is trained to map corresponding feature descriptors into identical codes. Unlike prior forest-based codebook learning methods, we utilize fine-grained descriptor labels and address the challenge of training a forest with an extremely large number of labels. Our codebook is used with various existing feature encoding schemes and also a variant we propose for importanceweighted aggregation of local features. We evaluate our approach on a public dataset of 25 landmarks and our new dataset of 620 landmarks (614K images). Our approach significantly outperforms the state of the art in landmark classification. Furthermore, our method is memory efficient and scalable.
count=21
* Alternating Regression Forests for Object Detection and Pose Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Schulter_Alternating_Regression_Forests_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Schulter_Alternating_Regression_Forests_2013_ICCV_paper.pdf)]
    * Title: Alternating Regression Forests for Object Detection and Pose Estimation
    * Year: `2013`
    * Authors: Samuel Schulter, Christian Leistner, Paul Wohlhart, Peter M. Roth, Horst Bischof
    * Abstract: We present Alternating Regression Forests (ARFs), a novel regression algorithm that learns a Random Forest by optimizing a global loss function over all trees. This interrelates the information of single trees during the training phase and results in more accurate predictions. ARFs can minimize any differentiable regression loss without sacrificing the appealing properties of Random Forests, like low computational complexity during both, training and testing. Inspired by recent developments for classification [19], we derive a new algorithm capable of dealing with different regression loss functions, discuss its properties and investigate the relations to other methods like Boosted Trees. We evaluate ARFs on standard machine learning benchmarks, where we observe better generalization power compared to both standard Random Forests and Boosted Trees. Moreover, we apply the proposed regressor to two computer vision applications: object detection and head pose estimation from depth images. ARFs outperform the Random Forest baselines in both tasks, illustrating the importance of optimizing a common loss function for all trees.
count=21
* Conformalized Quantile Regression
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/5103c3584b063c431bd1268e9b5e76fb-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf)]
    * Title: Conformalized Quantile Regression
    * Year: `2019`
    * Authors: Yaniv Romano, Evan Patterson, Emmanuel Candes
    * Abstract: Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.
count=21
* Understanding variable importances in forests of randomized trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/hash/e3796ae838835da0b6f6ea37bcf8bcb7-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/file/e3796ae838835da0b6f6ea37bcf8bcb7-Paper.pdf)]
    * Title: Understanding variable importances in forests of randomized trees
    * Year: `2013`
    * Authors: Gilles Louppe, Louis Wehenkel, Antonio Sutera, Pierre Geurts
    * Abstract: Despite growing interest and practical use in various scientific areas, variable importances derived from tree-based ensemble methods are not well understood from a theoretical point of view. In this work we characterize the Mean Decrease Impurity (MDI) variable importances as measured by an ensemble of totally randomized trees in asymptotic sample and ensemble size conditions. We derive a three-level decomposition of the information jointly provided by all input variables about the output in terms of i) the MDI importance of each input variable, ii) the degree of interaction of a given input variable with the other input variables, iii) the different interaction terms of a given degree. We then show that this MDI importance of a variable is equal to zero if and only if the variable is irrelevant and that the MDI importance of a relevant variable is invariant with respect to the removal or the addition of irrelevant variables. We illustrate these properties on a simple example and discuss how they may change in the case of non-totally randomized trees such as Random Forests and Extra-Trees.
count=20
* 3D Model-Based Continuous Emotion Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Chen_3D_Model-Based_Continuous_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Chen_3D_Model-Based_Continuous_2015_CVPR_paper.pdf)]
    * Title: 3D Model-Based Continuous Emotion Recognition
    * Year: `2015`
    * Authors: Hui Chen, Jiangdong Li, Fengjun Zhang, Yang Li, Hongan Wang
    * Abstract: We propose a real-time 3D model-based method that continuously recognizes dimensional emotions from facial expressions in natural communications. In our method, 3D facial models are restored from 2D images, which provide crucial clues for the enhancement of robustness to overcome large changes including out-of-plane head rotations, fast head motions and partial facial occlusions. To accurately recognize the emotion, a novel random forest-based algorithm which simultaneously integrates two regressions for 3D facial tracking and continuous emotion estimation is constructed. Moreover, via the reconstructed 3D facial model, temporal information and user-independent emotion presentations are also taken into account through our image fusion process. The experimental results show that our algorithm can achieve state-of-the-art result with higher Pearson's correlation coefficient of continuous emotion recognition in real time.
count=20
* Learning to Detect Motion Boundaries
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Weinzaepfel_Learning_to_Detect_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Weinzaepfel_Learning_to_Detect_2015_CVPR_paper.pdf)]
    * Title: Learning to Detect Motion Boundaries
    * Year: `2015`
    * Authors: Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, Cordelia Schmid
    * Abstract: We propose a learning-based approach for motion boundary detection. Precise localization of motion boundaries is essential for the success of optical flow estimation, as motion boundaries correspond to discontinuities of the optical flow field. The proposed approach allows to predict motion boundaries, using a structured random forest trained on the ground-truth of the MPI-Sintel dataset. The random forest leverages several cues at the patch level, namely appearance (RGB color) and motion cues (optical flow estimated by state-of-the-art algorithms). Experimental results show that the proposed approach is both robust and computationally efficient. It significantly outperforms state-of-the-art motion-difference approaches on the MPI-Sintel and Middlebury datasets. We compare the results obtained with several state-of-the-art optical flow approaches and study the impact of the different cues used in the random forest.Furthermore, we introduce a new dataset, the YouTube Motion Boundaries dataset (YMB), that comprises 60 sequences taken from real-world videos with manually annotated motion boundaries. On this dataset, our approach, although trained on MPI-Sintel, also outperforms by a large margin state-of-the-art optical flow algorithms.
count=20
* Dense Non-Rigid Shape Correspondence using Random Forests
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Rodola_Dense_Non-Rigid_Shape_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Rodola_Dense_Non-Rigid_Shape_2014_CVPR_paper.pdf)]
    * Title: Dense Non-Rigid Shape Correspondence using Random Forests
    * Year: `2014`
    * Authors: Emanuele Rodola, Samuel Rota Bulo, Thomas Windheuser, Matthias Vestner, Daniel Cremers
    * Abstract: We propose a shape matching method that produces dense correspondences tuned to a specific class of shapes and deformations. In a scenario where this class is represented by a small set of example shapes, the proposed method learns a shape descriptor capturing the variability of the deformations in the given class. The approach enables the wave kernel signature to extend the class of recognized deformations from near isometries to the deformations appearing in the example set by means of a random forest classifier. With the help of the introduced spatial regularization, the proposed method achieves significant improvements over the baseline approach and obtains state-of-the-art results while keeping short computation times.
count=19
* Speed Invariant Time Surface for Learning to Detect Corner Points With Event-Based Cameras
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Manderscheid_Speed_Invariant_Time_Surface_for_Learning_to_Detect_Corner_Points_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Manderscheid_Speed_Invariant_Time_Surface_for_Learning_to_Detect_Corner_Points_CVPR_2019_paper.pdf)]
    * Title: Speed Invariant Time Surface for Learning to Detect Corner Points With Event-Based Cameras
    * Year: `2019`
    * Authors: Jacques Manderscheid,  Amos Sironi,  Nicolas Bourdis,  Davide Migliore,  Vincent Lepetit
    * Abstract: We propose a learning approach to corner detection for event-based cameras that is stable even under fast and abrupt motions. Event-based cameras offer high temporal resolution, power efficiency, and high dynamic range. However, the properties of event-based data are very different compared to standard intensity images, and simple extensions of corner detection methods designed for these images do not perform well on event-based data. We first introduce an efficient way to compute a time surface that is invariant to the speed of the objects. We then show that we can train a Random Forest to recognize events generated by a moving corner from our time surface. Random Forests are also extremely efficient, and therefore a good choice to deal with the high capture frequency of event-based cameras ---our implementation processes up to 1.6Mev/s on a single CPU. Thanks to our time surface formulation and this learning approach, our method is significantly more robust to abrupt changes of direction of the corners compared to previous ones. Our method also naturally assigns a confidence score for the corners, which can be useful for postprocessing. Moreover, we introduce a high-resolution dataset suitable for quantitative evaluation and comparison of corner detection methods for event-based cameras. We call our approach SILC, for Speed Invariant Learned Corners, and compare it to the state-of-the-art with extensive experiments, showing better performance.
count=19
* Unified Face Analysis by Iterative Multi-Output Random Forests
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Zhao_Unified_Face_Analysis_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Zhao_Unified_Face_Analysis_2014_CVPR_paper.pdf)]
    * Title: Unified Face Analysis by Iterative Multi-Output Random Forests
    * Year: `2014`
    * Authors: Xiaowei Zhao, Tae-Kyun Kim, Wenhan Luo
    * Abstract: In this paper, we present a unified method for joint face image analysis, i.e., simultaneously estimating head pose, facial expression and landmark positions in real-world face images. To achieve this goal, we propose a novel iterative Multi-Output Random Forests (iMORF) algorithm, which explicitly models the relations among multiple tasks and iteratively exploits such relations to boost the performance of all tasks. Specifically, a hierarchical face analysis forest is learned to perform classification of pose and expression at the top level, while performing landmark positions regression at the bottom level. On one hand, the estimated pose and expression provide strong shape prior to constrain the variation of landmark positions. On the other hand, more discriminative shape-related features could be extracted from the estimated landmark positions to further improve the predictions of pose and expression. This relatedness of face analysis tasks is iteratively exploited through several cascaded hierarchical face analysis forests until convergence. Experiments conducted on publicly available real-world face datasets demonstrate that the performance of all individual tasks are significantly improved by the proposed iMORF algorithm. In addition, our method outperforms state-of-the-arts for all three face analysis tasks.
count=19
* Correlation Filters for Object Alignment
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Boddeti_Correlation_Filters_for_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Boddeti_Correlation_Filters_for_2013_CVPR_paper.pdf)]
    * Title: Correlation Filters for Object Alignment
    * Year: `2013`
    * Authors: Vishnu Naresh Boddeti, Takeo Kanade, B.V.K. Vijaya Kumar
    * Abstract: Alignment of 3D objects from 2D images is one of the most important and well studied problems in computer vision. A typical object alignment system consists of a landmark appearance model which is used to obtain an initial shape and a shape model which refines this initial shape by correcting the initialization errors. Since errors in landmark initialization from the appearance model propagate through the shape model, it is critical to have a robust landmark appearance model. While there has been much progress in designing sophisticated and robust shape models, there has been relatively less progress in designing robust landmark detection models. In this paper we present an efficient and robust landmark detection model which is designed specifically to minimize localization errors thereby leading to state-of-the-art object alignment performance. We demonstrate the efficacy and speed of the proposed approach on the challenging task of multi-view car alignment.
count=19
* Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Seyedhosseini_Image_Segmentation_with_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Seyedhosseini_Image_Segmentation_with_2013_ICCV_paper.pdf)]
    * Title: Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks
    * Year: `2013`
    * Authors: Mojtaba Seyedhosseini, Mehdi Sajjadi, Tolga Tasdizen
    * Abstract: Contextual information plays an important role in solving vision problems such as image segmentation. However, extracting contextual information and using it in an effective way remains a difficult problem. To address this challenge, we propose a multi-resolution contextual framework, called cascaded hierarchical model (CHM), which learns contextual information in a hierarchical framework for image segmentation. At each level of the hierarchy, a classifier is trained based on downsampled input images and outputs of previous levels. Our model then incorporates the resulting multi-resolution contextual information into a classifier to segment the input image at original resolution. We repeat this procedure by cascading the hierarchical framework to improve the segmentation accuracy. Multiple classifiers are learned in the CHM; therefore, a fast and accurate classifier is required to make the training tractable. The classifier also needs to be robust against overfitting due to the large number of parameters learned during training. We introduce a novel classification scheme, called logistic disjunctive normal networks (LDNN), which consists of one adaptive layer of feature detectors implemented by logistic sigmoid functions followed by two fixed layers of logical units that compute conjunctions and disjunctions, respectively. We demonstrate that LDNN outperforms state-of-theart classifiers and can be used in the CHM to improve object segmentation performance.
count=19
* Positive-Unlabeled Learning using Random Forests via Recursive Greedy Risk Minimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/98257285340854262185500e59bc0f28-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/98257285340854262185500e59bc0f28-Paper-Conference.pdf)]
    * Title: Positive-Unlabeled Learning using Random Forests via Recursive Greedy Risk Minimization
    * Year: `2022`
    * Authors: Jonathan Wilton, Abigail Koay, Ryan Ko, Miao Xu, Nan Ye
    * Abstract: The need to learn from positive and unlabeled data, or PU learning, arises in many applications and has attracted increasing interest. While random forests are known to perform well on many tasks with positive and negative data, recent PU algorithms are generally based on deep neural networks, and the potential of tree-based PU learning is under-explored. In this paper, we propose new random forest algorithms for PU-learning. Key to our approach is a new interpretation of decision tree algorithms for positive and negative data as \emph{recursive greedy risk minimization algorithms}. We extend this perspective to the PU setting to develop new decision tree learning algorithms that directly minimizes PU-data based estimators for the expected risk. This allows us to develop an efficient PU random forest algorithm, PU extra trees. Our approach features three desirable properties: it is robust to the choice of the loss function in the sense that various loss functions lead to the same decision trees; it requires little hyperparameter tuning as compared to neural network based PU learning; it supports a feature importance that directly measures a feature's contribution to risk minimization. Our algorithms demonstrate strong performance on several datasets. Our code is available at \url{https://github.com/puetpaper/PUExtraTrees}.
count=19
* A Debiased MDI Feature Importance Measure for Random Forests
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/702cafa3bb4c9c86e4a3b6834b45aedd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/702cafa3bb4c9c86e4a3b6834b45aedd-Paper.pdf)]
    * Title: A Debiased MDI Feature Importance Measure for Random Forests
    * Year: `2019`
    * Authors: Xiao Li, Yu Wang, Sumanta Basu, Karl Kumbier, Bin Yu
    * Abstract: Tree ensembles such as Random Forests have achieved impressive empirical success across a wide variety of applications. To understand how these models make predictions, people routinely turn to feature importance measures calculated from tree ensembles. It has long been known that Mean Decrease Impurity (MDI), one of the most widely used measures of feature importance, incorrectly assigns high importance to noisy features, leading to systematic bias in feature selection. In this paper, we address the feature selection bias of MDI from both theoretical and methodological perspectives. Based on the original definition of MDI by Breiman et al. \cite{Breiman1984} for a single tree, we derive a tight non-asymptotic bound on the expected bias of MDI importance of noisy features, showing that deep trees have higher (expected) feature selection bias than shallow ones. However, it is not clear how to reduce the bias of MDI using its existing analytical expression. We derive a new analytical expression for MDI, and based on this new expression, we are able to propose a debiased MDI feature importance measure using out-of-bag samples, called MDI-oob. For both the simulated data and a genomic ChIP dataset, MDI-oob achieves state-of-the-art performance in feature selection from Random Forests for both deep and shallow trees.
count=19
* Scalable Semi-Supervised Aggregation of Classifiers
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/ce78d1da254c0843eb23951ae077ff5f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/ce78d1da254c0843eb23951ae077ff5f-Paper.pdf)]
    * Title: Scalable Semi-Supervised Aggregation of Classifiers
    * Year: `2015`
    * Authors: Akshay Balsubramani, Yoav Freund
    * Abstract: We present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers. The algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements. It does this without making assumptions on the structure or origin of the ensemble, without parameters, and as scalably as linear learning. We empirically demonstrate these performance gains with random forests.
count=18
* Quantitative Evaluation of Confidence Measures in a Machine Learning World
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Poggi_Quantitative_Evaluation_of_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Poggi_Quantitative_Evaluation_of_ICCV_2017_paper.pdf)]
    * Title: Quantitative Evaluation of Confidence Measures in a Machine Learning World
    * Year: `2017`
    * Authors: Matteo Poggi, Fabio Tosi, Stefano Mattoccia
    * Abstract: Confidence measures aim at detecting unreliable depth measurements and play an important role for many purposes and in particular, as recently shown, to improve stereo accuracy. This topic has been thoroughly investigated by Hu and Mordohai in 2010 (and 2012) considering 17 confidence measures and two local algorithms on the two datasets available at that time. However, since then major breakthroughs happened in this field: the availability of much larger and challenging datasets, novel and more effective stereo algorithms including ones based on deep-learning and confidence measures leveraging on machine learning techniques. Therefore, this paper aims at providing an exhaustive and updated review and quantitative evaluation of 52 (actually, 76 considering variants) state-of-the-art confidence measures - focusing on recent ones mostly based on random-forests and deep-learning - with three algorithms on the challenging datasets available today. Moreover we deal with problems inherently induced by learning-based confidence measures. How are these methods able to generalize to new data? How a specific training improves their effectiveness? How more effective confidence measures can actually improve the overall stereo accuracy?
count=18
* Consistent Interpolating Ensembles via the Manifold-Hilbert Kernel
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/16371a9d5fed65d6d78ca3a7fa6e598c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/16371a9d5fed65d6d78ca3a7fa6e598c-Paper-Conference.pdf)]
    * Title: Consistent Interpolating Ensembles via the Manifold-Hilbert Kernel
    * Year: `2022`
    * Authors: Yutong Wang, Clay Scott
    * Abstract: Recent research in the theory of overparametrized learning has sought to establish generalization guarantees in the interpolating regime. Such results have been established for a few common classes of methods, but so far not for ensemble methods. We devise an ensemble classification method that simultaneously interpolates the training data, and is consistent for a broad class of data distributions. To this end, we define the manifold-Hilbert kernel for data distributed on a Riemannian manifold. We prove that kernel smoothing regression using the manifold-Hilbert kernel is weakly consistent in the setting of Devroye et al. 1998. For the sphere, we show that the manifold-Hilbert kernel can be realized as a weighted random partition kernel, which arises as an infinite ensemble of partition-based classifiers.
count=18
* From global to local MDI variable importances for random forests and when they are Shapley values
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/1cfa81af29c6f2d8cacb44921722e753-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/1cfa81af29c6f2d8cacb44921722e753-Paper.pdf)]
    * Title: From global to local MDI variable importances for random forests and when they are Shapley values
    * Year: `2021`
    * Authors: Antonio Sutera, Gilles Louppe, Van Anh Huynh-Thu, Louis Wehenkel, Pierre Geurts
    * Abstract: Random forests have been widely used for their ability to provide so-called importance measures, which give insight at a global (per dataset) level on the relevance of input variables to predict a certain output. On the other hand, methods based on Shapley values have been introduced to refine the analysis of feature relevance in tree-based models to a local (per instance) level. In this context, we first show that the global Mean Decrease of Impurity (MDI) variable importance scores correspond to Shapley values under some conditions. Then, we derive a local MDI importance measure of variable relevance, which has a very natural connection with the global MDI measure and can be related to a new notion of local feature relevance. We further link local MDI importances with Shapley values and discuss them in the light of related measures from the literature. The measures are illustrated through experiments on several classification and regression problems.
count=18
* Model Class Reliance for Random Forests
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/fd512441a1a791770a6fa573d688bff5-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/fd512441a1a791770a6fa573d688bff5-Paper.pdf)]
    * Title: Model Class Reliance for Random Forests
    * Year: `2020`
    * Authors: Gavin Smith, Roberto Mansilla, James Goulding
    * Abstract: Variable Importance (VI) has traditionally been cast as the process of estimating each variables contribution to a predictive model's overall performance. Analysis of a single model instance, however, guarantees no insight into a variables relevance to underlying generative processes. Recent research has sought to address this concern via analysis of Rashomon sets - sets of alternative model instances that exhibit equivalent predictive performance to some reference model, but which take different functional forms. Measures such as Model Class Reliance (MCR) have been proposed, that are computed against Rashomon sets, in order to ascertain how much a variable must be relied on to make robust predictions, or whether alternatives exist. If MCR range is tight, we have no choice but to use a variable; if range is high then there exists competing, perhaps fairer models, that provide alternative explanations of the phenomena being examined. Applications are wide, from enabling construction of `fairer' models in areas such as recidivism, health analytics and ethical marketing. Tractable estimation of MCR for non-linear models is currently restricted to Kernel Regression under squared loss \cite{fisher2019all}. In this paper we introduce a new technique that extends computation of Model Class Reliance (MCR) to Random Forest classifiers and regressors. The proposed approach addresses a number of open research questions, and in contrast to prior Kernel SVM MCR estimation, runs in linearithmic rather than polynomial time. Taking a fundamentally different approach to previous work, we provide a solution for this important model class, identifying situations where irrelevant covariates do not improve predictions.
count=18
* Random Tessellation Forests
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/043c2ec6c6390dd0ac5519190a57c88c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/043c2ec6c6390dd0ac5519190a57c88c-Paper.pdf)]
    * Title: Random Tessellation Forests
    * Year: `2019`
    * Authors: Shufei Ge, Shijia Wang, Yee Whye Teh, Liangliang Wang, Lloyd Elliott
    * Abstract: Space partitioning methods such as random forests and the Mondrian process are powerful machine learning methods for multi-dimensional and relational data, and are based on recursively cutting a domain. The flexibility of these methods is often limited by the requirement that the cuts be axis aligned. The Ostomachion process and the self-consistent binary space partitioning-tree process were recently introduced as generalizations of the Mondrian process for space partitioning with non-axis aligned cuts in the plane. Motivated by the need for a multi-dimensional partitioning tree with non-axis aligned cuts, we propose the Random Tessellation Process, a framework that includes the Mondrian process as a special case. We derive a sequential Monte Carlo algorithm for inference, and provide random forest methods. Our methods are self-consistent and can relax axis-aligned constraints, allowing complex inter-dimensional dependence to be captured. We present a simulation study and analyze gene expression data of brain tissue, showing improved accuracies over other methods.
count=17
* Human Pose Estimation Using Body Parts Dependent Joint Regressors
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Dantone_Human_Pose_Estimation_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Dantone_Human_Pose_Estimation_2013_CVPR_paper.pdf)]
    * Title: Human Pose Estimation Using Body Parts Dependent Joint Regressors
    * Year: `2013`
    * Authors: Matthias Dantone, Juergen Gall, Christian Leistner, Luc Van Gool
    * Abstract: In this work, we address the problem of estimating 2d human pose from still images. Recent methods that rely on discriminatively trained deformable parts organized in a tree model have shown to be very successful in solving this task. Within such a pictorial structure framework, we address the problem of obtaining good part templates by proposing novel, non-linear joint regressors. In particular, we employ two-layered random forests as joint regressors. The first layer acts as a discriminative, independent body part classifier. The second layer takes the estimated class distributions of the first one into account and is thereby able to predict joint locations by modeling the interdependence and co-occurrence of the parts. This results in a pose estimation framework that takes dependencies between body parts already for joint localization into account and is thus able to circumvent typical ambiguities of tree structures, such as for legs and arms. In the experiments, we demonstrate that our body parts dependent joint regressors achieve a higher joint localization accuracy than tree-based state-of-the-art methods.
count=17
* Smooth And Consistent Probabilistic Regression Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/8289889263db4a40463e3f358bb7c7a1-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/8289889263db4a40463e3f358bb7c7a1-Paper.pdf)]
    * Title: Smooth And Consistent Probabilistic Regression Trees
    * Year: `2020`
    * Authors: Sami Alkhoury, Emilie Devijver, Marianne Clausel, Myriam Tami, Eric Gaussier, georges Oppenheim
    * Abstract: We propose here a generalization of regression trees, referred to as Probabilistic Regression (PR) trees, that adapt to the smoothness of the prediction function relating input and output variables while preserving the interpretability of the prediction and being robust to noise. In PR trees, an observation is associated to all regions of a tree through a probability distribution that reflects how far the observation is to a region. We show that such trees are consistent, meaning that their error tends to 0 when the sample size tends to infinity, a property that has not been established for similar, previous proposals as Soft trees and Smooth Transition Regression trees. We further explain how PR trees can be used in different ensemble methods, namely Random Forests and Gradient Boosted Trees. Lastly, we assess their performance through extensive experiments that illustrate their benefits in terms of performance, interpretability and robustness to noise.
count=16
* Discriminative Feature-to-Point Matching in Image-Based Localization
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Donoser_Discriminative_Feature-to-Point_Matching_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Donoser_Discriminative_Feature-to-Point_Matching_2014_CVPR_paper.pdf)]
    * Title: Discriminative Feature-to-Point Matching in Image-Based Localization
    * Year: `2014`
    * Authors: Michael Donoser, Dieter Schmalstieg
    * Abstract: The prevalent approach to image-based localization is matching interest points detected in the query image to a sparse 3D point cloud representing the known world. The obtained correspondences are then used to recover a precise camera pose. The state-of-the-art in this field often ignores the availability of a set of 2D descriptors per 3D point, for example by representing each 3D point by only its centroid. In this paper we demonstrate that these sets contain useful information that can be exploited by formulating matching as a discriminative classification problem. Since memory demands and computational complexity are crucial in such a setup, we base our algorithm on the efficient and effective random fern principle. We propose an extension which projects features to fern-specific embedding spaces, which yields improved matching rates in short runtime. Experiments first show that our novel formulation provides improved matching performance in comparison to the standard nearest neighbor approach and that we outperform related randomization methods in our localization scenario.
count=15
* Rethinking Performance Estimation in Neural Architecture Search
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Zheng_Rethinking_Performance_Estimation_in_Neural_Architecture_Search_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Rethinking_Performance_Estimation_in_Neural_Architecture_Search_CVPR_2020_paper.pdf)]
    * Title: Rethinking Performance Estimation in Neural Architecture Search
    * Year: `2020`
    * Authors: Xiawu Zheng,  Rongrong Ji,  Qiang Wang,  Qixiang Ye,  Zhenguo Li,  Yonghong Tian,  Qi Tian
    * Abstract: Neural architecture search (NAS) remains a challenging problem, which is attributed to the indispensable and time-consuming component of performance estimation (PE). In this paper, we provide a novel yet systematic rethinking of PE in a resource constrained regime, termed budgeted PE (BPE), which precisely and effectively estimates the performance of an architecture sampled from an architecture space. Since searching an optimal BPE is extremely time-consuming as it requires to train a large number of networks for evaluation, we propose a Minimum Importance Pruning (MIP) approach. Given a dataset and a BPE search space, MIP estimates the importance of hyper-parameters using random forest and subsequently prunes the minimum one from the next iteration. In this way, MIP effectively prunes less important hyper-parameters to allocate more computational resource on more important ones, thus achieving an effective exploration. By combining BPE with various search algorithms including reinforcement learning, evolution algorithm, random search, and differentiable architecture search, we achieve 1, 000x of NAS speed up with a negligible performance drop comparing to the SOTA. All the NAS search codes are available at: https: //github.com/zhengxiawu/rethinking_performance_ estimation_in_NAS
count=15
* Learning to Detect Ground Control Points for Improving the Accuracy of Stereo Matching
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Spyropoulos_Learning_to_Detect_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Spyropoulos_Learning_to_Detect_2014_CVPR_paper.pdf)]
    * Title: Learning to Detect Ground Control Points for Improving the Accuracy of Stereo Matching
    * Year: `2014`
    * Authors: Aristotle Spyropoulos, Nikos Komodakis, Philippos Mordohai
    * Abstract: While machine learning has been instrumental to the ongoing progress in most areas of computer vision, it has not been applied to the problem of stereo matching with similar frequency or success. We present a supervised learning approach for predicting the correctness of stereo matches based on a random forest and a set of features that capture various forms of information about each pixel. We show highly competitive results in predicting the correctness of matches and in confidence estimation, which allows us to rank pixels according to the reliability of their assigned disparities. Moreover, we show how these confidence values can be used to improve the accuracy of disparity maps by integrating them with an MRF-based stereo algorithm. This is an important distinction from current literature that has mainly focused on sparsification by removing potentially erroneous disparities to generate quasi-dense disparity maps.
count=15
* Investigating Haze-relevant Features in A Learning Framework for Image Dehazing
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Tang_Investigating_Haze-relevant_Features_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Tang_Investigating_Haze-relevant_Features_2014_CVPR_paper.pdf)]
    * Title: Investigating Haze-relevant Features in A Learning Framework for Image Dehazing
    * Year: `2014`
    * Authors: Ketan Tang, Jianchao Yang, Jue Wang
    * Abstract: Haze is one of the major factors that degrade outdoor images. Removing haze from a single image is known to be severely ill-posed, and assumptions made in previous methods do not hold in many situations. In this paper, we systematically investigate different haze-relevant features in a learning framework to identify the best feature combination for image dehazing. We show that the dark-channel feature is the most informative one for this task, which confirms the observation of He et al. [8] from a learning perspective, while other haze-relevant features also contribute significantly in a complementary way. We also find that surprisingly, the synthetic hazy image patches we use for feature investigation serve well as training data for realworld images, which allows us to train specific models for specific applications. Experiment results demonstrate that the proposed algorithm outperforms state-of-the-art methods on both synthetic and real-world datasets.
count=15
* Improving Ferns Ensembles by Sparsifying and Quantising Posterior Probabilities
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Rodriguez_Improving_Ferns_Ensembles_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Rodriguez_Improving_Ferns_Ensembles_ICCV_2015_paper.pdf)]
    * Title: Improving Ferns Ensembles by Sparsifying and Quantising Posterior Probabilities
    * Year: `2015`
    * Authors: Antonio L. Rodriguez, Vitor Sequeira
    * Abstract: Ferns ensembles offer an accurate and efficient multiclass non-linear classification, commonly at the expense of consuming a large amount of memory. We introduce a two-fold contribution that produces large reductions in their memory consumption. First, an efficient L0 regularised cost optimisation finds a sparse representation of the posterior probabilities in the ensemble by discarding elements with zero contribution to valid responses in the training samples. As a by-product this can produce a prediction accuracy gain that, if required, can be traded for further reductions in memory size and prediction time. Secondly, posterior probabilities are quantised and stored in a memory-friendly sparse data structure. We reported a minimum of 75% memory reduction for different types of classification problems using generative and discriminative ferns ensembles, without increasing prediction time or classification error. For image patch recognition our proposal produced a 90% memory reduction, and improved in several percentage points the prediction accuracy.
count=15
* Learning a Descriptor-Specific 3D Keypoint Detector
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Salti_Learning_a_Descriptor-Specific_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Salti_Learning_a_Descriptor-Specific_ICCV_2015_paper.pdf)]
    * Title: Learning a Descriptor-Specific 3D Keypoint Detector
    * Year: `2015`
    * Authors: Samuele Salti, Federico Tombari, Riccardo Spezialetti, Luigi Di Stefano
    * Abstract: Keypoint detection represents the first stage in the majority of modern computer vision pipelines based on automatically established correspondences between local descriptors. However, no standard solution has emerged yet in the case of 3D data such as point clouds or meshes, which exhibit high variability in level of detail and noise. More importantly, existing proposals for 3D keypoint detection rely on geometric saliency functions that attempt to maximize repeatability rather than distinctiveness of the selected regions, which may lead to sub-optimal performance of the overall pipeline. To overcome these shortcomings, we cast 3D keypoint detection as a binary classification between points whose support can be correctly matched by a predefined 3D descriptor or not, thereby learning a descriptor-specific detector that adapts seamlessly to different scenarios. Through experiments on several public datasets, we show that this novel approach to the design of a keypoint detector represents a flexible solution that, nonetheless, can provide state-of-the-art descriptor matching performance.
count=15
* Predictive inference is free with the jackknife+-after-bootstrap
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/2b346a0aa375a07f5a90a344a61416c4-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/2b346a0aa375a07f5a90a344a61416c4-Paper.pdf)]
    * Title: Predictive inference is free with the jackknife+-after-bootstrap
    * Year: `2020`
    * Authors: Byol Kim, Chen Xu, Rina Barber
    * Abstract: Ensemble learning is widely used in applications to make predictions in complex decision problems---for example, averaging models fitted to a sequence of samples bootstrapped from the available training data. While such methods offer more accurate, stable, and robust predictions and model estimates, much less is known about how to perform valid, assumption-lean inference on the output of these types of procedures. In this paper, we propose the jackknife+-after-bootstrap (J+aB), a procedure for constructing a predictive interval, which uses only the available bootstrapped samples and their corresponding fitted models, and is therefore "free" in terms of the cost of model fitting. The J+aB offers a predictive coverage guarantee that holds with no assumptions on the distribution of the data, the nature of the fitted model, or the way in which the ensemble of models are aggregated---at worst, the failure rate of the predictive interval is inflated by a factor of 2. Our numerical experiments verify the coverage and accuracy of the resulting predictive intervals on real data.
count=14
* Adversarial Defense Through Network Profiling Based Path Extraction
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Qiu_Adversarial_Defense_Through_Network_Profiling_Based_Path_Extraction_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Qiu_Adversarial_Defense_Through_Network_Profiling_Based_Path_Extraction_CVPR_2019_paper.pdf)]
    * Title: Adversarial Defense Through Network Profiling Based Path Extraction
    * Year: `2019`
    * Authors: Yuxian Qiu,  Jingwen Leng,  Cong Guo,  Quan Chen,  Chao Li,  Minyi Guo,  Yuhao Zhu
    * Abstract: Recently, researchers have started decomposing deep neural network models according to their semantics or functions. Recent work has shown the effectiveness of decomposed functional blocks for defending adversarial attacks, which add small input perturbation to the input image to fool the DNN models. This work proposes a profiling-based method to decompose the DNN models to different functional blocks, which lead to the effective path as a new approach to exploring DNNs' internal organization. Specifically, the per-image effective path can be aggregated to the class-level effective path, through which we observe that adversarial images activate effective path different from normal images. We propose an effective path similarity-based method to detect adversarial images with an interpretable model, which achieve better accuracy and broader applicability than the state-of-the-art technique.
count=14
* Learning-by-Synthesis for Appearance-based 3D Gaze Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Sugano_Learning-by-Synthesis_for_Appearance-based_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Sugano_Learning-by-Synthesis_for_Appearance-based_2014_CVPR_paper.pdf)]
    * Title: Learning-by-Synthesis for Appearance-based 3D Gaze Estimation
    * Year: `2014`
    * Authors: Yusuke Sugano, Yasuyuki Matsushita, Yoichi Sato
    * Abstract: Inferring human gaze from low-resolution eye images is still a challenging task despite its practical importance in many application scenarios. This paper presents a learning-by-synthesis approach to accurate image-based gaze estimation that is person- and head pose-independent. Unlike existing appearance-based methods that assume person-specific training data, we use a large amount of cross-subject training data to train a 3D gaze estimator. We collect the largest and fully calibrated multi-view gaze dataset and perform a 3D reconstruction in order to generate dense training data of eye images. By using the synthesized dataset to learn a random regression forest, we show that our method outperforms existing methods that use low-resolution eye images.
count=14
* What Will Happen Next? Forecasting Player Moves in Sports Videos
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Felsen_What_Will_Happen_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Felsen_What_Will_Happen_ICCV_2017_paper.pdf)]
    * Title: What Will Happen Next? Forecasting Player Moves in Sports Videos
    * Year: `2017`
    * Authors: Panna Felsen, Pulkit Agrawal, Jitendra Malik
    * Abstract: A large number of very popular team sports involve the act of one team trying to score a goal against the other. During this game play, defending players constantly try to predict the next move of the attackers to prevent them from scoring, whereas attackers constantly try to predict the next move of the defenders in order to defy them and score. Such behavior is a prime example of the general human faculty to make predictions about the future and is an important facet of human intelligence. An algorithmic solution to learning a model of the external world from sensory inputs in order to make forecasts is an important unsolved problem. In this work we develop a generic framework for forecasting future events in team sports videos directly from visual inputs. We introduce water polo and basketball datasets towards this end and compare the predictions of the proposed methods against expert and non-expert humans.
count=14
* Fast and Accurate Head Pose Estimation via Random Projection Forests
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Lee_Fast_and_Accurate_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Lee_Fast_and_Accurate_ICCV_2015_paper.pdf)]
    * Title: Fast and Accurate Head Pose Estimation via Random Projection Forests
    * Year: `2015`
    * Authors: Donghoon Lee, Ming-Hsuan Yang, Songhwai Oh
    * Abstract: In this paper, we consider the problem of estimating the gaze direction of a person from a low-resolution image. Under this condition, reliably extracting facial features is very difficult. We propose a novel head pose estimation algorithm based on compressive sensing. Head image patches are mapped to a large feature space using the proposed extensive, yet efficient filter bank. The filter bank is designed to generate sparse responses of color and gradient information, which can be compressed using random projection, and classified by a random forest. Extensive experiments on challenging datasets show that the proposed algorithm performs favorably against the state-of-the-art methods on head pose estimation in low-resolution images degraded by noise, occlusion, and blurring.
count=14
* Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Aodha_Revisiting_Example_Dependent_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Aodha_Revisiting_Example_Dependent_2013_ICCV_paper.pdf)]
    * Title: Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees
    * Year: `2013`
    * Authors: Oisin Mac Aodha, Gabriel J. Brostow
    * Abstract: Typical approaches to classification treat class labels as disjoint. For each training example, it is assumed that there is only one class label that correctly describes it, and that all other labels are equally bad. We know however, that good and bad labels are too simplistic in many scenarios, hurting accuracy. In the realm of example dependent costsensitive learning, each label is instead a vector representing a data point's affinity for each of the classes. At test time, our goal is not to minimize the misclassification rate, but to maximize that affinity. We propose a novel example dependent cost-sensitive impurity measure for decision trees. Our experiments show that this new impurity measure improves test performance while still retaining the fast test times of standard classification trees. We compare our approach to classification trees and other cost-sensitive methods on three computer vision problems, tracking, descriptor matching, and optical flow, and show improvements in all three domains.
count=14
* Structured Forests for Fast Edge Detection
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Dollar_Structured_Forests_for_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Dollar_Structured_Forests_for_2013_ICCV_paper.pdf)]
    * Title: Structured Forests for Fast Edge Detection
    * Year: `2013`
    * Authors: Piotr Dollar, C. L. Zitnick
    * Abstract: Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets.
count=14
* Adversarial Learning for Feature Shift Detection and Correction
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/b3cd64ddad0a28da0f28a0e03a73ea7d-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/b3cd64ddad0a28da0f28a0e03a73ea7d-Paper-Conference.pdf)]
    * Title: Adversarial Learning for Feature Shift Detection and Correction
    * Year: `2023`
    * Authors: Míriam Barrabés, Daniel Mas Montserrat, Margarita Geleta, Xavier Giró-i-Nieto, Alexander Ioannidis
    * Abstract: Data shift is a phenomenon present in many real-world applications, and while there are multiple methods attempting to detect shifts, the task of localizing and correcting the features originating such shifts has not been studied in depth. Feature shifts can occur in many datasets, including in multi-sensor data, where some sensors are malfunctioning, or in tabular and structured data, including biomedical, financial, and survey data, where faulty standardization and data processing pipelines can lead to erroneous features. In this work, we explore using the principles of adversarial learning, where the information from several discriminators trained to distinguish between two distributions is used to both detect the corrupted features and fix them in order to remove the distribution shift between datasets. We show that mainstream supervised classifiers, such as random forest or gradient boosting trees, combined with simple iterative heuristics, can localize and correct feature shifts, outperforming current statistical and neural network-based techniques. The code is available at https://github.com/AI-sandbox/DataFix.
count=14
* Towards a Unified Framework for Uncertainty-aware Nonlinear Variable Selection with Theoretical Guarantees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/b153f11554e8f7926189e3f21185f00f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/b153f11554e8f7926189e3f21185f00f-Paper-Conference.pdf)]
    * Title: Towards a Unified Framework for Uncertainty-aware Nonlinear Variable Selection with Theoretical Guarantees
    * Year: `2022`
    * Authors: Wenying Deng, Beau Coker, Rajarshi Mukherjee, Jeremiah Liu, Brent Coull
    * Abstract: We develop a simple and unified framework for nonlinear variable importance estimation that incorporates uncertainty in the prediction function and is compatible with a wide range of machine learning models (e.g., tree ensembles, kernel methods, neural networks, etc). In particular, for a learned nonlinear model $f(\mathbf{x})$, we consider quantifying the importance of an input variable $\mathbf{x}^j$ using the integrated partial derivative $\Psi_j = \Vert \frac{\partial}{\partial \mathbf{x}^j} f(\mathbf{x})\Vert^2_{P_\mathcal{X}}$. We then (1) provide a principled approach for quantifying uncertainty in variable importance by deriving its posterior distribution, and (2) show that the approach is generalizable even to non-differentiable models such as tree ensembles. Rigorous Bayesian nonparametric theorems are derived to guarantee the posterior consistency and asymptotic uncertainty of the proposed approach. Extensive simulations and experiments on healthcare benchmark datasets confirm that the proposed algorithm outperforms existing classical and recent variable selection methods.
count=13
* Uncertainty-Driven 6D Pose Estimation of Objects and Scenes From a Single RGB Image
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Brachmann_Uncertainty-Driven_6D_Pose_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Brachmann_Uncertainty-Driven_6D_Pose_CVPR_2016_paper.pdf)]
    * Title: Uncertainty-Driven 6D Pose Estimation of Objects and Scenes From a Single RGB Image
    * Year: `2016`
    * Authors: Eric Brachmann, Frank Michel, Alexander Krull, Michael Ying Yang, Stefan Gumhold, carsten Rother
    * Abstract: In recent years, the task of estimating the 6D pose of object instances and complete scenes, i.e. camera localization, from a single input image has received considerable attention. Consumer RGB-D cameras have made this feasible, even for difficult, texture-less objects and scenes. In this work, we show that a single RGB image is sufficient to achieve visually convincing results. Our key concept is to model and exploit the uncertainty of the system at all stages of the processing pipeline. The uncertainty comes in the form of continuous distributions over 3D object coordinates and discrete distributions over object labels. We give three technical contributions. Firstly, we develop a regularized, auto-context regression framework which iteratively reduces uncertainty in object coordinate and object label predictions. Secondly, we introduce an efficient way to marginalize object coordinate distributions over depth. This is necessary to deal with missing depth information. Thirdly, we utilize the distributions over object labels to detect multiple objects simultaneously with a fixed budget of RANSAC hypotheses. We tested our system for object pose estimation and camera localization on commonly used data sets. We see a major improvement over competing systems.
count=13
* Online Learning With Bayesian Classification Trees
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Bulo_Online_Learning_With_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Bulo_Online_Learning_With_CVPR_2016_paper.pdf)]
    * Title: Online Learning With Bayesian Classification Trees
    * Year: `2016`
    * Authors: Samuel Rota Bulo, Peter Kontschieder
    * Abstract: Randomized classification trees are among the most popular machine learning tools and found successful applications in many areas. Although this classifier was originally designed as offline learning algorithm, there has been an increased interest in the last years to provide an online variant. In this paper, we propose an online learning algorithm for classification trees that adheres to Bayesian principles. In contrast to state-of-the-art approaches that produce large forests with complex trees, we aim at constructing small ensembles consisting of shallow trees with high generalization capabilities. Experiments on benchmark machine learning and body part recognition datasets show superior performance over state-of-the-art approaches.
count=13
* Constructing Robust Affinity Graphs for Spectral Clustering
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Zhu_Constructing_Robust_Affinity_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Zhu_Constructing_Robust_Affinity_2014_CVPR_paper.pdf)]
    * Title: Constructing Robust Affinity Graphs for Spectral Clustering
    * Year: `2014`
    * Authors: Xiatian Zhu, Chen Change Loy, Shaogang Gong
    * Abstract: Spectral clustering requires robust and meaningful affinity graphs as input in order to form clusters with desired structures that can well support human intuition. To construct such affinity graphs is non-trivial due to the ambiguity and uncertainty inherent in the raw data. In contrast to most existing clustering methods that typically employ all available features to construct affinity matrices with the Euclidean distance, which is often not an accurate representation of the underlying data structures, we propose a novel unsupervised approach to generating more robust affinity graphs via identifying and exploiting discriminative features for improving spectral clustering. Specifically, our model is capable of capturing and combining subtle similarity information distributed over discriminative feature subspaces for more accurately revealing the latent data distribution and thereby leading to improved data clustering, especially with heterogeneous data sources. We demonstrate the efficacy of the proposed approach on challenging image and video datasets.
count=13
* Conformal Prediction for Time Series with Modern Hopfield Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/aef75887979ae1287b5deb54a1e3cbda-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/aef75887979ae1287b5deb54a1e3cbda-Paper-Conference.pdf)]
    * Title: Conformal Prediction for Time Series with Modern Hopfield Networks
    * Year: `2023`
    * Authors: Andreas Auer, Martin Gauch, Daniel Klotz, Sepp Hochreiter
    * Abstract: To quantify uncertainty, conformal prediction methods are gaining continuously more interest and have already been successfully applied to various domains. However, they are difficult to apply to time series as the autocorrelative structure of time series violates basic assumptions required by conformal prediction. We propose HopCPT, a novel conformal prediction approach for time series that not only copes with temporal structures but leverages them. We show that our approach is theoretically well justified for time series where temporal dependencies are present. In experiments, we demonstrate that our new approach outperforms state-of-the-art conformal prediction methods on multiple real-world time series datasets from four different domains.
count=13
* On the Convergence of CART under Sufficient Impurity Decrease Condition
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/b418964bafb4fdd9aef9017301323a8a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/b418964bafb4fdd9aef9017301323a8a-Paper-Conference.pdf)]
    * Title: On the Convergence of CART under Sufficient Impurity Decrease Condition
    * Year: `2023`
    * Authors: Rahul Mazumder, Haoyue Wang
    * Abstract: The decision tree is a flexible machine-learning model that finds its success in numerous applications. It is usually fitted in a recursively greedy manner using CART. In this paper, we study the convergence rate of CART under a regression setting. First, we prove an upper bound on the prediction error of CART under a sufficient impurity decrease (SID) condition \cite{chi2020asymptotic} -- our result is an improvement over the known result by \cite{chi2020asymptotic} under a similar assumption. We show via examples that this error bound cannot be further improved by more than a constant or a log factor. Second, we introduce a few easy-to-check sufficient conditions of the SID condition. In particular, we show that the SID condition can be satisfied by an additive model when the component functions satisfy a ``locally reverse Poincare inequality". We discuss a few familiar function classes in non-parametric estimation to demonstrate the usefulness of this conception.
count=13
* Second Order PAC-Bayesian Bounds for the Weighted Majority Vote
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/386854131f58a556343e056f03626e00-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/386854131f58a556343e056f03626e00-Paper.pdf)]
    * Title: Second Order PAC-Bayesian Bounds for the Weighted Majority Vote
    * Year: `2020`
    * Authors: Andres Masegosa, Stephan Lorenzen, Christian Igel, Yevgeny Seldin
    * Abstract: We present a novel analysis of the expected risk of weighted majority vote in multiclass classification. The analysis takes correlation of predictions by ensemble members into account and provides a bound that is amenable to efficient minimization, which yields improved weighting for the majority vote. We also provide a specialized version of our bound for binary classification, which allows to exploit additional unlabeled data for tighter risk estimation. In experiments, we apply the bound to improve weighting of trees in random forests and show that, in contrast to the commonly used first order bound, minimization of the new bound typically does not lead to degradation of the test error of the ensemble.
count=13
* Joints in Random Forests
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/8396b14c5dff55d13eea57487bf8ed26-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/8396b14c5dff55d13eea57487bf8ed26-Paper.pdf)]
    * Title: Joints in Random Forests
    * Year: `2020`
    * Authors: Alvaro Correia, Robert Peharz, Cassio P. de Campos
    * Abstract: Decision Trees (DTs) and Random Forests (RFs) are powerful discriminative learners and tools of central importance to the everyday machine learning practitioner and data scientist. Due to their discriminative nature, however, they lack principled methods to process inputs with missing features or to detect outliers, which requires pairing them with imputation techniques or a separate generative model. In this paper, we demonstrate that DTs and RFs can naturally be interpreted as generative models, by drawing a connection to Probabilistic Circuits, a prominent class of tractable probabilistic models. This reinterpretation equips them with a full joint distribution over the feature space and leads to Generative Decision Trees (GeDTs) and Generative Forests (GeFs), a family of novel hybrid generative-discriminative models. This family of models retains the overall characteristics of DTs and RFs while additionally being able to handle missing features by means of marginalisation. Under certain assumptions, frequently made for Bayes consistency results, we show that consistency in GeDTs and GeFs extend to any pattern of missing input features, if missing at random. Empirically, we show that our models often outperform common routines to treat missing data, such as K-nearest neighbour imputation, and moreover, that our models can naturally detect outliers by monitoring the marginal probability of input features.
count=12
* Completing 3D Object Shape From One Depth Image
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Rock_Completing_3D_Object_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Rock_Completing_3D_Object_2015_CVPR_paper.pdf)]
    * Title: Completing 3D Object Shape From One Depth Image
    * Year: `2015`
    * Authors: Jason Rock, Tanmay Gupta, Justin Thorsen, JunYoung Gwak, Daeyun Shin, Derek Hoiem
    * Abstract: Our goal is to recover a complete 3D model from a depth image of an object. Existing approaches rely on user interaction or apply to a limited class of objects, such as chairs. We aim to fully automatically reconstruct a 3D model from any category. We take an exemplar-based approach: retrieve similar objects in a database of 3D models using view-based matching and transfer the symmetries and surfaces from retrieved models. We investigate completion of 3D models in three cases: novel view (model in database); novel model (models for other objects of the same category in database); and novel category (no models from the category in database).
count=12
* Pairwise Conditional Random Forests for Facial Expression Recognition
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Dapogny_Pairwise_Conditional_Random_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Dapogny_Pairwise_Conditional_Random_ICCV_2015_paper.pdf)]
    * Title: Pairwise Conditional Random Forests for Facial Expression Recognition
    * Year: `2015`
    * Authors: Arnaud Dapogny, Kevin Bailly, Severine Dubuisson
    * Abstract: Facial expression can be seen as the dynamic variation of one's appearance over time. Successful recognition thus involves finding representations of high-dimensional spatiotemporal patterns that can be generalized to unseen facial morphologies and variations of the expression dynamics. In this paper, we propose to learn Random Forests from heterogeneous derivative features (e.g. facial fiducial point movements or texture variations) upon pairs of images. Those forests are conditioned on the expression label of the first frame to reduce the variability of the ongoing expression transitions. When testing on a specific frame of a video, pairs are created between this frame and the previous ones. Predictions for each previous frame are used to draw trees from Pairwise Conditional Random Forests (PCRF) whose pairwise outputs are averaged over time to produce robust estimates. As such, PCRF appears as a natural extension of Random Forests to learn spatio-temporal patterns, that leads to significant improvements over standard Random Forests as well as state-of-the-art approaches on several facial expression benchmarks.
count=12
* FAST: a Fused and Accurate Shrinkage Tree for Heterogeneous Treatment Effects Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/01830c92c6558179fa6d7fb1edff692c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/01830c92c6558179fa6d7fb1edff692c-Paper-Conference.pdf)]
    * Title: FAST: a Fused and Accurate Shrinkage Tree for Heterogeneous Treatment Effects Estimation
    * Year: `2023`
    * Authors: Jia Gu, Caizhi Tang, Han Yan, Qing Cui, Longfei Li, Jun Zhou
    * Abstract: This paper proposes a novel strategy for estimating the heterogeneous treatment effect called the Fused and Accurate Shrinkage Tree ($\mathrm{FAST}$). Our approach utilizes both trial and observational data to improve the accuracy and robustness of the estimator. Inspired by the concept of shrinkage estimation in statistics, we develop an optimal weighting scheme and a corresponding estimator that balances the unbiased estimator based on the trial data with the potentially biased estimator based on the observational data. Specifically, combined with tree-based techniques, we introduce a new split criterion that utilizes both trial data and observational data to more accurately estimate the treatment effect. Furthermore, we confirm the consistency of our proposed tree-based estimator and demonstrate the effectiveness of our criterion in reducing prediction error through theoretical analysis. The advantageous finite sample performance of the $\mathrm{FAST}$ and its ensemble version over existing methods is demonstrated via simulations and real data analysis.
count=12
* When are ensembles really effective?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/30b6fa308e62ed52180c31ae3ba6bb0a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/30b6fa308e62ed52180c31ae3ba6bb0a-Paper-Conference.pdf)]
    * Title: When are ensembles really effective?
    * Year: `2023`
    * Authors: Ryan Theisen, Hyunsuk Kim, Yaoqing Yang, Liam Hodgkinson, Michael W. Mahoney
    * Abstract: Ensembling has a long history in statistical data analysis, with many impactful applications. However, in many modern machine learning settings, the benefits of ensembling are less ubiquitous and less obvious. We study, both theoretically and empirically, the fundamental question of when ensembling yields significant performance improvements in classification tasks. Theoretically, we prove new results relating the \emph{ensemble improvement rate} (a measure of how much ensembling decreases the error rate versus a single model, on a relative scale) to the \emph{disagreement-error ratio}. We show that ensembling improves performance significantly whenever the disagreement rate is large relative to the average error rate; and that, conversely, one classifier is often enough whenever the disagreement rate is low relative to the average error rate. On the way to proving these results, we derive, under a mild condition called \emph{competence}, improved upper and lower bounds on the average test error rate of the majority vote classifier.To complement this theory, we study ensembling empirically in a variety of settings, verifying the predictions made by our theory, and identifying practical scenarios where ensembling does and does not result in large performance improvements. Perhaps most notably, we demonstrate a distinct difference in behavior between interpolating models (popular in current practice) and non-interpolating models (such as tree-based methods, where ensembling is popular), demonstrating that ensembling helps considerably more in the latter case than in the former.
count=12
* Individual Arbitrariness and Group Fairness
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d891d240b5784656a0356bf4b00f5cdd-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d891d240b5784656a0356bf4b00f5cdd-Paper-Conference.pdf)]
    * Title: Individual Arbitrariness and Group Fairness
    * Year: `2023`
    * Authors: Carol Long, Hsiang Hsu, Wael Alghamdi, Flavio Calmon
    * Abstract: Machine learning tasks may admit multiple competing models that achieve similar performance yet produce conflicting outputs for individual samples---a phenomenon known as predictive multiplicity. We demonstrate that fairness interventions in machine learning optimized solely for group fairness and accuracy can exacerbate predictive multiplicity. Consequently, state-of-the-art fairness interventions can mask high predictive multiplicity behind favorable group fairness and accuracy metrics. We argue that a third axis of ``arbitrariness'' should be considered when deploying models to aid decision-making in applications of individual-level impact.To address this challenge, we propose an ensemble algorithm applicable to any fairness intervention that provably ensures more consistent predictions.
count=12
* Sparse Learning with CART
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/85fc37b18c57097425b52fc7afbb6969-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/85fc37b18c57097425b52fc7afbb6969-Paper.pdf)]
    * Title: Sparse Learning with CART
    * Year: `2020`
    * Authors: Jason Klusowski
    * Abstract: Decision trees with binary splits are popularly constructed using Classification and Regression Trees (CART) methodology. For regression models, this approach recursively divides the data into two near-homogenous daughter nodes according to a split point that maximizes the reduction in sum of squares error (the impurity) along a particular variable. This paper aims to study the statistical properties of regression trees constructed with CART. In doing so, we find that the training error is governed by the Pearson correlation between the optimal decision stump and response data in each node, which we bound by constructing a prior distribution on the split points and solving a nonlinear optimization problem. We leverage this connection between the training error and Pearson correlation to show that CART with cost-complexity pruning achieves an optimal complexity/goodness-of-fit tradeoff when the depth scales with the logarithm of the sample size. Data dependent quantities, which adapt to the dimensionality and latent structure of the regression model, are seen to govern the rates of convergence of the prediction error.
count=11
* Global Hypothesis Generation for 6D Object Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Michel_Global_Hypothesis_Generation_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Michel_Global_Hypothesis_Generation_CVPR_2017_paper.pdf)]
    * Title: Global Hypothesis Generation for 6D Object Pose Estimation
    * Year: `2017`
    * Authors: Frank Michel, Alexander Kirillov, Eric Brachmann, Alexander Krull, Stefan Gumhold, Bogdan Savchynskyy, Carsten Rother
    * Abstract: This paper addresses the task of estimating the 6D-pose of a known 3D object from a single RGB-D image. Most modern approaches solve this task in three steps: i) compute local features; ii) generate a pool of pose-hypotheses; iii) select and refine a pose from the pool. This work focuses on the second step. While all existing approaches generate the hypotheses pool via local reasoning, e.g. RANSAC or Hough-Voting, we are the first to show that global reasoning is beneficial at this stage. In particular, we formulate a novel fully-connected Conditional Random Field (CRF) that outputs a very small number of pose-hypotheses. Despite the potential functions of the CRF being non-Gaussian, we give a new, efficient two-step optimization procedure, with some guarantees for optimality. We utilize our global hypotheses generation procedure to produce results that exceed state-of-the-art for the challenging "Occluded Object Dataset".
count=11
* SemiContour: A Semi-Supervised Learning Approach for Contour Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_SemiContour_A_Semi-Supervised_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_SemiContour_A_Semi-Supervised_CVPR_2016_paper.pdf)]
    * Title: SemiContour: A Semi-Supervised Learning Approach for Contour Detection
    * Year: `2016`
    * Authors: Zizhao Zhang, Fuyong Xing, Xiaoshuang Shi, Lin Yang
    * Abstract: Supervised contour detection methods usually require many labeled training images to obtain satisfactory performance. However, a large set of annotated data might be unavailable or extremely labor intensive. In this paper, we investigate the usage of semi-supervised learning (SSL) to obtain competitive detection accuracy with very limited training data (three labeled images). Specifically, we propose a semi-supervised structured ensemble learning approach for contour detection built on structured random forests (SRF). To allow SRF to be applicable to unlabeled data, we present an effective sparse representation approach to capture inherent structure in image patches by finding a compact and discriminative low-dimensional subspace representation in an unsupervised manner, enabling the incorporation of abundant unlabeled patches with their estimated structured labels to help SRF perform better node splitting. We re-examine the role of sparsity and propose a novel and fast sparse coding algorithm to boost the overall learning efficiency. To the best of our knowledge, this is the first attempt to apply SSL for contour detection. Extensive experiments on the BSDS500 segmentation dataset and the NYU Depth dataset demonstrate the superiority of the proposed method.
count=11
* An Active Search Strategy for Efficient Object Class Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Gonzalez-Garcia_An_Active_Search_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Gonzalez-Garcia_An_Active_Search_2015_CVPR_paper.pdf)]
    * Title: An Active Search Strategy for Efficient Object Class Detection
    * Year: `2015`
    * Authors: Abel Gonzalez-Garcia, Alexander Vezhnevets, Vittorio Ferrari
    * Abstract: Object class detectors typically apply a window classifier to all the windows in a large set, either in a sliding window manner or using object proposals. In this paper, we develop an active search strategy that sequentially chooses the next window to evaluate based on all the information gathered before. This results in a substantial reduction in the number of classifier evaluations and in a more elegant approach in general. Our search strategy is guided by two forces. First, we exploit context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set. This enables to jump across distant regions in the image (e.g. observing a sky region suggests that cars might be far below) and is done efficiently in a Random Forest framework. Second, we exploit the score of the classifier to attract the search to promising areas surrounding a highly scored window, and to keep away from areas near low scored ones. Our search strategy can be applied on top of any classifier as it treats it as a black-box. In experiments with R-CNN on the challenging SUN2012 dataset, our method matches the detection accuracy of evaluating all windows independently, while evaluating 9x fewer windows.
count=11
* COUNT Forest: CO-Voting Uncertain Number of Targets Using Random Forest for Crowd Density Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Pham_COUNT_Forest_CO-Voting_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Pham_COUNT_Forest_CO-Voting_ICCV_2015_paper.pdf)]
    * Title: COUNT Forest: CO-Voting Uncertain Number of Targets Using Random Forest for Crowd Density Estimation
    * Year: `2015`
    * Authors: Viet-Quoc Pham, Tatsuo Kozakaya, Osamu Yamaguchi, Ryuzo Okada
    * Abstract: This paper presents a patch-based approach for crowd density estimation in public scenes. We formulate the problem of estimating density in a structured learning framework applied to random decision forests. Our approach learns the mapping between patch features and relative locations of all objects inside each patch, which contribute to generate the patch density map through Gaussian kernel density estimation. We build the forest in a coarse-to-fine manner with two split node layers, and further propose a crowdedness prior and an effective forest reduction method to improve the estimation accuracy and speed. Moreover, we introduce a semi-automatic training method to learn the estimator for a specific scene. We achieved state-of-the-art results on the public Mall dataset and UCSD dataset, and also proposed two potential applications in traffic counts and scene understanding with promising results.
count=11
* Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Levinkov_Sequential_Bayesian_Model_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Levinkov_Sequential_Bayesian_Model_2013_ICCV_paper.pdf)]
    * Title: Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling
    * Year: `2013`
    * Authors: Evgeny Levinkov, Mario Fritz
    * Abstract: Semantic road labeling is a key component of systems that aim at assisted or even autonomous driving. Considering that such systems continuously operate in the realworld, unforeseen conditions not represented in any conceivable training procedure are likely to occur on a regular basis. In order to equip systems with the ability to cope with such situations, we would like to enable adaptation to such new situations and conditions at runtime. Existing adaptive methods for image labeling either require labeled data from the new condition or even operate globally on a complete test set. None of this is a desirable mode of operation for a system as described above where new images arrive sequentially and conditions may vary. We study the effect of changing test conditions on scene labeling methods based on a new diverse street scene dataset. We propose a novel approach that can operate in such conditions and is based on a sequential Bayesian model update in order to robustly integrate the arriving images into the adapting procedure.
count=11
* Random Forests of Local Experts for Pedestrian Detection
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Marin_Random_Forests_of_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Marin_Random_Forests_of_2013_ICCV_paper.pdf)]
    * Title: Random Forests of Local Experts for Pedestrian Detection
    * Year: `2013`
    * Authors: Javier Marin, David Vazquez, Antonio M. Lopez, Jaume Amores, Bastian Leibe
    * Abstract: Pedestrian detection is one of the most challenging tasks in computer vision, and has received a lot of attention in the last years. Recently, some authors have shown the advantages of using combinations of part/patch-based detectors in order to cope with the large variability of poses and the existence of partial occlusions. In this paper, we propose a pedestrian detection method that efficiently combines multiple local experts by means of a Random Forest ensemble. The proposed method works with rich block-based representations such as HOG and LBP, in such a way that the same features are reused by the multiple local experts, so that no extra computational cost is needed with respect to a holistic method. Furthermore, we demonstrate how to integrate the proposed approach with a cascaded architecture in order to achieve not only high accuracy but also an acceptable efficiency. In particular, the resulting detector operates at five frames per second using a laptop machine. We tested the proposed method with well-known challenging datasets such as Caltech, ETH, Daimler, and INRIA. The method proposed in this work consistently ranks among the top performers in all the datasets, being either the best method or having a small difference with the best one.
count=11
* A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/82f39c7409155b74d15d73b048f06771-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/82f39c7409155b74d15d73b048f06771-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning
    * Year: `2023`
    * Authors: Valeriia Cherepanova, Roman Levin, Gowthami Somepalli, Jonas Geiping, C. Bayan Bruss, Andrew G. Wilson, Tom Goldstein, Micah Goldblum
    * Abstract: Academic tabular benchmarks often contain small sets of curated features. In contrast, data scientists typically collect as many features as possible into their datasets, and even engineer new features from existing ones. To prevent over-fitting in subsequent downstream modeling, practitioners commonly use automated feature selection methods that identify a reduced subset of informative features. Existing benchmarks for tabular feature selection consider classical downstream models, toy synthetic datasets, or do not evaluate feature selectors on the basis of downstream performance. We construct a challenging feature selection benchmark evaluated on downstream neural networks including transformers, using real datasets and multiple methods for generating extraneous features. We also propose an input-gradient-based analogue of LASSO for neural networks that outperforms classical feature selection methods on challenging problems such as selecting from corrupted or second-order features.
count=11
* A Direct Approximation of AIXI Using Logical State Abstractions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/ed91353f700d113e5d848c7e04a858b0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/ed91353f700d113e5d848c7e04a858b0-Paper-Conference.pdf)]
    * Title: A Direct Approximation of AIXI Using Logical State Abstractions
    * Year: `2022`
    * Authors: Samuel Yang-Zhao, Tianyu Wang, Kee Siong Ng
    * Abstract: We propose a practical integration of logical state abstraction with AIXI, a Bayesian optimality notion for reinforcement learning agents, to significantly expand the model class that AIXI agents can be approximated over to complex history-dependent and structured environments. The state representation and reasoning framework is based on higher-order logic, which can be used to define and enumerate complex features on non-Markovian and structured environments. We address the problem of selecting the right subset of features to form state abstractions by adapting the $\Phi$-MDP optimisation criterion from state abstraction theory. Exact Bayesian model learning is then achieved using a suitable generalisation of Context Tree Weighting over abstract state sequences. The resultant architecture can be integrated with different planning algorithms. Experimental results on controlling epidemics on large-scale contact networks validates the agent's performance.
count=11
* Tree in Tree: from Decision Trees to Decision Graphs
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf)]
    * Title: Tree in Tree: from Decision Trees to Decision Graphs
    * Year: `2021`
    * Authors: Bingzhao Zhu, Mahsa Shoaran
    * Abstract: Decision trees have been widely used as classifiers in many machine learning applications thanks to their lightweight and interpretable decision process. This paper introduces Tree in Tree decision graph (TnT), a framework that extends the conventional decision tree to a more generic and powerful directed acyclic graph. TnT constructs decision graphs by recursively growing decision trees inside the internal or leaf nodes instead of greedy training. The time complexity of TnT is linear to the number of nodes in the graph, therefore it can construct decision graphs on large datasets. Compared to decision trees, we show that TnT achieves better classification performance with reduced model size, both as a stand-alone classifier and as a base-estimator in bagging/AdaBoost ensembles. Our proposed model is a novel, more efficient and accurate alternative to the widely-used decision trees.
count=11
* Optimization over Continuous and Multi-dimensional Decisions with Observational Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/f337d999d9ad116a7b4f3d409fcc6480-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf)]
    * Title: Optimization over Continuous and Multi-dimensional Decisions with Observational Data
    * Year: `2018`
    * Authors: Dimitris Bertsimas, Christopher McCord
    * Abstract: We consider the optimization of an uncertain objective over continuous and multi-dimensional decision spaces in problems in which we are only provided with observational data. We propose a novel algorithmic framework that is tractable, asymptotically consistent, and superior to comparable methods on example problems. Our approach leverages predictive machine learning methods and incorporates information on the uncertainty of the predicted outcomes for the purpose of prescribing decisions. We demonstrate the efficacy of our method on examples involving both synthetic and real data sets.
count=10
* Towards Better Decision Forests: Forest Alternating Optimization
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Carreira-Perpinan_Towards_Better_Decision_Forests_Forest_Alternating_Optimization_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Carreira-Perpinan_Towards_Better_Decision_Forests_Forest_Alternating_Optimization_CVPR_2023_paper.pdf)]
    * Title: Towards Better Decision Forests: Forest Alternating Optimization
    * Year: `2023`
    * Authors: Miguel Á. Carreira-Perpiñán, Magzhan Gabidolla, Arman Zharmagambetov
    * Abstract: Decision forests are among the most accurate models in machine learning. This is remarkable given that the way they are trained is highly heuristic: neither the individual trees nor the overall forest optimize any well-defined loss. While diversity mechanisms such as bagging or boosting have been until now critical in the success of forests, we think that a better optimization should lead to better forests---ideally eliminating any need for an ensembling heuristic. However, unlike for most other models, such as neural networks, optimizing forests or trees is not easy, because they define a non-differentiable function. We show, for the first time, that it is possible to learn a forest by optimizing a desirable loss and regularization jointly over all its trees and parameters. Our algorithm, Forest Alternating Optimization, is based on defining a forest as a parametric model with a fixed number of trees and structure (rather than adding trees indefinitely as in bagging or boosting). It then iteratively updates each tree in alternation so that the objective function decreases monotonically. The algorithm is so effective at optimizing that it easily overfits, but this can be corrected by averaging. The result is a forest that consistently exceeds the accuracy of the state-of-the-art while using fewer, smaller trees.
count=10
* CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Batsos_CBMV_A_Coalesced_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Batsos_CBMV_A_Coalesced_CVPR_2018_paper.pdf)]
    * Title: CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation
    * Year: `2018`
    * Authors: Konstantinos Batsos, Changjiang Cai, Philippos Mordohai
    * Abstract: Recently, there has been a paradigm shift in stereo matching with learning-based methods achieving the best results on all popular benchmarks. The success of these methods is due to the availability of training data with ground truth; training learning-based systems on these datasets has allowed them to surpass the accuracy of conventional approaches based on heuristics and assumptions. Many of these assumptions, however, had been validated extensively and hold for the majority of possible inputs. In this paper, we generate a matching volume leveraging both data with ground truth and conventional wisdom. We accomplish this by coalescing diverse evidence from a bidirectional matching process via random forest classifiers. We show that the resulting matching volume estimation method achieves similar accuracy to purely data-driven alternatives on benchmarks and that it generalizes to unseen data much better. In fact, the results we submitted to the KITTI benchmarks were generated using a classifier trained on the Middlebury dataset.
count=10
* Person Count Localization in Videos From Noisy Foreground and Detections
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Chen_Person_Count_Localization_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Chen_Person_Count_Localization_2015_CVPR_paper.pdf)]
    * Title: Person Count Localization in Videos From Noisy Foreground and Detections
    * Year: `2015`
    * Authors: Sheng Chen, Alan Fern, Sinisa Todorovic
    * Abstract: This paper formulates and presents a solution to a new problem called person count localization. Given a video of a crowded scene, our goal is to output for each frame a set of: 1) Detections optimally covering both isolated individuals and cluttered groups of people; and 2) Counts of people inside these detections. This problem is a middle-ground between frame-level person counting, which does not localize counts, and person detection aimed at perfectly localizing people with count-one detections. Our problem formulation is important for a wide range of domains, where people appear frequently under severe occlusion within a crowd. As these crowds are often visually distinct from the rest of the scene, they can be viewed as ``visual phrases'' whose spatially tight localization and count assignment could facilitate higher-level video understanding. For count localization, we specify a novel framework of iterative error-driven revisions of a flow graph derived from noisy input of people detections and foreground segmentation. Each iteration creates and solves an integer program for count localization based on iterative revisions of the flow graph. The graph revisions are based on detected violations of basic integrity constraints. They in turn trigger learned modifications to the graph aimed at reducing noise in input features. For evaluation, we introduce a new metric that measures both count precision and localization of our approach on American football and pedestrian videos.
count=10
* Situational Object Boundary Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Uijlings_Situational_Object_Boundary_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Uijlings_Situational_Object_Boundary_2015_CVPR_paper.pdf)]
    * Title: Situational Object Boundary Detection
    * Year: `2015`
    * Authors: Jasper R. R. Uijlings, Vittorio Ferrari
    * Abstract: Intuitively, the appearance of true object boundaries varies from image to image. Hence the usual monolithic approach of training a single boundary predictor and applying it to all images regardless of their content is bound to be suboptimal. In this paper we therefore propose Situational Object Boundary Detection: we first define a variety of situations and train a specialized object boundary detector for each of them using [Dollar13]. Then given a test image, we classify it into these situations using its context, which we model by global image appearance. We apply the corresponding situational object boundary detectors, and fuse them based on the classification probabilities. In experiments on ImageNet [Russakovsky14], Microsoft COCO [Lin], and Pascal VOC 2012 segmentation [Everingham14] we show that our situational object boundary detection gives significant improvements over a monolithic approach. Additionally, our method substantially outperforms [Hariharan11iccv] on semantic contour detection on their SBD dataset.
count=10
* Gesture Recognition Portfolios for Personalization
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Yao_Gesture_Recognition_Portfolios_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Yao_Gesture_Recognition_Portfolios_2014_CVPR_paper.pdf)]
    * Title: Gesture Recognition Portfolios for Personalization
    * Year: `2014`
    * Authors: Angela Yao, Luc Van Gool, Pushmeet Kohli
    * Abstract: Human gestures, similar to speech and handwriting, are often unique to the individual. Training a generic classifier applicable to everyone can be very difficult and as such, it has become a standard to use personalized classifiers in speech and handwriting recognition. In this paper, we address the problem of personalization in the context of gesture recognition, and propose a novel and extremely efficient way of doing personalization. Unlike conventional personalization methods which learn a single classifier that later gets adapted, our approach learns a set (portfolio) of classifiers during training, one of which is selected for each test subject based on the personalization data. We formulate classifier personalization as a selection problem and propose several algorithms to compute the set of candidate classifiers. Our experiments show that such an approach is much more efficient than adapting the classifier parameters but can still achieve comparable or better results.
count=10
* Salient Object Detection: A Discriminative Regional Feature Integration Approach
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Jiang_Salient_Object_Detection_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Jiang_Salient_Object_Detection_2013_CVPR_paper.pdf)]
    * Title: Salient Object Detection: A Discriminative Regional Feature Integration Approach
    * Year: `2013`
    * Authors: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li
    * Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.
count=10
* Semi-supervised Node Splitting for Random Forest Construction
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Liu_Semi-supervised_Node_Splitting_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Liu_Semi-supervised_Node_Splitting_2013_CVPR_paper.pdf)]
    * Title: Semi-supervised Node Splitting for Random Forest Construction
    * Year: `2013`
    * Authors: Xiao Liu, Mingli Song, Dacheng Tao, Zicheng Liu, Luming Zhang, Chun Chen, Jiajun Bu
    * Abstract: Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper, we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular, we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmentation. Experimental results on publicly available datasets demonstrate the superiority of our method.
count=10
* Deep Neural Decision Forests
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf)]
    * Title: Deep Neural Decision Forests
    * Year: `2015`
    * Authors: Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, Samuel Rota Bulo
    * Abstract: We present Deep Neural Decision Forests - a novel approach that unifies classification trees with the representation learning functionality known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find on-par or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84%/6.38% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67% error obtained by the best GoogLeNet architecture (7 models, 144 crops).
count=10
* Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Krull_Learning_Analysis-by-Synthesis_for_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Krull_Learning_Analysis-by-Synthesis_for_ICCV_2015_paper.pdf)]
    * Title: Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images
    * Year: `2015`
    * Authors: Alexander Krull, Eric Brachmann, Frank Michel, Michael Ying Yang, Stefan Gumhold, Carsten Rother
    * Abstract: Analysis-by-synthesis has been a successful approach for many tasks in computer vision, such as 6D pose estimation of an object in an RGB-D image which is the topic of this work. The idea is to compare the observation with the output of a forward process, such as a rendered image of the object of interest in a particular pose. Due to occlusion or complicated sensor noise, it can be difficult to perform this comparison in a meaningful way. We propose an approach that ``learns to compare'', while taking these difficulties into account. This is done by describing the posterior density of a particular object pose with a convolutional neural network (CNN) that compares observed and rendered images. The network is trained with the maximum likelihood paradigm. We observe empirically that the CNN does not specialize to the geometry or appearance of specific objects. It can be used with objects of vastly different shapes and appearances, and in different backgrounds. Compared to state-of-the-art, we demonstrate a significant improvement on two different datasets which include a total of eleven objects, cluttered background, and heavy occlusion.
count=10
* GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/5eaafd67434a4cfb1cf829722c65f184-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/5eaafd67434a4cfb1cf829722c65f184-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection
    * Year: `2023`
    * Authors: Jianheng Tang, Fengrui Hua, Ziqi Gao, Peilin Zhao, Jia Li
    * Abstract: With a long history of traditional Graph Anomaly Detection (GAD) algorithms and recently popular Graph Neural Networks (GNNs), it is still not clear (1) how they perform under a standard comprehensive setting, (2) whether GNNs can outperform traditional algorithms such as tree ensembles, and (3) how about their efficiency on large-scale graphs. In response, we introduce GADBench---a benchmark tool dedicated to supervised anomalous node detection in static graphs. GADBench facilitates a detailed comparison across 29 distinct models on ten real-world GAD datasets, encompassing thousands to millions (~6M) nodes. Our main finding is that tree ensembles with simple neighborhood aggregation can outperform the latest GNNs tailored for the GAD task. We shed light on the current progress of GAD, setting a robust groundwork for subsequent investigations in this domain. GADBench is open-sourced at https://github.com/squareRoot3/GADBench.
count=10
* Probabilistic Forecasting: A Level-Set Approach
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/32b127307a606effdcc8e51f60a45922-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/32b127307a606effdcc8e51f60a45922-Paper.pdf)]
    * Title: Probabilistic Forecasting: A Level-Set Approach
    * Year: `2021`
    * Authors: Hilaf Hasson, Bernie Wang, Tim Januschowski, Jan Gasthaus
    * Abstract: Large-scale time series panels have become ubiquitous over the last years in areas such as retail, operational metrics, IoT, and medical domain (to name only a few). This has resulted in a need for forecasting techniques that effectively leverage all available data by learning across all time series in each panel. Among the desirable properties of forecasting techniques, being able to generate probabilistic predictions ranks among the top. In this paper, we therefore present Level Set Forecaster (LSF), a simple yet effective general approach to transform a point estimator into a probabilistic one. By recognizing the connection of our algorithm to random forests (RFs) and quantile regression forests (QRFs), we are able to prove consistency guarantees of our approach under mild assumptions on the underlying point estimator. As a byproduct, we prove the first consistency results for QRFs under the CART-splitting criterion. Empirical experiments show that our approach, equipped with tree-based models as the point estimator, rivals state-of-the-art deep learning models in terms of forecasting accuracy.
count=10
* Minimax Estimation of Conditional Moment Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/8fcd9e5482a62a5fa130468f4cf641ef-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/8fcd9e5482a62a5fa130468f4cf641ef-Paper.pdf)]
    * Title: Minimax Estimation of Conditional Moment Models
    * Year: `2020`
    * Authors: Nishanth Dikkala, Greg Lewis, Lester Mackey, Vasilis Syrgkanis
    * Abstract: We develop an approach for estimating models described via conditional moment restrictions, with a prototypical application being non-parametric instrumental variable regression. We introduce a min-max criterion function, under which the estimation problem can be thought of as solving a zero-sum game between a modeler who is optimizing over the hypothesis space of the target model and an adversary who identifies violating moments over a test function space. We analyze the statistical estimation rate of the resulting estimator for arbitrary hypothesis spaces, with respect to an appropriate analogue of the mean squared error metric, for ill-posed inverse problems. We show that when the minimax criterion is regularized with a second moment penalty on the test function and the test function space is sufficiently rich, then the estimation rate scales with the critical radius of the hypothesis and test function spaces, a quantity which typically gives tight fast rates. Our main result follows from a novel localized Rademacher analysis of statistical learning problems defined via minimax objectives. We provide applications of our main results for several hypothesis spaces used in practice such as: reproducing kernel Hilbert spaces, high dimensional sparse linear functions, spaces defined via shape constraints, ensemble estimators such as random forests, and neural networks. For each of these applications we provide computationally efficient optimization methods for solving the corresponding minimax problem (e.g. stochastic first-order heuristics for neural networks). In several applications, we show how our modified mean squared error rate, combined with conditions that bound the ill-posedness of the inverse problem, lead to mean squared error rates. We conclude with an extensive experimental analysis of the proposed methods.
count=10
* Cross-validation Confidence Intervals for Test Error
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/bce9abf229ffd7e570818476ee5d7dde-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/bce9abf229ffd7e570818476ee5d7dde-Paper.pdf)]
    * Title: Cross-validation Confidence Intervals for Test Error
    * Year: `2020`
    * Authors: Pierre Bayle, Alexandre Bayle, Lucas Janson, Lester Mackey
    * Abstract: This work develops central limit theorems for cross-validation and consistent estimators of the asymptotic variance under weak stability conditions on the learning algorithm. Together, these results provide practical, asymptotically-exact confidence intervals for k-fold test error and valid, powerful hypothesis tests of whether one learning algorithm has smaller k-fold test error than another. These results are also the first of their kind for the popular choice of leave-one-out cross-validation. In our experiments with diverse learning algorithms, the resulting intervals and tests outperform the most popular alternative methods from the literature.
count=10
* Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/3b2acfe2e38102074656ed938abf4ac3-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/3b2acfe2e38102074656ed938abf4ac3-Paper.pdf)]
    * Title: Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments
    * Year: `2019`
    * Authors: Vasilis Syrgkanis, Victor Lei, Miruna Oprescu, Maggie Hei, Keith Battocchi, Greg Lewis
    * Abstract: We consider the estimation of heterogeneous treatment effects with arbitrary machine learning methods in the presence of unobserved confounders with the aid of a valid instrument. Such settings arise in A/B tests with an intent-to-treat structure, where the experimenter randomizes over which user will receive a recommendation to take an action, and we are interested in the effect of the downstream action. We develop a statistical learning approach to the estimation of heterogeneous effects, reducing the problem to the minimization of an appropriate loss function that depends on a set of auxiliary models (each corresponding to a separate prediction task). The reduction enables the use of all recent algorithmic advances (e.g. neural nets, forests). We show that the estimated effect model is robust to estimation errors in the auxiliary models, by showing that the loss satisfies a Neyman orthogonality criterion. Our approach can be used to estimate projections of the true effect model on simpler hypothesis spaces. When these spaces are parametric, then the parameter estimates are asymptotically normal, which enables construction of confidence sets. We applied our method to estimate the effect of membership on downstream webpage engagement for a major travel webpage, using as an instrument an intent-to-treat A/B test among 4 million users, where some users received an easier membership sign-up process. We also validate our method on synthetic data and on public datasets for the effects of schooling on income.
count=10
* Partitioning Structure Learning for Segmented Linear Regression Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/7bb060764a818184ebb1cc0d43d382aa-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf)]
    * Title: Partitioning Structure Learning for Segmented Linear Regression Trees
    * Year: `2019`
    * Authors: Xiangyu Zheng, Song Xi Chen
    * Abstract: This paper proposes a partitioning structure learning method for segmented linear regression trees (SLRT), which assigns linear predictors over the terminal nodes. The recursive partitioning process is driven by an adaptive split selection algorithm that maximizes, at each node, a criterion function based on a conditional Kendall’s τ statistic that measures the rank dependence between the regressors and the fit- ted linear residuals. Theoretical analysis shows that the split selection algorithm permits consistent identification and estimation of the unknown segments. A suffi- ciently large tree is induced by applying the split selection algorithm recursively. Then the minimal cost-complexity tree pruning procedure is applied to attain the right-sized tree, that ensures (i) the nested structure of pruned subtrees and (ii) consistent estimation to the number of segments. Implanting the SLRT as the built-in base predictor, we obtain the ensemble predictors by random forests (RF) and the proposed weighted random forests (WRF). The practical performance of the SLRT and its ensemble versions are evaluated via numerical simulations and empirical studies. The latter shows their advantageous predictive performance over a set of state-of-the-art tree-based models on well-studied public datasets.
count=10
* Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/e22312179bf43e61576081a2f250f845-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/e22312179bf43e61576081a2f250f845-Paper.pdf)]
    * Title: Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate
    * Year: `2018`
    * Authors: Mikhail Belkin, Daniel J. Hsu, Partha Mitra
    * Abstract: Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for ``overfitted'' / interpolated classifiers appears to be ubiquitous in high-dimensional data, having been observed in deep networks, kernel machines, boosting and random forests. Their performance is consistently robust even when the data contain large amounts of label noise. Very little theory is available to explain these observations. The vast majority of theoretical analyses of generalization allows for interpolation only when there is little or no label noise. This paper takes a step toward a theoretical foundation for interpolated classifiers by analyzing local interpolating schemes, including geometric simplicial interpolation algorithm and singularly weighted $k$-nearest neighbor schemes. Consistency or near-consistency is proved for these schemes in classification and regression problems. Moreover, the nearest neighbor schemes exhibit optimal rates under some standard statistical assumptions. Finally, this paper suggests a way to explain the phenomenon of adversarial examples, which are seemingly ubiquitous in modern machine learning, and also discusses some connections to kernel machines and random forests in the interpolated regime.
count=10
* Context-Sensitive Decision Forests for Object Detection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/bcbe3365e6ac95ea2c0343a2395834dd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/bcbe3365e6ac95ea2c0343a2395834dd-Paper.pdf)]
    * Title: Context-Sensitive Decision Forests for Object Detection
    * Year: `2012`
    * Authors: Peter Kontschieder, Samuel Bulò, Antonio Criminisi, Pushmeet Kohli, Marcello Pelillo, Horst Bischof
    * Abstract: In this paper we introduce Context-Sensitive Decision Forests - A new perspective to exploit contextual information in the popular decision forest framework for the object detection problem. They are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time. This intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process. In addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information. In our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging TUD data set when compared to state-of-the-art methods.
count=9
* From Categories to Subcategories: Large-Scale Image Classification With Partial Class Label Refinement
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Ristin_From_Categories_to_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Ristin_From_Categories_to_2015_CVPR_paper.pdf)]
    * Title: From Categories to Subcategories: Large-Scale Image Classification With Partial Class Label Refinement
    * Year: `2015`
    * Authors: Marko Ristin, Juergen Gall, Matthieu Guillaumin, Luc Van Gool
    * Abstract: The number of digital images is growing extremely rapidly, and so is the need for their classification. But, as more images of pre-defined categories become available, they also become more diverse and cover finer semantic differences. Ultimately, the categories themselves need to be divided into subcategories to account for that semantic refinement. Image classification in general has improved significantly over the last few years, but it still requires a massive amount of manually annotated data. Subdividing categories into subcategories multiples the number of labels, aggravating the annotation problem. Hence, we can expect the annotations to be refined only for a subset of the already labeled data, and exploit coarser labeled data to improve classification. In this work, we investigate how coarse category labels can be used to improve the classification of subcategories. To this end, we adopt the framework of Random Forests and propose a regularized objective function that takes into account relations between categories and subcategories. Compared to approaches that disregard the extra coarse labeled data, we achieve a relative improvement in subcategory classification accuracy of up to 22% in our large-scale image classification experiments.
count=9
* Cascaded Hand Pose Regression
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Sun_Cascaded_Hand_Pose_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Sun_Cascaded_Hand_Pose_2015_CVPR_paper.pdf)]
    * Title: Cascaded Hand Pose Regression
    * Year: `2015`
    * Authors: Xiao Sun, Yichen Wei, Shuang Liang, Xiaoou Tang, Jian Sun
    * Abstract: We extends the previous 2D cascaded object pose regression work [9] in two aspects so that it works better for 3D articulated objects. Our first contribution is 3D pose-indexed features that generalize the previous 2D parameterized features and achieve better invariance to 3D transformations. Our second contribution is a principled hierarchical regression that is adapted to the articulated object structure. It is therefore more accurate and faster. Comprehensive experiments verify the state-of-the-art accuracy and efficiency of the proposed approach on the challenging 3D hand pose estimation problem, on a public dataset and our new dataset.
count=9
* Discrete-Continuous Gradient Orientation Estimation for Faster Image Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Donoser_Discrete-Continuous_Gradient_Orientation_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Donoser_Discrete-Continuous_Gradient_Orientation_2014_CVPR_paper.pdf)]
    * Title: Discrete-Continuous Gradient Orientation Estimation for Faster Image Segmentation
    * Year: `2014`
    * Authors: Michael Donoser, Dieter Schmalstieg
    * Abstract: The state-of-the-art in image segmentation builds hierarchical segmentation structures based on analyzing local feature cues in spectral settings. Due to their impressive performance, such segmentation approaches have become building blocks in many computer vision applications. Nevertheless, the main bottlenecks are still the computationally demanding processes of local feature processing and spectral analysis. In this paper, we demonstrate that based on a discrete-continuous optimization of oriented gradient signals, we are able to provide segmentation performance competitive to state-of-the-art on BSDS 500 (even without any spectral analysis) while reducing computation time by a factor of 40 and memory demands by a factor of 10.
count=9
* Face Alignment at 3000 FPS via Regressing Local Binary Features
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Ren_Face_Alignment_at_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Ren_Face_Alignment_at_2014_CVPR_paper.pdf)]
    * Title: Face Alignment at 3000 FPS via Regressing Local Binary Features
    * Year: `2014`
    * Authors: Shaoqing Ren, Xudong Cao, Yichen Wei, Jian Sun
    * Abstract: This paper presents a highly efficient, very accurate regression approach for face alignment. Our approach has two novel components: a set of local binary features, and a locality principle for learning those features. The locality principle guides us to learn a set of highly discriminative local binary features for each facial landmark independently. The obtained local binary features are used to jointly learn a linear regression for the final output. Our approach achieves the state-of-the-art results when tested on the current most challenging benchmarks. Furthermore, because extracting and regressing local binary features is computationally very cheap, our system is much faster than previous methods. It achieves over 3,000 fps on a desktop or 300 fps on a mobile phone for locating a few dozens of landmarks.
count=9
* SANet: Scene Agnostic Network for Camera Localization
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_SANet_Scene_Agnostic_Network_for_Camera_Localization_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SANet_Scene_Agnostic_Network_for_Camera_Localization_ICCV_2019_paper.pdf)]
    * Title: SANet: Scene Agnostic Network for Camera Localization
    * Year: `2019`
    * Authors: Luwei Yang,  Ziqian Bai,  Chengzhou Tang,  Honghua Li,  Yasutaka Furukawa,  Ping Tan
    * Abstract: This paper presents a scene agnostic neural architecture for camera localization, where model parameters and scenes are independent from each other.Despite recent advancement in learning based methods, most approaches require training for each scene one by one, not applicable for online applications such as SLAM and robotic navigation, where a model must be built on-the-fly.Our approach learns to build a hierarchical scene representation and predicts a dense scene coordinate map of a query RGB image on-the-fly given an arbitrary scene. The 6D camera pose of the query image can be estimated with the predicted scene coordinate map. Additionally, the dense prediction can be used for other online robotic and AR applications such as obstacle avoidance. We demonstrate the effectiveness and efficiency of our method on both indoor and outdoor benchmarks, achieving state-of-the-art performance.
count=9
* Localize Me Anywhere, Anytime: A Multi-Task Point-Retrieval Approach
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Lu_Localize_Me_Anywhere_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Lu_Localize_Me_Anywhere_ICCV_2015_paper.pdf)]
    * Title: Localize Me Anywhere, Anytime: A Multi-Task Point-Retrieval Approach
    * Year: `2015`
    * Authors: Guoyu Lu, Yan Yan, Li Ren, Jingkuan Song, Nicu Sebe, Chandra Kambhamettu
    * Abstract: Image-based localization is an essential complement to GPS localization. Current image-based localization methods are based on either 2D-to-3D or 3D-to-2D to find the correspondences, which ignore the real scene geometric attributes. The main contribution of our paper is that we use a 3D model reconstructed by a short video as the query to realize 3D-to-3D localization under a multi-task point retrieval framework. Firstly, the use of a 3D model as the query enables us to efficiently select location candidates. Furthermore, the reconstruction of 3D model exploits the correlations among different images, based on the fact that images captured from different views for SfM share information through matching features. By exploring shared information (matching features) across multiple related tasks (images of the same scene captured from different views), the visual feature's view-invariance property can be improved in order to get to a higher point retrieval accuracy. More specifically, we use multi-task point retrieval framework to explore the relationship between descriptors and the 3D points, which extracts the discriminant points for more accurate 3D-to-3D correspondences retrieval. We further apply multi-task learning (MTL) retrieval approach on thermal images to prove that our MTL retrieval framework also provides superior performance for the thermal domain. This application is exceptionally helpful to cope with the localization problem in an environment with limited light sources.
count=9
* The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/1403ab1a427050538ec59c7f570aec8b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/1403ab1a427050538ec59c7f570aec8b-Paper-Conference.pdf)]
    * Title: The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance
    * Year: `2023`
    * Authors: Jon Donnelly, Srikar Katta, Cynthia Rudin, Edward Browne
    * Abstract: Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We demonstrate through experiments that our framework recovers variable importance rankings for complex simulation setups where other methods fail. Further, we show that our framework accurately estimates the true importance of a variable for the underlying data distribution. We provide theoretical guarantees on the consistency and finite sample error rates for our estimator. Finally, we demonstrate its utility with a real-world case study exploring which genes are important for predicting HIV load in persons with HIV, highlighting an important gene that has not previously been studied in connection with HIV.
count=9
* Participatory Personalization in Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/2dbb8bfe4cd3875609b23799830ee865-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/2dbb8bfe4cd3875609b23799830ee865-Paper-Conference.pdf)]
    * Title: Participatory Personalization in Classification
    * Year: `2023`
    * Authors: Hailey Joren, Chirag Nagpal, Katherine A. Heller, Berk Ustun
    * Abstract: Machine learning models are often personalized based on information that is protected, sensitive, self-reported, or costly to acquire. These models use information about people, but do not facilitate nor inform their consent. Individuals cannot opt out of reporting information that a model needs to personalize their predictions nor tell if they benefit from personalization in the first place. We introduce a new family of prediction models, called participatory systems, that let individuals opt into personalization at prediction time. We present a model-agnostic algorithm to learn participatory systems for supervised learning tasks where models are personalized with categorical group attributes. We conduct a comprehensive empirical study of participatory systems in clinical prediction tasks, comparing them to common approaches for personalization and imputation. Our results show that participatory systems can facilitate and inform consent in a way that improves performance and privacy across all groups who report personal data.
count=9
* Gaussian Copula Embeddings
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/8ae260afda41b45ed77be58358a6c519-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/8ae260afda41b45ed77be58358a6c519-Paper-Conference.pdf)]
    * Title: Gaussian Copula Embeddings
    * Year: `2022`
    * Authors: Chien Lu, Jaakko Peltonen
    * Abstract: Learning latent vector representations via embedding models has been shown promising in machine learning. However, most of the embedding models are still limited to a single type of observation data. We propose a Gaussian copula embedding model to learn latent vector representations of items in a heterogeneous data setting. The proposed model can effectively incorporate different types of observed data and, at the same time, yield robust embeddings. We demonstrate the proposed model can effectively learn in many different scenarios, outperforming competing models in modeling quality and task performance.
count=9
* A Multilabel Classification Framework for Approximate Nearest Neighbor Search
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/e8752f3e51f33a2e06daf044c40ce412-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/e8752f3e51f33a2e06daf044c40ce412-Paper-Conference.pdf)]
    * Title: A Multilabel Classification Framework for Approximate Nearest Neighbor Search
    * Year: `2022`
    * Authors: Ville Hyvönen, Elias Jääsaari, Teemu Roos
    * Abstract: Both supervised and unsupervised machine learning algorithms have been used to learn partition-based index structures for approximate nearest neighbor (ANN) search. Existing supervised algorithms formulate the learning task as finding a partition in which the nearest neighbors of a training set point belong to the same partition element as the point itself, so that the nearest neighbor candidates can be retrieved by naive lookup or backtracking search. We formulate candidate set selection in ANN search directly as a multilabel classification problem where the labels correspond to the nearest neighbors of the query point, and interpret the partitions as partitioning classifiers for solving this task. Empirical results suggest that the natural classifier based on this interpretation leads to strictly improved performance when combined with any unsupervised or supervised partitioning strategy. We also prove a sufficient condition for consistency of a partitioning classifier for ANN search, and illustrate the result by verifying this condition for chronological $k$-d trees.
count=9
* Explaining Hyperparameter Optimization via Partial Dependence Plots
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/12ced2db6f0193dda91ba86224ea1cd8-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/12ced2db6f0193dda91ba86224ea1cd8-Paper.pdf)]
    * Title: Explaining Hyperparameter Optimization via Partial Dependence Plots
    * Year: `2021`
    * Authors: Julia Moosbauer, Julia Herbinger, Giuseppe Casalicchio, Marius Lindauer, Bernd Bischl
    * Abstract: Automated hyperparameter optimization (HPO) can support practitioners to obtain peak performance in machine learning models.However, there is often a lack of valuable insights into the effects of different hyperparameters on the final model performance.This lack of explainability makes it difficult to trust and understand the automated HPO process and its results.We suggest using interpretable machine learning (IML) to gain insights from the experimental data obtained during HPO with Bayesian optimization (BO).BO tends to focus on promising regions with potential high-performance configurations and thus induces a sampling bias.Hence, many IML techniques, such as the partial dependence plot (PDP), carry the risk of generating biased interpretations.By leveraging the posterior uncertainty of the BO surrogate model, we introduce a variant of the PDP with estimated confidence bands.We propose to partition the hyperparameter space to obtain more confident and reliable PDPs in relevant sub-regions.In an experimental study, we provide quantitative evidence for the increased quality of the PDPs within sub-regions.
count=9
* Regression with reject option and application to kNN
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/e8219d4c93f6c55c6b10fe6bfe997c6c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/e8219d4c93f6c55c6b10fe6bfe997c6c-Paper.pdf)]
    * Title: Regression with reject option and application to kNN
    * Year: `2020`
    * Authors: Ahmed Zaoui, Christophe Denis, Mohamed Hebiri
    * Abstract: We investigate the problem of regression where one is allowed to abstain from predicting. We refer to this framework as regression with reject option as an extension of classification with reject option. In this context, we focus on the case where the rejection rate is fixed and derive the optimal rule which relies on thresholding the conditional variance function. We provide a semi-supervised estimation procedure of the optimal rule involving two datasets: a first labeled dataset is used to estimate both regression function and conditional variance function while a second unlabeled dataset is exploited to calibrate the desired rejection rate. The resulting predictor with reject option is shown to be almost as good as the optimal predictor with reject option both in terms of risk and rejection rate. We additionally apply our methodology with kNN algorithm and establish rates of convergence for the resulting kNN predictor under mild conditions. Finally, a numerical study is performed to illustrate the benefit of using the proposed procedure.
count=9
* Sobolev Independence Criterion
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/79a3308b13cd31f096d8a4a34f96b66b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/79a3308b13cd31f096d8a4a34f96b66b-Paper.pdf)]
    * Title: Sobolev Independence Criterion
    * Year: `2019`
    * Authors: Youssef Mroueh, Tom Sercu, Mattia Rigotti, Inkit Padhi, Cicero Nogueira dos Santos
    * Abstract: We propose the Sobolev Independence Criterion (SIC), an interpretable dependency measure between a high dimensional random variable X and a response variable Y. SIC decomposes to the sum of feature importance scores and hence can be used for nonlinear feature selection. SIC can be seen as a gradient regularized Integral Probability Metric (IPM) between the joint distribution of the two random variables and the product of their marginals. We use sparsity inducing gradient penalties to promote input sparsity of the critic of the IPM. In the kernel version we show that SIC can be cast as a convex optimization problem by introducing auxiliary variables that play an important role in feature selection as they are normalized feature importance scores. We then present a neural version of SIC where the critic is parameterized as a homogeneous neural network, improving its representation power as well as its interpretability. We conduct experiments validating SIC for feature selection in synthetic and real-world experiments. We show that SIC enables reliable and interpretable discoveries, when used in conjunction with the holdout randomization test and knockoffs to control the False Discovery Rate. Code is available at http://github.com/ibm/sic.
count=9
* Pruning Random Forests for Prediction on a Budget
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/3948ead63a9f2944218de038d8934305-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/3948ead63a9f2944218de038d8934305-Paper.pdf)]
    * Title: Pruning Random Forests for Prediction on a Budget
    * Year: `2016`
    * Authors: Feng Nan, Joseph Wang, Venkatesh Saligrama
    * Abstract: We propose to prune a random forest (RF) for resource-constrained prediction. We first construct a RF and then prune it to optimize expected feature cost & accuracy. We pose pruning RFs as a novel 0-1 integer program with linear constraints that encourages feature re-use. We establish total unimodularity of the constraint set to prove that the corresponding LP relaxation solves the original integer program. We then exploit connections to combinatorial optimization and develop an efficient primal-dual algorithm, scalable to large datasets. In contrast to our bottom-up approach, which benefits from good RF initialization, conventional methods are top-down acquiring features based on their utility value and is generally intractable, requiring heuristics. Empirically, our pruning algorithm outperforms existing state-of-the-art resource-constrained algorithms.
count=8
* HyperDepth: Learning Depth From Structured Light Without Matching
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Fanello_HyperDepth_Learning_Depth_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Fanello_HyperDepth_Learning_Depth_CVPR_2016_paper.pdf)]
    * Title: HyperDepth: Learning Depth From Structured Light Without Matching
    * Year: `2016`
    * Authors: Sean Ryan Fanello, Christoph Rhemann, Vladimir Tankovich, Adarsh Kowdle, Sergio Orts Escolano, David Kim, Shahram Izadi
    * Abstract: Structured light sensors are popular due to their robustness to untextured scenes and multipath. These systems triangulate depth by solving a correspondence problem between each camera and projector pixel. This is often framed as a local stereo matching task, correlating patches of pixels in the observed and reference image. However, this is computationally intensive, leading to reduced depth accuracy and framerate. We contribute an algorithm for solving this correspondence problem efficiently, without compromising depth accuracy. For the first time, this problem is cast as a classification-regression task, which we solve extremely efficiently using an ensemble of cascaded random forests. Our algorithm scales in number of disparities, and each pixel can be processed independently, and in parallel. No matching or even access to the corresponding reference pattern is required at runtime, and regressed labels are directly mapped to depth. Our GPU-based algorithm runs at a 1KHz for 1.3MP input/output images, with disparity error of 0.1 subpixels. We show a prototype high framerate depth camera running at 375Hz, useful for solving tracking-related problems. We demonstrate our algorithmic performance, creating high resolution real-time depth maps that surpass the quality of current state of the art depth technologies, highlighting quantization-free results with reduced holes, edge fattening and other stereo-based depth artifacts.
count=8
* Flying Objects Detection From a Single Moving Camera
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Rozantsev_Flying_Objects_Detection_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Rozantsev_Flying_Objects_Detection_2015_CVPR_paper.pdf)]
    * Title: Flying Objects Detection From a Single Moving Camera
    * Year: `2015`
    * Authors: Artem Rozantsev, Vincent Lepetit, Pascal Fua
    * Abstract: We propose an approach to detect flying objects such as UAVs and aircrafts when they occupy a small portion of the field of view, possibly moving against complex backgrounds, and are filmed by a camera that itself moves. Solving such a difficult problem requires combining both appearance and motion cues. To this end we propose a regression-based approach to motion stabilization of local image patches that allows us to achieve effective classifica- tion on spatio-temporal image cubes and outperform state- of-the-art techniques. As the problem is relatively new, we collected two chal- lenging datasets for UAVs and Aircrafts, which can be used as benchmarks for flying objects detection and vision- guided collision avoidance.
count=8
* Discriminative Ferns Ensemble for Hand Pose Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Krupka_Discriminative_Ferns_Ensemble_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Krupka_Discriminative_Ferns_Ensemble_2014_CVPR_paper.pdf)]
    * Title: Discriminative Ferns Ensemble for Hand Pose Recognition
    * Year: `2014`
    * Authors: Eyal Krupka, Alon Vinnikov, Ben Klein, Aharon Bar Hillel, Daniel Freedman, Simon Stachniak
    * Abstract: We present the Discriminative Ferns Ensemble (DFE) classifier for efficient visual object recognition. The classifier architecture is designed to optimize both classification speed and accuracy when a large training set is available. Speed is obtained using simple binary features and direct indexing into a set of tables, and accuracy by using a large capacity model and careful discriminative optimization. The proposed framework is applied to the problem of hand pose recognition in depth and infra-red images, using a very large training set. Both the accuracy and the classification time obtained are considerably superior to relevant competing methods, allowing one to reach accuracy targets with run times orders of magnitude faster than the competition. We show empirically that using DFE, we can significantly reduce classification time by increasing training sample size for a fixed target accuracy. Finally a DFE result is shown for the MNIST dataset, showing the method's merit extends beyond depth images.
count=8
* Counterfactually Comparing Abstaining Classifiers
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/59fe467d5e71ba6b8d41bb3928da6f4c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/59fe467d5e71ba6b8d41bb3928da6f4c-Paper-Conference.pdf)]
    * Title: Counterfactually Comparing Abstaining Classifiers
    * Year: `2023`
    * Authors: Yo Joong Choe, Aditya Gangrade, Aaditya Ramdas
    * Abstract: Abstaining classifiers have the option to abstain from making predictions on inputs that they are unsure about. These classifiers are becoming increasingly popular in high-stakes decision-making problems, as they can withhold uncertain predictions to improve their reliability and safety. When evaluating black-box abstaining classifier(s), however, we lack a principled approach that accounts for what the classifier would have predicted on its abstentions. These missing predictions matter when they can eventually be utilized, either directly or as a backup option in a failure mode. In this paper, we introduce a novel approach and perspective to the problem of evaluating and comparing abstaining classifiers by treating abstentions as missing data. Our evaluation approach is centered around defining the counterfactual score of an abstaining classifier, defined as the expected performance of the classifier had it not been allowed to abstain. We specify the conditions under which the counterfactual score is identifiable: if the abstentions are stochastic, and if the evaluation data is independent of the training data (ensuring that the predictions are missing at random), then the score is identifiable. Note that, if abstentions are deterministic, then the score is unidentifiable because the classifier can perform arbitrarily poorly on its abstentions. Leveraging tools from observational causal inference, we then develop nonparametric and doubly robust methods to efficiently estimate this quantity under identification. Our approach is examined in both simulated and real data experiments.
count=8
* Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/8c2df4c35cdbee764ebb9e9d0acd5197-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/8c2df4c35cdbee764ebb9e9d0acd5197-Paper-Conference.pdf)]
    * Title: Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering
    * Year: `2023`
    * Authors: Noah Hollmann, Samuel Müller, Frank Hutter
    * Abstract: As the field of automated machine learning (AutoML) advances, it becomes increasingly important to incorporate domain knowledge into these systems.We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to iteratively generate additional semantically meaningful features for tabular datasets based on the description of the dataset. The method produces both Python code for creating new features and explanations for the utility of the generated features.Despite being methodologically simple, CAAFE improves performance on 11 out of 14 datasets -- boosting mean ROC AUC performance from 0.798 to 0.822 across all dataset - similar to the improvement achieved by using a random forest instead of logistic regression on our datasets. Furthermore, CAAFE is interpretable by providing a textual explanation for each generated feature.CAAFE paves the way for more extensive semi-automation in data science tasks and emphasizes the significance of context-aware solutions that can extend the scope of AutoML systems to semantic AutoML. We release our code, a simple demo and a python package.
count=8
* Metis: Understanding and Enhancing In-Network Regular Expressions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/f54bd48aba0dff7acdac86123188f1b6-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/f54bd48aba0dff7acdac86123188f1b6-Paper-Conference.pdf)]
    * Title: Metis: Understanding and Enhancing In-Network Regular Expressions
    * Year: `2023`
    * Authors: Zhengxin Zhang, Yucheng Huang, Guanglin Duan, Qing Li, Dan Zhao, Yong Jiang, Lianbo Ma, Xi Xiao, Hengyang Xu
    * Abstract: Regular expressions (REs) offer one-shot solutions for many networking tasks, e.g., network intrusion detection. However, REs purely rely on expert knowledge and cannot utilize labeled data for better accuracy. Today, neural networks (NNs) have shown superior accuracy and flexibility, thanks to their ability to learn from rich labeled data. Nevertheless, NNs are often incompetent in cold-start scenarios and too complex for deployment on network devices. In this paper, we propose Metis, a general framework that converts REs to network device affordable models for superior accuracy and throughput by taking advantage of REs' expert knowledge and NNs' learning ability. In Metis, we convert REs to byte-level recurrent neural networks (BRNNs) without training. The BRNNs preserve expert knowledge from REs and offer adequate accuracy in cold-start scenarios. When rich labeled data is available, the performance of BRNNs can be improved by training. Furthermore, we design a semi-supervised knowledge distillation to transform the BRNNs into pooling soft random forests (PSRFs) that can be deployed on network devices. To the best of our knowledge, this is the first method to employ model inference as an alternative to RE matching in network scenarios. We collect network traffic data on our campus for three weeks and evaluate Metis on them. Experimental results show that Metis is more accurate than original REs and other baselines, achieving superior throughput when deployed on network devices.
count=8
* Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted Majority Vote
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf)]
    * Title: Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted Majority Vote
    * Year: `2021`
    * Authors: Yi-Shan Wu, Andres Masegosa, Stephan Lorenzen, Christian Igel, Yevgeny Seldin
    * Abstract: We present a new second-order oracle bound for the expected risk of a weighted majority vote. The bound is based on a novel parametric form of the Chebyshev-Cantelli inequality (a.k.a. one-sided Chebyshev’s), which is amenable to efficient minimization. The new form resolves the optimization challenge faced by prior oracle bounds based on the Chebyshev-Cantelli inequality, the C-bounds [Germain et al., 2015], and, at the same time, it improves on the oracle bound based on second order Markov’s inequality introduced by Masegosa et al. [2020]. We also derive a new concentration of measure inequality, which we name PAC-Bayes-Bennett, since it combines PAC-Bayesian bounding with Bennett’s inequality. We use it for empirical estimation of the oracle bound. The PAC-Bayes-Bennett inequality improves on the PAC-Bayes-Bernstein inequality of Seldin et al. [2012]. We provide an empirical evaluation demonstrating that the new bounds can improve on the work of Masegosa et al. [2020]. Both the parametric form of the Chebyshev-Cantelli inequality and the PAC-Bayes-Bennett inequality may be of independent interest for the study of concentration of measure in other domains.
count=8
* Coresets for Decision Trees of Signals
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/fea9c11c4ad9a395a636ed944a28b51a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/fea9c11c4ad9a395a636ed944a28b51a-Paper.pdf)]
    * Title: Coresets for Decision Trees of Signals
    * Year: `2021`
    * Authors: Ibrahim Jubran, Ernesto Evgeniy Sanches Shayda, Ilan I Newman, Dan Feldman
    * Abstract: A $k$-decision tree $t$ (or $k$-tree) is a recursive partition of a matrix (2D-signal) into $k\geq 1$ block matrices (axis-parallel rectangles, leaves) where each rectangle is assigned a real label. Its regression or classification loss to a given matrix $D$ of $N$ entries (labels) is the sum of squared differences over every label in $D$ and its assigned label by $t$.Given an error parameter $\varepsilon\in(0,1)$, a $(k,\varepsilon)$-coreset $C$ of $D$ is a small summarization that provably approximates this loss to \emph{every} such tree, up to a multiplicative factor of $1\pm\varepsilon$. In particular, the optimal $k$-tree of $C$ is a $(1+\varepsilon)$-approximation to the optimal $k$-tree of $D$.We provide the first algorithm that outputs such a $(k,\varepsilon)$-coreset for \emph{every} such matrix $D$. The size $|C|$ of the coreset is polynomial in $k\log(N)/\varepsilon$, and its construction takes $O(Nk)$ time.This is by forging a link between decision trees from machine learning -- to partition trees in computational geometry. Experimental results on \texttt{sklearn} and \texttt{lightGBM} show that applying our coresets on real-world data-sets boosts the computation time of random forests and their parameter tuning by up to x$10$, while keeping similar accuracy. Full open source code is provided.
count=8
* Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/62d75fb2e3075506e8837d8f55021ab1-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/62d75fb2e3075506e8837d8f55021ab1-Paper.pdf)]
    * Title: Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation
    * Year: `2020`
    * Authors: Rasool Fakoor, Jonas W. Mueller, Nick Erickson, Pratik Chaudhari, Alexander J. Smola
    * Abstract: Automated machine learning (AutoML) can produce complex model ensembles by stacking, bagging, and boosting many individual models like trees, deep networks, and nearest neighbor estimators. While highly accurate, the resulting predictors are large, slow, and opaque as compared to their constituents. To improve the deployment of AutoML on tabular data, we propose FAST-DAD to distill arbitrarily-complex ensemble predictors into individual models like boosted trees, random forests, and deep networks. At the heart of our approach is a data augmentation strategy based on Gibbs sampling from a self-attention pseudolikelihood estimator. Across 30 datasets spanning regression and binary/multiclass classification tasks, FAST-DAD distillation produces significantly better individual models than one obtains through standard training on the original data. Our individual distilled models are over 10x faster and more accurate than ensemble predictors produced by AutoML tools like H2O/AutoSklearn.
count=8
* Fair regression via plug-in estimator and recalibration with statistical guarantees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/ddd808772c035aed516d42ad3559be5f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/ddd808772c035aed516d42ad3559be5f-Paper.pdf)]
    * Title: Fair regression via plug-in estimator and recalibration with statistical guarantees
    * Year: `2020`
    * Authors: Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, Massimiliano Pontil
    * Abstract: We study the problem of learning an optimal regression function subject to a fairness constraint. It requires that, conditionally on the sensitive feature, the distribution of the function output remains the same. This constraint naturally extends the notion of demographic parity, often used in classification, to the regression setting. We tackle this problem by leveraging on a proxy-discretized version, for which we derive an explicit expression of the optimal fair predictor. This result naturally suggests a two stage approach, in which we first estimate the (unconstrained) regression function from a set of labeled data and then we recalibrate it with another set of unlabeled data. The recalibration step can be efficiently performed via a smooth optimization. We derive rates of convergence of the proposed estimator to the optimal fair predictor both in terms of the risk and fairness constraint. Finally, we present numerical experiments illustrating that the proposed method is often superior or competitive with state-of-the-art methods.
count=8
* Efficient and Robust Automated Machine Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/11d0e6287202fced83f79975ec59a3a6-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf)]
    * Title: Efficient and Robust Automated Machine Learning
    * Year: `2015`
    * Authors: Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, Frank Hutter
    * Abstract: The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. In this work we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub auto-sklearn, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto-sklearn.
count=7
* Pushing the Envelope of Gradient Boosting Forests via Globally-Optimized Oblique Trees
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Gabidolla_Pushing_the_Envelope_of_Gradient_Boosting_Forests_via_Globally-Optimized_Oblique_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Gabidolla_Pushing_the_Envelope_of_Gradient_Boosting_Forests_via_Globally-Optimized_Oblique_CVPR_2022_paper.pdf)]
    * Title: Pushing the Envelope of Gradient Boosting Forests via Globally-Optimized Oblique Trees
    * Year: `2022`
    * Authors: Magzhan Gabidolla, Miguel Á. Carreira-Perpiñán
    * Abstract: Ensemble methods based on decision trees, such as Random Forests or boosted forests, have long been established as some of the most powerful, off-the-shelf machine learning models, and have been widely used in computer vision and other areas. In recent years, a specific form of boosting, gradient boosting (GB), has gained prominence. This is partly because of highly optimized implementations such as XGBoost or LightGBM, which incorporate many clever modifications and heuristics. However, one gaping hole remains unexplored in GB: the construction of individual trees. To date, all successful GB versions use axis-aligned trees trained in a suboptimal way via greedy recursive partitioning. We address this gap by using a more powerful type of trees (having hyperplane splits) and an algorithm that can optimize, globally over all the tree parameters, the objective function that GB dictates. We show, in several benchmarks of image and other data types, that GB forests of these stronger, well-optimized trees consistently exceed the test accuracy of axis-aligned forests from XGBoost, LightGBM and other strong baselines. Further, this happens using many fewer trees and sometimes even fewer parameters overall.
count=7
* Assessing Personally Perceived Image Quality via Image Features and Collaborative Filtering
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Korhonen_Assessing_Personally_Perceived_Image_Quality_via_Image_Features_and_Collaborative_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Korhonen_Assessing_Personally_Perceived_Image_Quality_via_Image_Features_and_Collaborative_CVPR_2019_paper.pdf)]
    * Title: Assessing Personally Perceived Image Quality via Image Features and Collaborative Filtering
    * Year: `2019`
    * Authors: Jari Korhonen
    * Abstract: During the past few years, different methods for optimizing the camera settings and post-processing techniques to improve the subjective quality of consumer photos have been studied extensively. However, most of the research in the prior art has focused on finding the optimal method for an average user. Since there is large deviation in personal opinions and aesthetic standards, the next challenge is to find the settings and post-processing techniques that fit to the individual users' personal taste. In this study, we aim to predict the personally perceived image quality by combining classical image feature analysis and collaboration filtering approach known from the recommendation systems. The experimental results for the proposed method show promising results. As a practical application, our work can be used for personalizing the camera settings or post-processing parameters for different users and images.
count=7
* Large Scale High-Resolution Land Cover Mapping With Multi-Resolution Data
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Robinson_Large_Scale_High-Resolution_Land_Cover_Mapping_With_Multi-Resolution_Data_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Robinson_Large_Scale_High-Resolution_Land_Cover_Mapping_With_Multi-Resolution_Data_CVPR_2019_paper.pdf)]
    * Title: Large Scale High-Resolution Land Cover Mapping With Multi-Resolution Data
    * Year: `2019`
    * Authors: Caleb Robinson,  Le Hou,  Kolya Malkin,  Rachel Soobitsky,  Jacob Czawlytko,  Bistra Dilkina,  Nebojsa Jojic
    * Abstract: In this paper we propose multi-resolution data fusion methods for deep learning-based high-resolution land cover mapping from aerial imagery. The land cover mapping problem, at country-level scales, is challenging for common deep learning methods due to the scarcity of high-resolution labels, as well as variation in geography and quality of input images. On the other hand, multiple satellite imagery and low-resolution ground truth label sources are widely available, and can be used to improve model training efforts. Our methods include: introducing low-resolution satellite data to smooth quality differences in high-resolution input, exploiting low-resolution labels with a dual loss function, and pairing scarce high-resolution labels with inputs from several points in time. We train models that are able to generalize from a portion of the Northeast United States, where we have high-resolution land cover labels, to the rest of the US. With these models, we produce the first high-resolution (1-meter) land cover map of the contiguous US, consisting of over 8 trillion pixels. We demonstrate the robustness and potential applications of this data in a case study with domain experts and develop a web application to share our results. This work is practically useful, and can be applied to other locations over the earth as high-resolution imagery becomes more widely available even as high-resolution labeled land cover data remains sparse.
count=7
* Learning Online Smooth Predictors for Realtime Camera Planning Using Recurrent Decision Trees
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Learning_Online_Smooth_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Learning_Online_Smooth_CVPR_2016_paper.pdf)]
    * Title: Learning Online Smooth Predictors for Realtime Camera Planning Using Recurrent Decision Trees
    * Year: `2016`
    * Authors: Jianhui Chen, Hoang M. Le, Peter Carr, Yisong Yue, James J. Little
    * Abstract: We study the problem of online prediction for realtime camera planning, where the goal is to predict smooth trajectories that correctly track and frame objects of interest (e.g., players in a basketball game). The conventional approach for training predictors does not directly consider temporal consistency, and often produces undesirable jitter. Although post-hoc smoothing (e.g., via a Kalman filter) can mitigate this issue to some degree, it is not ideal due to overly stringent modeling assumptions (e.g., Gaussian noise). We propose a recurrent decision tree framework that can directly incorporate temporal consistency into a data-driven predictor, as well as a learning algorithm that can efficiently learn such temporally smooth models. Our approach does not require any post-processing, making online smooth predictions much easier to generate when the noise model is unknown. We apply our approach to sports broadcasting: given noisy player detections, we learn where the camera should look based on human demonstrations. Our experiments exhibit significant improvements over conventional baselines and showcase the practicality of our approach.
count=7
* Contour Detection in Unstructured 3D Point Clouds
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Hackel_Contour_Detection_in_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Hackel_Contour_Detection_in_CVPR_2016_paper.pdf)]
    * Title: Contour Detection in Unstructured 3D Point Clouds
    * Year: `2016`
    * Authors: Timo Hackel, Jan D. Wegner, Konrad Schindler
    * Abstract: We describe a method to automatically detect contours, i.e. lines along which the surface orientation sharply changes, in large-scale outdoor point clouds. Contours are important intermediate features for structuring point clouds and converting them into high-quality surface or solid models, and are extensively used in graphics and mapping applications. Yet, detecting them in unstructured, inhomogeneous point clouds turns out to be surprisingly difficult, and existing line detection algorithms largely fail. We approach contour extraction as a two-stage discriminative learning problem. In the first stage, a contour score for each individual point is predicted with a binary classifier, using a set of features extracted from the point's neighborhood. The contour scores serve as a basis to construct an overcomplete graph of candidate contours. The second stage selects an optimal set of contours from the candidates. This amounts to a further binary classification in a higher-order MRF, whose cliques encode a preference for connected contours and penalize loose ends. The method can handle point clouds >10^7 points in a couple of minutes, and vastly outperforms a baseline that performs Canny-style edge detection on a range image representation of the point cloud.
count=7
* Multi-Forest Tracker: A Chameleon in Tracking
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Tan_Multi-Forest_Tracker_A_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Tan_Multi-Forest_Tracker_A_2014_CVPR_paper.pdf)]
    * Title: Multi-Forest Tracker: A Chameleon in Tracking
    * Year: `2014`
    * Authors: David J. Tan, Slobodan Ilic
    * Abstract: In this paper, we address the problem of object tracking in intensity images and depth data. We propose a generic framework that can be used either for tracking 2D templates in intensity images or for tracking 3D objects in depth images. To overcome problems like partial occlusions, strong illumination changes and motion blur, that notoriously make energy minimization-based tracking methods get trapped in a local minimum, we propose a learning-based method that is robust to all these problems. We use random forests to learn the relation between the parameters that defines the object's motion, and the changes they induce on the image intensities or the point cloud of the template. It follows that, to track the template when it moves, we use the changes on the image intensities or point cloud to predict the parameters of this motion. Our algorithm has an extremely fast tracking performance running at less than 2 ms per frame, and is robust to partial occlusions. Moreover, it demonstrates robustness to strong illumination changes when tracking templates using intensity images, and robustness in tracking 3D objects from arbitrary viewpoints even in the presence of motion blur that causes missing or erroneous data in depth images. Extensive experimental evaluation and comparison to the related approaches strongly demonstrates the benefits of our method.
count=7
* Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Lim_Sketch_Tokens_A_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Lim_Sketch_Tokens_A_2013_CVPR_paper.pdf)]
    * Title: Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection
    * Year: `2013`
    * Authors: Joseph J. Lim, C. L. Zitnick, Piotr Dollar
    * Abstract: We propose a novel approach to both learning and detecting local contour-based representations for mid-level features. Our features, called sketch tokens, are learned using supervised mid-level information in the form of hand drawn contours in images. Patches of human generated contours are clustered to form sketch token classes and a random forest classifier is used for efficient detection in novel images. We demonstrate our approach on both topdown and bottom-up tasks. We show state-of-the-art results on the top-down task of contour detection while being over 200x faster than competing methods. We also achieve large improvements in detection accuracy for the bottom-up tasks of pedestrian and object detection as measured on INRIA [5] and PASCAL [10], respectively. These gains are due to the complementary information provided by sketch tokens to low-level features such as gradient histograms.
count=7
* Joint Feature and Differentiable $ k $-NN Graph Learning using Dirichlet Energy
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/4d689f0f30199661a10aa2200488aebb-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/4d689f0f30199661a10aa2200488aebb-Paper-Conference.pdf)]
    * Title: Joint Feature and Differentiable $ k $-NN Graph Learning using Dirichlet Energy
    * Year: `2023`
    * Authors: Lei Xu, Lei Chen, Rong Wang, Feiping Nie, Xuelong Li
    * Abstract: Feature selection (FS) plays an important role in machine learning, which extracts important features and accelerates the learning process. In this paper, we propose a deep FS method that simultaneously conducts feature selection and differentiable $ k $-NN graph learning based on the Dirichlet Energy. The Dirichlet Energy identifies important features by measuring their smoothness on the graph structure, and facilitates the learning of a new graph that reflects the inherent structure in new feature subspace. We employ Optimal Transport theory to address the non-differentiability issue of learning $ k $-NN graphs in neural networks, which theoretically makes our method applicable to other graph neural networks for dynamic graph learning. Furthermore, the proposed framework is interpretable, since all modules are designed algorithmically. We validate the effectiveness of our model with extensive experiments on both synthetic and real-world datasets.
count=7
* Pareto Frontiers in Deep Feature Learning: Data, Compute, Width, and Luck
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/960573a3b797441aec39caa9f74bc793-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/960573a3b797441aec39caa9f74bc793-Paper-Conference.pdf)]
    * Title: Pareto Frontiers in Deep Feature Learning: Data, Compute, Width, and Luck
    * Year: `2023`
    * Authors: Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, Cyril Zhang
    * Abstract: In modern deep learning, algorithmic choices (such as width, depth, and learning rate) are known to modulate nuanced resource tradeoffs. This work investigates how these complexities necessarily arise for feature learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a multi-resource tradeoff frontier: successful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real problems requiring axis-aligned feature learning. We demonstrate improved sample efficiency on tabular classification benchmarks by using wide, sparsely-initialized MLP models; these networks sometimes outperform tuned random forests.
count=7
* Marginal Density Ratio for Off-Policy Evaluation in Contextual Bandits
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/a51f974947c42b40a40a882a7d9b2479-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/a51f974947c42b40a40a882a7d9b2479-Paper-Conference.pdf)]
    * Title: Marginal Density Ratio for Off-Policy Evaluation in Contextual Bandits
    * Year: `2023`
    * Authors: Muhammad Faaiz Taufiq, Arnaud Doucet, Rob Cornish, Jean-Francois Ton
    * Abstract: Off-Policy Evaluation (OPE) in contextual bandits is crucial for assessing new policies using existing data without costly experimentation. However, current OPE methods, such as Inverse Probability Weighting (IPW) and Doubly Robust (DR) estimators, suffer from high variance, particularly in cases of low overlap between target and behaviour policies or large action and context spaces. In this paper, we introduce a new OPE estimator for contextual bandits, the Marginal Ratio (MR) estimator, which focuses on the shift in the marginal distribution of outcomes $Y$ instead of the policies themselves. Through rigorous theoretical analysis, we demonstrate the benefits of the MR estimator compared to conventional methods like IPW and DR in terms of variance reduction. Additionally, we establish a connection between the MR estimator and the state-of-the-art Marginalized Inverse Propensity Score (MIPS) estimator, proving that MR achieves lower variance among a generalized family of MIPS estimators. We further illustrate the utility of the MR estimator in causal inference settings, where it exhibits enhanced performance in estimating Average Treatment Effects (ATE). Our experiments on synthetic and real-world datasets corroborate our theoretical findings and highlight the practical advantages of the MR estimator in OPE for contextual bandits.
count=7
* A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/aec5e2847c5ae90f939ab786774856cc-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/aec5e2847c5ae90f939ab786774856cc-Paper-Conference.pdf)]
    * Title: A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning
    * Year: `2023`
    * Authors: Alicia Curth, Alan Jeffares, Mihaela van der Schaar
    * Abstract: Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a _U-shaped curve_ reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count $p$ grows past sample size $n$ -- a phenomenon dubbed _double descent_. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include _linear regression, trees, and boosting_. In this work, we take a closer look at the evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of double descent truly extend the limits of a traditional U-shaped complexity-generalization curve therein. We show that once careful consideration is given to _what is being plotted_ on the x-axes of their double descent plots, it becomes apparent that there are implicitly multiple, distinct complexity axes along which the parameter count grows. We demonstrate that the second descent appears exactly (and _only_) when and where the transition between these underlying axes occurs, and that its location is thus _not_ inherently tied to the interpolation threshold $p=n$. We then gain further insight by adopting a classical nonparametric statistics perspective. We interpret the investigated methods as _smoothers_ and propose a generalized measure for the _effective_ number of parameters they use _on unseen examples_, using which we find that their apparent double descent curves do indeed fold back into more traditional convex shapes -- providing a resolution to the ostensible tension between double descent and traditional statistical intuition.
count=7
* RH-BrainFS: Regional Heterogeneous Multimodal Brain Networks Fusion Strategy
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/b9c353d02e565f0f7cba94c4f3584eaa-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/b9c353d02e565f0f7cba94c4f3584eaa-Paper-Conference.pdf)]
    * Title: RH-BrainFS: Regional Heterogeneous Multimodal Brain Networks Fusion Strategy
    * Year: `2023`
    * Authors: Hongting Ye, Yalu Zheng, Yueying Li, Ke Zhang, Youyong Kong, Yonggui Yuan
    * Abstract: Multimodal fusion has become an important research technique in neuroscience that completes downstream tasks by extracting complementary information from multiple modalities. Existing multimodal research on brain networks mainly focuses on two modalities, structural connectivity (SC) and functional connectivity (FC). Recently, extensive literature has shown that the relationship between SC and FC is complex and not a simple one-to-one mapping. The coupling of structure and function at the regional level is heterogeneous. However, all previous studies have neglected the modal regional heterogeneity between SC and FC and fused their representations via "simple patterns", which are inefficient ways of multimodal fusion and affect the overall performance of the model. In this paper, to alleviate the issue of regional heterogeneity of multimodal brain networks, we propose a novel Regional Heterogeneous multimodal Brain networks Fusion Strategy (RH-BrainFS). Briefly, we introduce a brain subgraph networks module to extract regional characteristics of brain networks, and further use a new transformer-based fusion bottleneck module to alleviate the issue of regional heterogeneity between SC and FC. To the best of our knowledge, this is the first paper to explicitly state the issue of structural-functional modal regional heterogeneity and to propose asolution. Extensive experiments demonstrate that the proposed method outperforms several state-of-the-art methods in a variety of neuroscience tasks.
count=7
* Feature Learning for Interpretable, Performant Decision Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d1b4076ae067dd23bad5ac2693547a01-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d1b4076ae067dd23bad5ac2693547a01-Paper-Conference.pdf)]
    * Title: Feature Learning for Interpretable, Performant Decision Trees
    * Year: `2023`
    * Authors: Jack Good, Torin Kovach, Kyle Miller, Artur Dubrawski
    * Abstract: Decision trees are regarded for high interpretability arising from their hierarchical partitioning structure built on simple decision rules. However, in practice, this is not realized because axis-aligned partitioning of realistic data results in deep trees, and because ensemble methods are used to mitigate overfitting. Even then, model complexity and performance remain sensitive to transformation of the input, and extensive expert crafting of features from the raw data is common. We propose the first system to alternate sparse feature learning with differentiable decision tree construction to produce small, interpretable trees with good performance. We benchmark against conventional tree-based models and demonstrate several notions of interpretation of a model and its predictions.
count=7
* Survival Permanental Processes for Survival Analysis with Time-Varying Covariates
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/e664650506f1cf2b4696df892147c06e-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/e664650506f1cf2b4696df892147c06e-Paper-Conference.pdf)]
    * Title: Survival Permanental Processes for Survival Analysis with Time-Varying Covariates
    * Year: `2023`
    * Authors: Hideaki Kim
    * Abstract: Survival or time-to-event data with time-varying covariates are common in practice, and exploring the non-stationarity in covariates is essential to accurately analyzing the nonlinear dependence of time-to-event outcomes on covariates. Traditional survival analysis methods such as Cox proportional hazards model have been extended to address the time-varying covariates through a counting process formulation, although sophisticated machine learning methods that can accommodate time-varying covariates have been limited. In this paper, we propose a non-parametric Bayesian survival model to analyze the nonlinear dependence of time-to-event outcomes on time-varying covariates. We focus on a computationally feasible Cox process called permanental process, which assumes the square root of hazard function to be generated from a Gaussian process, and tailor it for survival data with time-varying covariates. We verify that the proposed model holds with the representer theorem, a beneficial property for functional analysis, which offers us a fast Bayesian estimation algorithm that scales linearly with the number of observed events without relying on Markov Chain Monte Carlo computation. We evaluate our algorithm on synthetic and real-world data, and show that it achieves comparable predictive accuracy while being tens to hundreds of times faster than state-of-the-art methods.
count=7
* A General Framework for Auditing Differentially Private Machine Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/1add3bbdbc20c403a383482a665eb5a4-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/1add3bbdbc20c403a383482a665eb5a4-Paper-Conference.pdf)]
    * Title: A General Framework for Auditing Differentially Private Machine Learning
    * Year: `2022`
    * Authors: Fred Lu, Joseph Munoz, Maya Fuchs, Tyler LeBlond, Elliott Zaresky-Williams, Edward Raff, Francis Ferraro, Brian Testa
    * Abstract: We present a framework to statistically audit the privacy guarantee conferred by a differentially private machine learner in practice. While previous works have taken steps toward evaluating privacy loss through poisoning attacks or membership inference, they have been tailored to specific models or have demonstrated low statistical power. Our work develops a general methodology to empirically evaluate the privacy of differentially private machine learning implementations, combining improved privacy search and verification methods with a toolkit of influence-based poisoning attacks. We demonstrate significantly improved auditing power over previous approaches on a variety of models including logistic regression, Naive Bayes, and random forest. Our method can be used to detect privacy violations due to implementation errors or misuse. When violations are not present, it can aid in understanding the amount of information that can be leaked from a given dataset, algorithm, and privacy specification.
count=7
* Data Augmentation for Compositional Data: Advancing Predictive Models of the Microbiome
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/81a28be483155f802ddef448d6fc4b57-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/81a28be483155f802ddef448d6fc4b57-Paper-Conference.pdf)]
    * Title: Data Augmentation for Compositional Data: Advancing Predictive Models of the Microbiome
    * Year: `2022`
    * Authors: Elliott Gordon-Rodriguez, Thomas Quinn, John P. Cunningham
    * Abstract: Data augmentation plays a key role in modern machine learning pipelines. While numerous augmentation strategies have been studied in the context of computer vision and natural language processing, less is known for other data modalities. Our work extends the success of data augmentation to compositional data, i.e., simplex-valued data, which is of particular interest in microbiology, geochemistry, and other applications. Drawing on key principles from compositional data analysis, such as the \emph{Aitchison geometry of the simplex} and subcompositions, we define novel augmentation strategies for this data modality. Incorporating our data augmentations into standard supervised learning pipelines results in consistent performance gains across a wide range of standard benchmark datasets. In particular, we set a new state-of-the-art for key disease prediction tasks including colorectal cancer, type 2 diabetes, and Crohn's disease. In addition, our data augmentations enable us to define a novel contrastive learning model, which improves on previous representation learning approaches for microbiome compositional data.
count=7
* Assessing Fairness in the Presence of Missing Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/85dca1d270f7f9aef00c9d372f114482-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/85dca1d270f7f9aef00c9d372f114482-Paper.pdf)]
    * Title: Assessing Fairness in the Presence of Missing Data
    * Year: `2021`
    * Authors: Yiliang Zhang, Qi Long
    * Abstract: Missing data are prevalent and present daunting challenges in real data analysis. While there is a growing body of literature on fairness in analysis of fully observed data, there has been little theoretical work on investigating fairness in analysis of incomplete data. In practice, a popular analytical approach for dealing with missing data is to use only the set of complete cases, i.e., observations with all features fully observed to train a prediction algorithm. However, depending on the missing data mechanism, the distribution of complete cases and the distribution of the complete data may be substantially different. When the goal is to develop a fair algorithm in the complete data domain where there are no missing values, an algorithm that is fair in the complete case domain may show disproportionate bias towards some marginalized groups in the complete data domain. To fill this significant gap, we study the problem of estimating fairness in the complete data domain for an arbitrary model evaluated merely using complete cases. We provide upper and lower bounds on the fairness estimation error and conduct numerical experiments to assess our theoretical results. Our work provides the first known theoretical results on fairness guarantee in analysis of incomplete data.
count=7
* Counterfactual Predictions under Runtime Confounding
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/2b64c2f19d868305aa8bbc2d72902cc5-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf)]
    * Title: Counterfactual Predictions under Runtime Confounding
    * Year: `2020`
    * Authors: Amanda Coston, Edward Kennedy, Alexandra Chouldechova
    * Abstract: Algorithms are commonly used to predict outcomes under a particular decision or intervention, such as predicting likelihood of default if a loan is approved. Generally, to learn such counterfactual prediction models from observational data on historical decisions and corresponding outcomes, one must measure all factors that jointly affect the outcome and the decision taken. Motivated by decision support applications, we study the counterfactual prediction task in the setting where all relevant factors are captured in the historical data, but it is infeasible, undesirable, or impermissible to use some such factors in the prediction model. We refer to this setting as runtime confounding. We propose a doubly-robust procedure for learning counterfactual prediction models in this setting. Our theoretical analysis and experimental results suggest that our method often outperforms competing approaches. We also present a validation procedure for evaluating the performance of counterfactual prediction methods.
count=7
* MonoForest framework for tree ensemble analysis
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/1b9a80606d74d3da6db2f1274557e644-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/1b9a80606d74d3da6db2f1274557e644-Paper.pdf)]
    * Title: MonoForest framework for tree ensemble analysis
    * Year: `2019`
    * Authors: Igor Kuralenok, Vasilii Ershov, Igor Labutin
    * Abstract: In this work, we introduce a new decision tree ensemble representation framework: instead of using a graph model we transform each tree into a well-known polynomial form. We apply the new representation to three tasks: theoretical analysis, model reduction, and interpretation. The polynomial form of a tree ensemble allows a straightforward interpretation of the original model. In our experiments, it shows comparable results with state-of-the-art interpretation techniques. Another application of the framework is the ensemble-wise pruning: we can drop monomials from the polynomial, based on train data statistics. This way we reduce the model size up to 3 times without loss of its quality. It is possible to show the equivalence of tree shape classes that share the same polynomial. This fact gives us the ability to train a model in one tree's shape and exploit it in another, which is easier for computation or interpretation. We formulate a problem statement for optimal tree ensemble translation from one form to another and build a greedy solution to this problem.
count=7
* Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/ba51e6158bcaf80fd0d834950251e693-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/ba51e6158bcaf80fd0d834950251e693-Paper.pdf)]
    * Title: Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification
    * Year: `2019`
    * Authors: Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, Massimiliano Pontil
    * Abstract: We study the problem of fair binary classification using the notion of Equal Opportunity. It requires the true positive rate to distribute equally across the sensitive groups. Within this setting we show that the fair optimal classifier is obtained by recalibrating the Bayes classifier by a group-dependent threshold. We provide a constructive expression for the threshold. This result motivates us to devise a plug-in classification procedure based on both unlabeled and labeled datasets. While the latter is used to learn the output conditional probability, the former is used for calibration. The overall procedure can be computed in polynomial time and it is shown to be statistically consistent both in terms of the classification error and fairness measure. Finally, we present numerical experiments which indicate that our method is often superior or competitive with the state-of-the-art methods on benchmark datasets.
count=6
* Transition Forests: Learning Discriminative Temporal Transitions for Action Recognition and Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Garcia-Hernando_Transition_Forests_Learning_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Garcia-Hernando_Transition_Forests_Learning_CVPR_2017_paper.pdf)]
    * Title: Transition Forests: Learning Discriminative Temporal Transitions for Action Recognition and Detection
    * Year: `2017`
    * Authors: Guillermo Garcia-Hernando, Tae-Kyun Kim
    * Abstract: A human action can be seen as transitions between one's body poses over time, where the transition depicts a temporal relation between two poses. Recognizing actions thus involves learning a classifier sensitive to these pose transitions as well as to static poses. In this paper, we introduce a novel method called transitions forests, an ensemble of decision trees that both learn to discriminate static poses and transitions between pairs of two independent frames. During training, node splitting is driven by alternating two criteria: the standard classification objective that maximizes the discrimination power in individual frames, and the proposed one in pairwise frame transitions. Growing the trees tends to group frames that have similar associated transitions and share same action label incorporating temporal information that was not available otherwise. Unlike conventional decision trees where the best split in a node is determined independently of other nodes, the transition forests try to find the best split of nodes jointly (within a layer) for incorporating distant node transitions. When inferring the class label of a new frame, it is passed down the trees and the prediction is made based on previous frame predictions and the current one in an efficient and online manner. We apply our method on varied skeleton action recognition and online detection datasets showing its suitability over several baselines and state-of-the-art approaches.
count=6
* Monocular Depth Estimation Using Neural Regression Forest
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Roy_Monocular_Depth_Estimation_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Roy_Monocular_Depth_Estimation_CVPR_2016_paper.pdf)]
    * Title: Monocular Depth Estimation Using Neural Regression Forest
    * Year: `2016`
    * Authors: Anirban Roy, Sinisa Todorovic
    * Abstract: This paper presents a novel deep architecture, called neural regression forest (NRF), for depth estimation from a single image. NRF combines random forests and convolutional neural networks (CNNs). Scanning windows extracted from the image represent samples which are passed down the trees of NRF for predicting their depth. At every tree node, the sample is filtered with a CNN associated with that node. Results of the convolutional filtering are passed to left and right children nodes, i.e., corresponding CNNs, with a Bernoulli probability, until the leaves, where depth estimations are made. CNNs at every node are designed to have fewer parameters than seen in recent work, but their stacked processing along a path in the tree effectively amounts to a deeper CNN. NRF allows for parallelizable training of all "shallow" CNNs, and efficient enforcing of smoothness in depth estimation results. Our evaluation on the benchmark Make3D and NYUv2 datasets demonstrates that NRF outperforms the state of the art, and gracefully handles gradually decreasing training datasets.
count=6
* A Text Detection System for Natural Scenes With Convolutional Feature Learning and Cascaded Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_A_Text_Detection_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_A_Text_Detection_CVPR_2016_paper.pdf)]
    * Title: A Text Detection System for Natural Scenes With Convolutional Feature Learning and Cascaded Classification
    * Year: `2016`
    * Authors: Siyu Zhu, Richard Zanibbi
    * Abstract: We propose a system that finds text in natural scenes using a variety of cues. Our novel data-driven method incorporates coarse-to-fine detection of character pixels using convolutional features (Text-Conv), followed by extracting connected components (CCs) from characters using edge and color features, and finally performing a graph-based segmentation of CCs into words (Word-Graph). For Text-Conv, the initial detection is based on convolutional feature maps similar to those used in Convolutional Neural Networks (CNNs), but learned using Convolutional k-means. Convolution masks defined by local and neighboring patch features are used to improve detection accuracy. The Word-Graph algorithm uses contextual information to both improve word segmentation and prune false character/word detections. Different definitions for foreground (text) regions are used to train the detection stages, some based on bounding box intersection, and others on bounding box and pixel intersection. Our system obtains pixel, character, and word detection f-measures of 93.14%, 90.26%, and 86.77% respectively for the ICDAR 2015 Robust Reading Focused Scene Text dataset, out-performing state-of-the-art systems. This approach may work for other detection targets with homogenous color in natural scenes.
count=6
* Fast 2D Border Ownership Assignment
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Teo_Fast_2D_Border_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Teo_Fast_2D_Border_2015_CVPR_paper.pdf)]
    * Title: Fast 2D Border Ownership Assignment
    * Year: `2015`
    * Authors: Ching Teo, Cornelia Fermuller, Yiannis Aloimonos
    * Abstract: A method for efficient border ownership assignment in 2D images is proposed. Leveraging on recent advances using Structured Random Forests (SRF) for boundary detection, we impose a novel border ownership structure that detects both boundaries and border ownership at the same time. Key to this work are features that predict ownership cues from 2D images. To this end, we use several different local cues: shape, spectral properties of boundary patches, and semi-global grouping cues that are indicative of perceived depth. For shape, we use HoG-like descriptors that encode local curvature (convexity and concavity). For spectral properties, such as extremal edges, we first learn an orthonormal basis spanned by the top K eigenvectors via PCA over common types of contour tokens. For grouping, we introduce a novel mid-level descriptor that captures patterns near edges and indicates ownership information of the boundary. Experimental results over a subset of the Berkeley Segmentation Dataset (BSDS) and the NYU Depth V2 dataset show that our method's performance exceeds current state-of-the-art multi-stage approaches that use more complex features.
count=6
* Iterated Second-Order Label Sensitive Pooling for 3D Human Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Ionescu_Iterated_Second-Order_Label_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Ionescu_Iterated_Second-Order_Label_2014_CVPR_paper.pdf)]
    * Title: Iterated Second-Order Label Sensitive Pooling for 3D Human Pose Estimation
    * Year: `2014`
    * Authors: Catalin Ionescu, Joao Carreira, Cristian Sminchisescu
    * Abstract: Recently, the emergence of Kinect systems has demonstrated the benefits of predicting an intermediate body part labeling for 3D human pose estimation, in conjunction with RGB-D imagery. The availability of depth information plays a critical role, so an important question is whether a similar representation can be developed with sufficient robustness in order to estimate 3D pose from RGB images. This paper provides evidence for a positive answer, by leveraging (a) 2D human body part labeling in images, (b) second-order label-sensitive pooling over dynamically computed regions resulting from a hierarchical decomposition of the body, and (c) iterative structured-output modeling to contextualize the process based on 3D pose estimates. For robustness and generalization, we take advantage of a recent large-scale 3D human motion capture dataset, Human3.6M [18] that also has human body part labeling annotations available with images. We provide extensive experimental studies where alternative intermediate representations are compared and report a substantial 33% error reduction over competitive discriminative baselines that regress 3D human pose against global HOG features.
count=6
* Weakly Supervised Multiclass Video Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Liu_Weakly_Supervised_Multiclass_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Liu_Weakly_Supervised_Multiclass_2014_CVPR_paper.pdf)]
    * Title: Weakly Supervised Multiclass Video Segmentation
    * Year: `2014`
    * Authors: Xiao Liu, Dacheng Tao, Mingli Song, Ying Ruan, Chun Chen, Jiajun Bu
    * Abstract: The desire of enabling computers to learn semantic concepts from large quantities of Internet videos has motivated increasing interests on semantic video understanding, while video segmentation is important yet challenging for understanding videos. The main difficulty of video segmentation arises from the burden of labeling training samples, making the problem largely unsolved. In this paper, we present a novel nearest neighbor-based label transfer scheme for weakly supervised video segmentation. Whereas previous weakly supervised video segmentation methods have been limited to the two-class case, our proposed scheme focuses on more challenging multiclass video segmentation, which finds a semantically meaningful label for every pixel in a video. Our scheme enjoys several favorable properties when compared with conventional methods. First, a weakly supervised hashing procedure is carried out to handle both metric and semantic similarity. Second, the proposed nearest neighbor-based label transfer algorithm effectively avoids overfitting caused by weakly supervised data. Third, a multi-video graph model is built to encourage smoothness between regions that are spatiotemporally adjacent and similar in appearance. We demonstrate the effectiveness of the proposed scheme by comparing it with several other state-of-the-art weakly supervised segmentation methods on one new Wild8 dataset and two other publicly available datasets.
count=6
* Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Wanner_Globally_Consistent_Multi-label_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Wanner_Globally_Consistent_Multi-label_2013_CVPR_paper.pdf)]
    * Title: Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields
    * Year: `2013`
    * Authors: Sven Wanner, Christoph Straehle, Bastian Goldluecke
    * Abstract: We present the first variational framework for multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, features need to be extracted from the 2D projection of a three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.
count=6
* Conditioned Regression Models for Non-Blind Single Image Super-Resolution
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Riegler_Conditioned_Regression_Models_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Riegler_Conditioned_Regression_Models_ICCV_2015_paper.pdf)]
    * Title: Conditioned Regression Models for Non-Blind Single Image Super-Resolution
    * Year: `2015`
    * Authors: Gernot Riegler, Samuel Schulter, Matthias Ruther, Horst Bischof
    * Abstract: Single image super-resolution is an important task in the field of computer vision and finds many practical applications. Current state-of-the-art methods typically rely on machine learning algorithms to infer a mapping from low- to high-resolution images. These methods use a single fixed blur kernel during training and, consequently, assume the exact same kernel underlying the image formation process for all test images. However, this setting is not realistic for practical applications, because the blur is typically different for each test image. In this paper, we loosen this restrictive constraint and propose conditioned regression models (including convolutional neural networks and random forests) that can effectively exploit the additional kernel information during both, training and inference. This allows for training a single model, while previous methods need to be re-trained for every blur kernel individually to achieve good results, which we demonstrate in our evaluations. We also empirically show that the proposed conditioned regression models (i) can effectively handle scenarios where the blur kernel is different for each image and (ii) outperform related approaches trained for only a single kernel.
count=6
* Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/22a25fc3da528794d52664dacc7bd470-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/22a25fc3da528794d52664dacc7bd470-Paper-Conference.pdf)]
    * Title: Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings
    * Year: `2023`
    * Authors: Klim Kireev, Maksym Andriushchenko, Carmela Troncoso, Nicolas Flammarion
    * Abstract: Research on adversarial robustness is primarily focused on image and text data. Yet, many scenarios in which lack of robustness can result in serious risks, such as fraud detection, medical diagnosis, or recommender systems often do not rely on images or text but instead on tabular data. Adversarial robustness in tabular data poses two serious challenges. First, tabular datasets often contain categorical features, and therefore cannot be tackled directly with existing optimization procedures. Second, in the tabular domain, algorithms that are not based on deep networks are widely used and offer great performance, but algorithms to enhance robustness are tailored to neural networks (e.g. adversarial training).In this paper, we tackle both challenges. We present a method that allows us to train adversarially robust deep networks for tabular data and to transfer this robustness to other classifiers via universal robust embeddings tailored to categorical data. These embeddings, created using a bilevel alternating minimization framework, can be transferred to boosted trees or random forests making them robust without the need for adversarial training while preserving their high accuracy on tabular data. We show that our methods outperform existing techniques within a practical threat model suitable for tabular data.
count=6
* Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/8f61049e8fe5b9ed714860b951066f1e-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/8f61049e8fe5b9ed714860b951066f1e-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine
    * Year: `2023`
    * Authors: Emma Chen, Aman Kansal, Julie Chen, Boyang Tom Jin, Julia Reisler, David E. Kim, Pranav Rajpurkar
    * Abstract: We propose the Multimodal Clinical Benchmark for Emergency Care (MC-BEC), a comprehensive benchmark for evaluating foundation models in Emergency Medicine using a dataset of 100K+ continuously monitored Emergency Department visits from 2020-2022. MC-BEC focuses on clinically relevant prediction tasks at timescales from minutes to days, including predicting patient decompensation, disposition, and emergency department (ED) revisit, and includes a standardized evaluation framework with train-test splits and evaluation metrics. The multimodal dataset includes a wide range of detailed clinical data, including triage information, prior diagnoses and medications, continuously measured vital signs, electrocardiogram and photoplethysmograph waveforms, orders placed and medications administered throughout the visit, free-text reports of imaging studies, and information on ED diagnosis, disposition, and subsequent revisits. We provide performance baselines for each prediction task to enable the evaluation of multimodal, multitask models. We believe that MC-BEC will encourage researchers to develop more effective, generalizable, and accessible foundation models for multimodal clinical data.
count=6
* Explaining the Uncertain: Stochastic Shapley Values for Gaussian Process Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/9f0b1220028dfa2ee82ca0a0e0fc52d1-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/9f0b1220028dfa2ee82ca0a0e0fc52d1-Paper-Conference.pdf)]
    * Title: Explaining the Uncertain: Stochastic Shapley Values for Gaussian Process Models
    * Year: `2023`
    * Authors: Siu Lun Chau, Krikamol Muandet, Dino Sejdinovic
    * Abstract: We present a novel approach for explaining Gaussian processes (GPs) that can utilize the full analytical covariance structure present in GPs. Our method is based on the popular solution concept of Shapley values extended to stochastic cooperative games, resulting in explanations that are random variables. The GP explanations generated using our approach satisfy similar favorable axioms to standard Shapley values and possess a tractable covariance function across features and data observations. This covariance allows for quantifying explanation uncertainties and studying the statistical dependencies between explanations. We further extend our framework to the problem of predictive explanation, and propose a Shapley prior over the explanation function to predict Shapley values for new data based on previously computed ones. Our extensive illustrations demonstrate the effectiveness of the proposed approach.
count=6
* Why do tree-based models still outperform deep learning on typical tabular data?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/0378c7692da36807bdec87ab043cdadc-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/0378c7692da36807bdec87ab043cdadc-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Why do tree-based models still outperform deep learning on typical tabular data?
    * Year: `2022`
    * Authors: Leo Grinsztajn, Edouard Oyallon, Gael Varoquaux
    * Abstract: While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data ($\sim$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and neural networks. This leads to a series of challenges which should guide researchers aiming to build tabular-specific neural network: 1) be robust to uninformative features, 2) preserve the orientation of the data, and 3) be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20\,000 compute hours hyperparameter search for each learner.
count=6
* Debiased Causal Tree: Heterogeneous Treatment Effects Estimation with Unmeasured Confounding
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/2526d439030a3af95fc647dd20e9d049-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/2526d439030a3af95fc647dd20e9d049-Paper-Conference.pdf)]
    * Title: Debiased Causal Tree: Heterogeneous Treatment Effects Estimation with Unmeasured Confounding
    * Year: `2022`
    * Authors: Caizhi Tang, Huiyuan Wang, Xinyu Li, Qing Cui, Ya-Lin Zhang, Feng Zhu, Longfei Li, Jun Zhou, Linbo Jiang
    * Abstract: Unmeasured confounding poses a significant threat to the validity of causal inference. Despite that various ad hoc methods are developed to remove confounding effects, they are subject to certain fairly strong assumptions. In this work, we consider the estimation of conditional causal effects in the presence of unmeasured confounding using observational data and historical controls. Under an interpretable transportability condition, we prove the partial identifiability of conditional average treatment effect on the treated group (CATT). For tree-based models, a new notion, \emph{confounding entropy}, is proposed to measure the discrepancy introduced by unobserved confounders between the conditional outcome distribution of the treated and control groups. The confounding entropy generalizes conventional confounding bias, and can be estimated effectively using historical controls. We develop a new method, debiased causal tree, whose splitting rule is to minimize the empirical risk regularized by the confounding entropy. Notably, our method integrates current observational data (for empirical risk) and their historical controls (for confounding entropy) harmoniously. We highlight that, debiased causal tree can not only estimate CATT well in the presence of unmeasured confounding, but also is a robust estimator of conditional average treatment effect (CATE) against the imbalance of the treated and control populations when all confounders are observed. An extension of combining multiple debiased causal trees to further reduce biases by gradient boosting is considered. The computational feasibility and statistical power of our method are evidenced by simulations and a study of a credit card balance dataset.
count=6
* Benchmarking Heterogeneous Treatment Effect Models through the Lens of Interpretability
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/4fd7b4ed13f78b9ba7afcd9d01615896-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/4fd7b4ed13f78b9ba7afcd9d01615896-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Benchmarking Heterogeneous Treatment Effect Models through the Lens of Interpretability
    * Year: `2022`
    * Authors: Jonathan Crabbé, Alicia Curth, Ioana Bica, Mihaela van der Schaar
    * Abstract: Estimating personalized effects of treatments is a complex, yet pervasive problem. To tackle it, recent developments in the machine learning (ML) literature on heterogeneous treatment effect estimation gave rise to many sophisticated, but opaque, tools: due to their flexibility, modularity and ability to learn constrained representations, neural networks in particular have become central to this literature. Unfortunately, the assets of such black boxes come at a cost: models typically involve countless nontrivial operations, making it difficult to understand what they have learned. Yet, understanding these models can be crucial -- in a medical context, for example, discovered knowledge on treatment effect heterogeneity could inform treatment prescription in clinical practice. In this work, we therefore use post-hoc feature importance methods to identify features that influence the model's predictions. This allows us to evaluate treatment effect estimators along a new and important dimension that has been overlooked in previous work: We construct a benchmarking environment to empirically investigate the ability of personalized treatment effect models to identify predictive covariates -- covariates that determine differential responses to treatment. Our benchmarking environment then enables us to provide new insight into the strengths and weaknesses of different types of treatment effects models as we modulate different challenges specific to treatment effect estimation -- e.g. the ratio of prognostic to predictive information, the possible nonlinearity of potential outcomes and the presence and type of confounding.
count=6
* Exploring the Whole Rashomon Set of Sparse Decision Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/5afaa8b4dd18eb1eed055d2d821b58ae-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/5afaa8b4dd18eb1eed055d2d821b58ae-Paper-Conference.pdf)]
    * Title: Exploring the Whole Rashomon Set of Sparse Decision Trees
    * Year: `2022`
    * Authors: Rui Xin, Chudi Zhong, Zhi Chen, Takuya Takagi, Margo Seltzer, Cynthia Rudin
    * Abstract: In any given machine learning problem, there may be many models that could explain the data almost equally well. However, most learning algorithms return only one of these models, leaving practitioners with no practical way to explore alternative models that might have desirable properties beyond what could be expressed within a loss function. The Rashomon set is the set of these all almost-optimal models. Rashomon sets can be extremely complicated, particularly for highly nonlinear function classes that allow complex interaction terms, such as decision trees. We provide the first technique for completely enumerating the Rashomon set for sparse decision trees; in fact, our work provides the first complete enumeration of any Rashomon set for a non-trivial problem with a highly nonlinear discrete function class. This allows the user an unprecedented level of control over model choice among all models that are approximately equally good. We represent the Rashomon set in a specialized data structure that supports efficient querying and sampling. We show three applications of the Rashomon set: 1) it can be used to study variable importance for the set of almost-optimal trees (as opposed to a single tree), 2) the Rashomon set for accuracy enables enumeration of the Rashomon sets for balanced accuracy and F1-score, and 3) the Rashomon set for a full dataset can be used to produce Rashomon sets constructed with only subsets of the data set. Thus, we are able to examine Rashomon sets across problems with a new lens, enabling users to choose models rather than be at the mercy of an algorithm that produces only a single model.
count=6
* Tree ensemble kernels for Bayesian optimization with known constraints over  mixed-feature spaces
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/f3398b76d17792893ce6d4f660546353-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/f3398b76d17792893ce6d4f660546353-Paper-Conference.pdf)]
    * Title: Tree ensemble kernels for Bayesian optimization with known constraints over  mixed-feature spaces
    * Year: `2022`
    * Authors: Alexander Thebelt, Calvin Tsay, Robert Lee, Nathan Sudermann-Merx, David Walz, Behrang Shafei, Ruth Misener
    * Abstract: Tree ensembles can be well-suited for black-box optimization tasks such as algorithm tuning and neural architecture search, as they achieve good predictive performance with little or no manual tuning, naturally handle discrete feature spaces, and are relatively insensitive to outliers in the training data. Two well-known challenges in using tree ensembles for black-box optimization are (i) effectively quantifying model uncertainty for exploration and (ii) optimizing over the piece-wise constant acquisition function. To address both points simultaneously, we propose using the kernel interpretation of tree ensembles as a Gaussian Process prior to obtain model variance estimates, and we develop a compatible optimization formulation for the acquisition function. The latter further allows us to seamlessly integrate known constraints to improve sampling efficiency by considering domain-knowledge in engineering settings and modeling search space symmetries, e.g., hierarchical relationships in neural architecture search. Our framework performs as well as state-of-the-art methods for unconstrained black-box optimization over continuous/discrete features and outperforms competing methods for problems combining mixed-variable feature spaces and known input constraints.
count=6
* Classification with Valid and Adaptive Coverage
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/244edd7e85dc81602b7615cd705545f5-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/244edd7e85dc81602b7615cd705545f5-Paper.pdf)]
    * Title: Classification with Valid and Adaptive Coverage
    * Year: `2020`
    * Authors: Yaniv Romano, Matteo Sesia, Emmanuel Candes
    * Abstract: Conformal inference, cross-validation+, and the jackknife+ are hold-out methods that can be combined with virtually any machine learning algorithm to construct prediction sets with guaranteed marginal coverage. In this paper, we develop specialized versions of these techniques for categorical and unordered response labels that, in addition to providing marginal coverage, are also fully adaptive to complex data distributions, in the sense that they perform favorably in terms of approximate conditional coverage compared to alternative methods. The heart of our contribution is a novel conformity score, which we explicitly demonstrate to be powerful and intuitive for classification problems, but whose underlying principle is potentially far more general. Experiments on synthetic and real data demonstrate the practical value of our theoretical guarantees, as well as the statistical advantages of the proposed methods over the existing alternatives.
count=6
* Detecting Interactions from Neural Networks via Topological Analysis
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/473803f0f2ebd77d83ee60daaa61f381-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/473803f0f2ebd77d83ee60daaa61f381-Paper.pdf)]
    * Title: Detecting Interactions from Neural Networks via Topological Analysis
    * Year: `2020`
    * Authors: Zirui Liu, Qingquan Song, Kaixiong Zhou, Ting-Hsiang Wang, Ying Shan, Xia Hu
    * Abstract: Detecting statistical interactions between input features is a crucial and challenging task. Recent advances demonstrate that it is possible to extract learned interactions from trained neural networks. It has also been observed that, in neural networks, any interacting features must follow a strongly weighted connection to common hidden units. Motivated by the observation, in this paper, we propose to investigate the interaction detection problem from a novel topological perspective by analyzing the connectivity in neural networks. Specially, we propose a new measure for quantifying interaction strength, based upon the well-received theory of persistent homology. Based on this measure, a Persistence Interaction Dection (PID) algorithm is developed to efficiently detect interactions. Our proposed algorithm is evaluated across a number of interaction detection tasks on several synthetic and real-world datasets with different hyperparameters. Experimental results validate that the PID algorithm outperforms the state-of-the-art baselines.
count=6
* Symmetry-Based Disentangled Representation Learning requires Interaction with Environments
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/36e729ec173b94133d8fa552e4029f8b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/36e729ec173b94133d8fa552e4029f8b-Paper.pdf)]
    * Title: Symmetry-Based Disentangled Representation Learning requires Interaction with Environments
    * Year: `2019`
    * Authors: Hugo Caselles-Dupré, Michael Garcia Ortiz, David Filliat
    * Abstract: Finding a generally accepted formal definition of a disentangled representation in the context of an agent behaving in an environment is an important challenge towards the construction of data-efficient autonomous agents. Higgins et al. recently proposed Symmetry-Based Disentangled Representation Learning, a definition based on a characterization of symmetries in the environment using group theory. We build on their work and make observations, theoretical and empirical, that lead us to argue that Symmetry-Based Disentangled Representation Learning cannot only be based on static observations: agents should interact with the environment to discover its symmetries. Our experiments can be reproduced in Colab and the code is available on GitHub.
count=6
* Information Constraints on Auto-Encoding Variational Bayes
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/9a96a2c73c0d477ff2a6da3bf538f4f4-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/9a96a2c73c0d477ff2a6da3bf538f4f4-Paper.pdf)]
    * Title: Information Constraints on Auto-Encoding Variational Bayes
    * Year: `2018`
    * Authors: Romain Lopez, Jeffrey Regier, Michael I. Jordan, Nir Yosef
    * Abstract: Parameterizing the approximate posterior of a generative model with neural networks has become a common theme in recent machine learning research. While providing appealing flexibility, this approach makes it difficult to impose or assess structural constraints such as conditional independence. We propose a framework for learning representations that relies on Auto-Encoding Variational Bayes and whose search space is constrained via kernel-based measures of independence. In particular, our method employs the $d$-variable Hilbert-Schmidt Independence Criterion (dHSIC) to enforce independence between the latent representations and arbitrary nuisance factors. We show how to apply this method to a range of problems, including the problems of learning invariant representations and the learning of interpretable representations. We also present a full-fledged application to single-cell RNA sequencing (scRNA-seq). In this setting the biological signal in mixed in complex ways with sequencing errors and sampling effects. We show that our method out-performs the state-of-the-art in this domain.
count=6
* Probabilistic Matrix Factorization for Automated Machine Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/b59a51a3c0bf9c5228fde841714f523a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/b59a51a3c0bf9c5228fde841714f523a-Paper.pdf)]
    * Title: Probabilistic Matrix Factorization for Automated Machine Learning
    * Year: `2018`
    * Authors: Nicolo Fusi, Rishit Sheth, Melih Elibol
    * Abstract: In order to achieve state-of-the-art performance, modern machine learning techniques require careful data pre-processing and hyperparameter tuning. Moreover, given the ever increasing number of machine learning models being developed, model selection is becoming increasingly important. Automating the selection and tuning of machine learning pipelines, which can include different data pre-processing methods and machine learning models, has long been one of the goals of the machine learning community. In this paper, we propose to solve this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization. Specifically, we use a probabilistic matrix factorization model to transfer knowledge across experiments performed in hundreds of different datasets and use an acquisition function to guide the exploration of the space of possible ML pipelines. In our experiments, we show that our approach quickly identifies high-performing pipelines across a wide range of datasets, significantly outperforming the current state-of-the-art.
count=6
* Self-Normalizing Neural Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf)]
    * Title: Self-Normalizing Neural Networks
    * Year: `2017`
    * Authors: Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter
    * Abstract: Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep.
count=6
* Adaptive Classification for Prediction Under a Budget
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/d9ff90f4000eacd3a6c9cb27f78994cf-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/d9ff90f4000eacd3a6c9cb27f78994cf-Paper.pdf)]
    * Title: Adaptive Classification for Prediction Under a Budget
    * Year: `2017`
    * Authors: Feng Nan, Venkatesh Saligrama
    * Abstract: We propose a novel adaptive approximation approach for test-time resource-constrained prediction motivated by Mobile, IoT, health, security and other applications, where constraints in the form of computation, communication, latency and feature acquisition costs arise. We learn an adaptive low-cost system by training a gating and prediction model that limits utilization of a high-cost model to hard input instances and gates easy-to-handle input instances to a low-cost model. Our method is based on adaptively approximating the high-cost model in regions where low-cost models suffice for making highly accurate predictions. We pose an empirical loss minimization problem with cost constraints to jointly train gating and prediction models. On a number of benchmark datasets our method outperforms state-of-the-art achieving higher accuracy for the same cost.
count=6
* An urn model for majority voting in classification ensembles
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/d1a21da7bca4abff8b0b61b87597de73-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/d1a21da7bca4abff8b0b61b87597de73-Paper.pdf)]
    * Title: An urn model for majority voting in classification ensembles
    * Year: `2016`
    * Authors: Victor Soto, Alberto Suárez, Gonzalo Martinez-Muñoz
    * Abstract: In this work we analyze the class prediction of parallel randomized ensembles by majority voting as an urn model. For a given test instance, the ensemble can be viewed as an urn of marbles of different colors. A marble represents an individual classifier. Its color represents the class label prediction of the corresponding classifier. The sequential querying of classifiers in the ensemble can be seen as draws without replacement from the urn. An analysis of this classical urn model based on the hypergeometric distribution makes it possible to estimate the confidence on the outcome of majority voting when only a fraction of the individual predictions is known. These estimates can be used to speed up the prediction by the ensemble. Specifically, the aggregation of votes can be halted when the confidence in the final prediction is sufficiently high. If one assumes a uniform prior for the distribution of possible votes the analysis is shown to be equivalent to a previous one based on Dirichlet distributions. The advantage of the current approach is that prior knowledge on the possible vote outcomes can be readily incorporated in a Bayesian framework. We show how incorporating this type of problem-specific knowledge into the statistical analysis of majority voting leads to faster classification by the ensemble and allows us to estimate the expected average speed-up beforehand.
count=6
* Local Decorrelation For Improved Pedestrian Detection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/46ba9f2a6976570b0353203ec4474217-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/46ba9f2a6976570b0353203ec4474217-Paper.pdf)]
    * Title: Local Decorrelation For Improved Pedestrian Detection
    * Year: `2014`
    * Authors: Woonhyun Nam, Piotr Dollar, Joon Hee Han
    * Abstract: Even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets. While effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data. Given highly correlated data, decision trees with oblique (multiple feature) splits can be effective. Use of oblique splits, however, comes at considerable computational expense. Inspired by recent work on discriminative decorrelation of HOG features, we instead propose an efficient feature transform that removes correlations in local neighborhoods. The result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees. In fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost. The overall improvement in accuracy is dramatic: on the Caltech Pedestrian Dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.
count=5
* Supervised Anomaly Detection for Complex Industrial Images
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Baitieva_Supervised_Anomaly_Detection_for_Complex_Industrial_Images_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Baitieva_Supervised_Anomaly_Detection_for_Complex_Industrial_Images_CVPR_2024_paper.pdf)]
    * Title: Supervised Anomaly Detection for Complex Industrial Images
    * Year: `2024`
    * Authors: Aimira Baitieva, David Hurych, Victor Besnier, Olivier Bernard
    * Abstract: Automating visual inspection in industrial production lines is essential for increasing product quality across various industries. Anomaly detection (AD) methods serve as robust tools for this purpose. However existing public datasets primarily consist of images without anomalies limiting the practical application of AD methods in production settings. To address this challenge we present (1) the Valeo Anomaly Dataset (VAD) a novel real-world industrial dataset comprising 5000 images including 2000 instances of challenging real defects across more than 20 subclasses. Acknowledging that traditional AD methods struggle with this dataset we introduce (2) Segmentation-based Anomaly Detector (SegAD). First SegAD leverages anomaly maps as well as segmentation maps to compute local statistics. Next SegAD uses these statistics and an optional supervised classifier score as input features for a Boosted Random Forest (BRF) classifier yielding the final anomaly score. Our SegAD achieves state-of-the-art performance on both VAD (+2.1% AUROC) and the VisA dataset (+0.4% AUROC). The code and the models are publicly available.
count=5
* BridgeNet: A Continuity-Aware Probabilistic Network for Age Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_BridgeNet_A_Continuity-Aware_Probabilistic_Network_for_Age_Estimation_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_BridgeNet_A_Continuity-Aware_Probabilistic_Network_for_Age_Estimation_CVPR_2019_paper.pdf)]
    * Title: BridgeNet: A Continuity-Aware Probabilistic Network for Age Estimation
    * Year: `2019`
    * Authors: Wanhua Li,  Jiwen Lu,  Jianjiang Feng,  Chunjing Xu,  Jie Zhou,  Qi Tian
    * Abstract: Age estimation is an important yet very challenging problem in computer vision. Existing methods for age estimation usually apply a divide-and-conquer strategy to deal with heterogeneous data caused by the non-stationary aging process. However, the facial aging process is also a continuous process, and the continuity relationship between different components has not been effectively exploited. In this paper, we propose BridgeNet for age estimation, which aims to mine the continuous relation between age labels effectively. The proposed BridgeNet consists of local regressors and gating networks. Local regressors partition the data space into multiple overlapping subspaces to tackle heterogeneous data and gating networks learn continuity aware weights for the results of local regressors by employing the proposed bridge-tree structure, which introduces bridge connections into tree models to enforce the similarity between neighbor nodes. Moreover, these two components of BridgeNet can be jointly learned in an end-to-end way. We show experimental results on the MORPH II, FG-NET and Chalearn LAP 2015 datasets and find that BridgeNet outperforms the state-of-the-art methods.
count=5
* Deep Regression Forests for Age Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Regression_Forests_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Regression_Forests_CVPR_2018_paper.pdf)]
    * Title: Deep Regression Forests for Age Estimation
    * Year: `2018`
    * Authors: Wei Shen, Yilu Guo, Yan Wang, Kai Zhao, Bo Wang, Alan L. Yuille
    * Abstract: Age estimation from facial images is typically cast as a nonlinear regression problem. The main challenge of this problem is the facial feature space w.r.t. ages is inhomogeneous, due to the large variation in facial appearance across different persons of the same age and the non-stationary property of aging patterns. In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation. DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with inhomogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes. This joint learning follows an alternating strategy: First, by fixing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by fixing the split nodes, the leaf nodes are optimized by iterating a step-size free update rule derived from Variational Bounding. We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them.
count=5
* DSAC - Differentiable RANSAC for Camera Localization
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Brachmann_DSAC_-_Differentiable_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Brachmann_DSAC_-_Differentiable_CVPR_2017_paper.pdf)]
    * Title: DSAC - Differentiable RANSAC for Camera Localization
    * Year: `2017`
    * Authors: Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, Carsten Rother
    * Abstract: RANSAC is an important algorithm in robust optimization and a central building block for many computer vision applications. In recent years, traditionally hand-crafted pipelines have been replaced by deep learning pipelines, which can be trained in an end-to-end fashion. However, RANSAC has so far not been used as part of such deep learning pipelines, because its hypothesis selection procedure is non-differentiable. In this work, we present two different ways to overcome this limitation. The most promising approach is inspired by reinforcement learning, namely to replace the deterministic hypothesis selection by a probabilistic selection for which we can derive the expected loss w.r.t. to all learnable parameters. We call this approach DSAC, the differentiable counterpart of RANSAC. We apply DSAC to the problem of camera localization, where deep learning has so far failed to improve on traditional approaches. We demonstrate that by directly minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we achieve an increase in accuracy. In the future, any deep learning pipeline can use DSAC as a robust optimization component.
count=5
* Personalizing Gesture Recognition Using Hierarchical Bayesian Neural Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Joshi_Personalizing_Gesture_Recognition_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Joshi_Personalizing_Gesture_Recognition_CVPR_2017_paper.pdf)]
    * Title: Personalizing Gesture Recognition Using Hierarchical Bayesian Neural Networks
    * Year: `2017`
    * Authors: Ajjen Joshi, Soumya Ghosh, Margrit Betke, Stan Sclaroff, Hanspeter Pfister
    * Abstract: Building robust classifiers trained on data susceptible to group or subject-specific variations is a challenging pattern recognition problem. We develop hierarchical Bayesian neural networks to capture subject-specific variations and share statistical strength across subjects. Leveraging recent work on learning Bayesian neural networks, we build fast, scalable algorithms for inferring the posterior distribution over all network weights in the hierarchy. We also develop methods for adapting our model to new subjects when a small number of subject-specific personalization data is available. Finally, we investigate active learning algorithms for interactively labeling personalization data in resource-constrained scenarios. Focusing on the problem of gesture recognition where inter-subject variations are commonplace, we demonstrate the effectiveness of our proposed techniques. We test our framework on three widely used gesture recognition datasets, achieving personalization performance competitive with the state-of-the-art.
count=5
* Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Doumanoglou_Recovering_6D_Object_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Doumanoglou_Recovering_6D_Object_CVPR_2016_paper.pdf)]
    * Title: Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd
    * Year: `2016`
    * Authors: Andreas Doumanoglou, Rigas Kouskouridas, Sotiris Malassiotis, Tae-Kyun Kim
    * Abstract: Object detection and 6D pose estimation in the crowd (scenes with multiple object instances, severe foreground occlusions and background distractors), has become an important problem in many rapidly evolving technological areas such as robotics and augmented reality. Single shot-based 6D pose estimators with manually designed features are still unable to tackle the above challenges, motivating the research towards unsupervised feature learning and next-best-view estimation. In this work, we present a complete framework for both single shot-based 6D object pose estimation and next-best-view prediction based on Hough Forests, the state of the art object pose estimator that performs classification and regression jointly. Rather than using manually designed features we a) propose an unsupervised feature learnt from depth-invariant patches using a Sparse Autoencoder and b) offer an extensive evaluation of various state of the art features. Furthermore, taking advantage of the clustering performed in the leaf nodes of Hough Forests, we learn to estimate the reduction of uncertainty in other views, formulating the problem of selecting the next-best-view. To further improve pose estimation, we propose an improved joint registration and hypotheses verification module as a final refinement step to reject false detections. We provide two additional challenging datasets inspired from realistic scenarios to extensively evaluate the state of the art and our framework. One is related to domestic environments and the other depicts a bin-picking scenario mostly found in industrial settings. We show that our framework significantly outperforms state of the art both on public and on our datasets.
count=5
* Robust 3D Hand Pose Estimation in Single Depth Images: From Single-View CNN to Multi-View CNNs
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Ge_Robust_3D_Hand_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Ge_Robust_3D_Hand_CVPR_2016_paper.pdf)]
    * Title: Robust 3D Hand Pose Estimation in Single Depth Images: From Single-View CNN to Multi-View CNNs
    * Year: `2016`
    * Authors: Liuhao Ge, Hui Liang, Junsong Yuan, Daniel Thalmann
    * Abstract: Articulated hand pose estimation plays an important role in human-computer interaction. Despite the recent progress, the accuracy of existing methods is still not satisfactory, partially due to the difficulty of embedded high-dimensional and non-linear regression problem. Different from the existing discriminative methods that regress for the hand pose with a single depth image, we propose to first project the query depth image onto three orthogonal planes and utilize these multi-view projections to regress for 2D heat-maps which estimate the joint positions on each plane. These multi-view heat-maps are then fused to produce final 3D hand pose estimation with learned pose priors. Experiments show that the proposed method largely outperforms state-of-the-arts on a challenging dataset. Moreover, a cross-dataset experiment also demonstrates the good generalization ability of the proposed method.
count=5
* Scene Labeling Using Sparse Precision Matrix
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Souly_Scene_Labeling_Using_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Souly_Scene_Labeling_Using_CVPR_2016_paper.pdf)]
    * Title: Scene Labeling Using Sparse Precision Matrix
    * Year: `2016`
    * Authors: Nasim Souly, Mubarak Shah
    * Abstract: Scene labeling task is to segment the image into meaningful regions and categorize them into classes of objects which comprised the image. Commonly used methods typically find the local features for each segment and label them using classifiers. Afterwards, labeling is smoothed in order to make sure that neighboring regions receive similar labels. However, these methods ignore expressive connections between labels and non-local dependencies among regions. In this paper, we propose to use a sparse estimation of precision matrix (also called concentration matrix), which is the inverse of covariance matrix of data obtained by graphical lasso to find interaction between labels and regions. To do this, we formulate the problem as an energy minimization over a graph, whose structure is captured by applying sparse constraint on the elements of the precision matrix. This graph encodes (or represents) only significant interactions and avoids a fully connected graph, which is typically used to reflect the long distance associations. We use local and global information to achieve better labeling. We assess our approach on three datasets and obtained promising results.
count=5
* Oriented Edge Forests for Boundary Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Hallman_Oriented_Edge_Forests_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Hallman_Oriented_Edge_Forests_2015_CVPR_paper.pdf)]
    * Title: Oriented Edge Forests for Boundary Detection
    * Year: `2015`
    * Authors: Sam Hallman, Charless C. Fowlkes
    * Abstract: We present a simple, efficient model for learning boundary detection based on a random forest classifier. Our approach combines (1) efficient clustering of training examples based on a simple partitioning of the space of local edge orientations and (2) scale-dependent calibration of individual tree output probabilities prior to multiscale combination. The resulting model outperforms published results on the challenging BSDS500 boundary detection benchmark. Further, on large datasets our model requires substantially less memory for training and speeds up training time by a factor of 10 over the structured forest model.
count=5
* Toward User-Specific Tracking by Detection of Human Shapes in Multi-Cameras
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Huang_Toward_User-Specific_Tracking_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Huang_Toward_User-Specific_Tracking_2015_CVPR_paper.pdf)]
    * Title: Toward User-Specific Tracking by Detection of Human Shapes in Multi-Cameras
    * Year: `2015`
    * Authors: Chun-Hao Huang, Edmond Boyer, Bibiana do Canto Angonese, Nassir Navab, Slobodan Ilic
    * Abstract: Human shape tracking consists in fitting a template model to temporal sequences of visual observations. It usually comprises an association step, that finds correspondences between the model and the input data, and a deformation step, that fits the model to the observations given correspondences. Most current approaches find their common ground with the Iterative-Closest-Point (ICP) algorithm, which facilitates the association step with local distance considerations. It fails when large deformations occur, and errors in the association tend to propagate over time. In this paper, we propose a discriminative alternative for the association, that leverages random forests to infer correspondences in one shot. It allows for large deformations and prevents tracking errors from accumulating. The approach is successfully integrated to a surface tracking framework that recovers human shapes and poses jointly. When combined with ICP, this discriminative association proves to yield better accuracy in registration, more stability when tracking over time, and faster convergence. Evaluations on existing datasets demonstrate the benefits with respect to the state-of-the-art.
count=5
* Learning To Look Up: Realtime Monocular Gaze Correction Using Machine Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Kononenko_Learning_To_Look_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Kononenko_Learning_To_Look_2015_CVPR_paper.pdf)]
    * Title: Learning To Look Up: Realtime Monocular Gaze Correction Using Machine Learning
    * Year: `2015`
    * Authors: Daniil Kononenko, Victor Lempitsky
    * Abstract: We revisit the well-known problem of gaze correction and present a solution based on supervised machine learning. At training time, our system observes pairs of images, where each pair contains the face of the same person with a fixed angular difference in gaze direction. It then learns to synthesize the second image of a pair from the first one. After learning, the system becomes able to redirect the gaze of a previously unseen person by the same angular difference. Unlike many previous solutions to gaze problem in videoconferencing, ours is purely monocular, i.e. it does not require any hardware apart from an in-built web-camera of a laptop. We base our machine learning implementation on a special kind of decision forests that predict a displacement (flow) vector for each pixel in the input image. As a result, our system is highly efficient (runs in real-time on a single core of a modern laptop). In the paper, we demonstrate results on a variety of videoconferencing frames and evaluate the method quantitatively on the hold-out set of registered images. The supplementary video shows example sessions of our system at work.
count=5
* Multiclass Semantic Video Segmentation With Object-Level Active Inference
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Liu_Multiclass_Semantic_Video_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Liu_Multiclass_Semantic_Video_2015_CVPR_paper.pdf)]
    * Title: Multiclass Semantic Video Segmentation With Object-Level Active Inference
    * Year: `2015`
    * Authors: Buyu Liu, Xuming He
    * Abstract: We address the problem of integrating object reasoning with supervoxel labeling in multiclass semantic video segmentation. To this end, we first propose an object-augmented dense CRF in spatio-temporal domain, which captures long-range dependency between supervoxels, and imposes consistency between object and supervoxel labels. We develop an efficient mean field inference algorithm to jointly infer the supervoxel labels, object activations and their occlusion relations for a moderate number of object proposals. To scale up our method, we adopt an active inference strategy to improve the efficiency, which adaptively selects object subgraphs in the object-augmented dense CRF. We formulate the problem as a Markov Decision Process, which learns an approximate optimal policy based on a reward of accuracy improvement and a set of well-designed model and input features. We evaluate our method on three publicly available multiclass video semantic segmentation datasets and demonstrate superior efficiency and accuracy.
count=5
* Neural Decision Forests for Semantic Image Labelling
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Bulo_Neural_Decision_Forests_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Bulo_Neural_Decision_Forests_2014_CVPR_paper.pdf)]
    * Title: Neural Decision Forests for Semantic Image Labelling
    * Year: `2014`
    * Authors: Samuel Rota Bulo, Peter Kontschieder
    * Abstract: In this work we present Neural Decision Forests, a novel approach to jointly tackle data representation- and discriminative learning within randomized decision trees. Recent advances of deep learning architectures demonstrate the power of embedding representation learning within the classifier – An idea that is intuitively supported by the hierarchical nature of the decision forest model where the input space is typically left unchanged during training and testing. We bridge this gap by introducing randomized Multi- Layer Perceptrons (rMLP) as new split nodes which are capable of learning non-linear, data-specific representations and taking advantage of them by finding optimal predictions for the emerging child nodes. To prevent overfitting, we i) randomly select the image data fed to the input layer, ii) automatically adapt the rMLP topology to meet the complexity of the data arriving at the node and iii) introduce an l1-norm based regularization that additionally sparsifies the network. The key findings in our experiments on three different semantic image labelling datasets are consistently improved results and significantly compressed trees compared to conventional classification trees.
count=5
* Predicting Matchability
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Hartmann_Predicting_Matchability_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Hartmann_Predicting_Matchability_2014_CVPR_paper.pdf)]
    * Title: Predicting Matchability
    * Year: `2014`
    * Authors: Wilfried Hartmann, Michal Havlena, Konrad Schindler
    * Abstract: The initial steps of many computer vision algorithms are interest point extraction and matching. In larger image sets the pairwise matching of interest point descriptors between images is an important bottleneck. For each descriptor in one image the (approximate) nearest neighbor in the other one has to be found and checked against the second-nearest neighbor to ensure the correspondence is unambiguous. Here, we asked the question how to best decimate the list of interest points without losing matches, i.e. we aim to speed up matching by filtering out, in advance, those points which would not survive the matching stage. It turns out that the best filtering criterion is not the response of the interest point detector, which in fact is not surprising: the goal of detection are repeatable and well-localized points, whereas the objective of the selection are points whose descriptors can be matched successfully. We show that one can in fact learn to predict which descriptors are matchable, and thus reduce the number of interest points significantly without losing too many matches. We show that this strategy, as simple as it is, greatly improves the matching success with the same number of points per image. Moreover, we embed the prediction in a state-of-the-art Structure-from-Motion pipeline and demonstrate that it also outperforms other selection methods at system level.
count=5
* Orientation Robust Text Line Detection in Natural Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Kang_Orientation_Robust_Text_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Kang_Orientation_Robust_Text_2014_CVPR_paper.pdf)]
    * Title: Orientation Robust Text Line Detection in Natural Images
    * Year: `2014`
    * Authors: Le Kang, Yi Li, David Doermann
    * Abstract: In this paper, higher-order correlation clustering (HOCC) is used for text line detection in natural images. We treat text line detection as a graph partitioning problem, where each vertex is represented by a Maximally Stable Extremal Region (MSER). First, weak hypothesises are proposed by coarsely grouping MSERs based on their spatial alignment and appearance consistency. Then, higher-order correlation clustering (HOCC) is used to partition the MSERs into text line candidates, using the hypotheses as soft constraints to enforce long range interactions. We further propose a regularization method to solve the Semidefinite Programming problem in the inference. Finally we use a simple texton-based texture classifier to filter out the non-text areas. This framework allows us to naturally handle multiple orientations, languages and fonts. Experiments show that our approach achieves competitive performance compared to the state of the art.
count=5
* Optimal Decisions from Probabilistic Models: The Intersection-over-Union Case
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Nowozin_Optimal_Decisions_from_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Nowozin_Optimal_Decisions_from_2014_CVPR_paper.pdf)]
    * Title: Optimal Decisions from Probabilistic Models: The Intersection-over-Union Case
    * Year: `2014`
    * Authors: Sebastian Nowozin
    * Abstract: A probabilistic model allows us to reason about the world and make statistically optimal decisions using Bayesian decision theory. However, in practice the intractability of the decision problem forces us to adopt simplistic loss functions such as the 0/1 loss or Hamming loss and as result we make poor decisions through MAP estimates or through low-order marginal statistics. In this work we investigate optimal decision making for more realistic loss functions. Specifically we consider the popular intersection-over-union (IoU) score used in image segmentation benchmarks and show that it results in a hard combinatorial decision problem. To make this problem tractable we propose a statistical approximation to the objective function, as well as an approximate algorithm based on parametric linear programming. We apply the algorithm on three benchmark datasets and obtain improved intersection-over-union scores compared to maximum-posterior-marginal decisions. Our work points out the difficulties of using realistic loss functions with probabilistic computer vision models.
count=5
* Spatial Inference Machines
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Shapovalov_Spatial_Inference_Machines_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Shapovalov_Spatial_Inference_Machines_2013_CVPR_paper.pdf)]
    * Title: Spatial Inference Machines
    * Year: `2013`
    * Authors: Roman Shapovalov, Dmitry Vetrov, Pushmeet Kohli
    * Abstract: This paper addresses the problem of semantic segmentation of 3D point clouds. We extend the inference machines framework of Ross et al. by adding spatial factors that model mid-range and long-range dependencies inherent in the data. The new model is able to account for semantic spatial context. During training, our method automatically isolates and retains factors modelling spatial dependencies between variables that are relevant for achieving higher prediction accuracy. We evaluate the proposed method by using it to predict 17-category semantic segmentations on sets of stitched Kinect scans. Experimental results show that the spatial dependencies learned by our method significantly improve the accuracy of segmentation. They also show that our method outperforms the existing segmentation technique of Koppula et al.
count=5
* Fast Object Detection with Entropy-Driven Evaluation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Sznitman_Fast_Object_Detection_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Sznitman_Fast_Object_Detection_2013_CVPR_paper.pdf)]
    * Title: Fast Object Detection with Entropy-Driven Evaluation
    * Year: `2013`
    * Authors: Raphael Sznitman, Carlos Becker, Francois Fleuret, Pascal Fua
    * Abstract: Cascade-style approaches to implementing ensemble classifiers can deliver significant speed-ups at test time. While highly effective, they remain challenging to tune and their overall performance depends on the availability of large validation sets to estimate rejection thresholds. These characteristics are often prohibitive and thus limit their applicability. We introduce an alternative approach to speeding-up classifier evaluation which overcomes these limitations. It involves maintaining a probability estimate of the class label at each intermediary response and stopping when the corresponding uncertainty becomes small enough. As a result, the evaluation terminates early based on the sequence of responses observed. Furthermore, it does so independently of the type of ensemble classifier used or the way it was trained. We show through extensive experimentation that our method provides 2 to 10 fold speed-ups, over existing state-of-the-art methods, at almost no loss in accuracy on a number of object classification tasks.
count=5
* Contour Detection and Characterization for Asynchronous Event Sensors
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Barranco_Contour_Detection_and_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Barranco_Contour_Detection_and_ICCV_2015_paper.pdf)]
    * Title: Contour Detection and Characterization for Asynchronous Event Sensors
    * Year: `2015`
    * Authors: Francisco Barranco, Ching L. Teo, Cornelia Fermuller, Yiannis Aloimonos
    * Abstract: The bio-inspired, asynchronous event-based dynamic vision sensor records temporal changes in the luminance of the scene at high temporal resolution. Since events are only triggered at significant luminance changes, most events occur at the boundary of objects and their parts. The detection of these contours is an essential step for further interpretation of the scene. This paper presents an approach to learn the location of contours and their border ownership using Structured Random Forests on event-based features that encode motion, timing, texture, and spatial orientations. The classifier integrates elegantly information over time by utilizing the classification results previously computed. Finally, the contour detection and boundary assignment are demonstrated in a layer-segmentation of the scene. Experimental results demonstrate good performance in boundary detection and segmentation.
count=5
* Car That Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Jain_Car_That_Knows_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Jain_Car_That_Knows_ICCV_2015_paper.pdf)]
    * Title: Car That Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models
    * Year: `2015`
    * Authors: Ashesh Jain, Hema S. Koppula, Bharad Raghavan, Shane Soh, Ashutosh Saxena
    * Abstract: Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger. In this work we anticipate driving maneuvers a few seconds before they occur. For this purpose we equip a car with cameras and a computing device to capture the driving context from both inside and outside of the car. We propose an Autoregressive Input-Output HMM to model the contextual information alongwith the maneuvers. We evaluate our approach on a diverse data set with 1180 miles of natural freeway and city driving and show that we can anticipate maneuvers 3.5 seconds before they occur with over 80% F1-score in real-time.
count=5
* Detection and Segmentation of 2D Curved Reflection Symmetric Structures
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Teo_Detection_and_Segmentation_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Teo_Detection_and_Segmentation_ICCV_2015_paper.pdf)]
    * Title: Detection and Segmentation of 2D Curved Reflection Symmetric Structures
    * Year: `2015`
    * Authors: Ching L. Teo, Cornelia Fermuller, Yiannis Aloimonos
    * Abstract: Symmetry, as one of the key components of Gestalt theory, provides an important mid-level cue that serves as input to higher visual processes such as segmentation. In this work, we propose a complete approach that links the detection of curved reflection symmetries to produce symmetry-constrained segments of structures/regions in real images with clutter. For curved reflection symmetry detection, we leverage on patch-based symmetric features to train a Structured Random Forest classifier that detects multiscaled curved symmetries in 2D images. Next, using these curved symmetries, we modulate a novel symmetry-constrained foreground-background segmentation by their symmetry scores so that we enforce global symmetrical consistency in the final segmentation. This is achieved by imposing a pairwise symmetry prior that encourages symmetric pixels to have the same labels over a MRF-based representation of the input image edges, and the final segmentation is obtained via graph-cuts. Experimental results over four publicly available datasets containing annotated symmetric structures: 1) SYMMAX-300, 2) BSD-Parts, 3) Weizmann Horse and 4) NY-roads demonstrate the approach's applicability to different environments with state-of-the-art performance.
count=5
* Zero-shot causal learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/15ddb1773510075ef44981cdb204330b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/15ddb1773510075ef44981cdb204330b-Paper-Conference.pdf)]
    * Title: Zero-shot causal learning
    * Year: `2023`
    * Authors: Hamed Nilforoshan, Michael Moor, Yusuf Roohani, Yining Chen, Anja Šurina, Michihiro Yasunaga, Sara Oblak, Jure Leskovec
    * Abstract: Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (e.g., a newly invented drug), which these methods do not address.Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, its recipients, and its nonrecipients. By leveraging both intervention information (e.g., a drug's attributes) and individual features (e.g., a patient's history), CaML is able to predict the personalized effects of novel interventions that do not exist at the time of training. Experimental results on real world datasets in large-scale medical claims and cell-line perturbations demonstrate the effectiveness of our approach. Most strikingly, CaML's zero-shot predictions outperform even strong baselines trained directly on data from the test interventions.
count=5
* EMBERSim: A Large-Scale Databank for Boosting Similarity Search in Malware Analysis
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/54bf430f5d3090502ea021941e9cb18e-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/54bf430f5d3090502ea021941e9cb18e-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: EMBERSim: A Large-Scale Databank for Boosting Similarity Search in Malware Analysis
    * Year: `2023`
    * Authors: Dragos Georgian Corlatescu, Alexandru Dinu, Mihaela Petruta Gaman, Paul Sumedrea
    * Abstract: In recent years there has been a shift from heuristics based malware detection towards machine learning, which proves to be more robust in the current heavily adversarial threat landscape. While we acknowledge machine learning to be better equipped to mine for patterns in the increasingly high amounts of similar-looking files, we also note a remarkable scarcity of the data available for similarity targeted research. Moreover, we observe that the focus in the few related works falls on quantifying similarity in malware, often overlooking the clean data. This one-sided quantification is especially dangerous in the context of detection bypass. We propose to address the deficiencies in the space of similarity research on binary files, starting from EMBER — one of the largest malware classification datasets. We enhance EMBER with similarity information as well as malware class tags, to enable further research in the similarity space. Our contribution is threefold: (1) we publish EMBERSim, an augmented version of EMBER, that includes similarity informed tags; (2) we enrich EMBERSim with automatically determined malware class tags using the open-source tool AVClass on VirusTotal data and (3) we describe and share the implementation for our class scoring technique and leaf similarity method.
count=5
* Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/55a49718689fdecef31b6a2386df6fe1-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/55a49718689fdecef31b6a2386df6fe1-Paper-Conference.pdf)]
    * Title: Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions
    * Year: `2023`
    * Authors: Hao Wang, Luxi He, Rui Gao, Flavio Calmon
    * Abstract: Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and epistemic discrimination, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate that state-of-the-art fairness interventions are effective at removing epistemic discrimination on standard (overused) tabular datasets. However, when data has missing values, there is still significant room for improvement in handling aleatoric discrimination.
count=5
* Generalized equivalences between subsampling and ridge regularization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/f976982cd1c1b9e076c096787ef6652e-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/f976982cd1c1b9e076c096787ef6652e-Paper-Conference.pdf)]
    * Title: Generalized equivalences between subsampling and ridge regularization
    * Year: `2023`
    * Authors: Pratik Patil, Jin-Hong Du
    * Abstract: We establish precise structural and risk equivalences between subsampling and ridge regularization for ensemble ridge estimators. Specifically, we prove that linear and quadratic functionals of subsample ridge estimators, when fitted with different ridge regularization levels $\lambda$ and subsample aspect ratios $\psi$, are asymptotically equivalent along specific paths in the $(\lambda,\psi)$-plane (where $\psi$ is the ratio of the feature dimension to the subsample size). Our results only require bounded moment assumptions on feature and response distributions and allow for arbitrary joint distributions. Furthermore, we provide a data-dependent method to determine the equivalent paths of $(\lambda,\psi)$. An indirect implication of our equivalences is that optimally tuned ridge regression exhibits a monotonic prediction risk in the data aspect ratio. This resolves a recent open problem raised by Nakkiran et al. for general data distributions under proportional asymptotics, assuming a mild regularity condition that maintains regression hardness through linearized signal-to-noise ratios.
count=5
* On Margins and Generalisation for Voting Classifiers
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/3f8675af3da6da231c9e75b889b7f047-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/3f8675af3da6da231c9e75b889b7f047-Paper-Conference.pdf)]
    * Title: On Margins and Generalisation for Voting Classifiers
    * Year: `2022`
    * Authors: Felix Biggs, Valentina Zantedeschi, Benjamin Guedj
    * Abstract: We study the generalisation properties of majority voting on finite ensembles of classifiers, proving margin-based generalisation bounds via the PAC-Bayes theory. These provide state-of-the-art guarantees on a number of classification tasks. Our central results leverage the Dirichlet posteriors studied recently by Zantedeschi et al. (2021) for training voting classifiers; in contrast to that work our bounds apply to non-randomised votes via the use of margins. Our contributions add perspective to the debate on the ``margins theory'' proposed by Schapire et al. (1998) for the generalisation of ensemble classifiers.
count=5
* Instance-Based Uncertainty Estimation for Gradient-Boosted Regression Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/48088756ec0ce6ba362bddc7ebeb3915-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/48088756ec0ce6ba362bddc7ebeb3915-Paper-Conference.pdf)]
    * Title: Instance-Based Uncertainty Estimation for Gradient-Boosted Regression Trees
    * Year: `2022`
    * Authors: Jonathan Brophy, Daniel Lowd
    * Abstract: Gradient-boosted regression trees (GBRTs) are hugely popular for solving tabular regression problems, but provide no estimate of uncertainty. We propose Instance-Based Uncertainty estimation for Gradient-boosted regression trees (IBUG), a simple method for extending any GBRT point predictor to produce probabilistic predictions. IBUG computes a non-parametric distribution around a prediction using the $k$-nearest training instances, where distance is measured with a tree-ensemble kernel. The runtime of IBUG depends on the number of training examples at each leaf in the ensemble, and can be improved by sampling trees or training instances. Empirically, we find that IBUG achieves similar or better performance than the previous state-of-the-art across 22 benchmark regression datasets. We also find that IBUG can achieve improved probabilistic performance by using different base GBRT models, and can more flexibly model the posterior distribution of a prediction than competing methods. We also find that previous methods suffer from poor probabilistic calibration on some datasets, which can be mitigated using a scalar factor tuned on the validation data. Source code is available at https://github.com/jjbrophy47/ibug.
count=5
* Linear Label Ranking with Bounded Noise
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/64792f7bd5d400c9ac310c6fef97ef2d-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/64792f7bd5d400c9ac310c6fef97ef2d-Paper-Conference.pdf)]
    * Title: Linear Label Ranking with Bounded Noise
    * Year: `2022`
    * Authors: Dimitris Fotakis, Alkis Kalavasis, Vasilis Kontonis, Christos Tzamos
    * Abstract: Label Ranking (LR) is the supervised task of learning a sorting function that maps feature vectors $x \in \mathbb{R}^d$ to rankings $\sigma(x) \in \mathbb S_k$ over a finite set of $k$ labels. We focus on the fundamental case of learning linear sorting functions (LSFs) under Gaussian marginals: $x$ is sampled from the $d$-dimensional standard normal and the ground truth ranking $\sigma^\star(x)$ is the ordering induced by sorting the coordinates of the vector $W^\star x$, where $W^\star \in \mathbb{R}^{k \times d}$ is unknown. We consider learning LSFs in the presence of bounded noise: assuming that a noiseless example is of the form $(x, \sigma^\star(x))$, we observe $(x, \pi)$, where for any pair of elements $i \neq j$, the probability that the order of $i, j$ is different in $\pi$ than in $\sigma^\star(x)$ is at most $\eta < 1/2$. We design efficient non-proper and proper learning algorithms that learn hypotheses within normalized Kendall's Tau distance $\epsilon$ from the ground truth with $N= \widetilde{O}(d\log(k)/\epsilon)$ labeled examples and runtime $\mathrm{poly}(N, k)$. For the more challenging top-$r$ disagreement loss, we give an efficient proper learning algorithm that achieves $\epsilon$ top-$r$ disagreement with the ground truth with $N = \widetilde{O}(d k r /\epsilon)$ samples and $\mathrm{poly}(N)$ runtime.
count=5
* FACT: Learning Governing Abstractions Behind Integer Sequences
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/72372ec86dd49238900fc0b68bad63f8-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/72372ec86dd49238900fc0b68bad63f8-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: FACT: Learning Governing Abstractions Behind Integer Sequences
    * Year: `2022`
    * Authors: Peter Belcak, Ard Kastrati, Flavio Schenker, Roger Wattenhofer
    * Abstract: Integer sequences are of central importance to the modeling of concepts admitting complete finitary descriptions. We introduce a novel view on the learning of such concepts and lay down a set of benchmarking tasks aimed at conceptual understanding by machine learning models. These tasks indirectly assess model ability to abstract, and challenge them to reason both interpolatively and extrapolatively from the knowledge gained by observing representative examples. To further aid research in knowledge representation and reasoning, we present FACT, the Finitary Abstraction Comprehension Toolkit. The toolkit surrounds a large dataset of integer sequences comprising both organic and synthetic entries, a library for data pre-processing and generation, a set of model performance evaluation tools, and a collection of baseline model implementations, enabling the making of the future advancements with ease.
count=5
* Pushing the limits of fairness impossibility: Who's the fairest of them all?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/d3222559698f41247261b7a6c2bbaedc-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/d3222559698f41247261b7a6c2bbaedc-Paper-Conference.pdf)]
    * Title: Pushing the limits of fairness impossibility: Who's the fairest of them all?
    * Year: `2022`
    * Authors: Brian Hsu, Rahul Mazumder, Preetam Nandy, Kinjal Basu
    * Abstract: The impossibility theorem of fairness is a foundational result in the algorithmic fairness literature. It states that outside of special cases, one cannot exactly and simultaneously satisfy all three common and intuitive definitions of fairness - demographic parity, equalized odds, and predictive rate parity. This result has driven most works to focus on solutions for one or two of the metrics. Rather than follow suit, in this paper we present a framework that pushes the limits of the impossibility theorem in order to satisfy all three metrics to the best extent possible. We develop an integer-programming based approach that can yield a certifiably optimal post-processing method for simultaneously satisfying multiple fairness criteria under small violations. We show experiments demonstrating that our post-processor can improve fairness across the different definitions simultaneously with minimal model performance reduction. We also discuss applications of our framework for model selection and fairness explainability, thereby attempting to answer the question: Who's the fairest of them all?
count=5
* JAWS: Auditing Predictive Uncertainty Under Covariate Shift
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/e944bacecce6b06374ac39b260348db0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/e944bacecce6b06374ac39b260348db0-Paper-Conference.pdf)]
    * Title: JAWS: Auditing Predictive Uncertainty Under Covariate Shift
    * Year: `2022`
    * Authors: Drew Prinster, Anqi Liu, Suchi Saria
    * Abstract: We propose \textbf{JAWS}, a series of wrapper methods for distribution-free uncertainty quantification tasks under covariate shift, centered on the core method \textbf{JAW}, the \textbf{JA}ckknife+ \textbf{W}eighted with data-dependent likelihood-ratio weights. JAWS also includes computationally efficient \textbf{A}pproximations of JAW using higher-order influence functions: \textbf{JAWA}. Theoretically, we show that JAW relaxes the jackknife+'s assumption of data exchangeability to achieve the same finite-sample coverage guarantee even under covariate shift. JAWA further approaches the JAW guarantee in the limit of the sample size or the influence function order under common regularity assumptions. Moreover, we propose a general approach to repurposing predictive interval-generating methods and their guarantees to the reverse task: estimating the probability that a prediction is erroneous, based on user-specified error criteria such as a safe or acceptable tolerance threshold around the true label. We then propose \textbf{JAW-E} and \textbf{JAWA-E} as the repurposed proposed methods for this \textbf{E}rror assessment task. Practically, JAWS outperform state-of-the-art predictive inference baselines in a variety of biased real world data sets for interval-generation and error-assessment predictive uncertainty auditing tasks.
count=5
* Learning Stochastic Majority Votes by Minimizing a PAC-Bayes Generalization Bound
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf)]
    * Title: Learning Stochastic Majority Votes by Minimizing a PAC-Bayes Generalization Bound
    * Year: `2021`
    * Authors: Valentina Zantedeschi, Paul Viallard, Emilie Morvant, Rémi Emonet, Amaury Habrard, Pascal Germain, Benjamin Guedj
    * Abstract: We investigate a stochastic counterpart of majority votes over finite ensembles of classifiers, and study its generalization properties. While our approach holds for arbitrary distributions, we instantiate it with Dirichlet distributions: this allows for a closed-form and differentiable expression for the expected risk, which then turns the generalization bound into a tractable training objective.The resulting stochastic majority vote learning algorithm achieves state-of-the-art accuracy and benefits from (non-vacuous) tight generalization bounds, in a series of numerical experiments when compared to competing algorithms which also minimize PAC-Bayes objectives -- both with uninformed (data-independent) and informed (data-dependent) priors.
count=5
* Learning Interpretable Decision Rule Sets: A Submodular Optimization Approach
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/eaa32c96f620053cf442ad32258076b9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/eaa32c96f620053cf442ad32258076b9-Paper.pdf)]
    * Title: Learning Interpretable Decision Rule Sets: A Submodular Optimization Approach
    * Year: `2021`
    * Authors: Fan Yang, Kai He, Linxiao Yang, Hongxia Du, Jingbang Yang, Bo Yang, Liang Sun
    * Abstract: Rule sets are highly interpretable logical models in which the predicates for decision are expressed in disjunctive normal form (DNF, OR-of-ANDs), or, equivalently, the overall model comprises an unordered collection of if-then decision rules. In this paper, we consider a submodular optimization based approach for learning rule sets. The learning problem is framed as a subset selection task in which a subset of all possible rules needs to be selected to form an accurate and interpretable rule set. We employ an objective function that exhibits submodularity and thus is amenable to submodular optimization techniques. To overcome the difficulty arose from dealing with the exponential-sized ground set of rules, the subproblem of searching a rule is casted as another subset selection task that asks for a subset of features. We show it is possible to write the induced objective function for the subproblem as a difference of two submodular (DS) functions to make it approximately solvable by DS optimization algorithms. Overall, the proposed approach is simple, scalable, and likely to be benefited from further research on submodular optimization. Experiments on real datasets demonstrate the effectiveness of our method.
count=5
* Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/f1507aba9fc82ffa7cc7373c58f8a613-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/f1507aba9fc82ffa7cc7373c58f8a613-Paper.pdf)]
    * Title: Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning
    * Year: `2021`
    * Authors: Jannik Kossen, Neil Band, Clare Lyle, Aidan N. Gomez, Thomas Rainforth, Yarin Gal
    * Abstract: We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input. To this end, we introduce a general-purpose deep learning architecture that takes as input the entire dataset instead of processing one datapoint at a time. Our approach uses self-attention to reason about relationships between datapoints explicitly, which can be seen as realizing non-parametric models using parametric attention mechanisms. However, unlike conventional non-parametric models, we let the model learn end-to-end from the data how to make use of other datapoints for prediction. Empirically, our models solve cross-datapoint lookup and complex reasoning tasks unsolvable by traditional deep learning models. We show highly competitive results on tabular data, early results on CIFAR-10, and give insight into how the model makes use of the interactions between points.
count=5
* Model Agnostic Multilevel Explanations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/426f990b332ef8193a61cc90516c1245-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/426f990b332ef8193a61cc90516c1245-Paper.pdf)]
    * Title: Model Agnostic Multilevel Explanations
    * Year: `2020`
    * Authors: Karthikeyan Natesan Ramamurthy, Bhanukiran Vinzamuri, Yunfeng Zhang, Amit Dhurandhar
    * Abstract: In recent years, post-hoc local instance-level and global dataset-level explainability of black-box models has received a lot of attention. Lesser attention has been given to obtaining insights at intermediate or group levels, which is a need outlined in recent works that study the challenges in realizing the guidelines in the General Data Protection Regulation (GDPR). In this paper, we propose a meta-method that, given a typical local explainability method, can build a multilevel explanation tree. The leaves of this tree correspond to local explanations, the root corresponds to global explanation, and intermediate levels correspond to explanations for groups of data points that it automatically clusters. The method can also leverage side information, where users can specify points for which they may want the explanations to be similar. We argue that such a multilevel structure can also be an effective form of communication, where one could obtain few explanations that characterize the entire dataset by considering an appropriate level in our explanation tree. Explanations for novel test points can be cost-efficiently obtained by associating them with the closest training points. When the local explainability technique is generalized additive (viz. LIME, GAMs), we develop fast approximate algorithm for building the multilevel tree and study its convergence behavior. We show that we produce high fidelity sparse explanations on several public datasets and also validate the effectiveness of the proposed technique based on two human studies -- one with experts and the other with non-expert users -- on real world datasets.
count=5
* Adaptive Learned Bloom Filter (Ada-BF): Efficient Utilization of the Classifier with Application to Real-Time Information Filtering on the Web
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/86b94dae7c6517ec1ac767fd2c136580-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/86b94dae7c6517ec1ac767fd2c136580-Paper.pdf)]
    * Title: Adaptive Learned Bloom Filter (Ada-BF): Efficient Utilization of the Classifier with Application to Real-Time Information Filtering on the Web
    * Year: `2020`
    * Authors: Zhenwei Dai, Anshumali Shrivastava
    * Abstract: Recent work suggests improving the performance of Bloom filter by incorporating a machine learning model as a binary classifier. However, such learned Bloom filter does not take full advantage of the predicted probability scores. We propose new algorithms that generalize the learned Bloom filter by using the complete spectrum of the score regions. We prove our algorithms have lower false positive rate (FPR) and memory usage compared with the existing approaches to learned Bloom filter. We also demonstrate the improved performance of our algorithms on real-world information filtering tasks over the web.
count=5
* N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/2f3926f0a9613f3c3cc21d52a3cdb4d9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/2f3926f0a9613f3c3cc21d52a3cdb4d9-Paper.pdf)]
    * Title: N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules
    * Year: `2019`
    * Authors: Shengchao Liu, Mehmet F. Demirel, Yingyu Liang
    * Abstract: Machine learning techniques have recently been adopted in various applications in medicine, biology, chemistry, and material engineering. An important task is to predict the properties of molecules, which serves as the main subroutine in many downstream applications such as virtual screening and drug design. Despite the increasing interest, the key challenge is to construct proper representations of molecules for learning algorithms. This paper introduces the N-gram graph, a simple unsupervised representation for molecules. The method first embeds the vertices in the molecule graph. It then constructs a compact representation for the graph by assembling the vertex embeddings in short walks in the graph, which we show is equivalent to a simple graph neural network that needs no training. The representations can thus be efficiently computed and then used with supervised learning methods for prediction. Experiments on 60 tasks from 10 benchmark datasets demonstrate its advantages over both popular graph neural networks and traditional representation methods. This is complemented by theoretical analysis showing its strong representation and prediction power.
count=5
* Conformal Prediction Under Covariate Shift
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/8fb21ee7a2207526da55a679f0332de2-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf)]
    * Title: Conformal Prediction Under Covariate Shift
    * Year: `2019`
    * Authors: Ryan J. Tibshirani, Rina Foygel Barber, Emmanuel Candes, Aaditya Ramdas
    * Abstract: We extend conformal prediction methodology beyond the case of exchangeable data. In particular, we show that a weighted version of conformal prediction can be used to compute distribution-free prediction intervals for problems in which the test and training covariate distributions differ, but the likelihood ratio between the two distributions is known---or, in practice, can be estimated accurately from a set of unlabeled data (test covariate points). Our weighted extension of conformal prediction also applies more broadly, to settings in which the data satisfies a certain weighted notion of exchangeability. We discuss other potential applications of our new conformal methodology, including latent variable and missing data problems.
count=5
* Why Is My Classifier Discriminatory?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf)]
    * Title: Why Is My Classifier Discriminatory?
    * Year: `2018`
    * Authors: Irene Chen, Fredrik D. Johansson, David Sontag
    * Abstract: Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.
count=5
* Multitask Boosting for Survival Analysis with Competing Risks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/2afe4567e1bf64d32a5527244d104cea-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf)]
    * Title: Multitask Boosting for Survival Analysis with Competing Risks
    * Year: `2018`
    * Authors: Alexis Bellot, Mihaela van der Schaar
    * Abstract: The co-occurrence of multiple diseases among the general population is an important problem as those patients have more risk of complications and represent a large share of health care expenditure. Learning to predict time-to-event probabilities for these patients is a challenging problem because the risks of events are correlated (there are competing risks) with often only few patients experiencing individual events of interest, and of those only a fraction are actually observed in the data. We introduce in this paper a survival model with the flexibility to leverage a common representation of related events that is designed to correct for the strong imbalance in observed outcomes. The procedure is sequential: outcome-specific survival distributions form the components of nonparametric multivariate estimators which we combine into an ensemble in such a way as to ensure accurate predictions on all outcome types simultaneously. Our algorithm is general and represents the first boosting-like method for time-to-event data with multiple outcomes. We demonstrate the performance of our algorithm on synthetic and real data.
count=5
* Learning Confidence Sets using Support Vector Machines
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/8b4224068a41c5d37f5e2d54f3995089-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/8b4224068a41c5d37f5e2d54f3995089-Paper.pdf)]
    * Title: Learning Confidence Sets using Support Vector Machines
    * Year: `2018`
    * Authors: Wenbo Wang, Xingye Qiao
    * Abstract: The goal of confidence-set learning in the binary classification setting is to construct two sets, each with a specific probability guarantee to cover a class. An observation outside the overlap of the two sets is deemed to be from one of the two classes, while the overlap is an ambiguity region which could belong to either class. Instead of plug-in approaches, we propose a support vector classifier to construct confidence sets in a flexible manner. Theoretically, we show that the proposed learner can control the non-coverage rates and minimize the ambiguity with high probability. Efficient algorithms are developed and numerical studies illustrate the effectiveness of the proposed method.
count=5
* Rectangular Bounding Process
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/8ce87bdda85cd44f14de9afb86491884-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/8ce87bdda85cd44f14de9afb86491884-Paper.pdf)]
    * Title: Rectangular Bounding Process
    * Year: `2018`
    * Authors: Xuhui Fan, Bin Li, Scott SIsson
    * Abstract: Stochastic partition models divide a multi-dimensional space into a number of rectangular regions, such that the data within each region exhibit certain types of homogeneity. Due to the nature of their partition strategy, existing partition models may create many unnecessary divisions in sparse regions when trying to describe data in dense regions. To avoid this problem we introduce a new parsimonious partition model -- the Rectangular Bounding Process (RBP) -- to efficiently partition multi-dimensional spaces, by employing a bounding strategy to enclose data points within rectangular bounding boxes. Unlike existing approaches, the RBP possesses several attractive theoretical properties that make it a powerful nonparametric partition prior on a hypercube. In particular, the RBP is self-consistent and as such can be directly extended from a finite hypercube to infinite (unbounded) space. We apply the RBP to regression trees and relational models as a flexible partition prior. The experimental results validate the merit of the RBP {in rich yet parsimonious expressiveness} compared to the state-of-the-art methods.
count=5
* Multivariate Time Series Imputation with Generative Adversarial Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/96b9bff013acedfb1d140579e2fbeb63-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/96b9bff013acedfb1d140579e2fbeb63-Paper.pdf)]
    * Title: Multivariate Time Series Imputation with Generative Adversarial Networks
    * Year: `2018`
    * Authors: Yonghong Luo, Xiangrui Cai, Ying ZHANG, Jun Xu, Yuan xiaojie
    * Abstract: Multivariate time series usually contain a large number of missing values, which hinders the application of advanced analysis methods on multivariate time series data. Conventional approaches to addressing the challenge of missing values, including mean/zero imputation, case deletion, and matrix factorization-based imputation, are all incapable of modeling the temporal dependencies and the nature of complex distribution in multivariate time series. In this paper, we treat the problem of missing value imputation as data generation. Inspired by the success of Generative Adversarial Networks (GAN) in image generation, we propose to learn the overall distribution of a multivariate time series dataset with GAN, which is further used to generate the missing values for each sample. Different from the image data, the time series data are usually incomplete due to the nature of data recording process. A modified Gate Recurrent Unit is employed in GAN to model the temporal irregularity of the incomplete time series. Experiments on two multivariate time series datasets show that the proposed model outperformed the baselines in terms of accuracy of imputation. Experimental results also showed that a simple model on the imputed data can achieve state-of-the-art results on the prediction tasks, demonstrating the benefits of our model in downstream applications.
count=5
* Cost efficient gradient boosting
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/4fac9ba115140ac4f1c22da82aa0bc7f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/4fac9ba115140ac4f1c22da82aa0bc7f-Paper.pdf)]
    * Title: Cost efficient gradient boosting
    * Year: `2017`
    * Authors: Sven Peter, Ferran Diego, Fred A. Hamprecht, Boaz Nadler
    * Abstract: Many applications require learning classifiers or regressors that are both accurate and cheap to evaluate. Prediction cost can be drastically reduced if the learned predictor is constructed such that on the majority of the inputs, it uses cheap features and fast evaluations. The main challenge is to do so with little loss in accuracy. In this work we propose a budget-aware strategy based on deep boosted regression trees. In contrast to previous approaches to learning with cost penalties, our method can grow very deep trees that on average are nonetheless cheap to compute. We evaluate our method on a number of datasets and find that it outperforms the current state of the art by a large margin. Our algorithm is easy to implement and its learning time is comparable to that of the original gradient boosting. Source code is made available at http://github.com/svenpeter42/LightGBM-CEGB.
count=5
* Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/6a508a60aa3bf9510ea6acb021c94b48-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/6a508a60aa3bf9510ea6acb021c94b48-Paper.pdf)]
    * Title: Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes
    * Year: `2017`
    * Authors: Ahmed M. Alaa, Mihaela van der Schaar
    * Abstract: Predicated on the increasing abundance of electronic health records, we investigate the problem of inferring individualized treatment effects using observational data. Stemming from the potential outcomes model, we propose a novel multi-task learning framework in which factual and counterfactual outcomes are modeled as the outputs of a function in a vector-valued reproducing kernel Hilbert space (vvRKHS). We develop a nonparametric Bayesian method for learning the treatment effects using a multi-task Gaussian process (GP) with a linear coregionalization kernel as a prior over the vvRKHS. The Bayesian approach allows us to compute individualized measures of confidence in our estimates via pointwise credible intervals, which are crucial for realizing the full potential of precision medicine. The impact of selection bias is alleviated via a risk-based empirical Bayes method for adapting the multi-task GP prior, which jointly minimizes the empirical error in factual outcomes and the uncertainty in (unobserved) counterfactual outcomes. We conduct experiments on observational datasets for an interventional social program applied to premature infants, and a left ventricular assist device applied to cardiac patients wait-listed for a heart transplant. In both experiments, we show that our method significantly outperforms the state-of-the-art.
count=5
* Learning shape correspondence with anisotropic convolutional neural networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/228499b55310264a8ea0e27b6e7c6ab6-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf)]
    * Title: Learning shape correspondence with anisotropic convolutional neural networks
    * Year: `2016`
    * Authors: Davide Boscaini, Jonathan Masci, Emanuele Rodolà, Michael Bronstein
    * Abstract: Convolutional neural networks have achieved extraordinary results in many computer vision and pattern recognition applications; however, their adoption in the computer graphics and geometry processing communities is limited due to the non-Euclidean structure of their data. In this paper, we propose Anisotropic Convolutional Neural Network (ACNN), a generalization of classical CNNs to non-Euclidean domains, where classical convolutions are replaced by projections over a set of oriented anisotropic diffusion kernels. We use ACNNs to effectively learn intrinsic dense correspondences between deformable shapes, a fundamental problem in geometry processing, arising in a wide variety of applications. We tested ACNNs performance in very challenging settings, achieving state-of-the-art results on some of the most difficult recent correspondence benchmarks.
count=4
* A Meta-Learning Approach to Predicting Performance and Data Requirements
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Jain_A_Meta-Learning_Approach_to_Predicting_Performance_and_Data_Requirements_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_A_Meta-Learning_Approach_to_Predicting_Performance_and_Data_Requirements_CVPR_2023_paper.pdf)]
    * Title: A Meta-Learning Approach to Predicting Performance and Data Requirements
    * Year: `2023`
    * Authors: Achin Jain, Gurumurthy Swaminathan, Paolo Favaro, Hao Yang, Avinash Ravichandran, Hrayr Harutyunyan, Alessandro Achille, Onkar Dabeer, Bernt Schiele, Ashwin Swaminathan, Stefano Soatto
    * Abstract: We propose an approach to estimate the number of samples required for a model to reach a target performance. We find that the power law, the de facto principle to estimate model performance, leads to large error when using a small dataset (e.g., 5 samples per class) for extrapolation. This is because the log-performance error against the log-dataset size follows a nonlinear progression in the few-shot regime followed by a linear progression in the high-shot regime. We introduce a novel piecewise power law (PPL) that handles the two data regimes differently. To estimate the parameters of the PPL, we introduce a random forest regressor trained via meta learning that generalizes across classification/detection tasks, ResNet/ViT based architectures, and random/pre-trained initializations. The PPL improves the performance estimation on average by 37% across 16 classification datasets and 33% across 10 detection datasets, compared to the power law. We further extend the PPL to provide a confidence bound and use it to limit the prediction horizon that reduces over-estimation of data by 76% on classification and 91% on detection datasets.
count=4
* Satellite Image Time Series Classification With Pixel-Set Encoders and Temporal Self-Attention
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Garnot_Satellite_Image_Time_Series_Classification_With_Pixel-Set_Encoders_and_Temporal_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Garnot_Satellite_Image_Time_Series_Classification_With_Pixel-Set_Encoders_and_Temporal_CVPR_2020_paper.pdf)]
    * Title: Satellite Image Time Series Classification With Pixel-Set Encoders and Temporal Self-Attention
    * Year: `2020`
    * Authors: Vivien Sainte Fare Garnot,  Loic Landrieu,  Sebastien Giordano,  Nesrine Chehata
    * Abstract: Satellite image time series, bolstered by their growing availability, are at the forefront of an extensive effort towards automated Earth monitoring by international institutions. In particular, large-scale control of agricultural parcels is an issue of major political and economic importance. In this regard, hybrid convolutional-recurrent neural architectures have shown promising results for the automated classification of satellite image time series. We propose an alternative approach in which the convolutional layers are advantageously replaced with encoders operating on unordered sets of pixels to exploit the typically coarse resolution of publicly available satellite images. We also propose to extract temporal features using a bespoke neural architecture based on self-attention instead of recurrent networks. We demonstrate experimentally that our method not only outperforms previous state-of-the-art approaches in terms of precision, but also significantly decreases processing time and memory requirements. Lastly, we release a large open-access annotated dataset as a benchmark for future work on satellite image time series.
count=4
* Unifying Heterogeneous Classifiers With Distillation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Vongkulbhisal_Unifying_Heterogeneous_Classifiers_With_Distillation_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Vongkulbhisal_Unifying_Heterogeneous_Classifiers_With_Distillation_CVPR_2019_paper.pdf)]
    * Title: Unifying Heterogeneous Classifiers With Distillation
    * Year: `2019`
    * Authors: Jayakorn Vongkulbhisal,  Phongtharin Vinayavekhin,  Marco Visentini-Scarzanella
    * Abstract: In this paper, we study the problem of unifying knowledge from a set of classifiers with different architectures and target classes into a single classifier, given only a generic set of unlabelled data. We call this problem Unifying Heterogeneous Classifiers (UHC). This problem is motivated by scenarios where data is collected from multiple sources, but the sources cannot share their data, e.g., due to privacy concerns, and only privately trained models can be shared. In addition, each source may not be able to gather data to train all classes due to data availability at each source, and may not be able to train the same classification model due to different computational resources. To tackle this problem, we propose a generalisation of knowledge distillation to merge HCs. We derive a probabilistic relation between the outputs of HCs and the probability over all classes. Based on this relation, we propose two classes of methods based on cross-entropy minimisation and matrix factorisation, which allow us to estimate soft labels over all classes from unlabelled samples and use them in lieu of ground truth labels to train a unified classifier. Our extensive experiments on ImageNet, LSUN, and Places365 datasets show that our approaches significantly outperform a naive extension of distillation and can achieve almost the same accuracy as classifiers that are trained in a centralised, supervised manner.
count=4
* Learning Less Is More - 6D Camera Localization via 3D Surface Regression
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Brachmann_Learning_Less_Is_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Brachmann_Learning_Less_Is_CVPR_2018_paper.pdf)]
    * Title: Learning Less Is More - 6D Camera Localization via 3D Surface Regression
    * Year: `2018`
    * Authors: Eric Brachmann, Carsten Rother
    * Abstract: Popular research areas like autonomous driving and augmented reality have renewed the interest in image-based camera localization. In this work, we address the task of predicting the 6D camera pose from a single RGB image in a given 3D environment. With the advent of neural networks, previous works have either learned the entire camera localization process, or multiple components of a camera localization pipeline. Our key contribution is to demonstrate and explain that learning a single component of this pipeline is sufficient. This component is a fully convolutional neural network for densely regressing so-called scene coordinates, defining the correspondence between the input image and the 3D scene space. The neural network is prepended to a new end-to-end trainable pipeline. Our system is efficient, highly accurate, robust in training, and exhibits outstanding generalization capabilities. It exceeds state-of-the-art consistently on indoor and outdoor datasets. Interestingly, our approach surpasses existing techniques even without utilizing a 3D model of the scene during training, since the network is able to discover 3D scene geometry automatically, solely from single-view constraints.
count=4
* PoseAgent: Budget-Constrained 6D Object Pose Estimation via Reinforcement Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Krull_PoseAgent_Budget-Constrained_6D_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Krull_PoseAgent_Budget-Constrained_6D_CVPR_2017_paper.pdf)]
    * Title: PoseAgent: Budget-Constrained 6D Object Pose Estimation via Reinforcement Learning
    * Year: `2017`
    * Authors: Alexander Krull, Eric Brachmann, Sebastian Nowozin, Frank Michel, Jamie Shotton, Carsten Rother
    * Abstract: State-of-the-art computer vision algorithms often achieve efficiency by making discrete choices about which hypotheses to explore next. This allows allocation of computational resources to promising candidates, however, such decisions are non-differentiable. As a result, these algorithms are hard to train in an end-to-end fashion. In this work we propose to learn an efficient algorithm for the task of 6D object pose estimation. Our system optimizes the parameters of an existing state-of-the art pose estimation system using reinforcement learning, where the pose estimation system now becomes the stochastic policy, parametrized by a CNN. Additionally, we present an efficient training algorithm that dramatically reduces computation time. We show empirically that our learned pose estimation procedure makes better use of limited resources and improves upon the state-of-the-art on a challenging dataset. Our approach enables differentiable end-to-end training of complex algorithmic pipelines and learns to make optimal use of a given computational budget.
count=4
* Learning to Predict Stereo Reliability Enforcing Local Consistency of Confidence Maps
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Poggi_Learning_to_Predict_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Poggi_Learning_to_Predict_CVPR_2017_paper.pdf)]
    * Title: Learning to Predict Stereo Reliability Enforcing Local Consistency of Confidence Maps
    * Year: `2017`
    * Authors: Matteo Poggi, Stefano Mattoccia
    * Abstract: Confidence measures estimate unreliable disparity assignments performed by a stereo matching algorithm and, as recently proved, can be used for several purposes. This paper aims at increasing, by means of a deep network, the effectiveness of state-of-the-art confidence measures exploiting the local consistency assumption. We exhaustively evaluated our proposal on 23 confidence measures, including 5 top-performing ones based on random-forests and CNNs, training our networks with two popular stereo algorithms and a small subset (25 out of 194 frames) of the KITTI 2012 dataset. Experimental results show that our approach dramatically increases the effectiveness of all the 23 confidence measures on the remaining frames. Moreover, without re-training, we report a further cross-evaluation on KITTI 2015 and Middlebury 2014 confirming that our proposal provides remarkable improvements for each confidence measure even when dealing with significantly different input data. To the best of our knowledge, this is the first method to move beyond conventional pixel-wise confidence estimation.
count=4
* Structured Prediction of Unobserved Voxels From a Single Depth Image
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Firman_Structured_Prediction_of_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Firman_Structured_Prediction_of_CVPR_2016_paper.pdf)]
    * Title: Structured Prediction of Unobserved Voxels From a Single Depth Image
    * Year: `2016`
    * Authors: Michael Firman, Oisin Mac Aodha, Simon Julier, Gabriel J. Brostow
    * Abstract: Building a complete 3D model of a scene, given only a single depth image, is underconstrained. To gain a full volumetric model, one needs either multiple views, or a single view together with a library of unambiguous 3D models that will fit the shape of each individual object in the scene. We hypothesize that objects of dissimilar semantic classes often share similar 3D shape components, enabling a limited dataset to model the shape of a wide range of objects, and hence estimate their hidden geometry. Exploring this hypothesis, we propose an algorithm that can complete the unobserved geometry of tabletop-sized objects, based on a supervised model trained on already available volumetric elements. Our model maps from a local observation in a single depth image to an estimate of the surface shape in the surrounding neighborhood. We validate our approach both qualitatively and quantitatively on a range of indoor object collections and challenging real scenes.
count=4
* Volumetric 3D Tracking by Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Huang_Volumetric_3D_Tracking_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Volumetric_3D_Tracking_CVPR_2016_paper.pdf)]
    * Title: Volumetric 3D Tracking by Detection
    * Year: `2016`
    * Authors: Chun-Hao Huang, Benjamin Allain, Jean-Sebastien Franco, Nassir Navab, Slobodan Ilic, Edmond Boyer
    * Abstract: In this paper, we propose a new framework for 3D tracking by detection based on fully volumetric representations. On one hand, 3D tracking by detection has shown robust use in the context of interaction (Kinect) and surface tracking. On the other hand, volumetric representations have recently been proven efficient both for building 3D features and for addressing the 3D tracking problem. We leverage these benefits by unifying both families of approaches into a single, fully volumetric tracking-by-detection framework. We use a centroidal Voronoi tessellation (CVT) representation to compactly tessellate shapes with optimal discretization, construct a feature space, and perform the tracking according to the correspondences provided by trained random forests. Our results show improved tracking and training computational efficiency and improved memory performance. This in turn enables the use of larger training databases than state of the art approaches, which we leverage by proposing a cross-tracking subject training scheme to benefit from all subject sequences for all tracking situations, thus yielding better detection and less overfitting.
count=4
* Using Self-Contradiction to Learn Confidence Measures in Stereo Vision
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Mostegel_Using_Self-Contradiction_to_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Mostegel_Using_Self-Contradiction_to_CVPR_2016_paper.pdf)]
    * Title: Using Self-Contradiction to Learn Confidence Measures in Stereo Vision
    * Year: `2016`
    * Authors: Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer, Horst Bischof
    * Abstract: Learned confidence measures gain increasing importance for outlier removal and quality improvement in stereo vision. However, acquiring the necessary training data is typically a tedious and time consuming task that involves manual interaction, active sensing devices and/or synthetic scenes. To overcome this problem, we propose a new, flexible, and scalable way for generating training data that only requires a set of stereo images as input. The key idea of our approach is to use different view points for reasoning about contradictions and consistencies between multiple depth maps generated with the same stereo algorithm. This enables us to generate a huge amount of training data in a fully automated manner. Among other experiments, we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the KITTI2012 dataset by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data.
count=4
* A Dual-Source Approach for 3D Pose Estimation From a Single Image
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Yasin_A_Dual-Source_Approach_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Yasin_A_Dual-Source_Approach_CVPR_2016_paper.pdf)]
    * Title: A Dual-Source Approach for 3D Pose Estimation From a Single Image
    * Year: `2016`
    * Authors: Hashim Yasin, Umar Iqbal, Bjorn Kruger, Andreas Weber, Juergen Gall
    * Abstract: One major challenge for 3D pose estimation from a single RGB image is the acquisition of sufficient training data. In particular, collecting large amounts of training data that contain unconstrained images and are annotated with accurate 3D poses is infeasible. We therefore propose to use two independent training sources. The first source consists of images with annotated 2D poses and the second source consists of accurate 3D motion capture data. To integrate both sources, we propose a dual-source approach that combines 2D pose estimation with efficient and robust 3D pose retrieval. In our experiments, we show that our approach achieves state-of-the-art results and is even competitive when the skeleton structure of the two sources differ substantially.
count=4
* Classifier Based Graph Construction for Video Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Khoreva_Classifier_Based_Graph_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Khoreva_Classifier_Based_Graph_2015_CVPR_paper.pdf)]
    * Title: Classifier Based Graph Construction for Video Segmentation
    * Year: `2015`
    * Authors: Anna Khoreva, Fabio Galasso, Matthias Hein, Bernt Schiele
    * Abstract: Video segmentation has become an important and active research area with a large diversity of proposed approaches. Graph-based methods, enabling topperformance on recent benchmarks, consist of three essential components: 1. powerful features account for object appearance and motion similarities; 2. spatio-temporal neighborhoods of pixels or superpixels (the graph edges) are modeled using a combination of those features; 3. video segmentation is formulated as a graph partitioning problem. While a wide variety of features have been explored and various graph partition algorithms have been proposed, there is surprisingly little research on how to construct a graph to obtain the best video segmentation performance. This is the focus of our paper. We propose to combine features by means of a classifier, use calibrated classifier outputs as edge weights and define the graph topology by edge selection. By learning the graph (without changes to the graph partitioning method), we improve the results of the best performing video segmentation algorithm by 6% on the challenging VSB100 benchmark, while reducing its runtime by 55%, as the learnt graph is much sparser.
count=4
* 3D All The Way: Semantic Segmentation of Urban Scenes From Start to End in 3D
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Martinovic_3D_All_The_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Martinovic_3D_All_The_2015_CVPR_paper.pdf)]
    * Title: 3D All The Way: Semantic Segmentation of Urban Scenes From Start to End in 3D
    * Year: `2015`
    * Authors: Andelo Martinovic, Jan Knopp, Hayko Riemenschneider, Luc Van Gool
    * Abstract: We propose a new approach for semantic segmentation of 3D city models. Starting from an SfM reconstruction of a street-side scene, we perform classification and facade splitting purely in 3D, obviating the need for slow image-based semantic segmentation methods. We show that a properly trained pure-3D approach produces high quality labelings, with significant speed benefits (20x faster) allowing us to analyze entire streets in a matter of minutes. Additionally, if speed is not of the essence, the 3D labeling can be combined with the results of a state-of-the-art 2D classifier, further boosting the performance. Further, we propose a novel facade separation based on semantic nuances between facades. Finally, inspired by the use of architectural principles for 2D facade labeling, we propose new 3D-specific principles and an efficient optimization scheme based on an integer quadratic programming formulation.
count=4
* Leveraging Stereo Matching With Learning-Based Confidence Measures
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Park_Leveraging_Stereo_Matching_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Park_Leveraging_Stereo_Matching_2015_CVPR_paper.pdf)]
    * Title: Leveraging Stereo Matching With Learning-Based Confidence Measures
    * Year: `2015`
    * Authors: Min-Gyu Park, Kuk-Jin Yoon
    * Abstract: We propose a new approach to associate supervised learning-based confidence prediction with the stereo matching problem. First of all, we analyze the characteristics of various confidence measures in the regression forest framework to select effective confidence measures using training data. We then train regression forests again to predict the correctness (confidence) of a match by using selected confidence measures. In addition, we present a confidence-based matching cost modulation scheme based on the predicted correctness for improving the robustness and accuracy of various stereo matching algorithms. We apply the proposed scheme to the semi-global matching algorithm to make it robust under unexpected difficulties that can occur in outdoor environments. We verify the proposed confidence measure selection and cost modulation methods through extensive experimentation with various aspects using KITTI and challenging outdoor datasets.
count=4
* PAIGE: PAirwise Image Geometry Encoding for Improved Efficiency in Structure-From-Motion
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Schonberger_PAIGE_PAirwise_Image_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Schonberger_PAIGE_PAirwise_Image_2015_CVPR_paper.pdf)]
    * Title: PAIGE: PAirwise Image Geometry Encoding for Improved Efficiency in Structure-From-Motion
    * Year: `2015`
    * Authors: Johannes L. Schonberger, Alexander C. Berg, Jan-Michael Frahm
    * Abstract: Large-scale Structure-from-Motion systems typically spend major computational effort on pairwise image matching and geometric verification in order to discover connected components in large-scale, unordered image collections. In recent years, the research community has spent significant effort on improving the efficiency of this stage. In this paper, we present a comprehensive overview of various state-of-the-art methods, evaluating and analyzing their performance. Based on the insights of this evaluation, we propose a learning-based approach, the PAirwise Image Geometry Encoding (PAIGE), to efficiently identify image pairs with scene overlap without the need to perform exhaustive putative matching and geometric verification. PAIGE achieves state-of-the-art performance and integrates well into existing Structure-from-Motion pipelines.
count=4
* Symmetry-Based Text Line Detection in Natural Scenes
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Zhang_Symmetry-Based_Text_Line_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Zhang_Symmetry-Based_Text_Line_2015_CVPR_paper.pdf)]
    * Title: Symmetry-Based Text Line Detection in Natural Scenes
    * Year: `2015`
    * Authors: Zheng Zhang, Wei Shen, Cong Yao, Xiang Bai
    * Abstract: Recently, a variety of real-world applications have triggered huge demand for techniques that can extract textual information from natural scenes. Therefore, scene text detection and recognition have become active research topics in computer vision. In this work, we investigate the problem of scene text detection from an alternative perspective and propose a novel algorithm for it. Different from traditional methods, which mainly make use of the properties of single characters or strokes, the proposed algorithm exploits the symmetry property of character groups and allows for direct extraction of text lines from natural images. The experiments on the latest ICDAR benchmarks demonstrate that the proposed algorithm achieves state-of-the-art performance. Moreover, compared to conventional approaches, the proposed algorithm shows stronger adaptability to texts in challenging scenarios.
count=4
* Curvilinear Structure Tracking by Low Rank Tensor Approximation with Model Propagation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Cheng_Curvilinear_Structure_Tracking_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Cheng_Curvilinear_Structure_Tracking_2014_CVPR_paper.pdf)]
    * Title: Curvilinear Structure Tracking by Low Rank Tensor Approximation with Model Propagation
    * Year: `2014`
    * Authors: Erkang Cheng, Yu Pang, Ying Zhu, Jingyi Yu, Haibin Ling
    * Abstract: Robust tracking of deformable object like catheter or vascular structures in X-ray images is an important technique used in image guided medical interventions for effective motion compensation and dynamic multi-modality image fusion. Tracking of such anatomical structures and devices is very challenging due to large degrees of appearance changes, low visibility of X-ray images and the deformable nature of the underlying motion field as a result of complex 3D anatomical movements projected into 2D images. To address these issues, we propose a new deformable tracking method using the tensor-based algorithm with model propagation. Specifically, the deformable tracking is formulated as a multi-dimensional assignment problem which is solved by rank-1 l1 tensor approximation. The model prior is propagated in the course of deformable tracking. Both the higher order information and the model prior provide powerful discriminative cues for reducing ambiguity arising from the complex background, and consequently improve the tracking robustness. To validate the proposed approach, we applied it to catheter and vascular structures tracking and tested on X-ray fluoroscopic sequences obtained from 17 clinical cases. The results show, both quantitatively and qualitatively, that our approach achieves a mean tracking error of 1.4 pixels for vascular structure and 1.3 pixels for catheter tracking.
count=4
* Predicting Object Dynamics in Scenes
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Fouhey_Predicting_Object_Dynamics_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Fouhey_Predicting_Object_Dynamics_2014_CVPR_paper.pdf)]
    * Title: Predicting Object Dynamics in Scenes
    * Year: `2014`
    * Authors: David F. Fouhey, C. L. Zitnick
    * Abstract: Given a static scene, a human can trivially enumerate the myriad of things that can happen next and characterize the relative likelihood of each. In the process, we make use of enormous amounts of commonsense knowledge about how the world works. In this paper, we investigate learning this commonsense knowledge from data. To overcome a lack of densely annotated spatiotemporal data, we learn from sequences of abstract images gathered using crowdsourcing. The abstract scenes provide both object location and attribute information. We demonstrate qualitatively and quantitatively that our models produce plausible scene predictions on both the abstract images, as well as natural images taken from the Internet.
count=4
* Strokelets: A Learned Multi-Scale Representation for Scene Text Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Yao_Strokelets_A_Learned_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Yao_Strokelets_A_Learned_2014_CVPR_paper.pdf)]
    * Title: Strokelets: A Learned Multi-Scale Representation for Scene Text Recognition
    * Year: `2014`
    * Authors: Cong Yao, Xiang Bai, Baoguang Shi, Wenyu Liu
    * Abstract: Driven by the wide range of applications, scene text detection and recognition have become active research topics in computer vision. Though extensively studied, localizing and reading text in uncontrolled environments remain extremely challenging, due to various interference factors. In this paper, we propose a novel multi-scale representation for scene text recognition. This representation consists of a set of detectable primitives, termed as strokelets, which capture the essential substructures of characters at different granularities. Strokelets possess four distinctive advantages: (1) Usability: automatically learned from bounding box labels; (2) Robustness: insensitive to interference factors; (3) Generality: applicable to variant languages; and (4) Expressivity: effective at describing characters. Extensive experiments on standard benchmarks verify the advantages of strokelets and demonstrate the effectiveness of the proposed algorithm for text recognition.
count=4
* Ensemble Learning for Confidence Measures in Stereo Vision
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Haeusler_Ensemble_Learning_for_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Haeusler_Ensemble_Learning_for_2013_CVPR_paper.pdf)]
    * Title: Ensemble Learning for Confidence Measures in Stereo Vision
    * Year: `2013`
    * Authors: Ralf Haeusler, Rahul Nair, Daniel Kondermann
    * Abstract: With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawn from the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm, which is the best performing realtime capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset.
count=4
* GeoF: Geodesic Forests for Learning Coupled Predictors
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Kontschieder_GeoF_Geodesic_Forests_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Kontschieder_GeoF_Geodesic_Forests_2013_CVPR_paper.pdf)]
    * Title: GeoF: Geodesic Forests for Learning Coupled Predictors
    * Year: `2013`
    * Authors: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi
    * Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.
count=4
* Learning Separable Filters
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Rigamonti_Learning_Separable_Filters_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Rigamonti_Learning_Separable_Filters_2013_CVPR_paper.pdf)]
    * Title: Learning Separable Filters
    * Year: `2013`
    * Authors: Roberto Rigamonti, Amos Sironi, Vincent Lepetit, Pascal Fua
    * Abstract: Learning filters to produce sparse image representations in terms of overcomplete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes, and we show that we significantly outperform state-of-theart methods on the linear structure extraction task, in terms of both accuracy and speed. Moreover, our approach is general and can be used on generic filter banks to reduce the complexity of the convolutions.
count=4
* A Higher-Order CRF Model for Road Network Extraction
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Wegner_A_Higher-Order_CRF_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Wegner_A_Higher-Order_CRF_2013_CVPR_paper.pdf)]
    * Title: A Higher-Order CRF Model for Road Network Extraction
    * Year: `2013`
    * Authors: Jan D. Wegner, Javier A. Montoya-Zegarra, Konrad Schindler
    * Abstract: The aim of this work is to extract the road network from aerial images. What makes the problem challenging is the complex structure of the prior: roads form a connected network of smooth, thin segments which meet at junctions and crossings. This type of a-priori knowledge is more difficult to turn into a tractable model than standard smoothness or co-occurrence assumptions. We develop a novel CRF formulation for road labeling, in which the prior is represented by higher-order cliques that connect sets of superpixels along straight line segments. These long-range cliques have asymmetric P es-potentials, which express a preference to assign all rather than just some of their constituent superpixels to the road class. Thus, the road likelihood is amplified for thin chains of superpixels, while the CRF is still amenable to optimization with graph cuts. Since the number of such cliques of arbitrary length is huge, we furthermore propose a sampling scheme which concentrates on those cliques which are most relevant for the optimization. In experiments on two different databases the model significantly improves both the per-pixel accuracy and the topological correctness of the extracted roads, and outperforms both a simple smoothness prior and heuristic rulebased road completion.
count=4
* Homogeneous Architecture Augmentation for Neural Predictor
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Homogeneous_Architecture_Augmentation_for_Neural_Predictor_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Homogeneous_Architecture_Augmentation_for_Neural_Predictor_ICCV_2021_paper.pdf)]
    * Title: Homogeneous Architecture Augmentation for Neural Predictor
    * Year: `2021`
    * Authors: Yuqiao Liu, Yehui Tang, Yanan Sun
    * Abstract: Neural Architecture Search (NAS) can automatically design well-performed architectures of Deep Neural Networks (DNNs) for the tasks at hand. However, one bottleneck of NAS is the prohibitively computational cost largely due to the expensive performance evaluation. The neural predictors can directly estimate the performance without any training of the DNNs to be evaluated, thus have drawn increasing attention from researchers. Despite their popularity, they also suffer a severe limitation: the shortage of annotated DNN architectures for effectively training the neural predictors. In this paper, we proposed Homogeneous Architecture Augmentation for Neural Predictor (HAAP) of DNN architectures to address the issue aforementioned. Specifically, a homogeneous architecture augmentation algorithm is proposed in HAAP to generate sufficient training data taking the use of homogeneous representation. Furthermore, the one-hot encoding strategy is introduced into HAAP to make the representation of DNN architectures more effective. The experiments have been conducted on both NAS-Benchmark-101 and NAS-Bench-201 dataset. The experimental results demonstrate that the proposed HAAP algorithm outperforms the state of the arts compared, yet with much less training data. In addition, the ablation studies on both benchmark datasets have also shown the universality of the homogeneous architecture augmentation. Our code has been made available at https://github.com/lyq998/HAAP.
count=4
* Expert Sample Consensus Applied to Camera Re-Localization
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Brachmann_Expert_Sample_Consensus_Applied_to_Camera_Re-Localization_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Brachmann_Expert_Sample_Consensus_Applied_to_Camera_Re-Localization_ICCV_2019_paper.pdf)]
    * Title: Expert Sample Consensus Applied to Camera Re-Localization
    * Year: `2019`
    * Authors: Eric Brachmann,  Carsten Rother
    * Abstract: Fitting model parameters to a set of noisy data points is a common problem in computer vision. In this work, we fit the 6D camera pose to a set of noisy correspondences between the 2D input image and a known 3D environment. We estimate these correspondences from the image using a neural network. Since the correspondences often contain outliers, we utilize a robust estimator such as Random Sample Consensus (RANSAC) or Differentiable RANSAC (DSAC) to fit the pose parameters. When the problem domain, e.g. the space of all 2D-3D correspondences, is large or ambiguous, a single network does not cover the domain well. Mixture of Experts (MoE) is a popular strategy to divide a problem domain among an ensemble of specialized networks, so called experts, where a gating network decides which expert is responsible for a given input. In this work, we introduce Expert Sample Consensus (ESAC), which integrates DSAC in a MoE. Our main technical contribution is an efficient method to train ESAC jointly and end-to-end. We demonstrate experimentally that ESAC handles two real-world problems better than competing methods, i.e. scalability and ambiguity. We apply ESAC to fitting simple geometric models to synthetic images, and to camera re-localization for difficult, real datasets.
count=4
* Bounding Boxes, Segmentations and Object Coordinates: How Important Is Recognition for 3D Scene Flow Estimation in Autonomous Driving Scenarios?
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Behl_Bounding_Boxes_Segmentations_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Behl_Bounding_Boxes_Segmentations_ICCV_2017_paper.pdf)]
    * Title: Bounding Boxes, Segmentations and Object Coordinates: How Important Is Recognition for 3D Scene Flow Estimation in Autonomous Driving Scenarios?
    * Year: `2017`
    * Authors: Aseem Behl, Omid Hosseini Jafari, Siva Karthik Mustikovela, Hassan Abu Alhaija, Carsten Rother, Andreas Geiger
    * Abstract: Existing methods for 3D scene flow estimation often fail in the presence of large displacement or local ambiguities, e.g., at texture-less or reflective surfaces. However, these challenges are omnipresent in dynamic road scenes, which is the focus of this work. Our main contribution is to overcome these 3D motion estimation problems by exploiting recognition. In particular, we investigate the importance of recognition granularity, from coarse 2D bounding box estimates over 2D instance segmentations to fine-grained 3D object part predictions. We compute these cues using CNNs trained on a newly annotated dataset of stereo images and integrate them into a CRF-based model for robust 3D scene flow estimation - an approach we term Instance Scene Flow. We analyze the importance of each recognition cue in an ablation study and observe that the instance segmentation cue is by far strongest, in our setting. We demonstrate the effectiveness of our method on the challenging KITTI 2015 scene flow benchmark where we achieve state-of-the-art performance at the time of submission.
count=4
* Monocular Free-Head 3D Gaze Tracking With Deep Learning and Geometry Constraints
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Monocular_Free-Head_3D_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Monocular_Free-Head_3D_ICCV_2017_paper.pdf)]
    * Title: Monocular Free-Head 3D Gaze Tracking With Deep Learning and Geometry Constraints
    * Year: `2017`
    * Authors: Wangjiang Zhu, Haoping Deng
    * Abstract: Free-head 3D gaze tracking outputs both the eye location and the gaze vector in 3D space, and it has wide applications in scenarios such as driver monitoring, advertisement analysis and surveillance. A reliable and low-cost monocular solution is critical for pervasive usage in these areas. Noticing that a gaze vector is a composition of head pose and eyeball movement in a geometrically deterministic way, we propose a novel gaze transform layer to connect separate head pose and eyeball movement models. The proposed decomposition does not suffer from head-gaze correlation overfitting and makes it possible to use datasets existing for other tasks. To add stronger supervision for better network training, we propose a two-step training strategy, which first trains sub-tasks with rough labels and then jointly trains with accurate gaze labels. To enable good cross-subject performance under various conditions, we collect a large dataset which has full coverage of head poses and eyeball movements, contains 200 subjects, and has diverse illumination conditions. Our deep solution achieves state-of-the-art gaze tracking accuracy, reaching 5.6 degrees cross-subject prediction error using a small network running at 1000 fps on a s ingle CPU (excluding face alignment time) and 4.3 degrees cross-subject error with a deeper network.
count=4
* Robust Nonrigid Registration by Convex Optimization
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Chen_Robust_Nonrigid_Registration_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Chen_Robust_Nonrigid_Registration_ICCV_2015_paper.pdf)]
    * Title: Robust Nonrigid Registration by Convex Optimization
    * Year: `2015`
    * Authors: Qifeng Chen, Vladlen Koltun
    * Abstract: We present an approach to nonrigid registration of 3D surfaces. We cast isometric embedding as MRF optimization and apply efficient global optimization algorithms based on linear programming relaxations. The Markov random field perspective suggests a natural connection with robust statistics and motivates robust forms of the intrinsic distortion functional. Our approach outperforms a large body of prior work by a significant margin, increasing registration precision on real data by a factor of 3.
count=4
* Flowing ConvNets for Human Pose Estimation in Videos
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Pfister_Flowing_ConvNets_for_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Pfister_Flowing_ConvNets_for_ICCV_2015_paper.pdf)]
    * Title: Flowing ConvNets for Human Pose Estimation in Videos
    * Year: `2015`
    * Authors: Tomas Pfister, James Charles, Andrew Zisserman
    * Abstract: The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow. To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps; (ii) spatial fusion layers that learn an implicit spatial model; (iii) optical flow is used to align heatmap predictions from neighbouring frames; and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map. We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, one that regresses joint coordinates directly, and one that predicts heatmaps without spatial fusion. The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset, and outperforms other deep methods that don't use a graphical model on the single-image FLIC benchmark (and also Chen & Yuille and Tompson et al. in the high precision region).
count=4
* Projection Onto the Manifold of Elongated Structures for Accurate Extraction
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Sironi_Projection_Onto_the_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Sironi_Projection_Onto_the_ICCV_2015_paper.pdf)]
    * Title: Projection Onto the Manifold of Elongated Structures for Accurate Extraction
    * Year: `2015`
    * Authors: Amos Sironi, Vincent Lepetit, Pascal Fua
    * Abstract: Detection of elongated structures in 2D images and 3D image stacks is a critical prerequisite in many applications and Machine Learning-based approaches have recently been shown to deliver superior performance. However, these methods essentially classify individual locations and do not explicitly model the strong relationship that exists between neighboring ones. As a result, isolated erroneous responses, discontinuities, and topological errors are present in the resulting score maps. We solve this problem by projecting patches of the score map to their nearest neighbors in a set of ground truth training patches. Our algorithm induces global spatial consistency on the classifier score map and returns results that are provably geometrically consistent. We apply our algorithm to challenging datasets in four different domains and show that it compares favorably to state-of-the-art methods.
count=4
* A Versatile Learning-Based 3D Temporal Tracker: Scalable, Robust, Online
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Tan_A_Versatile_Learning-Based_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Tan_A_Versatile_Learning-Based_ICCV_2015_paper.pdf)]
    * Title: A Versatile Learning-Based 3D Temporal Tracker: Scalable, Robust, Online
    * Year: `2015`
    * Authors: David Joseph Tan, Federico Tombari, Slobodan Ilic, Nassir Navab
    * Abstract: This paper proposes a temporal tracking algorithm based on Random Forest that uses depth images to estimate and track the 3D pose of a rigid object in real-time. Compared to the state of the art aimed at the same goal, our algorithm holds important attributes such as high robustness against holes and occlusion, low computational cost of both learning and tracking stages, and low memory consumption. These are obtained (a) by a novel formulation of the learning strategy, based on a dense sampling of the camera viewpoints and learning independent trees from a single image for each camera view; as well as, (b) by an insightful occlusion handling strategy that enforces the forest to recognize the object's local and global structures. Due to these attributes, we report state-of-the-art tracking accuracy on benchmark datasets, and accomplish remarkable scalability with the number of targets, being able to simultaneously track the pose of over a hundred objects at 30 fps with an off-the-shelf CPU. In addition, the fast learning time enables us to extend our algorithm as a robust online tracker for model-free 3D objects under different viewpoints and appearance changes as demonstrated by the experiments.
count=4
* Ensemble Projection for Semi-supervised Image Classification
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Dai_Ensemble_Projection_for_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Dai_Ensemble_Projection_for_2013_ICCV_paper.pdf)]
    * Title: Ensemble Projection for Semi-supervised Image Classification
    * Year: `2013`
    * Authors: Dengxin Dai, Luc Van Gool
    * Abstract: This paper investigates the problem of semi-supervised classification. Unlike previous methods to regularize classifying boundaries with unlabeled data, our method learns a new image representation from all available data (labeled and unlabeled) and performs plain supervised learning with the new feature. In particular, an ensemble of image prototype sets are sampled automatically from the available data, to represent a rich set of visual categories/attributes. Discriminative functions are then learned on these prototype sets, and image are represented by the concatenation of their projected values onto the prototypes (similarities to them) for further classification. Experiments on four standard datasets show three interesting phenomena: (1) our method consistently outperforms previous methods for semi-supervised image classification; (2) our method lets itself combine well with these methods; and (3) our method works well for self-taught image classification where unlabeled data are not coming from the same distribution as labeled ones, but rather from a random collection of images.
count=4
* Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Huang_Text_Localization_in_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Huang_Text_Localization_in_2013_ICCV_paper.pdf)]
    * Title: Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors
    * Year: `2013`
    * Authors: Weilin Huang, Zhe Lin, Jianchao Yang, Jue Wang
    * Abstract: In this paper, we present a new approach for text localization in natural images, by discriminating text and non-text regions at three levels: pixel, component and textline levels. Firstly, a powerful low-level filter called the Stroke Feature Transform (SFT) is proposed, which extends the widely-used Stroke Width Transform (SWT) by incorporating color cues of text pixels, leading to significantly enhanced performance on inter-component separation and intra-component connection. Secondly, based on the output of SFT, we apply two classifiers, a text component classifier and a text-line classifier, sequentially to extract text regions, eliminating the heuristic procedures that are commonly used in previous approaches. The two classifiers are built upon two novel Text Covariance Descriptors (TCDs) that encode both the heuristic properties and the statistical characteristics of text stokes. Finally, text regions are located by simply thresholding the text-line confident map. Our method was evaluated on two benchmark datasets: ICDAR 2005 and ICDAR 2011, and the corresponding Fmeasure values are 0.72 and 0.73, respectively, surpassing previous methods in accuracy by a large margin.
count=4
* Efficient 3D Scene Labeling Using Fields of Trees
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Kahler_Efficient_3D_Scene_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Kahler_Efficient_3D_Scene_2013_ICCV_paper.pdf)]
    * Title: Efficient 3D Scene Labeling Using Fields of Trees
    * Year: `2013`
    * Authors: Olaf Kahler, Ian Reid
    * Abstract: We address the problem of 3D scene labeling in a structured learning framework. Unlike previous work which uses structured Support Vector Machines, we employ the recently described Decision Tree Field and Regression Tree Field frameworks, which learn the unary and binary terms of a Conditional Random Field from training data. We show this has significant advantages in terms of inference speed, while maintaining similar accuracy. We also demonstrate empirically the importance for overall labeling accuracy of features that make use of prior knowledge about the coarse scene layout such as the location of the ground plane. We show how this coarse layout can be estimated by our framework automatically, and that this information can be used to bootstrap improved accuracy in the detailed labeling.
count=4
* Model Recommendation with Virtual Probes for Egocentric Hand Detection
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Li_Model_Recommendation_with_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Li_Model_Recommendation_with_2013_ICCV_paper.pdf)]
    * Title: Model Recommendation with Virtual Probes for Egocentric Hand Detection
    * Year: `2013`
    * Authors: Cheng Li, Kris M. Kitani
    * Abstract: Egocentric cameras can be used to benefit such tasks as analyzing fine motor skills, recognizing gestures and learning about hand-object manipulation. To enable such technology, we believe that the hands must detected on the pixellevel to gain important information about the shape of the hands and fingers. We show that the problem of pixel-wise hand detection can be effectively solved, by posing the problem as a model recommendation task. As such, the goal of a recommendation system is to recommend the n-best hand detectors based on the probe set a small amount of labeled data from the test distribution. This requirement of a probe set is a serious limitation in many applications, such as ego-centric hand detection, where the test distribution may be continually changing. To address this limitation, we propose the use of virtual probes which can be automatically extracted from the test distribution. The key idea is that many features, such as the color distribution or relative performance between two detectors, can be used as a proxy to the probe set. In our experiments we show that the recommendation paradigm is well-equipped to handle complex changes in the appearance of the hands in firstperson vision. In particular, we show how our system is able to generalize to new scenarios by testing our model across multiple users.
count=4
* Conservation Tracking
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Schiegg_Conservation_Tracking_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Schiegg_Conservation_Tracking_2013_ICCV_paper.pdf)]
    * Title: Conservation Tracking
    * Year: `2013`
    * Authors: Martin Schiegg, Philipp Hanslovsky, Bernhard X. Kausler, Lars Hufnagel, Fred A. Hamprecht
    * Abstract: The quality of any tracking-by-assignment hinges on the accuracy of the foregoing target detection / segmentation step. In many kinds of images, errors in this first stage are unavoidable. These errors then propagate to, and corrupt, the tracking result. Our main contribution is the first probabilistic graphical model that can explicitly account for overand undersegmentation errors even when the number of tracking targets is unknown and when they may divide, as in cell cultures. The tracking model we present implements global consistency constraints for the number of targets comprised by each detection and is solved to global optimality on reasonably large 2D+t and 3D+t datasets. In addition, we empirically demonstrate the effectiveness of a postprocessing that allows to establish target identity even across occlusion / undersegmentation. The usefulness and efficiency of this new tracking method is demonstrated on three different and challenging 2D+t and 3D+t datasets from developmental biology.
count=4
* Auditing Fairness by Betting
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/1338c277525011f20166cf740952bb47-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/1338c277525011f20166cf740952bb47-Paper-Conference.pdf)]
    * Title: Auditing Fairness by Betting
    * Year: `2023`
    * Authors: Ben Chugg, Santiago Cortes-Gomez, Bryan Wilder, Aaditya Ramdas
    * Abstract: We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics---the ``testing by betting'' framework in particular. These connections ensure that our methods are interpretable, fast, and easy to implement. We demonstrate the efficacy of our approach on three benchmark fairness datasets.
count=4
* Joint Training of Deep Ensembles Fails Due to Learner Collusion
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/2bde8fef08f7ebe42b584266cbcfc909-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/2bde8fef08f7ebe42b584266cbcfc909-Paper-Conference.pdf)]
    * Title: Joint Training of Deep Ensembles Fails Due to Learner Collusion
    * Year: `2023`
    * Authors: Alan Jeffares, Tennison Liu, Jonathan Crabbé, Mihaela van der Schaar
    * Abstract: Ensembles of machine learning models have been well established as a powerful method of improving performance over a single model. Traditionally, ensembling algorithms train their base learners independently or sequentially with the goal of optimizing their joint performance. In the case of deep ensembles of neural networks, we are provided with the opportunity to directly optimize the true objective: the joint performance of the ensemble as a whole. Surprisingly, however, directly minimizing the loss of the ensemble appears to rarely be applied in practice. Instead, most previous research trains individual models independently with ensembling performed post hoc. In this work, we show that this is for good reason - joint optimization of ensemble loss results in degenerate behavior. We approach this problem by decomposing the ensemble objective into the strength of the base learners and the diversity between them. We discover that joint optimization results in a phenomenon in which base learners collude to artificially inflate their apparent diversity. This pseudo-diversity fails to generalize beyond the training data, causing a larger generalization gap. We proceed to comprehensively demonstrate the practical implications of this effect on a range of standard machine learning tasks and architectures by smoothly interpolating between independent training and joint optimization.
count=4
* Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/6aa9a05b929fb08ff46a58cab6cf860d-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/6aa9a05b929fb08ff46a58cab6cf860d-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark
    * Year: `2023`
    * Authors: Lasse Hansen, Nabeel Seedat, Mihaela van der Schaar, Andrija Petrovic
    * Abstract: Synthetic data serves as an alternative in training machine learning models, particularly when real-world data is limited or inaccessible. However, ensuring that synthetic data mirrors the complex nuances of real-world data is a challenging task. This paper addresses this issue by exploring the potential of integrating data-centric AI techniques which profile the data to guide the synthetic data generation process. Moreover, we shed light on the often ignored consequences of neglecting these data profiles during synthetic data generation --- despite seemingly high statistical fidelity. Subsequently, we propose a novel framework to evaluate the integration of data profiles to guide the creation of more representative synthetic data. In an empirical study, we evaluate the performance of five state-of-the-art models for tabular data generation on eleven distinct tabular datasets. The findings offer critical insights into the successes and limitations of current synthetic data generation techniques. Finally, we provide practical recommendations for integrating data-centric insights into the synthetic data generation process, with a specific focus on classification performance, model selection, and feature selection. This study aims to reevaluate conventional approaches to synthetic data generation and promote the application of data-centric AI techniques in improving the quality and effectiveness of synthetic data.
count=4
* Model Shapley: Equitable Model Valuation with Black-box Access
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/86bcae6da75c72e32f30a5553f094c06-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/86bcae6da75c72e32f30a5553f094c06-Paper-Conference.pdf)]
    * Title: Model Shapley: Equitable Model Valuation with Black-box Access
    * Year: `2023`
    * Authors: Xinyi Xu, Thanh Lam, Chuan Sheng Foo, Bryan Kian Hsiang Low
    * Abstract: Valuation methods of data and machine learning (ML) models are essential to the establishment of AI marketplaces. Importantly, certain practical considerations (e.g., operational constraints, legal restrictions) favor the use of model valuation over data valuation. Also, existing marketplaces that involve trading of pre-trained ML models call for an equitable model valuation method to price them. In particular, we investigate the black-box access setting which allows querying a model (to observe predictions) without disclosing model-specific information (e.g., architecture and parameters). By exploiting a Dirichlet abstraction of a model’s predictions, we propose a novel and equitable model valuation method called model Shapley. We also leverage a Lipschitz continuity of model Shapley to design a learning approach for predicting the model Shapley values (MSVs) of many vendors’ models (e.g., 150) in a large-scale marketplace. We perform extensive empirical validation on the effectiveness of model Shapley using various real-world datasets and heterogeneous model types.
count=4
* Disentanglement via Latent Quantization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/8e63972d4d9d81b31459d787466ce271-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/8e63972d4d9d81b31459d787466ce271-Paper-Conference.pdf)]
    * Title: Disentanglement via Latent Quantization
    * Year: `2023`
    * Authors: Kyle Hsu, William Dorrell, James Whittington, Jiajun Wu, Chelsea Finn
    * Abstract: In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards encoding to and decoding from an organized latent space. Concretely, we do this by (i) quantizing the latent space into discrete code vectors with a separate learnable scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the latent space design forces the encoder to combinatorially construct codes from a small number of distinct scalar values, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this approach by adding it to both basic data-reconstructing (vanilla autoencoder) and latent-reconstructing (InfoGAN) generative models. For reliable evaluation, we also propose InfoMEC, a new set of metrics for disentanglement that is cohesively grounded in information theory and fixes well-established shortcomings in previous metrics. Together with regularization, latent quantization dramatically improves the modularity and explicitness of learned representations on a representative suite of benchmark datasets. In particular, our quantized-latent autoencoder (QLAE) consistently outperforms strong methods from prior work in these key disentanglement properties without compromising data reconstruction.
count=4
* High Precision Causal Model Evaluation with Conditional Randomization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/a4dfcbcfa1f0425cd18aafa35a68019a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/a4dfcbcfa1f0425cd18aafa35a68019a-Paper-Conference.pdf)]
    * Title: High Precision Causal Model Evaluation with Conditional Randomization
    * Year: `2023`
    * Authors: Chao Ma, Cheng Zhang
    * Abstract: The gold standard for causal model evaluation involves comparing model predictions with true effects estimated from randomized controlled trials (RCT). However, RCTs are not always feasible or ethical to perform. In contrast, conditionally randomized experiments based on inverse probability weighting (IPW) offer a more realistic approach but may suffer from high estimation variance. To tackle this challenge and enhance causal model evaluation in real-world conditional randomization settings, we introduce a novel low-variance estimator for causal error, dubbed as the pairs estimator. By applying the same IPW estimator to both the model and true experimental effects, our estimator effectively cancels out the variance due to IPW and achieves a smaller asymptotic variance. Empirical studies demonstrate the improved of our estimator, highlighting its potential on achieving near-RCT performance. Our method offers a simple yet powerful solution to evaluate causal inference models in conditional randomization settings without complicated modification of the IPW estimator itself, paving the way for more robust and reliable model assessments.
count=4
* Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/b456a00e145ad56f6f251f79f8c8a7de-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/b456a00e145ad56f6f251f79f8c8a7de-Paper-Conference.pdf)]
    * Title: Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation
    * Year: `2023`
    * Authors: Seunghwan An, Jong-June Jeon
    * Abstract: The Gaussianity assumption has been consistently criticized as a main limitation of the Variational Autoencoder (VAE) despite its efficiency in computational modeling. In this paper, we propose a new approach that expands the model capacity (i.e., expressive power of distributional family) without sacrificing the computational advantages of the VAE framework. Our VAE model's decoder is composed of an infinite mixture of asymmetric Laplace distribution, which possesses general distribution fitting capabilities for continuous variables. Our model is represented by a special form of a nonparametric M-estimator for estimating general quantile functions, and we theoretically establish the relevance between the proposed model and quantile estimation. We apply the proposed model to synthetic data generation, and particularly, our model demonstrates superiority in easily adjusting the level of data privacy.
count=4
* An Efficient Doubly-Robust Test for the Kernel Treatment Effect
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/bccdd196d798a51a4961989984a9ed4a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/bccdd196d798a51a4961989984a9ed4a-Paper-Conference.pdf)]
    * Title: An Efficient Doubly-Robust Test for the Kernel Treatment Effect
    * Year: `2023`
    * Authors: Diego Martinez Taboada, Aaditya Ramdas, Edward Kennedy
    * Abstract: The average treatment effect, which is the difference in expectation of the counterfactuals, is probably the most popular target effect in causal inference with binary treatments. However, treatments may have effects beyond the mean, for instance decreasing or increasing the variance. We propose a new kernel-based test for distributional effects of the treatment. It is, to the best of our knowledge, the first kernel-based, doubly-robust test with provably valid type-I error. Furthermore, our proposed algorithm is computationally efficient, avoiding the use of permutations.
count=4
* Understanding the Limitations of Deep Models for Molecular property prediction: Insights and Solutions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/cc83e97320000f4e08cb9e293b12cf7e-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/cc83e97320000f4e08cb9e293b12cf7e-Paper-Conference.pdf)]
    * Title: Understanding the Limitations of Deep Models for Molecular property prediction: Insights and Solutions
    * Year: `2023`
    * Authors: Jun Xia, Lecheng Zhang, Xiao Zhu, Yue Liu, Zhangyang Gao, Bozhen Hu, Cheng Tan, Jiangbin Zheng, Siyuan Li, Stan Z. Li
    * Abstract: Molecular Property Prediction (MPP) is a crucial task in the AI-driven Drug Discovery (AIDD) pipeline, which has recently gained considerable attention thanks to advancements in deep learning. However, recent research has revealed that deep models struggle to beat traditional non-deep ones on MPP. In this study, we benchmark 12 representative models (3 non-deep models and 9 deep models) on 15 molecule datasets. Through the most comprehensive study to date, we make the following key observations: \textbf{(\romannumeral 1)} Deep models are generally unable to outperform non-deep ones; \textbf{(\romannumeral 2)} The failure of deep models on MPP cannot be solely attributed to the small size of molecular datasets; \textbf{(\romannumeral 3)} In particular, some traditional models including XGB and RF that use molecular fingerprints as inputs tend to perform better than other competitors. Furthermore, we conduct extensive empirical investigations into the unique patterns of molecule data and inductive biases of various models underlying these phenomena. These findings stimulate us to develop a simple-yet-effective feature mapping method for molecule data prior to feeding them into deep models. Empirically, deep models equipped with this mapping method can beat non-deep ones in most MoleculeNet datasets. Notably, the effectiveness is further corroborated by extensive experiments on cutting-edge dataset related to COVID-19 and activity cliff datasets.
count=4
* Feature Selection in the Contrastive Analysis Setting
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d083980ec9f874025550136b776a96a9-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d083980ec9f874025550136b776a96a9-Paper-Conference.pdf)]
    * Title: Feature Selection in the Contrastive Analysis Setting
    * Year: `2023`
    * Authors: Ethan Weinberger, Ian Covert, Su-In Lee
    * Abstract: Contrastive analysis (CA) refers to the exploration of variations uniquely enriched in a target dataset as compared to a corresponding background dataset generated from sources of variation that are irrelevant to a given task. For example, a biomedical data analyst may wish to find a small set of genes to use as a proxy for variations in genomic data only present among patients with a given disease (target) as opposed to healthy control subjects (background). However, as of yet the problem of feature selection in the CA setting has received little attention from the machine learning community. In this work we present contrastive feature selection (CFS),a method for performing feature selection in the CA setting. We motivate our approach with a novel information-theoretic analysis of representation learning in the CA setting, and we empirically validate CFS on a semi-synthetic dataset and four real-world biomedical datasets. We find that our method consistently outperforms previously proposed state-of-the-art supervised and fully unsupervised feature selection methods not designed for the CA setting. An open-source implementation of our method is available at https://github.com/suinleelab/CFS.
count=4
* Semi-Supervised Learning with Decision Trees: Graph Laplacian Tree Alternating Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/104f7b25495a0e40e65fb7c7eee37ed9-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/104f7b25495a0e40e65fb7c7eee37ed9-Paper-Conference.pdf)]
    * Title: Semi-Supervised Learning with Decision Trees: Graph Laplacian Tree Alternating Optimization
    * Year: `2022`
    * Authors: Arman Zharmagambetov, Miguel A. Carreira-Perpinan
    * Abstract: Semi-supervised learning seeks to learn a machine learning model when only a small amount of the available data is labeled. The most widespread approach uses a graph prior, which encourages similar instances to have similar predictions. This has been very successful with models ranging from kernel machines to neural networks, but has remained inapplicable to decision trees, for which the optimization problem is much harder. We solve this based on a reformulation of the problem which requires iteratively solving two simpler problems: a supervised tree learning problem, which can be solved by the Tree Alternating Optimization algorithm; and a label smoothing problem, which can be solved through a sparse linear system. The algorithm is scalable and highly effective even with very few labeled instances, and makes it possible to learn accurate, interpretable models based on decision trees in such situations.
count=4
* Falsification before Extrapolation in Causal Effect Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/28b5dfc51e5ae12d84fb7c6172a00df4-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/28b5dfc51e5ae12d84fb7c6172a00df4-Paper-Conference.pdf)]
    * Title: Falsification before Extrapolation in Causal Effect Estimation
    * Year: `2022`
    * Authors: Zeshan M Hussain, Michael Oberst, Ming-Chieh Shih, David Sontag
    * Abstract: Randomized Controlled Trials (RCTs) represent a gold standard when developing policy guidelines. However, RCTs are often narrow, and lack data on broader populations of interest. Causal effects in these populations are often estimated using observational datasets, which may suffer from unobserved confounding and selection bias. Given a set of observational estimates (e.g., from multiple studies), we propose a meta-algorithm that attempts to reject observational estimates that are biased. We do so using validation effects, causal effects that can be inferred from both RCT and observational data. After rejecting estimators that do not pass this test, we generate conservative confidence intervals on the extrapolated causal effects for subgroups not observed in the RCT. Under the assumption that at least one observational estimator is asymptotically normal and consistent for both the validation and extrapolated effects, we provide guarantees on the coverage probability of the intervals output by our algorithm. To facilitate hypothesis testing in settings where causal effect transportation across datasets is necessary, we give conditions under which a doubly-robust estimator of group average treatment effects is asymptotically normal, even when flexible machine learning methods are used for estimation of nuisance parameters. We illustrate the properties of our approach on semi-synthetic experiments based on the IHDP dataset, and show that it compares favorably to standard meta-analysis techniques.
count=4
* On the Safety of Interpretable Machine Learning: A Maximum Deviation Approach
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/402e12102d6ec3ea3df40ce1b23d423a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/402e12102d6ec3ea3df40ce1b23d423a-Paper-Conference.pdf)]
    * Title: On the Safety of Interpretable Machine Learning: A Maximum Deviation Approach
    * Year: `2022`
    * Authors: Dennis Wei, Rahul Nair, Amit Dhurandhar, Kush R. Varshney, Elizabeth Daly, Moninder Singh
    * Abstract: Interpretable and explainable machine learning has seen a recent surge of interest. We focus on safety as a key motivation behind the surge and make the relationship between interpretability and safety more quantitative. Toward assessing safety, we introduce the concept of maximum deviation via an optimization problem to find the largest deviation of a supervised learning model from a reference model regarded as safe. We then show how interpretability facilitates this safety assessment. For models including decision trees, generalized linear and additive models, the maximum deviation can be computed exactly and efficiently. For tree ensembles, which are not regarded as interpretable, discrete optimization techniques can still provide informative bounds. For a broader class of piecewise Lipschitz functions, we leverage the multi-armed bandit literature to show that interpretability produces tighter (regret) bounds on the maximum deviation. We present case studies, including one on mortgage approval, to illustrate our methods and the insights about models that may be obtained from deviation maximization.
count=4
* Sparse Interaction Additive Networks via Feature Interaction Detection and Sparse Selection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/5a3674849d6d6d23ac088b9a2552f323-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/5a3674849d6d6d23ac088b9a2552f323-Paper-Conference.pdf)]
    * Title: Sparse Interaction Additive Networks via Feature Interaction Detection and Sparse Selection
    * Year: `2022`
    * Authors: James Enouen, Yan Liu
    * Abstract: There is currently a large gap in performance between the statistically rigorous methods like linear regression or additive splines and the powerful deep methods using neural networks. Previous works attempting to close this gap have failed to fully consider the exponentially growing number of feature combinations which deep networks consider automatically during training. In this work, we develop a tractable selection algorithm to efficiently identify the necessary feature combinations by leveraging techniques in feature interaction detection.Our proposed Sparse Interaction Additive Networks (SIAN) construct a bridge from these simple and interpretable models to a fully connected neural network. SIAN achieves competitive performance against state-of-the-art methods across multiple large-scale tabular datasets and consistently finds an optimal tradeoff between the modeling capacity of neural networks and the generalizability of simpler methods.
count=4
* Washing The Unwashable : On The (Im)possibility of Fairwashing Detection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/5b84864ff8474fd742c66f219b2eaac1-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/5b84864ff8474fd742c66f219b2eaac1-Paper-Conference.pdf)]
    * Title: Washing The Unwashable : On The (Im)possibility of Fairwashing Detection
    * Year: `2022`
    * Authors: Ali Shahin Shamsabadi, Mohammad Yaghini, Natalie Dullerud, Sierra Wyllie, Ulrich Aïvodji, Aisha Alaagib, Sébastien Gambs, Nicolas Papernot
    * Abstract: The use of black-box models (e.g., deep neural networks) in high-stakes decision-making systems, whose internal logic is complex, raises the need for providing explanations about their decisions. Model explanation techniques mitigate this problem by generating an interpretable and high-fidelity surrogate model (e.g., a logistic regressor or decision tree) to explain the logic of black-box models. In this work, we investigate the issue of fairwashing, in which model explanation techniques are manipulated to rationalize decisions taken by an unfair black-box model using deceptive surrogate models. More precisely, we theoretically characterize and analyze fairwashing, proving that this phenomenon is difficult to avoid due to an irreducible factor---the unfairness of the black-box model. Based on the theory developed, we propose a novel technique, called FRAUD-Detect (FaiRness AUDit Detection), to detect fairwashed models by measuring a divergence over subpopulation-wise fidelity measures of the interpretable model. We empirically demonstrate that this divergence is significantly larger in purposefully fairwashed interpretable models than in honest ones. Furthermore, we show that our detector is robust to an informed adversary trying to bypass our detector. The code implementing FRAUD-Detect is available at https://github.com/cleverhans-lab/FRAUD-Detect.
count=4
* Fair Wrapping for Black-box Predictions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/876b45367d9069f0e91e359c57155ab1-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/876b45367d9069f0e91e359c57155ab1-Paper-Conference.pdf)]
    * Title: Fair Wrapping for Black-box Predictions
    * Year: `2022`
    * Authors: Alexander Soen, Ibrahim M. Alabdulmohsin, Sanmi Koyejo, Yishay Mansour, Nyalleng Moorosi, Richard Nock, Ke Sun, Lexing Xie
    * Abstract: We introduce a new family of techniques to post-process (``wrap") a black-box classifier in order to reduce its bias. Our technique builds on the recent analysis of improper loss functions whose optimization can correct any twist in prediction, unfairness being treated as a twist. In the post-processing, we learn a wrapper function which we define as an $\alpha$-tree, which modifies the prediction. We provide two generic boosting algorithms to learn $\alpha$-trees. We show that our modification has appealing properties in terms of composition of $\alpha$-trees, generalization, interpretability, and KL divergence between modified and original predictions. We exemplify the use of our technique in three fairness notions: conditional value-at-risk, equality of opportunity, and statistical parity; and provide experiments on several readily available datasets.
count=4
* GLOBEM Dataset: Multi-Year Datasets for Longitudinal Human Behavior Modeling Generalization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/9c7e8a0821dfcb58a9a83cbd37cc8131-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/9c7e8a0821dfcb58a9a83cbd37cc8131-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: GLOBEM Dataset: Multi-Year Datasets for Longitudinal Human Behavior Modeling Generalization
    * Year: `2022`
    * Authors: Xuhai Xu, Han Zhang, Yasaman Sefidgar, Yiyi Ren, Xin Liu, Woosuk Seo, Jennifer Brown, Kevin Kuehn, Mike Merrill, Paula Nurius, Shwetak Patel, Tim Althoff, Margaret Morris, Eve Riskin, Jennifer Mankoff, Anind Dey
    * Abstract: Recent research has demonstrated the capability of behavior signals captured by smartphones and wearables for longitudinal behavior modeling. However, there is a lack of a comprehensive public dataset that serves as an open testbed for fair comparison among algorithms. Moreover, prior studies mainly evaluate algorithms using data from a single population within a short period, without measuring the cross-dataset generalizability of these algorithms. We present the first multi-year passive sensing datasets, containing over 700 user-years and 497 unique users’ data collected from mobile and wearable sensors, together with a wide range of well-being metrics. Our datasets can support multiple cross-dataset evaluations of behavior modeling algorithms’ generalizability across different users and years. As a starting point, we provide the benchmark results of 18 algorithms on the task of depression detection. Our results indicate that both prior depression detection algorithms and domain generalization techniques show potential but need further research to achieve adequate cross-dataset generalizability. We envision our multi-year datasets can support the ML community in developing generalizable longitudinal behavior modeling algorithms.
count=4
* Removing Inter-Experimental Variability from Functional Data in Systems Neuroscience
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/1e5eeb40a3fce716b244599862fd2200-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/1e5eeb40a3fce716b244599862fd2200-Paper.pdf)]
    * Title: Removing Inter-Experimental Variability from Functional Data in Systems Neuroscience
    * Year: `2021`
    * Authors: Dominic Gonschorek, Larissa Höfling, Klaudia P. Szatko, Katrin Franke, Timm Schubert, Benjamin Dunn, Philipp Berens, David Klindt, Thomas Euler
    * Abstract: Integrating data from multiple experiments is common practice in systems neuroscience but it requires inter-experimental variability to be negligible compared to the biological signal of interest. This requirement is rarely fulfilled; systematic changes between experiments can drastically affect the outcome of complex analysis pipelines. Modern machine learning approaches designed to adapt models across multiple data domains offer flexible ways of removing inter-experimental variability where classical statistical methods often fail. While applications of these methods have been mostly limited to single-cell genomics, in this work, we develop a theoretical framework for domain adaptation in systems neuroscience. We implement this in an adversarial optimization scheme that removes inter-experimental variability while preserving the biological signal. We compare our method to previous approaches on a large-scale dataset of two-photon imaging recordings of retinal bipolar cell responses to visual stimuli. This dataset provides a unique benchmark as it contains biological signal from well-defined cell types that is obscured by large inter-experimental variability. In a supervised setting, we compare the generalization performance of cell type classifiers across experiments, which we validate with anatomical cell type distributions from electron microscopy data. In an unsupervised setting, we remove inter-experimental variability from the data which can then be fed into arbitrary downstream analyses. In both settings, we find that our method achieves the best trade-off between removing inter-experimental variability and preserving biological signal. Thus, we offer a flexible approach to remove inter-experimental variability and integrate datasets across experiments in systems neuroscience. Code available at https://github.com/eulerlab/rave.
count=4
* On Inductive Biases for Heterogeneous Treatment Effect Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/8526e0962a844e4a2f158d831d5fddf7-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/8526e0962a844e4a2f158d831d5fddf7-Paper.pdf)]
    * Title: On Inductive Biases for Heterogeneous Treatment Effect Estimation
    * Year: `2021`
    * Authors: Alicia Curth, Mihaela van der Schaar
    * Abstract: We investigate how to exploit structural similarities of an individual's potential outcomes (POs) under different treatments to obtain better estimates of conditional average treatment effects in finite samples. Especially when it is unknown whether a treatment has an effect at all, it is natural to hypothesize that the POs are similar -- yet, some existing strategies for treatment effect estimation employ regularization schemes that implicitly encourage heterogeneity even when it does not exist and fail to fully make use of shared structure. In this paper, we investigate and compare three end-to-end learning strategies to overcome this problem -- based on regularization, reparametrization and a flexible multi-task architecture -- each encoding inductive bias favoring shared behavior across POs. To build understanding of their relative strengths, we implement all strategies using neural networks and conduct a wide range of semi-synthetic experiments. We observe that all three approaches can lead to substantial improvements upon numerous baselines and gain insight into performance differences across various experimental settings.
count=4
* Learning Riemannian metric for disease progression modeling
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/c7b90b0fc23725f299b47c5224e6ec0d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/c7b90b0fc23725f299b47c5224e6ec0d-Paper.pdf)]
    * Title: Learning Riemannian metric for disease progression modeling
    * Year: `2021`
    * Authors: Samuel Gruffaz, Pierre-Emmanuel Poulet, Etienne Maheux, Bruno Jedynak, Stanley DURRLEMAN
    * Abstract: Linear mixed-effect models provide a natural baseline for estimating disease progression using longitudinal data. They provide interpretable models at the cost of modeling assumptions on the progression profiles and their variability across subjects. A significant improvement is to embed the data in a Riemannian manifold and learn patient-specific trajectories distributed around a central geodesic. A few interpretable parameters characterize subject trajectories at the cost of a prior choice of the metric, which determines the shape of the trajectories. We extend this approach by learning the metric from the data allowing more flexibility while keeping the interpretability. Specifically, we learn the metric as the push-forward of the Euclidean metric by a diffeomorphism. This diffeomorphism is estimated iteratively as the composition of radial basis functions belonging to a reproducible kernel Hilbert space. The metric update allows us to improve the forecasting of imaging and clinical biomarkers in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) cohort. Our results compare favorably to the 56 methods benchmarked in the TADPOLE challenge.
count=4
* Stronger NAS with Weaker Predictors
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf)]
    * Title: Stronger NAS with Weaker Predictors
    * Year: `2021`
    * Authors: Junru Wu, Xiyang Dai, Dongdong Chen, Yinpeng Chen, Mengchen Liu, Ye Yu, Zhangyang Wang, Zicheng Liu, Mei Chen, Lu Yuan
    * Abstract: Neural Architecture Search (NAS) often trains and evaluates a large number of architectures. Recent predictor-based NAS approaches attempt to alleviate such heavy computation costs with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Given limited samples, these predictors, however, are far from accurate to locate top architectures due to the difficulty of fitting the huge search space. This paper reflects on a simple yet crucial question: if our final goal is to find the best architecture, do we really need to model the whole space well?. We propose a paradigm shift from fitting the whole architecture space using one strong predictor, to progressively fitting a search path towards the high-performance sub-space through a set of weaker predictors. As a key property of the weak predictors, their probabilities of sampling better architectures keep increasing. Hence we only sample a few well-performed architectures guided by the previously learned predictor and estimate a new better weak predictor. This embarrassingly easy framework, dubbed WeakNAS, produces coarse-to-fine iteration to gradually refine the ranking of sampling space. Extensive experiments demonstrate that WeakNAS costs fewer samples to find top-performance architectures on NAS-Bench-101 and NAS-Bench-201. Compared to state-of-the-art (SOTA) predictor-based NAS methods, WeakNAS outperforms all with notable margins, e.g., requiring at least 7.5x less samples to find global optimal on NAS-Bench-101. WeakNAS can also absorb their ideas to boost performance more. Further, WeakNAS strikes the new SOTA result of 81.3% in the ImageNet MobileNet Search Space. The code is available at: https://github.com/VITA-Group/WeakNAS.
count=4
* Scalable Rule-Based Representation Learning for Interpretable Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/ffbd6cbb019a1413183c8d08f2929307-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/ffbd6cbb019a1413183c8d08f2929307-Paper.pdf)]
    * Title: Scalable Rule-Based Representation Learning for Interpretable Classification
    * Year: `2021`
    * Authors: Zhuo Wang, Wei Zhang, Ning Liu, Jianyong Wang
    * Abstract: Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on nine small and four large data sets show that RRL outperforms the competitive interpretable approaches and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios. Our code is available at: https://github.com/12wang3/rrl.
count=4
* Universal guarantees for decision tree induction via a higher-order splitting criterion
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/6b5617315c9ac918215fc7514bef514b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/6b5617315c9ac918215fc7514bef514b-Paper.pdf)]
    * Title: Universal guarantees for decision tree induction via a higher-order splitting criterion
    * Year: `2020`
    * Authors: Guy Blanc, Neha Gupta, Jane Lange, Li-Yang Tan
    * Abstract: We propose a simple extension of {\sl top-down decision tree learning heuristics} such as ID3, C4.5, and CART. Our algorithm achieves provable guarantees for all target functions $f: \{-1,1\}^n \to \{-1,1\}$ with respect to the uniform distribution, circumventing impossibility results showing that existing heuristics fare poorly even for simple target functions. The crux of our extension is a new splitting criterion that takes into account the correlations between $f$ and {\sl small subsets} of its attributes. The splitting criteria of existing heuristics (e.g. Gini impurity and information gain), in contrast, are based solely on the correlations between $f$ and its {\sl individual} attributes. Our algorithm satisfies the following guarantee: for all target functions $f : \{-1,1\}^n \to \{-1,1\}$, sizes $s\in \N$, and error parameters $\eps$, it constructs a decision tree of size $s^{\tilde{O}((\log s)^2/\eps^2)}$ that achieves error $\le O(\opt_s) + \eps$, where $\opt_s$ denotes the error of the optimal size-$s$ decision tree for $f$. A key technical notion that drives our analysis is the {\sl noise stability} of $f$, a well-studied smoothness measure of $f$.
count=4
* An Efficient Adversarial Attack for Tree Ensembles
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/ba3e9b6a519cfddc560b5d53210df1bd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/ba3e9b6a519cfddc560b5d53210df1bd-Paper.pdf)]
    * Title: An Efficient Adversarial Attack for Tree Ensembles
    * Year: `2020`
    * Authors: Chong Zhang, Huan Zhang, Cho-Jui Hsieh
    * Abstract: We study the problem of efficient adversarial attacks on tree based ensembles such as gradient boosting decision trees (GBDTs) and random forests (RFs). Since these models are non-continuous step functions and gradient does not exist, most existing efficient adversarial attacks are not applicable. Although decision-based black-box attacks can be applied, they cannot utilize the special structure of trees. In our work, we transform the attack problem into a discrete search problem specially designed for tree ensembles, where the goal is to find a valid ``leaf tuple'' that leads to mis-classification while having the shortest distance to the original input. With this formulation, we show that a simple yet effective greedy algorithm can be applied to iteratively optimize the adversarial example by moving the leaf tuple to its neighborhood within hamming distance 1. Experimental results on several large GBDT and RF models with up to hundreds of trees demonstrate that our method can be thousands of times faster than the previous mixed-integer linear programming (MILP) based approach, while also providing smaller (better) adversarial examples than decision-based black-box attacks on general $\ell_p$ ($p=1, 2, \infty$) norm perturbations.
count=4
* Can I Trust My Fairness Metric? Assessing Fairness with Unlabeled Data and Bayesian Inference
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/d83de59e10227072a9c034ce10029c39-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/d83de59e10227072a9c034ce10029c39-Paper.pdf)]
    * Title: Can I Trust My Fairness Metric? Assessing Fairness with Unlabeled Data and Bayesian Inference
    * Year: `2020`
    * Authors: Disi Ji, Padhraic Smyth, Mark Steyvers
    * Abstract: Group fairness is measured via parity of quantitative metrics across different protected demographic groups. In this paper, we investigate the problem of reliably assessing group fairness metrics when labeled examples are few but unlabeled examples are plentiful. We propose a general Bayesian framework that can augment labeled data with unlabeled data to produce more accurate and lower-variance estimates compared to methods based on labeled data alone. Our approach estimates calibrated scores (for unlabeled examples) of each group using a hierarchical latent variable model conditioned on labeled examples. This in turn allows for inference of posterior distributions for an array of group fairness metrics with a notion of uncertainty. We demonstrate that our approach leads to significant and consistent reductions in estimation error across multiple well-known fairness datasets, sensitive attributes, and predictive models. The results clearly show the benefits of using both unlabeled data and Bayesian inference in assessing whether a prediction model is fair or not.
count=4
* Meta-Surrogate Benchmarking for Hyperparameter Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/0668e20b3c9e9185b04b3d2a9dc8fa2d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/0668e20b3c9e9185b04b3d2a9dc8fa2d-Paper.pdf)]
    * Title: Meta-Surrogate Benchmarking for Hyperparameter Optimization
    * Year: `2019`
    * Authors: Aaron Klein, Zhenwen Dai, Frank Hutter, Neil Lawrence, Javier Gonzalez
    * Abstract: Despite the recent progress in hyperparameter optimization (HPO), available benchmarks that resemble real-world scenarios consist of a few and very large problem instances that are expensive to solve. This blocks researchers and practitioners no only from systematically running large-scale comparisons that are needed to draw statistically significant results but also from reproducing experiments that were conducted before. This work proposes a method to alleviate these issues by means of a meta-surrogate model for HPO tasks trained on off-line generated data. The model combines a probabilistic encoder with a multi-task model such that it can generate inexpensive and realistic tasks of the class of problems of interest. We demonstrate that benchmarking HPO methods on samples of the generative model allows us to draw more coherent and statistically significant conclusions that can be reached orders of magnitude faster than using the original tasks. We provide evidence of our findings for various HPO methods on a wide class of problems.
count=4
* Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/1ce83e5d4135b07c0b82afffbe2b3436-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/1ce83e5d4135b07c0b82afffbe2b3436-Paper.pdf)]
    * Title: Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder
    * Year: `2019`
    * Authors: Ji Feng, Qi-Zhi Cai, Zhi-Hua Zhou
    * Abstract: In this work, we consider one challenging training time attack by modifying training data with bounded perturbation, hoping to manipulate the behavior (both targeted or non-targeted) of any corresponding trained classifier during test time when facing clean samples. To achieve this, we proposed to use an auto-encoder-like network to generate such adversarial perturbations on the training data together with one imaginary victim differentiable classifier. The perturbation generator will learn to update its weights so as to produce the most harmful noise, aiming to cause the lowest performance for the victim classifier during test time. This can be formulated into a non-linear equality constrained optimization problem. Unlike GANs, solving such problem is computationally challenging, we then proposed a simple yet effective procedure to decouple the alternating updates for the two networks for stability. By teaching the perturbation generator to hijacking the training trajectory of the victim classifier, the generator can thus learn to move against the victim classifier step by step. The method proposed in this paper can be easily extended to the label specific setting where the attacker can manipulate the predictions of the victim classifier according to some predefined rules rather than only making wrong predictions. Experiments on various datasets including CIFAR-10 and a reduced version of ImageNet confirmed the effectiveness of the proposed method and empirical results showed that, such bounded perturbations have good transferability across different types of victim classifiers.
count=4
* Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/d54e99a6c03704e95e6965532dec148b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/d54e99a6c03704e95e6965532dec148b-Paper.pdf)]
    * Title: Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds
    * Year: `2019`
    * Authors: Nathan Kallus, Angela Zhou
    * Abstract: Personalized interventions in social services, education, and healthcare leverage individual-level causal effect predictions in order to give the best treatment to each individual or to prioritize program interventions for the individuals most likely to benefit. While the sensitivity of these domains compels us to evaluate the fairness of such policies, we show that actually auditing their disparate impacts per standard observational metrics, such as true positive rates, is impossible since ground truths are unknown. Whether our data is experimental or observational, an individual's actual outcome under an intervention different than that received can never be known, only predicted based on features. We prove how we can nonetheless point-identify these quantities under the additional assumption of monotone treatment response, which may be reasonable in many applications. We further provide a sensitivity analysis for this assumption via sharp partial-identification bounds under violations of monotonicity of varying strengths. We show how to use our results to audit personalized interventions using partially-identified ROC and xROC curves and demonstrate this in a case study of a French job training dataset.
count=4
* PIDForest: Anomaly Detection via Partial Identification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/eb6dc8aba23375061b6f07b137617096-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/eb6dc8aba23375061b6f07b137617096-Paper.pdf)]
    * Title: PIDForest: Anomaly Detection via Partial Identification
    * Year: `2019`
    * Authors: Parikshit Gopalan, Vatsal Sharan, Udi Wieder
    * Abstract: We consider the problem of detecting anomalies in a large dataset. We propose a framework called Partial Identification which captures the intuition that anomalies are easy to distinguish from the overwhelming majority of points by relatively few attribute values. Formalizing this intuition, we propose a geometric anomaly measure for a point that we call PIDScore, which measures the minimum density of data points over all subcubes containing the point. We present PIDForest: a random forest based algorithm that finds anomalies based on this definition. We show that it performs favorably in comparison to several popular anomaly detection methods, across a broad range of benchmarks. PIDForest also provides a succinct explanation for why a point is labelled anomalous, by providing a set of features and ranges for them which are relatively uncommon in the dataset.
count=4
* Alternating optimization of decision trees, with application to learning sparse oblique trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/185c29dc24325934ee377cfda20e414c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/185c29dc24325934ee377cfda20e414c-Paper.pdf)]
    * Title: Alternating optimization of decision trees, with application to learning sparse oblique trees
    * Year: `2018`
    * Authors: Miguel A. Carreira-Perpinan, Pooya Tavallali
    * Abstract: Learning a decision tree from data is a difficult optimization problem. The most widespread algorithm in practice, dating to the 1980s, is based on a greedy growth of the tree structure by recursively splitting nodes, and possibly pruning back the final tree. The parameters (decision function) of an internal node are approximately estimated by minimizing an impurity measure. We give an algorithm that, given an input tree (its structure and the parameter values at its nodes), produces a new tree with the same or smaller structure but new parameter values that provably lower or leave unchanged the misclassification error. This can be applied to both axis-aligned and oblique trees and our experiments show it consistently outperforms various other algorithms while being highly scalable to large datasets and trees. Further, the same algorithm can handle a sparsity penalty, so it can learn sparse oblique trees, having a structure that is a subset of the original tree and few nonzero parameters. This combines the best of axis-aligned and oblique trees: flexibility to model correlated data, low generalization error, fast inference and interpretable nodes that involve only a few features in their decision.
count=4
* DeepPINK: reproducible feature selection in deep neural networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/29daf9442f3c0b60642b14c081b4a556-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/29daf9442f3c0b60642b14c081b4a556-Paper.pdf)]
    * Title: DeepPINK: reproducible feature selection in deep neural networks
    * Year: `2018`
    * Authors: Yang Lu, Yingying Fan, Jinchi Lv, William Stafford Noble
    * Abstract: Deep learning has become increasingly popular in both supervised and unsupervised machine learning thanks to its outstanding empirical performance. However, because of their intrinsic complexity, most deep learning methods are largely treated as black box tools with little interpretability. Even though recent attempts have been made to facilitate the interpretability of deep neural networks (DNNs), existing methods are susceptible to noise and lack of robustness. Therefore, scientists are justifiably cautious about the reproducibility of the discoveries, which is often related to the interpretability of the underlying statistical models. In this paper, we describe a method to increase the interpretability and reproducibility of DNNs by incorporating the idea of feature selection with controlled error rate. By designing a new DNN architecture and integrating it with the recently proposed knockoffs framework, we perform feature selection with a controlled error rate, while maintaining high power. This new method, DeepPINK (Deep feature selection using Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data sets to demonstrate its empirical utility.
count=4
* Removing Hidden Confounding by Experimental Grounding
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/566f0ea4f6c2e947f36795c8f58ba901-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/566f0ea4f6c2e947f36795c8f58ba901-Paper.pdf)]
    * Title: Removing Hidden Confounding by Experimental Grounding
    * Year: `2018`
    * Authors: Nathan Kallus, Aahlad Manas Puli, Uri Shalit
    * Abstract: Observational data is increasingly used as a means for making individual-level causal predictions and intervention recommendations. The foremost challenge of causal inference from observational data is hidden confounding, whose presence cannot be tested in data and can invalidate any causal conclusion. Experimental data does not suffer from confounding but is usually limited in both scope and scale. We introduce a novel method of using limited experimental data to correct the hidden confounding in causal effect models trained on larger observational data, even if the observational data does not fully overlap with the experimental data. Our method makes strictly weaker assumptions than existing approaches, and we prove conditions under which it yields a consistent estimator. We demonstrate our method's efficacy using real-world data from a large educational experiment.
count=4
* Nonparametric learning from Bayesian models with randomized objective functions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/b4d168b48157c623fbd095b4a565b5bb-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/b4d168b48157c623fbd095b4a565b5bb-Paper.pdf)]
    * Title: Nonparametric learning from Bayesian models with randomized objective functions
    * Year: `2018`
    * Authors: Simon Lyddon, Stephen Walker, Chris C. Holmes
    * Abstract: Bayesian learning is built on an assumption that the model space contains a true reflection of the data generating mechanism. This assumption is problematic, particularly in complex data environments. Here we present a Bayesian nonparametric approach to learning that makes use of statistical models, but does not assume that the model is true. Our approach has provably better properties than using a parametric model and admits a Monte Carlo sampling scheme that can afford massive scalability on modern computer architectures. The model-based aspect of learning is particularly attractive for regularizing nonparametric inference when the sample size is small, and also for correcting approximate approaches such as variational Bayes (VB). We demonstrate the approach on a number of examples including VB classifiers and Bayesian random forests.
count=4
* Variable Importance Using Decision Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Paper.pdf)]
    * Title: Variable Importance Using Decision Trees
    * Year: `2017`
    * Authors: Jalil Kazemitabar, Arash Amini, Adam Bloniarz, Ameet S. Talwalkar
    * Abstract: Decision trees and random forests are well established models that not only offer good predictive performance, but also provide rich feature importance information. While practitioners often employ variable importance methods that rely on this impurity-based information, these methods remain poorly characterized from a theoretical perspective. We provide novel insights into the performance of these methods by deriving finite sample performance guarantees in a high-dimensional setting under various modeling assumptions. We further demonstrate the effectiveness of these impurity-based methods via an extensive set of simulations.
count=3
* Learning Triangular Distribution in Visual World
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Learning_Triangular_Distribution_in_Visual_World_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Learning_Triangular_Distribution_in_Visual_World_CVPR_2024_paper.pdf)]
    * Title: Learning Triangular Distribution in Visual World
    * Year: `2024`
    * Authors: Ping Chen, Xingpeng Zhang, Chengtao Zhou, Dichao Fan, Peng Tu, Le Zhang, Yanlin Qian
    * Abstract: Convolution neural network is successful in pervasive vision tasks including label distribution learning which usually takes the form of learning an injection from the non-linear visual features to the well-defined labels. However how the discrepancy between features is mapped to the label discrepancy is ambient and its correctness is not guaranteed.To address these problems we study the mathematical connection between feature and its label presenting a general and simple framework for label distribution learning. We propose a so-called Triangular Distribution Transform (TDT) to build an injective function between feature and label guaranteeing that any symmetric feature discrepancy linearly reflects the difference between labels. The proposed TDT can be used as a plug-in in mainstream backbone networks to address different label distribution learning tasks. Experiments on Facial Age Recognition Illumination Chromaticity Estimation and Aesthetics assessment show that TDT achieves on-par or better results than the prior arts.
count=3
* TULIP: Multi-camera 3D Precision Assessment of Parkinson's Disease
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Kim_TULIP_Multi-camera_3D_Precision_Assessment_of_Parkinsons_Disease_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_TULIP_Multi-camera_3D_Precision_Assessment_of_Parkinsons_Disease_CVPR_2024_paper.pdf)]
    * Title: TULIP: Multi-camera 3D Precision Assessment of Parkinson's Disease
    * Year: `2024`
    * Authors: Kyungdo Kim, Sihan Lyu, Sneha Mantri, Timothy W. Dunn
    * Abstract: Parkinson's disease (PD) is a devastating movement disorder accelerating in global prevalence but a lack of precision symptom measurement has made the development of effective therapies challenging. The Unified Parkinson's Disease Rating Scale (UPDRS) is the gold-standard for assessing motor symptom severity yet its manual scoring criteria are vague and subjective resulting in coarse and noisy clinical assessments. Machine learning approaches have the potential to modernize PD symptom assessments by making them more quantitative objective and scalable. However the lack of benchmark video datasets for PD motor exams hinders model development. Here we introduce the TULIP dataset to bridge this gap. TULIP emphasizes precision and comprehensiveness comprising multi-view video recordings (6 cameras) of all 25 UPDRS motor exam components together with ratings by 3 clinical experts in a cohort of Parkinson's patients and healthy controls. The multi-view recordings enable 3D reconstructions of body movement that better capture disease signatures than more conventional 2D methods. Using the dataset we establish a baseline model for predicting UPDRS scores from 3D poses illustrating how existing diagnostics could be automated. Looking ahead TULIP could aid the development of new precision diagnostics that transcend UPDRS scores providing a deeper understanding of PD and its potential treatments.
count=3
* Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Learning_without_Exact_Guidance_Updating_Large-scale_High-resolution_Land_Cover_Maps_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Learning_without_Exact_Guidance_Updating_Large-scale_High-resolution_Land_Cover_Maps_CVPR_2024_paper.pdf)]
    * Title: Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels
    * Year: `2024`
    * Authors: Zhuohong Li, Wei He, Jiepan Li, Fangxiao Lu, Hongyan Zhang
    * Abstract: Large-scale high-resolution (HR) land-cover mapping is a vital task to survey the Earth's surface and resolve many challenges facing humanity. However it is still a non-trivial task hindered by complex ground details various landforms and the scarcity of accurate training labels over a wide-span geographic area. In this paper we propose an efficient weakly supervised framework (Paraformer) to guide large-scale HR land-cover mapping with easy-access historical land-cover data of low resolution (LR). Specifically existing land-cover mapping approaches reveal the dominance of CNNs in preserving local ground details but still suffer from insufficient global modeling in various landforms. Therefore we design a parallel CNN-Transformer feature extractor in Paraformer consisting of a downsampling-free CNN branch and a Transformer branch to jointly capture local and global contextual information. Besides facing the spatial mismatch of training data a pseudo-label-assisted training (PLAT) module is adopted to reasonably refine LR labels for weakly supervised semantic segmentation of HR images. Experiments on two large-scale datasets demonstrate the superiority of Paraformer over other state-of-the-art methods for automatically updating HR land-cover maps from LR historical labels.
count=3
* Thermal Spread Functions (TSF): Physics-Guided Material Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Dashpute_Thermal_Spread_Functions_TSF_Physics-Guided_Material_Classification_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Dashpute_Thermal_Spread_Functions_TSF_Physics-Guided_Material_Classification_CVPR_2023_paper.pdf)]
    * Title: Thermal Spread Functions (TSF): Physics-Guided Material Classification
    * Year: `2023`
    * Authors: Aniket Dashpute, Vishwanath Saragadam, Emma Alexander, Florian Willomitzer, Aggelos Katsaggelos, Ashok Veeraraghavan, Oliver Cossairt
    * Abstract: Robust and non-destructive material classification is a challenging but crucial first-step in numerous vision applications. We propose a physics-guided material classification framework that relies on thermal properties of the object. Our key observation is that the rate of heating and cooling of an object depends on the unique intrinsic properties of the material, namely the emissivity and diffusivity. We leverage this observation by gently heating the objects in the scene with a low-power laser for a fixed duration and then turning it off, while a thermal camera captures measurements during the heating and cooling process. We then take this spatial and temporal "thermal spread function" (TSF) to solve an inverse heat equation using the finite-differences approach, resulting in a spatially varying estimate of diffusivity and emissivity. These tuples are then used to train a classifier that produces a fine-grained material label at each spatial pixel. Our approach is extremely simple requiring only a small light source (low power laser) and a thermal camera, and produces robust classification results with 86% accuracy over 16 classes
count=3
* Contour-Hugging Heatmaps for Landmark Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/McCouat_Contour-Hugging_Heatmaps_for_Landmark_Detection_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/McCouat_Contour-Hugging_Heatmaps_for_Landmark_Detection_CVPR_2022_paper.pdf)]
    * Title: Contour-Hugging Heatmaps for Landmark Detection
    * Year: `2022`
    * Authors: James McCouat, Irina Voiculescu
    * Abstract: We propose an effective and easy-to-implement method for simultaneously performing landmark detection in images and obtaining an ingenious uncertainty measurement for each landmark. Uncertainty measurements for landmarks are particularly useful in medical imaging applications: rather than giving an erroneous reading, a landmark detection system is more useful when it flags its level of confidence in its prediction. When an automated system is unsure of its predictions, the accuracy of the results can be further improved manually by a human. In the medical domain, being able to review an automated system's level of certainty significantly improves a clinician's trust in it. This paper obtains landmark predictions with uncertainty measurements using a three stage method: 1) We train our network on one-hot heatmap images, 2) We calibrate the uncertainty of the network using temperature scaling, 3) We calculate a novel statistic called 'Expected Radial Error' to obtain uncertainty measurements. We find that this method not only achieves localisation results on par with other state-of-the-art methods but also an uncertainty score which correlates with the true error for each landmark thereby bringing an overall step change in what a generic computer vision method for landmark detection should be capable of. In addition, we show that our uncertainty measurement can be used to classify, with good accuracy, what landmark predictions are likely to be inaccurate. Code available at: https://github.com/jfm15/ContourHuggingHeatmaps.git
count=3
* Learning Camera Localization via Dense Scene Matching
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Tang_Learning_Camera_Localization_via_Dense_Scene_Matching_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Tang_Learning_Camera_Localization_via_Dense_Scene_Matching_CVPR_2021_paper.pdf)]
    * Title: Learning Camera Localization via Dense Scene Matching
    * Year: `2021`
    * Authors: Shitao Tang, Chengzhou Tang, Rui Huang, Siyu Zhu, Ping Tan
    * Abstract: Camera localization aims to estimate 6 DoF camera poses from RGB images. Traditional methods detect and match interest points between a query image and a pre-built 3D model. Recent learning-based approaches encode scene structures into a specific convolutional neural network(CNN) and thus are able to predict dense coordinates from RGB images. However, most of them require re-training or re-adaption for a new scene and have difficulties in handling large-scale scenes due to limited network capacity. We present a new method for scene agnostic camera localization using dense scene matching (DSM), where the cost volume is constructed between a query image and a scene. The cost volume and the corresponding coordinates are processed by a CNN to predict dense coordinates. Camera poses can then be solved by PnP algorithms. In addition, our method can be extended to temporal domain, giving extra performance boost during testing time. Our scene-agnostic approach achieves comparable accuracy as the existing scene-specific approaches on the 7scenes and Cambridge benchmark. This approach also remarkably outperforms state-of-the-art scene-agnostic dense coordinate regression network SANet.
count=3
* ReNAS: Relativistic Evaluation of Neural Architecture Search
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Xu_ReNAS_Relativistic_Evaluation_of_Neural_Architecture_Search_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_ReNAS_Relativistic_Evaluation_of_Neural_Architecture_Search_CVPR_2021_paper.pdf)]
    * Title: ReNAS: Relativistic Evaluation of Neural Architecture Search
    * Year: `2021`
    * Authors: Yixing Xu, Yunhe Wang, Kai Han, Yehui Tang, Shangling Jui, Chunjing Xu, Chang Xu
    * Abstract: An effective and efficient architecture performance evaluation scheme is essential for the success of Neural Architecture Search (NAS). To save computational cost, most of existing NAS algorithms often train and evaluate intermediate neural architectures on a small proxy dataset with limited training epochs. But it is difficult to expect an accurate performance estimation of an architecture in such a coarse evaluation way. This paper advocates a new neural architecture evaluation scheme, which aims to determine which architecture would perform better instead of accurately predict the absolute architecture performance. Therefore, we propose a relativistic architecture performance predictor in NAS (ReNAS). We encode neural architectures into feature tensors, and further refining the representations with the predictor. The proposed relativistic performance predictor can be deployed in discrete searching methods to search for the desired architectures without additional evaluation. Experimental results on NAS-Bench-101 dataset suggests that, sampling 424 (0.1% of the entire search space) neural architectures and their corresponding validation performance is already enough for learning an accurate architecture performance predictor. The accuracies of our searched neural architectures on NAS-Bench-101 and NAS-Bench-201 datasets are higher than that of the state-of-the-art methods and show the priority of the proposed method.
count=3
* Attention-Driven Cropping for Very High Resolution Facial Landmark Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Chandran_Attention-Driven_Cropping_for_Very_High_Resolution_Facial_Landmark_Detection_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chandran_Attention-Driven_Cropping_for_Very_High_Resolution_Facial_Landmark_Detection_CVPR_2020_paper.pdf)]
    * Title: Attention-Driven Cropping for Very High Resolution Facial Landmark Detection
    * Year: `2020`
    * Authors: Prashanth Chandran,  Derek Bradley,  Markus Gross,  Thabo Beeler
    * Abstract: Facial landmark detection is a fundamental task for many consumer and high-end applications and is almost entirely solved by machine learning methods today. Existing datasets used to train such algorithms are primarily made up of only low resolution images, and current algorithms are limited to inputs of comparable quality and resolution as the training dataset. On the other hand, high resolution imagery is becoming increasingly more common as consumer cameras improve in quality every year. Therefore, there is need for algorithms that can leverage the rich information available in high resolution imagery. Naively attempting to reuse existing network architectures on high resolution imagery is prohibitive due to memory bottlenecks on GPUs. The only current solution is to downsample the images, sacrificing resolution and quality. Building on top of recent progress in attention-based networks, we present a novel, fully convolutional regional architecture that is specially designed for predicting landmarks on very high resolution facial images without downsampling. We demonstrate the flexibility of our architecture by training the proposed model with images of resolutions ranging from 256 x 256 to 4K. In addition to being the first method for facial landmark detection on high resolution images, our approach achieves superior performance over traditional (holistic) state-of-the-art architectures across ALL resolutions, leading to a general-purpose, extremely flexible, high quality landmark detector.
count=3
* HyperSTAR: Task-Aware Hyperparameters for Deep Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Mittal_HyperSTAR_Task-Aware_Hyperparameters_for_Deep_Networks_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Mittal_HyperSTAR_Task-Aware_Hyperparameters_for_Deep_Networks_CVPR_2020_paper.pdf)]
    * Title: HyperSTAR: Task-Aware Hyperparameters for Deep Networks
    * Year: `2020`
    * Authors: Gaurav Mittal,  Chang Liu,  Nikolaos Karianakis,  Victor Fragoso,  Mei Chen,  Yun Fu
    * Abstract: While deep neural networks excel in solving visual recognition tasks, they require significant effort to find hyperparameters that make them work optimally. Hyperparameter Optimization (HPO) approaches have automated the process of finding good hyperparameters but they do not adapt to a given task (task-agnostic), making them computationally inefficient. To reduce HPO time, we present HyperSTAR (System for Task Aware Hyperparameter Recommendation), a task-aware method to warm-start HPO for deep neural networks. HyperSTAR ranks and recommends hyperparameters by predicting their performance conditioned on a joint dataset-hyperparameter space. It learns a dataset (task) representation along with the performance predictor directly from raw images in an end-to-end fashion. The recommendations, when integrated with an existing HPO method, make it task-aware and significantly reduce the time to achieve optimal performance. We conduct extensive experiments on 10 publicly available large-scale image classification datasets over two different network architectures, validating that HyperSTAR evaluates 50% less configurations to achieve the best performance compared to existing methods. We further demonstrate that HyperSTAR makes Hyperband (HB) task-aware, achieving the optimal accuracy in just 25% of the budget required by both vanilla HB and Bayesian Optimized HB (BOHB).
count=3
* A Semi-Supervised Assessor of Neural Architectures
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Tang_A_Semi-Supervised_Assessor_of_Neural_Architectures_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Tang_A_Semi-Supervised_Assessor_of_Neural_Architectures_CVPR_2020_paper.pdf)]
    * Title: A Semi-Supervised Assessor of Neural Architectures
    * Year: `2020`
    * Authors: Yehui Tang,  Yunhe Wang,  Yixing Xu,  Hanting Chen,  Boxin Shi,  Chao Xu,  Chunjing Xu,  Qi Tian,  Chang Xu
    * Abstract: Neural architecture search (NAS) aims to automatically design deep neural networks of satisfactory performance. Wherein, architecture performance predictor is critical to efficiently value an intermediate neural architecture. But for the training of this predictor, a number of neural architectures and their corresponding real performance often have to be collected. In contrast with classical performance predictor optimized in a fully supervised way, this paper suggests a semi-supervised assessor of neural architectures. We employ an auto-encoder to discover meaningful representations of neural architectures. Taking each neural architecture as an individual instance in the search space, we construct a graph to capture their intrinsic similarities, where both labeled and unlabeled architectures are involved. A graph convolutional neural network is introduced to predict the performance of architectures based on the learned representations and their relation modeled by the graph. Extensive experimental results on the NAS-Benchmark-101 dataset demonstrated that our method is able to make a significant reduction on the required fully trained architectures for finding efficient architectures.
count=3
* PMS-Net: Robust Haze Removal Based on Patch Map for Single Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_PMS-Net_Robust_Haze_Removal_Based_on_Patch_Map_for_Single_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_PMS-Net_Robust_Haze_Removal_Based_on_Patch_Map_for_Single_CVPR_2019_paper.pdf)]
    * Title: PMS-Net: Robust Haze Removal Based on Patch Map for Single Images
    * Year: `2019`
    * Authors: Wei-Ting Chen,  Jian-Jiun Ding,  Sy-Yen Kuo
    * Abstract: In this paper, we proposed a novel haze removal algorithm based on a new feature called the patch map. Conventional patch-based haze removal algorithms (e.g. the Dark Channel prior) usually performs dehazing with a fixed patch size. However, it may produce several problems in recovered results such as oversaturation and color distortion. Therefore, in this paper, we designed an adaptive and automatic patch size selection model called the Patch Map Selection Network (PMS-Net) to select the patch size corresponding to each pixel. This network is designed based on the convolutional neural network (CNN), which can generate the patch map from the image to image. Experimental results on both synthesized and real-world hazy images show that, with the combination of the proposed PMS-Net, the performance in haze removal is much better than that of other state-of-the-art algorithms and we can address the problems caused by the fixed patch size.
count=3
* Mixed Effects Neural Networks (MeNets) With Applications to Gaze Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Xiong_Mixed_Effects_Neural_Networks_MeNets_With_Applications_to_Gaze_Estimation_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Xiong_Mixed_Effects_Neural_Networks_MeNets_With_Applications_to_Gaze_Estimation_CVPR_2019_paper.pdf)]
    * Title: Mixed Effects Neural Networks (MeNets) With Applications to Gaze Estimation
    * Year: `2019`
    * Authors: Yunyang Xiong,  Hyunwoo J. Kim,  Vikas Singh
    * Abstract: There is much interest in computer vision to utilize commodity hardware for gaze estimation. A number of papers have shown that algorithms based on deep convolutional architectures are approaching accuracies where streaming data from mass-market devices can offer good gaze tracking performance, although a gap still remains between what is possible and the performance users will expect in real deployments. We observe that one obvious avenue for improvement relates to a gap between some basic technical assumptions behind most existing approaches and the statistical properties of the data used for training. Specifically, most training datasets involve tens of users with a few hundreds (or more) repeated acquisitions per user. The non i.i.d. nature of this data suggests better estimation may be possible if the model explicitly made use of such "repeated measurements" from each user as is commonly done in classical statistical analysis using so-called mixed effects models. The goal of this paper is to adapt these "mixed effects" ideas from statistics within a deep neural network architecture for gaze estimation, based on eye images. Such a formulation seeks to specifically utilize information regarding the hierarchical structure of the training data -- each node in the hierarchy is a user who provides tens or hundreds of repeated samples. This modification yields an architecture that offers state of the art performance on various publicly available datasets improving results by 10-20%.
count=3
* Planar Shape Detection at Structural Scales
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Fang_Planar_Shape_Detection_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Fang_Planar_Shape_Detection_CVPR_2018_paper.pdf)]
    * Title: Planar Shape Detection at Structural Scales
    * Year: `2018`
    * Authors: Hao Fang, Florent Lafarge, Mathieu Desbrun
    * Abstract: Interpreting 3D data such as point clouds or surface meshes depends heavily on the scale of observation. Yet, existing algorithms for shape detection rely on trial-and-error parameter tunings to output configurations representative of a structural scale. We present a framework to automatically extract a set of representations that capture the shape and structure of man-made objects at different key abstraction levels. A shape-collapsing process first generates a fine-to-coarse sequence of shape representations by exploiting local planarity. This sequence is then analyzed to identify significant geometric variations between successive representations through a supervised energy minimization. Our framework is flexible enough to learn how to detect both existing structural formalisms such as the CityGML Levels Of Details, and expert-specified levels of abstraction. Experiments on different input data and classes of man-made objects, as well as comparisons with existing shape detection methods, illustrate the strengths of our approach in terms of efficiency and flexibility.
count=3
* Hand PointNet: 3D Hand Pose Estimation Using Point Sets
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Ge_Hand_PointNet_3D_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Ge_Hand_PointNet_3D_CVPR_2018_paper.pdf)]
    * Title: Hand PointNet: 3D Hand Pose Estimation Using Point Sets
    * Year: `2018`
    * Authors: Liuhao Ge, Yujun Cai, Junwu Weng, Junsong Yuan
    * Abstract: Convolutional Neural Network (CNN) has shown promising results for 3D hand pose estimation in depth images. Different from existing CNN-based hand pose estimation methods that take either 2D images or 3D volumes as the input, our proposed Hand PointNet directly processes the 3D point cloud that models the visible surface of the hand for pose regression. Taking the normalized point cloud as the input, our proposed hand pose regression network is able to capture complex hand structures and accurately regress a low dimensional representation of the 3D hand pose. In order to further improve the accuracy of fingertips, we design a fingertip refinement network that directly takes the neighboring points of the estimated fingertip location as input to refine the fingertip location. Experiments on three challenging hand pose datasets show that our proposed method outperforms state-of-the-art methods.
count=3
* V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation From a Single Depth Map
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Moon_V2V-PoseNet_Voxel-to-Voxel_Prediction_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Moon_V2V-PoseNet_Voxel-to-Voxel_Prediction_CVPR_2018_paper.pdf)]
    * Title: V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation From a Single Depth Map
    * Year: `2018`
    * Authors: Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee
    * Abstract: Most of the existing deep learning-based methods for 3D hand and human pose estimation from a single depth map are based on a common framework that takes a 2D depth map and directly regresses the 3D coordinates of keypoints, such as hand or human body joints, via 2D convolutional neural networks (CNNs). The first weakness of this approach is the presence of perspective distortion in the 2D depth map. While the depth map is intrinsically 3D data, many previous methods treat depth maps as 2D images that can distort the shape of the actual object through projection from 3D to 2D space. This compels the network to perform perspective distortion-invariant estimation. The second weakness of the conventional approach is that directly regressing 3D coordinates from a 2D image is a highly non-linear mapping, which causes difficulty in the learning procedure. To overcome these weaknesses, we firstly cast the 3D hand and human pose estimation problem from a single depth map into a voxel-to-voxel prediction that uses a 3D voxelized grid and estimates the per-voxel likelihood for each keypoint. We design our model as a 3D CNN that provides accurate estimates while running in real-time. Our system outperforms previous methods in almost all publicly available 3D hand and human pose estimation datasets and placed first in the HANDS 2017 frame-based 3D hand pose estimation challenge. The code is available in https://github.com/mks0601/V2V-PoseNet_RELEASE.
count=3
* Interpret Neural Networks by Identifying Critical Data Routing Paths
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Interpret_Neural_Networks_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Interpret_Neural_Networks_CVPR_2018_paper.pdf)]
    * Title: Interpret Neural Networks by Identifying Critical Data Routing Paths
    * Year: `2018`
    * Authors: Yulong Wang, Hang Su, Bo Zhang, Xiaolin Hu
    * Abstract: Interpretability of a deep neural network aims to explain the rationale behind its decisions and enable the users to understand the intelligent agents, which has become an important issue due to its importance in practical applications. To address this issue, we develop a Distillation Guided Routing method, which is a flexible framework to interpret a deep neural network by identifying critical data routing paths and analyzing the functional processing behavior of the corresponding layers. Specifically, we propose to discover the critical nodes on the data routing paths during network inferring prediction for individual input samples by learning associated control gates for each layer's output channel. The routing paths can, therefore, be represented based on the responses of concatenated control gates from all the layers, which reflect the network's semantic selectivity regarding to the input patterns and more detailed functional process across different layer levels. Based on the discoveries, we propose an adversarial sample detection algorithm by learning a classifier to discriminate whether the critical data routing paths are from real or adversarial samples. Experiments demonstrate that our algorithm can effectively achieve high defense rate with minor training overhead.
count=3
* Iterative Learning With Open-Set Noisy Labels
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Iterative_Learning_With_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Iterative_Learning_With_CVPR_2018_paper.pdf)]
    * Title: Iterative Learning With Open-Set Noisy Labels
    * Year: `2018`
    * Authors: Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, Shu-Tao Xia
    * Abstract: Large-scale datasets possessing clean label annotations are crucial for training Convolutional Neural Networks (CNNs). However, labeling large-scale data can be very costly and error-prone, and even high-quality datasets are likely to contain noisy (incorrect) labels. Existing works usually employ a closed-set assumption, whereby the samples associated with noisy labels possess a true class contained within the set of known classes in the training data. However, such an assumption is too restrictive for many applications, since samples associated with noisy labels might in fact possess a true class that is not present in the training data. We refer to this more complex scenario as the open-set noisy label problem and show that it is nontrivial in order to make accurate predictions. To address this problem, we propose a novel iterative learning framework for training CNNs on datasets with open-set noisy labels. Our approach detects noisy labels and learns deep discriminative features in an iterative fashion. To benefit from the noisy label detection, we design a Siamese network to encourage clean labels and noisy labels to be dissimilar. A reweighting module is also applied to simultaneously emphasize the learning from clean labels and reduce the effect caused by noisy labels. Experiments on CIFAR-10, ImageNet and real-world noisy (web-search) datasets demonstrate that our proposed model can robustly train CNNs in the presence of a high proportion of open-set as well as closed-set noisy labels.
count=3
* On-The-Fly Adaptation of Regression Forests for Online Camera Relocalisation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Cavallari_On-The-Fly_Adaptation_of_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Cavallari_On-The-Fly_Adaptation_of_CVPR_2017_paper.pdf)]
    * Title: On-The-Fly Adaptation of Regression Forests for Online Camera Relocalisation
    * Year: `2017`
    * Authors: Tommaso Cavallari, Stuart Golodetz, Nicholas A. Lord, Julien Valentin, Luigi Di Stefano, Philip H. S. Torr
    * Abstract: Camera relocalisation is an important problem in computer vision, with applications in simultaneous localisation and mapping, virtual/augmented reality and navigation. Common techniques either match the current image against keyframes with known poses coming from a tracker, or establish 2D-to-3D correspondences between keypoints in the current image and points in the scene in order to estimate the camera pose. Recently, regression forests have become a popular alternative to establish such correspondences. They achieve accurate results, but must be trained offline on the target scene, preventing relocalisation in new environments. In this paper, we show how to circumvent this limitation by adapting a pre-trained forest to a new scene on the fly. Our adapted forests achieve relocalisation performance that is on par with that of offline forests, and our approach runs in under 150ms, making it desirable for real-time systems that require online relocalisation.
count=3
* Simultaneous Facial Landmark Detection, Pose and Deformation Estimation Under Facial Occlusion
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Wu_Simultaneous_Facial_Landmark_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wu_Simultaneous_Facial_Landmark_CVPR_2017_paper.pdf)]
    * Title: Simultaneous Facial Landmark Detection, Pose and Deformation Estimation Under Facial Occlusion
    * Year: `2017`
    * Authors: Yue Wu, Chao Gou, Qiang Ji
    * Abstract: Facial landmark detection, head pose estimation, and facial deformation analysis are typical facial behavior analysis tasks in computer vision. The existing methods usually perform each task independently and sequentially, ignoring their interactions. To tackle this problem, we propose a unified framework for simultaneous facial landmark detection, head pose estimation, and facial deformation analysis, and the proposed model is robust to facial occlusion. Following a cascade procedure augmented with model-based head pose estimation, we iteratively update the facial landmark locations, facial occlusion, head pose and facial deformation until convergence. The experimental results on benchmark databases demonstrate the effectiveness of the proposed method for simultaneous facial landmark detection, head pose and facial deformation estimation, even if the images are under facial occlusion.
count=3
* Fractal Dimension Invariant Filtering and Its CNN-Based Implementation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Xu_Fractal_Dimension_Invariant_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Fractal_Dimension_Invariant_CVPR_2017_paper.pdf)]
    * Title: Fractal Dimension Invariant Filtering and Its CNN-Based Implementation
    * Year: `2017`
    * Authors: Hongteng Xu, Junchi Yan, Nils Persson, Weiyao Lin, Hongyuan Zha
    * Abstract: Fractal analysis has been widely used in computer vision, especially in texture image processing and texture analysis. The key concept of fractal-based image model is the fractal dimension, which is invariant to bi-Lipschitz transformation of image, and thus capable of representing intrinsic structural information of image robustly. However, the invariance of fractal dimension generally does not hold after filtering, which limits the application of fractal-based image model. In this paper, we propose a novel fractal dimension invariant filtering (FDIF) method, extending the invariance of fractal dimension to filtering operations. Utilizing the notion of local self-similarity, we first develop a local fractal model for images. By adding a nonlinear post-processing step behind anisotropic filter banks, we demonstrate that the proposed filtering method is capable of preserving the local invariance of the fractal dimension of image. Meanwhile, we show that the FDIF method can be re-instantiated approximately via a CNN-based architecture, where the convolution layer extracts anisotropic structure of image and the nonlinear layer enhances the structure via preserving local fractal dimension of image. The proposed filtering method provides us with a novel geometric interpretation of CNN-based image model. Focusing on a challenging image processing task --- detecting complicated curves from the texture-like images, the proposed method obtains superior results to the state-of-art approaches.
count=3
* CASENet: Deep Category-Aware Semantic Edge Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_CASENet_Deep_Category-Aware_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_CASENet_Deep_Category-Aware_CVPR_2017_paper.pdf)]
    * Title: CASENet: Deep Category-Aware Semantic Edge Detection
    * Year: `2017`
    * Authors: Zhiding Yu, Chen Feng, Ming-Yu Liu, Srikumar Ramalingam
    * Abstract: Boundary and edge cues are highly beneficial in improving a wide variety of vision tasks such as semantic segmentation, object recognition, stereo, and object proposal generation. Recently, the problem of edge detection has been revisited and significant progress has been made with deep learning. While classical edge detection is a challenging binary problem in itself, the category-aware semantic edge detection by nature is an even more challenging multi-label problem. We model the problem such that each edge pixel can be associated with more than one class as they appear in contours or junctions belonging to two or more semantic classes. To this end, we propose a novel end-to-end deep semantic edge learning architecture based on ResNet and a new skip-layer architecture where category-wise edge activations at the top convolution layer share and are fused with the same set of bottom layer features. We then propose a multi-label loss function to supervise the fused activations. We show that our proposed architecture benefits this problem with better performance, and we outperform the current state-of-the-art semantic edge detection methods by a large margin on standard data sets such as SBD and Cityscapes.
count=3
* Personalizing Human Video Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Charles_Personalizing_Human_Video_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Charles_Personalizing_Human_Video_CVPR_2016_paper.pdf)]
    * Title: Personalizing Human Video Pose Estimation
    * Year: `2016`
    * Authors: James Charles, Tomas Pfister, Derek Magee, David Hogg, Andrew Zisserman
    * Abstract: We propose a personalized ConvNet pose estimator that automatically adapts itself to the uniqueness of a person's appearance to improve pose estimation in long videos. We make the following contributions: (i) we show that given a few high-precision pose annotations, e.g. from a generic ConvNet pose estimator, additional annotations can be generated throughout the video using a combination of image-based matching for temporally distant frames, and dense optical flow for temporally local frames; (ii) we develop an occlusion aware self-evaluation model that is able to automatically select the high-quality and reject the erroneous additional annotations; and (iii) we demonstrate that these high-quality annotations can be used to fine-tune a ConvNet pose estimator and thereby personalize it to lock on to key discriminative features of the person's appearance. The outcome is a substantial improvement in the pose estimates for the target video using the personalized ConvNet compared to the original generic ConvNet. Our method outperforms the state of the art (including top ConvNet methods) by a large margin on three standard benchmarks, as well as on a new challenging YouTube video dataset. Furthermore, we show that training from the automatically generated annotations can be used to improve the performance of a generic ConvNet on other benchmarks.
count=3
* Learning Deep Representation for Imbalanced Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Huang_Learning_Deep_Representation_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Learning_Deep_Representation_CVPR_2016_paper.pdf)]
    * Title: Learning Deep Representation for Imbalanced Classification
    * Year: `2016`
    * Authors: Chen Huang, Yining Li, Chen Change Loy, Xiaoou Tang
    * Abstract: Data in vision domain often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary classification methods based on deep convolutional neural network (CNN) typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain both inter-cluster and inter-class margins. This tighter constraint effectively reduces the class imbalance inherent in the local data neighborhood. We show that the margins can be easily deployed in standard deep learning framework through quintuplet instance sampling and the associated triple-header hinge loss. The representation learned by our approach, when combined with a simple k-nearest neighbor (kNN) algorithm, shows significant improvements over existing methods on both high- and low-level vision classification tasks that exhibit imbalanced class distribution.
count=3
* Moral Lineage Tracing
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Jug_Moral_Lineage_Tracing_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Jug_Moral_Lineage_Tracing_CVPR_2016_paper.pdf)]
    * Title: Moral Lineage Tracing
    * Year: `2016`
    * Authors: Florian Jug, Evgeny Levinkov, Corinna Blasse, Eugene W. Myers, Bjoern Andres
    * Abstract: Lineage tracing, the tracking of living cells as they move and divide, is a central problem in biological image analysis. Solutions, called lineage forests, are key to understanding how the structure of multicellular organisms emerges. We propose an integer linear program (ILP) whose feasible solutions define, for every image in a sequence, a decomposition into cells (segmentation) and, across images, a lineage forest of cells (tracing). In this ILP, path-cut inequalities enforce the morality of lineages, i.e., the constraint that cells do not merge. To find feasible solutions of this NP-hard problem, with certified bounds to the global optimum, we define efficient separation procedures and apply these as part of a branch-and-cut algorithm. To show the effectiveness of this approach, we analyze feasible solutions for real microscopy data in terms of bounds and run-time, and by their weighted edit distance to lineage forests traced by humans.
count=3
* Learning Action Maps of Large Environments via First-Person Vision
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Rhinehart_Learning_Action_Maps_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Rhinehart_Learning_Action_Maps_CVPR_2016_paper.pdf)]
    * Title: Learning Action Maps of Large Environments via First-Person Vision
    * Year: `2016`
    * Authors: Nicholas Rhinehart, Kris M. Kitani
    * Abstract: When people observe and interact with physical spaces, they are able to associate functionality to regions in the environment. Our goal is to automate functional understanding of large spaces by leveraging activity demonstrations recorded from an ego-centric viewpoint. The method we describe enables functionality estimation in both large scenes where people have behaved, as well as novel scenes where no behaviors are available. Our method learns and predicts "Action Maps", which encode the ability for a user to perform activities at various locations. With the usage of an egocentric camera to observe demonstrations, our method scales with the size of the scene without the need for mounting multiple static surveillance cameras, and is well-suited to the task of observing activities up-close. We demonstrate that by capturing appearance-based attributes of the environment and associating these attributes with activity demonstrations, our mathematical framework allows for the prediction of Action Maps in new environments. Additionally, we take a preliminary look at the breadth of applicability of Action Maps by demonstrating a proof-of-concept application in which they are used in concert with activity detections to perform localization.
count=3
* Material Classification Using Raw Time-Of-Flight Measurements
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Su_Material_Classification_Using_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Su_Material_Classification_Using_CVPR_2016_paper.pdf)]
    * Title: Material Classification Using Raw Time-Of-Flight Measurements
    * Year: `2016`
    * Authors: Shuochen Su, Felix Heide, Robin Swanson, Jonathan Klein, Clara Callenberg, Matthias Hullin, Wolfgang Heidrich
    * Abstract: We propose a material classification method using raw time-of-flight (ToF) measurements. ToF cameras capture the correlation between a reference signal and the temporal response of material to incident illumination. Such measurements encode unique signatures of the material, i.e. the degree of subsurface scattering inside a volume. Subsequently, it offers an orthogonal domain of feature representation compared to conventional spatial and angular reflectance-based approaches. We demonstrate the effectiveness, robustness, and efficiency of our method through experiments and comparisons of real-world materials.
count=3
* The Global Patch Collider
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_The_Global_Patch_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_The_Global_Patch_CVPR_2016_paper.pdf)]
    * Title: The Global Patch Collider
    * Year: `2016`
    * Authors: Shenlong Wang, Sean Ryan Fanello, Christoph Rhemann, Shahram Izadi, Pushmeet Kohli
    * Abstract: This paper proposes a novel extremely efficient, fully-parallelizable, task-specific algorithm for the computation of global point-wise correspondences in images and videos. Our algorithm, the Global Patch Collider, is based on detecting unique collisions between image points using a collection of learned tree structures that act as conditional hash functions. In contrast to conventional approaches that rely on pairwise distance computation, our algorithm isolates distinctive pixel pairs that hit the same leaf during traversal through multiple learned tree structures. The split functions stored at the intermediate nodes of the trees are trained to ensure that only visually similar patches or their geometric or photometric transformed versions fall into the same leaf node. The matching process involves passing all pixel positions in the images under analysis through the tree structures. We then compute matches by isolating points that uniquely collide with each other ie. fell in the same empty leaf in multiple trees. Our algorithm is linear in the number of pixels but can be made constant time on a parallel computation architecture as the tree traversal for individual image points is decoupled. We demonstrate the efficacy of our method by using it to perform optical flow matching and stereo matching on some challenging benchmarks. Experimental results show that not only is our method extremely computationally efficient, but it is also able to match or outperform state of the art methods that are much more complex.
count=3
* Learned Binary Spectral Shape Descriptor for 3D Shape Correspondence
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Xie_Learned_Binary_Spectral_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_Learned_Binary_Spectral_CVPR_2016_paper.pdf)]
    * Title: Learned Binary Spectral Shape Descriptor for 3D Shape Correspondence
    * Year: `2016`
    * Authors: Jin Xie, Meng Wang, Yi Fang
    * Abstract: Dense 3D shape correspondence is an important problem in computer vision and computer graphics. Recently, the local shape descriptor based 3D shape correspondence approaches have been widely studied, where the local shape descriptor is a real-valued vector to characterize the geometrical structure of the shape. Different from these real-valued local shape descriptors, in this paper, we propose to learn a novel binary spectral shape descriptor with the deep neural network for 3D shape correspondence. The binary spectral shape descriptor can require less storage space and enable fast matching. First, based on the eigenvectors of the Laplace-Beltrami operator, we construct a neural network to form a nonlinear spectral representation to characterize the shape. Then, for the defined positive and negative points on the shapes, we train the constructed neural network by minimizing the errors between the outputs and their corresponding binary descriptors, minimizing the variations of the outputs of the positive points and maximizing the variations of the outputs of the negative points, simultaneously. Finally, we binarize the output of the neural network to form the binary spectral shape descriptor for shape correspondence. The proposed binary spectral shape descriptor is evaluated on the SCAPE and TOSCA 3D shape datasets for shape correspondence. The experimental results demonstrate the effectiveness of the proposed binary shape descriptor for the shape correspondence task.
count=3
* Unconstrained Face Alignment via Cascaded Compositional Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_Unconstrained_Face_Alignment_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_Unconstrained_Face_Alignment_CVPR_2016_paper.pdf)]
    * Title: Unconstrained Face Alignment via Cascaded Compositional Learning
    * Year: `2016`
    * Authors: Shizhan Zhu, Cheng Li, Chen-Change Loy, Xiaoou Tang
    * Abstract: We present a practical approach to address the problem of unconstrained face alignment for a single image. In our unconstrained problem, we need to deal with large shape and appearance variations under extreme head poses and rich shape deformation. To equip cascaded regressors with the capability to handle global shape variation and irregular appearance-shape relation in the unconstrained scenario, we partition the optimisation space into multiple domains of homogeneous descent, and predict a shape as a composition of estimations from multiple domain-specific regressors. With a specially formulated learning objective and a novel tree splitting function, our approach is capable of estimating a robust and meaningful composition. In addition to achieving state-of-the-art accuracy over existing approaches, our framework is also an efficient solution (350 FPS), thanks to the on-the-fly domain exclusion mechanism and the capability of leveraging the fast pixel feature.
count=3
* Multi-View Feature Engineering and Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Dong_Multi-View_Feature_Engineering_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Dong_Multi-View_Feature_Engineering_2015_CVPR_paper.pdf)]
    * Title: Multi-View Feature Engineering and Learning
    * Year: `2015`
    * Authors: Jingming Dong, Nikolaos Karianakis, Damek Davis, Joshua Hernandez, Jonathan Balzer, Stefano Soatto
    * Abstract: We frame the problem of local representation of imaging data as the computation of minimal sufficient statistics that are invariant to nuisance variability induced by viewpoint and illumination. We show that, under very stringent conditions, these are related to "feature descriptors" commonly used in Computer Vision. Such conditions can be relaxed if multiple views of the same scene are available. We propose a sampling-based and a point-estimate based approximation of such a representation, compared empirically on image-to-(multiple)image matching, for which we introduce a multi-view wide-baseline matching benchmark, consisting of a mixture of real and synthetic objects with ground truth camera motion and dense three-dimensional geometry.
count=3
* Random Tree Walk Toward Instantaneous 3D Human Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Jung_Random_Tree_Walk_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Jung_Random_Tree_Walk_2015_CVPR_paper.pdf)]
    * Title: Random Tree Walk Toward Instantaneous 3D Human Pose Estimation
    * Year: `2015`
    * Authors: Ho Yub Jung, Soochahn Lee, Yong Seok Heo, Il Dong Yun
    * Abstract: The availability of accurate depth cameras have made real-time human pose estimation possible; however, there are still demands for faster algorithms on low power processors. This paper introduces 1000 frames per second pose estimation method on a single core CPU. A large computation gain is achieved by random walk sub-sampling. Instead of training trees for pixel-wise classification, a regression tree is trained to estimate the probability distribution to the direction toward the particular joint, relative to the current position. At test time, the direction for the random walk is randomly chosen from a set of representative directions. The new position is found by a constant step toward the direction, and the distribution for next direction is found at the new position. The continual random walk through 3D space will eventually produce an expectation of step positions, which we estimate as the joint position. A regression tree is built separately for each joint. The number of random walk steps can be assigned for each joint so that the computation time is consistent regardless of the size of body segmentation. The experiments show that even with large computation gain, the accuracy is higher or comparable to the state-of-the-art pose estimation methods.
count=3
* Kernel Fusion for Better Image Deblurring
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Mai_Kernel_Fusion_for_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Mai_Kernel_Fusion_for_2015_CVPR_paper.pdf)]
    * Title: Kernel Fusion for Better Image Deblurring
    * Year: `2015`
    * Authors: Long Mai, Feng Liu
    * Abstract: Kernel estimation for image deblurring is a challenging task and a large number of algorithms have been developed. Our hypothesis is that while individual kernels estimated using different methods alone are sometimes inadequate, they often complement each other. This paper addresses the problem of fusing multiple kernels estimated using different methods into a more accurate one that can better support image deblurring than each individual kernel. In this paper, we develop a data-driven approach to kernel fusion that learns how each kernel contributes to the final kernel and how they interact with each other. We discuss various kernel fusion models and find that kernel fusion using Gaussian Conditional Random Fields performs best. This Gaussian Conditional Random Fields-based kernel fusion method not only models how individual kernels are fused at each kernel element but also the interaction of kernel fusion among multiple kernel elements. Our experiments show that our method can significantly improve image deblurring by combining kernels from multiple methods into a better one.
count=3
* Social Saliency Prediction
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Park_Social_Saliency_Prediction_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Park_Social_Saliency_Prediction_2015_CVPR_paper.pdf)]
    * Title: Social Saliency Prediction
    * Year: `2015`
    * Authors: Hyun Soo Park, Jianbo Shi
    * Abstract: This paper presents a method to predict social saliency, the likelihood of joint attention, given an input image or video by leveraging the social interaction data captured by first person cameras. Inspired by electric dipole moments, we introduce a social formation feature that encodes the geometric relationship between joint attention and its social formation. We learn this feature from the first person social interaction data where we can precisely measure the locations of joint attention and its associated members in 3D. An ensemble classifier is trained to learn the geometric relationship. Using the trained classifier, we predict social saliency in real-world scenes with multiple social groups including scenes from team sports captured in a third person view. Our representation does not require directional measurements such as gaze directions. A geometric analysis of social interactions in terms of the F-formation theory is also presented.
count=3
* Deep Networks for Saliency Detection via Local Estimation and Global Search
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Wang_Deep_Networks_for_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Wang_Deep_Networks_for_2015_CVPR_paper.pdf)]
    * Title: Deep Networks for Saliency Detection via Local Estimation and Global Search
    * Year: `2015`
    * Authors: Lijun Wang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang
    * Abstract: This paper presents a saliency detection algorithm by integrating both local estimation and global search. In the local estimation stage, we detect local saliency by using a deep neural network (DNN-L) which learns local patch features to determine the saliency value of each pixel. The estimated local saliency maps are further refined by exploring the high level object concept. In the global search stage, the local saliency map together with global contrast and geometric information are used as global features to describe a set of object candidate regions. Another deep neural network (DNN-G) is trained to predict the saliency score of each object region based on the global features. The final saliency map is generated by a weighted sum of salient object regions. Our method presents two interesting insights. First, local features learned by a supervised scheme can effectively capture local contrast, texture and shape information for saliency detection. Second, complex relationship between different global saliency cues can be captured by deep networks and exploited principally rather than heuristically. Quantitative and qualitative experiments on large benchmark data sets demonstrate that our algorithm performs favorably against the state-of-the-art methods.
count=3
* Face Alignment by Coarse-to-Fine Shape Searching
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Zhu_Face_Alignment_by_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Zhu_Face_Alignment_by_2015_CVPR_paper.pdf)]
    * Title: Face Alignment by Coarse-to-Fine Shape Searching
    * Year: `2015`
    * Authors: Shizhan Zhu, Cheng Li, Chen Change Loy, Xiaoou Tang
    * Abstract: We present a novel face alignment framework based on coarse-to-fine shape searching. Unlike the conventional cascaded regression approaches that start with an initial shape and refine the shape in a cascaded manner, our approach begins with a coarse search over a shape space that contains diverse shapes, and employs the coarse solution to constrain subsequent finer search of shapes. The unique stage-by-stage progressive and adaptive search i) prevents the final solution from being trapped in local optima due to poor initialisation, a common problem encountered by cascaded regression approaches; and ii) improves the robustness in coping with large pose variations. The framework demonstrates real-time performance and state-of-theart results on various benchmarks including the challenging 300-W dataset.
count=3
* Latent Regression Forest: Structured Estimation of 3D Articulated Hand Posture
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Tang_Latent_Regression_Forest_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Tang_Latent_Regression_Forest_2014_CVPR_paper.pdf)]
    * Title: Latent Regression Forest: Structured Estimation of 3D Articulated Hand Posture
    * Year: `2014`
    * Authors: Danhang Tang, Hyung Jin Chang, Alykhan Tejani, Tae-Kyun Kim
    * Abstract: In this paper we present the Latent Regression Forest (LRF), a novel framework for real-time, 3D hand pose estimation from a single depth image. In contrast to prior forest-based methods, which take dense pixels as input, classify them independently and then estimate joint positions afterwards; our method can be considered as a structured coarse-to-fine search, starting from the centre of mass of a point cloud until locating all the skeletal joints. The searching process is guided by a learnt Latent Tree Model which reflects the hierarchical topology of the hand. Our main contributions can be summarised as follows: (i) Learning the topology of the hand in an unsupervised, data-driven manner. (ii) A new forest-based, discriminative framework for structured search in images, as well as an error regression step to avoid error accumulation. (iii) A new multi-view hand pose dataset containing 180K annotated images from 10 different subjects. Our experiments show that the LRF out-performs state-of-the-art methods in both accuracy and efficiency.
count=3
* Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Vemulapalli_Human_Action_Recognition_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Vemulapalli_Human_Action_Recognition_2014_CVPR_paper.pdf)]
    * Title: Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group
    * Year: `2014`
    * Authors: Raviteja Vemulapalli, Felipe Arrate, Rama Chellappa
    * Abstract: Recently introduced cost-effective depth sensors coupled with the real-time skeleton estimation algorithm of Shotton et al. [16] have generated a renewed interest in skeleton-based human action recognition. Most of the existing skeleton-based approaches use either the joint locations or the joint angles to represent a human skeleton. In this paper, we propose a new skeletal representation that explicitly models the 3D geometric relationships between various body parts using rotations and translations in 3D space. Since 3D rigid body motions are members of the special Euclidean group SE(3), the proposed skeletal representation lies in the Lie group SE(3)×. . .×SE(3), which is a curved manifold. Using the proposed representation, human actions can be modeled as curves in this Lie group. Since classification of curves in this Lie group is not an easy task, we map the action curves from the Lie group to its Lie algebra, which is a vector space. We then perform classification using a combination of dynamic time warping, Fourier temporal pyramid representation and linear SVM. Experimental results on three action datasets show that the proposed representation performs better than many existing skeletal representations. The proposed approach also outperforms various state-of-the-art skeleton-based human action recognition approaches.
count=3
* Real-time Simultaneous Pose and Shape Estimation for Articulated Objects Using a Single Depth Camera
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Ye_Real-time_Simultaneous_Pose_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Ye_Real-time_Simultaneous_Pose_2014_CVPR_paper.pdf)]
    * Title: Real-time Simultaneous Pose and Shape Estimation for Articulated Objects Using a Single Depth Camera
    * Year: `2014`
    * Authors: Mao Ye, Ruigang Yang
    * Abstract: In this paper we present a novel real-time algorithm for simultaneous pose and shape estimation for articulated objects, such as human beings and animals. The key of our pose estimation component is to embed the articulated deformation model with exponential-maps-based parametrization into a Gaussian Mixture Model. Benefiting from the probabilistic measurement model, our algorithm requires no explicit point correspondences as opposed to most existing methods. Consequently, our approach is less sensitive to local minimum and well handles fast and complex motions. Extensive evaluations on publicly available datasets demonstrate that our method outperforms most state-of-art pose estimation algorithms with large margin, especially in the case of challenging motions. Moreover, our novel shape adaptation algorithm based on the same probabilistic model automatically captures the shape of the subjects during the dynamic pose estimation process. Experiments show that our shape estimation method achieves comparable accuracy with state of the arts, yet requires neither parametric model nor extra calibration procedure.
count=3
* Tracking Sports Players with Context-Conditioned Motion Models
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Liu_Tracking_Sports_Players_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Liu_Tracking_Sports_Players_2013_CVPR_paper.pdf)]
    * Title: Tracking Sports Players with Context-Conditioned Motion Models
    * Year: `2013`
    * Authors: Jingchen Liu, Peter Carr, Robert T. Collins, Yanxi Liu
    * Abstract: We employ hierarchical data association to track players in team sports. Player movements are often complex and highly correlated with both nearby and distant players. A single model would require many degrees of freedom to represent the full motion diversity and could be difficult to use in practice. Instead, we introduce a set of Game Context Features extracted from noisy detections to describe the current state of the match, such as how the players are spatially distributed. Our assumption is that players react to the current situation in only a finite number of ways. As a result, we are able to select an appropriate simplified affinity model for each player and time instant using a random decision forest based on current track and game context features. Our context-conditioned motion models implicitly incorporate complex inter-object correlations while remaining tractable. We demonstrate significant performance improvements over existing multi-target tracking algorithms on basketball and field hockey sequences several minutes in duration and containing 10 and 20 players respectively.
count=3
* Bayesian Grammar Learning for Inverse Procedural Modeling
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Martinovic_Bayesian_Grammar_Learning_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Martinovic_Bayesian_Grammar_Learning_2013_CVPR_paper.pdf)]
    * Title: Bayesian Grammar Learning for Inverse Procedural Modeling
    * Year: `2013`
    * Authors: Andelo Martinovic, Luc Van Gool
    * Abstract: Within the fields of urban reconstruction and city modeling, shape grammars have emerged as a powerful tool for both synthesizing novel designs and reconstructing buildings. Traditionally, a human expert was required to write grammars for specific building styles, which limited the scope of method applicability. We present an approach to automatically learn two-dimensional attributed stochastic context-free grammars (2D-ASCFGs) from a set of labeled building facades. To this end, we use Bayesian Model Merging, a technique originally developed in the field of natural language processing, which we extend to the domain of two-dimensional languages. Given a set of labeled positive examples, we induce a grammar which can be sampled to create novel instances of the same building style. In addition, we demonstrate that our learned grammar can be used for parsing existing facade imagery. Experiments conducted on the dataset of Haussmannian buildings in Paris show that our parsing with learned grammars not only outperforms bottom-up classifiers but is also on par with approaches that use a manually designed style grammar.
count=3
* SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Wu_SCaLE_Supervised_and_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Wu_SCaLE_Supervised_and_2013_CVPR_paper.pdf)]
    * Title: SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors
    * Year: `2013`
    * Authors: Ruobing Wu, Yizhou Yu, Wenping Wang
    * Abstract: Recognizing the category of a visual object remains a challenging computer vision problem. In this paper we develop a novel deep learning method that facilitates examplebased visual object category recognition. Our deep learning architecture consists of multiple stacked layers and computes an intermediate representation that can be fed to a nearest-neighbor classifier. This intermediate representation is discriminative and structure-preserving. It is also capable of extracting essential characteristics shared by objects in the same category while filtering out nonessential differences among them. Each layer in our model is a nonlinear mapping, whose parameters are learned through two sequential steps that are designed to achieve the aforementioned properties. The first step computes a discrete mapping called supervised Laplacian Eigenmap. The second step computes a continuous mapping from the discrete version through nonlinear regression. We have extensively tested our method and it achieves state-of-the-art recognition rates on a number of benchmark datasets.
count=3
* Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Yu_Unconstrained_Monocular_3D_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Yu_Unconstrained_Monocular_3D_2013_CVPR_paper.pdf)]
    * Title: Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest
    * Year: `2013`
    * Authors: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla
    * Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE) from a novel perspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.
count=3
* Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Patel_Interpretation_of_Emergent_Communication_in_Heterogeneous_Collaborative_Embodied_Agents_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Patel_Interpretation_of_Emergent_Communication_in_Heterogeneous_Collaborative_Embodied_Agents_ICCV_2021_paper.pdf)]
    * Title: Interpretation of Emergent Communication in Heterogeneous Collaborative Embodied Agents
    * Year: `2021`
    * Authors: Shivansh Patel, Saim Wani, Unnat Jain, Alexander G. Schwing, Svetlana Lazebnik, Manolis Savva, Angel X. Chang
    * Abstract: Communication between embodied AI agents has received increasing attention in recent years. Despite its use, it is still unclear whether the learned communication is interpretable and grounded in perception. To study the grounding of emergent forms of communication, we first introduce the collaborative multi-object navigation task 'CoMON.' In this task, an 'oracle agent' has detailed environment information in the form of a map. It communicates with a 'navigator agent' that perceives the environment visually and is tasked to find a sequence of goals. To succeed at the task, effective communication is essential. CoMON hence serves as a basis to study different communication mechanisms between heterogeneous agents, that is, agents with different capabilities and roles. We study two common communication mechanisms and analyze their communication patterns through an egocentric and spatial lens. We show that the emergent communication can be grounded to the agent observations and the spatial structure of the 3D environment.
count=3
* Deep Survival Analysis With Longitudinal X-Rays for COVID-19
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Shu_Deep_Survival_Analysis_With_Longitudinal_X-Rays_for_COVID-19_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Shu_Deep_Survival_Analysis_With_Longitudinal_X-Rays_for_COVID-19_ICCV_2021_paper.pdf)]
    * Title: Deep Survival Analysis With Longitudinal X-Rays for COVID-19
    * Year: `2021`
    * Authors: Michelle Shu, Richard Strong Bowen, Charles Herrmann, Gengmo Qi, Michele Santacatterina, Ramin Zabih
    * Abstract: Time-to-event analysis is an important statistical tool for allocating clinical resources such as ICU beds. However, classical techniques like the Cox model cannot directly incorporate images due to their high dimensionality. We propose a deep learning approach that naturally incorporates multiple, time-dependent imaging studies as well as non-imaging data into time-to-event analysis. Our techniques are benchmarked on a clinical dataset of 1,894 COVID-19 patients, and show that image sequences significantly improve predictions. For example, classical time-to-event methods produce a concordance error of around 30-40% for predicting hospital admission, while our error is 25% without images and 20% with multiple X-rays included. Ablation studies suggest that our models are not learning spurious features such as scanner artifacts. While our focus and evaluation is on COVID-19, the methods we develop are broadly applicable.
count=3
* CamNet: Coarse-to-Fine Retrieval for Camera Re-Localization
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Ding_CamNet_Coarse-to-Fine_Retrieval_for_Camera_Re-Localization_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_CamNet_Coarse-to-Fine_Retrieval_for_Camera_Re-Localization_ICCV_2019_paper.pdf)]
    * Title: CamNet: Coarse-to-Fine Retrieval for Camera Re-Localization
    * Year: `2019`
    * Authors: Mingyu Ding,  Zhe Wang,  Jiankai Sun,  Jianping Shi,  Ping Luo
    * Abstract: Camera re-localization is an important but challenging task in applications like robotics and autonomous driving. Recently, retrieval-based methods have been considered as a promising direction as they can be easily generalized to novel scenes. Despite significant progress has been made, we observe that the performance bottleneck of previous methods actually lies in the retrieval module. These methods use the same features for both retrieval and relative pose regression tasks which have potential conflicts in learning. To this end, here we present a coarse-to-fine retrieval-based deep learning framework, which includes three steps, i.e., image-based coarse retrieval, pose-based fine retrieval and precise relative pose regression. With our carefully designed retrieval module, the relative pose regression task can be surprisingly simpler. We design novel retrieval losses with batch hard sampling criterion and two-stage retrieval to locate samples that adapt to the relative pose regression task. Extensive experiments show that our model (CamNet) outperforms the state-of-the-art methods by a large margin on both indoor and outdoor datasets.
count=3
* Deep Functional Maps: Structured Prediction for Dense Shape Correspondence
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Litany_Deep_Functional_Maps_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Litany_Deep_Functional_Maps_ICCV_2017_paper.pdf)]
    * Title: Deep Functional Maps: Structured Prediction for Dense Shape Correspondence
    * Year: `2017`
    * Authors: Or Litany, Tal Remez, Emanuele Rodola, Alex Bronstein, Michael Bronstein
    * Abstract: We introduce a new framework for learning dense correspondence between deformable 3D shapes. Existing learning based approaches model shape correspondence as a labelling problem, where each point of a query shape receives a label identifying a point on some reference domain; the correspondence is then constructed a posteriori by composing the label predictions of two input shapes. We propose a paradigm shift and design a structured prediction model in the space of functional maps, linear operators that provide a compact representation of the correspondence. We model the learning process via a deep residual network which takes dense descriptor fields defined on two shapes as input, and outputs a soft map between the two given objects. The resulting correspondence is shown to be accurate on several challenging benchmarks comprising multiple categories, synthetic models, real scans with acquisition artifacts, topological noise, and partiality.
count=3
* 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-Scale 3D Point Clouds
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Liu_3DCNN-DQN-RNN_A_Deep_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_3DCNN-DQN-RNN_A_Deep_ICCV_2017_paper.pdf)]
    * Title: 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-Scale 3D Point Clouds
    * Year: `2017`
    * Authors: Fangyu Liu, Shuaipeng Li, Liqiang Zhang, Chenghu Zhou, Rongtian Ye, Yuebin Wang, Jiwen Lu
    * Abstract: Semantic parsing of large-scale 3D point clouds is an important research topic in computer vision and remote sensing fields. Most existing approaches utilize hand-crafted features for each modality independently and combine them in a heuristic manner. They often fail to consider the consistency and complementary information among features adequately, which makes them difficult to capture high-level semantic structures. The features learned by most of the current deep learning methods can obtain high-quality image classification results. However, these methods are hard to be applied to recognize 3D point clouds due to unorganized distribution and various point density of data. In this paper, we propose a 3DCNN-DQN-RNN method which fuses the 3D convolutional neural network (CNN), Deep Q-Network (DQN) and Residual recurrent neural network (RNN) for an efficient semantic parsing of large-scale 3D point clouds. In our method, an eye window under control of the 3D CNN and DQN can localize and segment the points of the class objects efficiently. The 3D CNN and Residual RNN further extract robust and discriminative features of the points in the eye window, and thus greatly enhance the parsing accuracy of large-scale point clouds. Our method provides an automatic process that maps the raw data to the classification results. It also integrates object localization, segmentation and classification into one framework. Experimental results demonstrate that the proposed method outperforms the state-of-the-art point cloud classification methods.
count=3
* BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects Without Using Depth
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Rad_BB8_A_Scalable_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Rad_BB8_A_Scalable_ICCV_2017_paper.pdf)]
    * Title: BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects Without Using Depth
    * Year: `2017`
    * Authors: Mahdi Rad, Vincent Lepetit
    * Abstract: We introduce a novel method for 3D object detection and pose estimation from color images only. We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background. By contrast with recent patch-based methods, we rely on a "holistic" approach: We apply to the detected objects a Convolutional Neural Network (CNN) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes. This, however, is not sufficient for handling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging. We solve this problem by restricting the range of poses used for training, and by introducing a classifier to identify the range of a pose at run-time before estimating it. We also use an optional additional step that refines the predicted poses. We improve the state-of-the-art on the LINEMOD dataset from 73.7% to 89.3% of correctly registered RGB frames. We are also the first to report results on the Occlusion dataset using color images only. We obtain 54% of frames passing the Pose 6D criterion on average on several sequences of the T-LESS dataset, compared to the 67% of the state-of-the-art on the same sequences which uses both color and depth. The full approach is also scalable, as a single network can be trained for multiple objects simultaneously.
count=3
* Spatiotemporal Modeling for Crowd Counting in Videos
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Xiong_Spatiotemporal_Modeling_for_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Xiong_Spatiotemporal_Modeling_for_ICCV_2017_paper.pdf)]
    * Title: Spatiotemporal Modeling for Crowd Counting in Videos
    * Year: `2017`
    * Authors: Feng Xiong, Xingjian Shi, Dit-Yan Yeung
    * Abstract: Region of Interest (ROI) crowd counting can be formulated as a regression problem of learning a mapping from an image or a video frame to a crowd density map. Recently, convolutional neural network (CNN) models have achieved promising results for crowd counting. However, even when dealing with video data, CNN-based methods still consider each video frame independently, ignoring the strong temporal correlation between neighboring frames. To exploit the otherwise very useful temporal information in video sequences, we propose a variant of a recent deep learning model called convolutional LSTM (ConvLSTM) for crowd counting. Unlike the previous CNN-based methods, our method fully captures both spatial and temporal dependencies. Furthermore, we extend the ConvLSTM model to a bidirectional ConvLSTM model which can access long-range information in both directions. Extensive experiments using four publicly available datasets demonstrate the reliability of our approach and the effectiveness of incorporating temporal information to boost the accuracy of crowd counting. In addition, we also conduct some transfer learning experiments to show that once our model is trained on one dataset, its learning experience can be transferred easily to a new dataset which consists of only very few video frames for model adaptation.
count=3
* Truncating Wide Networks Using Binary Tree Architectures
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Zhang_Truncating_Wide_Networks_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Truncating_Wide_Networks_ICCV_2017_paper.pdf)]
    * Title: Truncating Wide Networks Using Binary Tree Architectures
    * Year: `2017`
    * Authors: Yan Zhang, Mete Ozay, Shuohao Li, Takayuki Okatani
    * Abstract: In this paper, we propose a binary tree architecture to truncate architecture of wide networks by reducing the width of the networks. More precisely, in the proposed architecture, the width is incrementally reduced from lower layers to higher layers in order to increase the expressive capacity of networks with a less increase on parameter size. Also, in order to ease the gradient vanishing problem, features obtained at different layers are concatenated to form the output of our architecture. By employing the proposed architecture on a baseline wide network, we can construct and train a new network with same depth but considerably less number of parameters. In our experimental analyses, we observe that the proposed architecture enables us to obtain better parameter size and accuracy trade-off compared to baseline networks using various benchmark image classification datasets. The results show that our model can decrease the classification error of a baseline from 20.43% to 19.22% on Cifar-100 using only 28% of parameters that the baseline has.
count=3
* HICO: A Benchmark for Recognizing Human-Object Interactions in Images
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Chao_HICO_A_Benchmark_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Chao_HICO_A_Benchmark_ICCV_2015_paper.pdf)]
    * Title: HICO: A Benchmark for Recognizing Human-Object Interactions in Images
    * Year: `2015`
    * Authors: Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, Jia Deng
    * Abstract: We introduce a new benchmark "Humans Interacting with Common Objects" (HICO) for recognizing human-object interactions (HOI). We demonstrate the key features of HICO: a diverse set of interactions with common object categories, a list of well-defined, sense-based HOI categories, and an exhaustive labeling of co-occurring interactions with an object category in each image. We perform an in-depth analysis of representative current approaches and show that DNNs enjoy a significant edge. In addition, we show that semantic knowledge can significantly improve HOI recognition, especially for uncommon categories.
count=3
* Real-Time Pose Estimation Piggybacked on Object Detection
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Juranek_Real-Time_Pose_Estimation_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Juranek_Real-Time_Pose_Estimation_ICCV_2015_paper.pdf)]
    * Title: Real-Time Pose Estimation Piggybacked on Object Detection
    * Year: `2015`
    * Authors: Roman Juranek, Adam Herout, Marketa Dubska, Pavel Zemcik
    * Abstract: We present an object detector coupled with pose estimation directly in a single compact and simple model, where the detector shares extracted image features with the pose estimator. The output of the classification of each candidate window consists of both object score and likelihood map of poses. This extension introduces negligible overhead during detection so that the detector is still capable of real time operation. We evaluated the proposed approach on the problem of vehicle detection. We used existing datasets with viewpoint/pose annotation (WCVP, 3D objects, KITTI). Besides that, we collected a new traffic surveillance dataset COD20k which fills certain gaps of the existing datasets and we make it public. The experimental results show that the proposed approach is comparable with state-of-the-art approaches in terms of accuracy, but it is considerably faster - easily operating in real time (Matlab with C++ code). The source codes and the collected COD20k dataset are made public along with the paper.
count=3
* Person Re-Identification With Discriminatively Trained Viewpoint Invariant Dictionaries
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Karanam_Person_Re-Identification_With_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Karanam_Person_Re-Identification_With_ICCV_2015_paper.pdf)]
    * Title: Person Re-Identification With Discriminatively Trained Viewpoint Invariant Dictionaries
    * Year: `2015`
    * Authors: Srikrishna Karanam, Yang Li, Richard J. Radke
    * Abstract: This paper introduces a new approach to address the person re-identification problem in cameras with non-overlapping fields of view. Unlike previous approaches that learn Mahalanobis-like distance metrics in some transformed feature space, we propose to learn a dictionary that is capable of discriminatively and sparsely encoding features representing different people. Our approach directly addresses two key challenges in person re-identification: viewpoint variations and discriminability. First, to tackle viewpoint and associated appearance changes, we learn a single dictionary to represent both gallery and probe images in the training phase. We then discriminatively train the dictionary by enforcing explicit constraints on the associated sparse representations of the feature vectors. In the testing phase, we re-identify a probe image by simply determining the gallery image that has the closest sparse representation to that of the probe image in the Euclidean sense. Extensive performance evaluations on three publicly available multi-shot re-identification datasets demonstrate the advantages of our algorithm over several state-of-the-art dictionary learning, temporal sequence matching, and spatial appearance and metric learning based techniques.
count=3
* Efficient Classifier Training to Minimize False Merges in Electron Microscopy Segmentation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Parag_Efficient_Classifier_Training_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Parag_Efficient_Classifier_Training_ICCV_2015_paper.pdf)]
    * Title: Efficient Classifier Training to Minimize False Merges in Electron Microscopy Segmentation
    * Year: `2015`
    * Authors: Toufiq Parag, Dan C. Ciresan, Alessandro Giusti
    * Abstract: The prospect of neural reconstruction from Electron Microscopy (EM) images has been elucidated by the automatic segmentation algorithms. Although segmentation algorithms eliminate the necessity of tracing the neurons by hand, significant manual effort is still essential for correcting the mistakes they make. A considerable amount of human labor is also required for annotating groundtruth volumes for training the classifiers of a segmentation framework. It is critically important to diminish the dependence on human interaction in the overall reconstruction system. This study proposes a novel classifier training algorithm for EM segmentation aimed to reduce the amount of manual effort demanded by the groundtruth annotation and error refinement tasks. Instead of using an exhaustive pixel level groundtruth, an active learning algorithm is proposed for sparse labeling of pixel and boundaries of superpixels. Because over-segmentation errors are in general more tolerable and easier to correct than the under-segmentation errors, our algorithm is designed to prioritize minimization of false-merges over false-split mistakes. Our experiments on both 2D and 3D data suggest that the proposed method yields segmentation outputs that are more amenable to neural reconstruction than those of existing methods.
count=3
* Naive Bayes Super-Resolution Forest
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Salvador_Naive_Bayes_Super-Resolution_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Salvador_Naive_Bayes_Super-Resolution_ICCV_2015_paper.pdf)]
    * Title: Naive Bayes Super-Resolution Forest
    * Year: `2015`
    * Authors: Jordi Salvador, Eduardo Perez-Pellitero
    * Abstract: This paper presents a fast, high-performance method for super resolution with external learning. The first contribution leading to the excellent performance is a bimodal tree for clustering, which successfully exploits the antipodal invariance of the coarse-to-high-res mapping of natural image patches and provides scalability to finer partitions of the underlying coarse patch space. During training an ensemble of such bimodal trees is computed, providing different linearizations of the mapping. The second and main contribution is a fast inference algorithm, which selects the most suitable mapping function within the tree ensemble for each patch by adopting a Local Naive Bayes formulation. The experimental validation shows promising scalability properties that reflect the suitability of the proposed model, which may also be generalized to other tasks. The resulting method is beyond one order of magnitude faster and performs objectively and subjectively better than the current state of the art.
count=3
* PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Duffner_PixelTrack_A_Fast_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Duffner_PixelTrack_A_Fast_2013_ICCV_paper.pdf)]
    * Title: PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects
    * Year: `2013`
    * Authors: Stefan Duffner, Christophe Garcia
    * Abstract: In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-theart tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.
count=3
* Learning to Predict Gaze in Egocentric Video
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Li_Learning_to_Predict_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Li_Learning_to_Predict_2013_ICCV_paper.pdf)]
    * Title: Learning to Predict Gaze in Egocentric Video
    * Year: `2013`
    * Authors: Yin Li, Alireza Fathi, James M. Rehg
    * Abstract: We present a model for gaze prediction in egocentric video by leveraging the implicit cues that exist in camera wearer's behaviors. Specifically, we compute the camera wearer's head motion and hand location from the video and combine them to estimate where the eyes look. We further model the dynamic behavior of the gaze, in particular fixations, as latent variables to improve the gaze prediction. Our gaze prediction results outperform the state-of-the-art algorithms by a large margin on publicly available egocentric vision datasets. In addition, we demonstrate that we get a significant performance boost in recognizing daily actions and segmenting foreground objects by plugging in our gaze predictions into state-of-the-art methods.
count=3
* Shape Anchors for Data-Driven Multi-view Reconstruction
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Owens_Shape_Anchors_for_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Owens_Shape_Anchors_for_2013_ICCV_paper.pdf)]
    * Title: Shape Anchors for Data-Driven Multi-view Reconstruction
    * Year: `2013`
    * Authors: Andrew Owens, Jianxiong Xiao, Antonio Torralba, William Freeman
    * Abstract: We present a data-driven method for building dense 3D reconstructions using a combination of recognition and multi-view cues. Our approach is based on the idea that there are image patches that are so distinctive that we can accurately estimate their latent 3D shapes solely using recognition. We call these patches shape anchors, and we recoggnioon.WWWeccalltteeseepatchhesshhapeaanchorrs,anndwwe that transfers dense, complex geometry between scenes. We "anchor" our 3D interpretation from these patches, using them to predict geometry for parts of the scene that are relatively ambiguous. The resulting algorithm produces dense reconstructions from stereo point clouds that are sparse and noisy, and we demonstrate it on a challenging dataset of real-world, indoor scenes.
count=3
* Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Tang_Real-Time_Articulated_Hand_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Tang_Real-Time_Articulated_Hand_2013_ICCV_paper.pdf)]
    * Title: Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests
    * Year: `2013`
    * Authors: Danhang Tang, Tsz-Ho Yu, Tae-Kyun Kim
    * Abstract: This paper presents the first semi-supervised transductive algorithm for real-time articulated hand pose estimation. Noisy data and occlusions are the major challenges of articulated hand pose estimation. In addition, the discrepancies among realistic and synthetic pose data undermine the performances of existing approaches that use synthetic data extensively in training. We therefore propose the Semi-supervised Transductive Regression (STR) forest which learns the relationship between a small, sparsely labelled realistic dataset and a large synthetic dataset. We also design a novel data-driven, pseudo-kinematic technique to refine noisy or occluded joints. Our contributions include: (i) capturing the benefits of both realistic and synthetic data via transductive learning; (ii) showing accuracies can be improved by considering unlabelled data; and (iii) introducing a pseudo-kinematic technique to refine articulations efficiently. Experimental results show not only the promising performance of our method with respect to noise and occlusions, but also its superiority over state-ofthe-arts in accuracy, robustness and speed.
count=3
* Sieving Regression Forest Votes for Facial Feature Detection in the Wild
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Yang_Sieving_Regression_Forest_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Yang_Sieving_Regression_Forest_2013_ICCV_paper.pdf)]
    * Title: Sieving Regression Forest Votes for Facial Feature Detection in the Wild
    * Year: `2013`
    * Authors: Heng Yang, Ioannis Patras
    * Abstract: In this paper we propose a method for the localization of multiple facial features on challenging face images. In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. Several sieves of the second type, one associated with each individual facial point, filter out distant votes. We propose a method that adjusts onthe-fly the proximity threshold of each second type sieve by applying a classifier which, based on middle-level features extracted from voting maps for the facial feature in question, makes a sequence of decisions on whether the threshold should be reduced or not. We validate our proposed method on two challenging datasets with images collected from the Internet in which we obtain state of the art results without resorting to explicit facial shape models. We also show the benefits of our method for proximity threshold adjustment especially on 'difficult' face images.
count=3
* Optimal Transport for Treatment Effect Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/1160e7f31d0a74abbbe1bbf7924b949c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/1160e7f31d0a74abbbe1bbf7924b949c-Paper-Conference.pdf)]
    * Title: Optimal Transport for Treatment Effect Estimation
    * Year: `2023`
    * Authors: Hao Wang, Jiajun Fan, Zhichao Chen, Haoxuan Li, Weiming Liu, Tianqiao Liu, Quanyu Dai, Yichao Wang, Zhenhua Dong, Ruiming Tang
    * Abstract: Estimating individual treatment effects from observational data is challenging due to treatment selection bias. Prevalent methods mainly mitigate this issue by aligning different treatment groups in the latent space, the core of which is the calculation of distribution discrepancy. However, two issues that are often overlooked can render these methods invalid:(1) mini-batch sampling effects (MSE), where the calculated discrepancy is erroneous in non-ideal mini-batches with outcome imbalance and outliers;(2) unobserved confounder effects (UCE), where the unobserved confounders are not considered in the discrepancy calculation.Both of these issues invalidate the calculated discrepancy, mislead the training of estimators, and thus impede the handling of treatment selection bias.To tackle these issues, we propose Entire Space CounterFactual Regression (ESCFR), which is a new take on optimal transport technology in the context of causality.Specifically, based on the canonical optimal transport framework, we propose a relaxed mass-preserving regularizer to address the MSE issue and design a proximal factual outcome regularizer to handle the UCE issue.Extensive experiments demonstrate that ESCFR estimates distribution discrepancy accurately, handles the treatment selection bias effectively, and outperforms prevalent competitors significantly.
count=3
* NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/14f656f21d09a4114666f60a45aab1aa-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/14f656f21d09a4114666f60a45aab1aa-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics
    * Year: `2023`
    * Authors: Anwar Said, Roza Bayrak, Tyler Derr, Mudassir Shabbir, Daniel Moyer, Catie Chang, Xenofon Koutsoukos
    * Abstract: Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional magnetic resonance imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain has been challenging due to the expansive number of potential preprocessing pipelines and the large parameter search space for graph-based dataset construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets, and demonstrated its utility for predicting multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by crafting 35 datasets that encompass static and dynamic brain connectivity, running in excess of 15 baseline methods for benchmarking. Additionally, we provide generic frameworks for learning on both static and dynamic graphs. Our extensive experiments lead to several key observations. Notably, using correlation vectors as node features, incorporating larger number of regions of interest, and employing sparser graphs lead to improved performance. To foster further advancements in graph-based data driven neuroimaging analysis, we offer a comprehensive open-source Python package that includes the benchmark datasets, baseline implementations, model training, and standard evaluation.
count=3
* Explaining Predictive Uncertainty with Information Theoretic Shapley Values
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/16e4be78e61a3897665fa01504e9f452-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/16e4be78e61a3897665fa01504e9f452-Paper-Conference.pdf)]
    * Title: Explaining Predictive Uncertainty with Information Theoretic Shapley Values
    * Year: `2023`
    * Authors: David Watson, Joshua O'Hara, Niek Tax, Richard Mudd, Ido Guy
    * Abstract: Researchers in explainable artificial intelligence have developed numerous methods for helping users understand the predictions of complex supervised learning models. By contrast, explaining the $\textit{uncertainty}$ of model outputs has received relatively little attention. We adapt the popular Shapley value framework to explain various types of predictive uncertainty, quantifying each feature's contribution to the conditional entropy of individual model outputs. We consider games with modified characteristic functions and find deep connections between the resulting Shapley values and fundamental quantities from information theory and conditional independence testing. We outline inference procedures for finite sample error rate control with provable guarantees, and implement efficient algorithms that perform well in a range of experiments on real and simulated data. Our method has applications to covariate shift detection, active learning, feature selection, and active feature-value acquisition.
count=3
* Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/3df874367ce2c43891aab1ab23ae6959-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/3df874367ce2c43891aab1ab23ae6959-Paper-Conference.pdf)]
    * Title: Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning
    * Year: `2023`
    * Authors: Amit Dhurandhar, Karthikeyan Natesan Ramamurthy, Kartik Ahuja, Vijay Arya
    * Abstract: Locally interpretable model agnostic explanations (LIME) method is one of the most popular methods used to explain black-box models at a per example level. Although many variants have been proposed, few provide a simple way to produce high fidelity explanations that are also stable and intuitive. In this work, we provide a novel perspective by proposing a model agnostic local explanation method inspired by the invariant risk minimization (IRM) principle -- originally proposed for (global) out-of-distribution generalization -- to provide such high fidelity explanations that are also stable and unidirectional across nearby examples. Our method is based on a game theoretic formulation where we theoretically show that our approach has a strong tendency to eliminate features where the gradient of the black-box function abruptly changes sign in the locality of the example we want to explain, while in other cases it is more careful and will choose a more conservative (feature) attribution, a behavior which can be highly desirable for recourse. Empirically, we show on tabular, image and text data that the quality of our explanations with neighborhoods formed using random perturbations are much better than LIME and in some cases even comparable to other methods that use realistic neighbors sampled from the data manifold. This is desirable given that learning a manifold to either create realistic neighbors or to project explanations is typically expensive or may even be impossible. Moreover, our algorithm is simple and efficient to train, and can ascertain stable input features for local decisions of a black-box without access to side information such as a (partial) causal graph as has been seen in some recent works.
count=3
* Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/421f83663c02cdaec8c3c38337709989-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/421f83663c02cdaec8c3c38337709989-Paper-Conference.pdf)]
    * Title: Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension
    * Year: `2023`
    * Authors: Moritz Haas, David Holzmüller, Ulrike Luxburg, Ingo Steinwart
    * Abstract: The success of over-parameterized neural networks trained to near-zero training error has caused great interest in the phenomenon of benign overfitting, where estimators are statistically consistent even though they interpolate noisy training data. While benign overfitting in fixed dimension has been established for some learning methods, current literature suggests that for regression with typical kernel methods and wide neural networks, benign overfitting requires a high-dimensional setting, where the dimension grows with the sample size. In this paper, we show that the smoothness of the estimators, and not the dimension, is the key: benign overfitting is possible if and only if the estimator's derivatives are large enough. We generalize existing inconsistency results to non-interpolating models and more kernels to show that benign overfitting with moderate derivatives is impossible in fixed dimension. Conversely, we show that benign overfitting is possible for regression with a sequence of spiky-smooth kernels with large derivatives. Using neural tangent kernels, we translate our results to wide neural networks. We prove that while infinite-width networks do not overfit benignly with the ReLU activation, this can be fixed by adding small high-frequency fluctuations to the activation function. Our experiments verify that such neural networks, while overfitting, can indeed generalize well even on low-dimensional data sets.
count=3
* ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/44a6769fe6c695f8dfb347c649f7c9f0-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/44a6769fe6c695f8dfb347c649f7c9f0-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning
    * Year: `2023`
    * Authors: Julia Kaltenborn, Charlotte Lange, Venkatesh Ramesh, Philippe Brouillard, Yaniv Gurwicz, Chandni Nagda, Jakob Runge, Peer Nowack, David Rolnick
    * Abstract: Climate models have been key for assessing the impact of climate change and simulating future climate scenarios. The machine learning (ML) community has taken an increased interest in supporting climate scientists’ efforts on various tasks such as climate model emulation, downscaling, and prediction tasks. Many of those tasks have been addressed on datasets created with single climate models. However, both the climate science and ML communities have suggested that to address those tasks at scale, we need large, consistent, and ML-ready climate model datasets. Here, we introduce ClimateSet, a dataset containing the inputs and outputs of 36 climate models from the Input4MIPs and CMIP6 archives. In addition, we provide a modular dataset pipeline for retrieving and preprocessing additional climate models and scenarios. We showcase the potential of our dataset by using it as a benchmark for ML-based climate model emulation. We gain new insights about the performance and generalization capabilities of the different ML models by analyzing their performance across different climate models. Furthermore, the dataset can be used to train an ML emulator on several climate models instead of just one. Such a “super emulator” can quickly project new climate change scenarios, complementing existing scenarios already provided to policymakers. We believe ClimateSet will create the basis needed for the ML community to tackle climate-related tasks at scale.
count=3
* Decision Tree for Locally Private Estimation with Public Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/88237ac4e9941b1be5c6d3c1ad408184-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/88237ac4e9941b1be5c6d3c1ad408184-Paper-Conference.pdf)]
    * Title: Decision Tree for Locally Private Estimation with Public Data
    * Year: `2023`
    * Authors: Yuheng Ma, Han Zhang, Yuchao Cai, Hanfang Yang
    * Abstract: We propose conducting locally differentially private (LDP) estimation with the aid of a small amount of public data to enhance the performance of private estimation. Specifically, we introduce an efficient algorithm called Locally differentially Private Decision Tree (LPDT) for LDP regression. We first use the public data to grow a decision tree partition and then fit an estimator according to the partition privately. From a theoretical perspective, we show that LPDT is $\varepsilon$-LDP and has a mini-max optimal convergence rate under a mild assumption of similarity between public and private data, whereas the lower bound of the convergence rate of LPDT without public data is strictly slower, which implies that the public data helps to improve the convergence rates of LDP estimation. We conduct experiments on both synthetic and real-world data to demonstrate the superior performance of LPDT compared with other state-of-the-art LDP regression methods. Moreover, we show that LPDT remains effective despite considerable disparities between public and private data.
count=3
* AirDelhi: Fine-Grained Spatio-Temporal Particulate Matter Dataset From Delhi For ML based Modeling
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/ee799aff607fcf39c01df6391e96f92c-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/ee799aff607fcf39c01df6391e96f92c-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: AirDelhi: Fine-Grained Spatio-Temporal Particulate Matter Dataset From Delhi For ML based Modeling
    * Year: `2023`
    * Authors: Sachin Chauhan, Zeel Bharatkumar Patel, Sayan Ranu, Rijurekha Sen, Nipun Batra
    * Abstract: Air pollution poses serious health concerns in developing countries, such as India, necessitating large-scale measurement for correlation analysis, policy recommendations, and informed decision-making. However, fine-grained data collection is costly. Specifically, static sensors for pollution measurement cost several thousand dollars per unit, leading to inadequate deployment and coverage. To complement the existing sparse static sensor network, we propose a mobile sensor network utilizing lower-cost PM2.5 sensors mounted on public buses in the Delhi-NCR region of India. Through this exercise, we introduce a novel dataset AirDelhi comprising PM2.5 and PM10 measurements. This dataset is made publicly available, at https://www.cse.iitd.ac.in/pollutiondata, serving as a valuable resource for machine learning (ML) researchers and environmentalists. We present three key contributions with the release of this dataset. Firstly, through in-depth statistical analysis, we demonstrate that the released dataset significantly differs from existing pollution datasets, highlighting its uniqueness and potential for new insights. Secondly, the dataset quality been validated against existing expensive sensors. Thirdly, we conduct a benchmarking exercise (https://github.com/sachin-iitd/DelhiPMDatasetBenchmark), evaluating state-of-the-art methods for interpolation, feature imputation, and forecasting on this dataset, which is the largest publicly available PM dataset to date. The results of the benchmarking exercise underscore the substantial disparities in accuracy between the proposed dataset and other publicly available datasets. This finding highlights the complexity and richness of our dataset, emphasizing its value for advancing research in the field of air pollution.
count=3
* Harnessing the power of choices in decision tree learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/fddad60891bdf85aac8041f80ed022df-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/fddad60891bdf85aac8041f80ed022df-Paper-Conference.pdf)]
    * Title: Harnessing the power of choices in decision tree learning
    * Year: `2023`
    * Authors: Guy Blanc, Jane Lange, Chirag Pabbaraju, Colin Sullivan, Li-Yang Tan, Mo Tiwari
    * Abstract: We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as possible splits instead of just the single best attribute. We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a greediness hierarchy theorem showing that for every $k\in \mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there are data distributions for which the former achieves accuracy $1-\epsilon$, whereas the latter only achieves accuracy $\frac{1}{2}+\epsilon$. We then show, through extensive experiments, that Top-$k$ outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent ``optimal decision tree'' algorithms. On one hand, Top-$k$ consistently enjoys significant accuracy gains over greedy algorithms across a wide range of benchmarks. On the other hand, Top-$k$ is markedly more scalable than optimal decision tree algorithms and is able to handle dataset and feature set sizes that remain far beyond the reach of these algorithms. The code to reproduce our results is available at https://github.com/SullivanC19/pydl8.5-topk.
count=3
* A Reduction to Binary Approach for Debiasing Multiclass Datasets
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/10eaa0aae94b34308e9b3fa7b677cbe1-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/10eaa0aae94b34308e9b3fa7b677cbe1-Paper-Conference.pdf)]
    * Title: A Reduction to Binary Approach for Debiasing Multiclass Datasets
    * Year: `2022`
    * Authors: Ibrahim M. Alabdulmohsin, Jessica Schrouff, Sanmi Koyejo
    * Abstract: We propose a novel reduction-to-binary (R2B) approach that enforces demographic parity for multiclass classification with non-binary sensitive attributes via a reduction to a sequence of binary debiasing tasks. We prove that R2B satisfies optimality and bias guarantees and demonstrate empirically that it can lead to an improvement over two baselines: (1) treating multiclass problems as multi-label by debiasing labels independently and (2) transforming the features instead of the labels. Surprisingly, we also demonstrate that independent label debiasing yields competitive results in most (but not all) settings. We validate these conclusions on synthetic and real-world datasets from social science, computer vision, and healthcare.
count=3
* Conformalized Fairness via Quantile Regression
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/4b52b3c50110fc10f6a1a86055682ea2-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/4b52b3c50110fc10f6a1a86055682ea2-Paper-Conference.pdf)]
    * Title: Conformalized Fairness via Quantile Regression
    * Year: `2022`
    * Authors: Meichen Liu, Lei Ding, Dengdeng Yu, Wulong Liu, Linglong Kong, Bei Jiang
    * Abstract: Algorithmic fairness has received increased attention in socially sensitive domains. While rich literature on mean fairness has been established, research on quantile fairness remains sparse but vital. To fulfill great needs and advocate the significance of quantile fairness, we propose a novel framework to learn a real-valued quantile function under the fairness requirement of Demographic Parity with respect to sensitive attributes, such as race or gender, and thereby derive a reliable fair prediction interval. Using optimal transport and functional synchronization techniques, we establish theoretical guarantees of distribution-free coverage and exact fairness for the induced prediction interval constructed by fair quantiles. A hands-on pipeline is provided to incorporate flexible quantile regressions with an efficient fairness adjustment post-processing algorithm. We demonstrate the superior empirical performance of this approach on several benchmark datasets. Our results show the model’s ability to uncover the mechanism underlying the fairness-accuracy trade-off in a wide range of societal and medical applications.
count=3
* LIFT: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/4ce7fe1d2730f53cb3857032952cd1b8-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/4ce7fe1d2730f53cb3857032952cd1b8-Paper-Conference.pdf)]
    * Title: LIFT: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks
    * Year: `2022`
    * Authors: Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dimitris Papailiopoulos, Kangwook Lee
    * Abstract: Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling "no-code machine learning with LMs." We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g., context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. Our code is available at https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.
count=3
* Normalizing Flows for Knockoff-free Controlled Feature Selection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/66f09010d989c83faeeac2617464b6a4-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/66f09010d989c83faeeac2617464b6a4-Paper-Conference.pdf)]
    * Title: Normalizing Flows for Knockoff-free Controlled Feature Selection
    * Year: `2022`
    * Authors: Derek Hansen, Brian Manzo, Jeffrey Regier
    * Abstract: Controlled feature selection aims to discover the features a response depends on while limiting the false discovery rate (FDR) to a predefined level. Recently, multiple deep-learning-based methods have been proposed to perform controlled feature selection through the Model-X knockoff framework. We demonstrate, however, that these methods often fail to control the FDR for two reasons. First, these methods often learn inaccurate models of features. Second, the "swap" property, which is required for knockoffs to be valid, is often not well enforced. We propose a new procedure called FlowSelect to perform controlled feature selection that does not suffer from either of these two problems. To more accurately model the features, FlowSelect uses normalizing flows, the state-of-the-art method for density estimation. Instead of enforcing the "swap" property, FlowSelect uses a novel MCMC-based procedure to calculate p-values for each feature directly. Asymptotically, FlowSelect computes valid p-values. Empirically, FlowSelect consistently controls the FDR on both synthetic and semi-synthetic benchmarks, whereas competing knockoff-based approaches do not. FlowSelect also demonstrates greater power on these benchmarks. Additionally, FlowSelect correctly infers the genetic variants associated with specific soybean traits from GWAS data.
count=3
* Agreement-on-the-line: Predicting the Performance of Neural Networks under Distribution Shift
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/7a8d388b7a17df480856dff1cc079b08-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/7a8d388b7a17df480856dff1cc079b08-Paper-Conference.pdf)]
    * Title: Agreement-on-the-line: Predicting the Performance of Neural Networks under Distribution Shift
    * Year: `2022`
    * Authors: Christina Baek, Yiding Jiang, Aditi Raghunathan, J. Zico Kolter
    * Abstract: Recently, Miller et al. showed that a model's in-distribution (ID) accuracy has a strong linear correlation with its out-of-distribution (OOD) accuracy, on several OOD benchmarks, a phenomenon they dubbed ``accuracy-on-the-line''. While a useful tool for model selection (i.e., the model most likely to perform the best OOD is the one with highest ID accuracy), this fact does not help to estimate the actual OOD performance of models without access to a labeled OOD validation set. In this paper, we show a similar surprising phenomena also holds for the agreement between pairs of neural network classifiers: whenever accuracy-on-the-line holds, we observe that the OOD agreement between the predictions of any two pairs of neural networks (with potentially different architectures) also observes a strong linear correlation with their ID agreement. Furthermore, we observe that the slope and bias of OOD vs ID agreement closely matches that of OOD vs ID accuracy. This phenomenon which we call agreement-on-the-line, has important practical applications: without any labeled data, we can predict the OOD accuracy of classifiers, since OOD agreement can be estimated with just unlabeled data. Our prediction algorithm outperforms previous methods both in shifts where agreement-on-the-line holds and, surprisingly, when accuracy is not on the line. This phenomenon also provides new insights into neural networks: unlike accuracy-on-the-line, agreement-on-the-line only appears to hold for neural network classifiers.
count=3
* RISE: Robust Individualized Decision Learning with Sensitive Variables
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/7b2f0758334389b8ad0665a9bd165463-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/7b2f0758334389b8ad0665a9bd165463-Paper-Conference.pdf)]
    * Title: RISE: Robust Individualized Decision Learning with Sensitive Variables
    * Year: `2022`
    * Authors: Xiaoqing Tan, Zhengling Qi, Christopher Seymour, Lu Tang
    * Abstract: This paper introduces RISE, a robust individualized decision learning framework with sensitive variables, where sensitive variables are collectible data and important to the intervention decision, but their inclusion in decision making is prohibited due to reasons such as delayed availability or fairness concerns. A naive baseline is to ignore these sensitive variables in learning decision rules, leading to significant uncertainty and bias. To address this, we propose a decision learning framework to incorporate sensitive variables during offline training but not include them in the input of the learned decision rule during model deployment. Specifically, from a causal perspective, the proposed framework intends to improve the worst-case outcomes of individuals caused by sensitive variables that are unavailable at the time of decision. Unlike most existing literature that uses mean-optimal objectives, we propose a robust learning framework by finding a newly defined quantile- or infimum-optimal decision rule. The reliable performance of the proposed method is demonstrated through synthetic experiments and three real-world applications.
count=3
* ToDD: Topological Compound Fingerprinting in Computer-Aided Drug Discovery
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/b31f6d65f2584b3c4347148db36fe07f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/b31f6d65f2584b3c4347148db36fe07f-Paper-Conference.pdf)]
    * Title: ToDD: Topological Compound Fingerprinting in Computer-Aided Drug Discovery
    * Year: `2022`
    * Authors: Andaç Demir, Baris Coskunuzer, Yulia Gel, Ignacio Segovia-Dominguez, Yuzhou Chen, Bulent Kiziltan
    * Abstract: In computer-aided drug discovery (CADD), virtual screening (VS) is used for comparing a library of compounds against known active ligands to identify the drug candidates that are most likely to bind to a molecular target. Most VS methods to date have focused on using canonical compound representations (e.g., SMILES strings, Morgan fingerprints) or generating alternative fingerprints of the compounds by training progressively more complex variational autoencoders (VAEs) and graph neural networks (GNNs). Although VAEs and GNNs led to significant improvements in VS performance, these methods suffer from reduced performance when scaling to large virtual compound datasets. The performance of these methods has shown only incremental improvements in the past few years. To address this problem, we developed a novel method using multiparameter persistence (MP) homology that produces topological fingerprints of the compounds as multidimensional vectors. Our primary contribution is framing the VS process as a new topology-based graph ranking problem by partitioning a compound into chemical substructures informed by the periodic properties of its atoms and extracting their persistent homology features at multiple resolution levels. We show that the margin loss fine-tuning of pretrained Triplet networks attains highly competitive results in differentiating between compounds in the embedding space and ranking their likelihood of becoming effective drug candidates. We further establish theoretical guarantees for the stability properties of our proposed MP signatures, and demonstrate that our models, enhanced by the MP signatures, outperform state-of-the-art methods on benchmark datasets by a wide and highly statistically significant margin (e.g., 93\% gain for Cleves-Jain and 54\% gain for DUD-E Diverse dataset).
count=3
* Batch Bayesian optimisation via density-ratio estimation with guarantees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/c0d5a28eb3949efbedbe3e41751e3ffc-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/c0d5a28eb3949efbedbe3e41751e3ffc-Paper-Conference.pdf)]
    * Title: Batch Bayesian optimisation via density-ratio estimation with guarantees
    * Year: `2022`
    * Authors: Rafael Oliveira, Louis Tiao, Fabio T. Ramos
    * Abstract: Bayesian optimisation (BO) algorithms have shown remarkable success in applications involving expensive black-box functions. Traditionally BO has been set as a sequential decision-making process which estimates the utility of query points via an acquisition function and a prior over functions, such as a Gaussian process. Recently, however, a reformulation of BO via density-ratio estimation (BORE) allowed reinterpreting the acquisition function as a probabilistic binary classifier, removing the need for an explicit prior over functions and increasing scalability. In this paper, we present a theoretical analysis of BORE's regret and an extension of the algorithm with improved uncertainty estimates. We also show that BORE can be naturally extended to a batch optimisation setting by recasting the problem as approximate Bayesian inference. The resulting algorithms come equipped with theoretical performance guarantees and are assessed against other batch and sequential BO baselines in a series of experiments.
count=3
* WeightedSHAP: analyzing and improving Shapley based feature attributions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/de1739eba209c682a90ec3669229ab2d-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/de1739eba209c682a90ec3669229ab2d-Paper-Conference.pdf)]
    * Title: WeightedSHAP: analyzing and improving Shapley based feature attributions
    * Year: `2022`
    * Authors: Yongchan Kwon, James Y. Zou
    * Abstract: Shapley value is a popular approach for measuring the influence of individual features. While Shapley feature attribution is built upon desiderata from game theory, some of its constraints may be less natural in certain machine learning settings, leading to unintuitive model interpretation. In particular, the Shapley value uses the same weight for all marginal contributions---i.e. it gives the same importance when a large number of other features are given versus when a small number of other features are given. This property can be problematic if larger feature sets are more or less informative than smaller feature sets. Our work performs a rigorous analysis of the potential limitations of Shapley feature attribution. We identify simple settings where the Shapley value is mathematically suboptimal by assigning larger attributions for less influential features. Motivated by this observation, we propose WeightedSHAP, which generalizes the Shapley value and learns which marginal contributions to focus directly from data. On several real-world datasets, we demonstrate that the influential features identified by WeightedSHAP are better able to recapitulate the model's predictions compared to the features identified by the Shapley value.
count=3
* Doubly Robust Counterfactual Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/e124f1547f7ac87e33d348b827d4291b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/e124f1547f7ac87e33d348b827d4291b-Paper-Conference.pdf)]
    * Title: Doubly Robust Counterfactual Classification
    * Year: `2022`
    * Authors: Kwangho Kim, Edward Kennedy, Jose Zubizarreta
    * Abstract: We study counterfactual classification as a new tool for decision-making under hypothetical (contrary to fact) scenarios. We propose a doubly-robust nonparametric estimator for a general counterfactual classifier, where we can incorporate flexible constraints by casting the classification problem as a nonlinear mathematical program involving counterfactuals. We go on to analyze the rates of convergence of the estimator and provide a closed-form expression for its asymptotic distribution. Our analysis shows that the proposed estimator is robust against nuisance model misspecification, and can attain fast $\sqrt{n}$ rates with tractable inference even when using nonparametric machine learning approaches. We study the empirical performance of our methods by simulation and apply them for recidivism risk prediction.
count=3
* Beyond Adult and COMPAS: Fair Multi-Class Prediction via Information Projection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/fd5013ea0c3f96931dec77174eaf9d80-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/fd5013ea0c3f96931dec77174eaf9d80-Paper-Conference.pdf)]
    * Title: Beyond Adult and COMPAS: Fair Multi-Class Prediction via Information Projection
    * Year: `2022`
    * Authors: Wael Alghamdi, Hsiang Hsu, Haewon Jeong, Hao Wang, Peter Michalak, Shahab Asoodeh, Flavio Calmon
    * Abstract: We consider the problem of producing fair probabilistic classifiers for multi-class classification tasks. We formulate this problem in terms of ``projecting'' a pre-trained (and potentially unfair) classifier onto the set of models that satisfy target group-fairness requirements. The new, projected model is given by post-processing the outputs of the pre-trained classifier by a multiplicative factor. We provide a parallelizable, iterative algorithm for computing the projected classifier and derive both sample complexity and convergence guarantees. Comprehensive numerical comparisons with state-of-the-art benchmarks demonstrate that our approach maintains competitive performance in terms of accuracy-fairness trade-off curves, while achieving favorable runtime on large datasets. We also evaluate our method at scale on an open dataset with multiple classes, multiple intersectional groups, and over 1M samples.
count=3
* The Adaptive Doubly Robust Estimator and a Paradox Concerning Logging Policy
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/09e7655fc1dc8fa7c9d6c4478313d5e6-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/09e7655fc1dc8fa7c9d6c4478313d5e6-Paper.pdf)]
    * Title: The Adaptive Doubly Robust Estimator and a Paradox Concerning Logging Policy
    * Year: `2021`
    * Authors: Masahiro Kato, Kenichiro McAlinn, Shota Yasui
    * Abstract: The doubly robust (DR) estimator, which consists of two nuisance parameters, the conditional mean outcome and the logging policy (the probability of choosing an action), is crucial in causal inference. This paper proposes a DR estimator for dependent samples obtained from adaptive experiments. To obtain an asymptotically normal semiparametric estimator from dependent samples without non-Donsker nuisance estimators, we propose adaptive-fitting as a variant of sample-splitting. We also report an empirical paradox that our proposed DR estimator tends to show better performances compared to other estimators utilizing the true logging policy. While a similar phenomenon is known for estimators with i.i.d. samples, traditional explanations based on asymptotic efficiency cannot elucidate our case with dependent samples. We confirm this hypothesis through simulation studies.
count=3
* Conditioning Sparse Variational Gaussian Processes for Online Decision-making
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/325eaeac5bef34937cfdc1bd73034d17-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/325eaeac5bef34937cfdc1bd73034d17-Paper.pdf)]
    * Title: Conditioning Sparse Variational Gaussian Processes for Online Decision-making
    * Year: `2021`
    * Authors: Wesley J. Maddox, Samuel Stanton, Andrew G. Wilson
    * Abstract: With a principled representation of uncertainty and closed form posterior updates, Gaussian processes (GPs) are a natural choice for online decision making. However, Gaussian processes typically require at least $\mathcal{O}(n^2)$ computations for $n$ training points, limiting their general applicability. Stochastic variational Gaussian processes (SVGPs) can provide scalable inference for a dataset of fixed size, but are difficult to efficiently condition on new data. We propose online variational conditioning (OVC), a procedure for efficiently conditioning SVGPs in an online setting that does not require re-training through the evidence lower bound with the addition of new data. OVC enables the pairing of SVGPs with advanced look-ahead acquisition functions for black-box optimization, even with non-Gaussian likelihoods. We show OVC provides compelling performance in a range of applications including active learning of malaria incidence, and reinforcement learning on MuJoCo simulated robotic control tasks.
count=3
* A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/43c656628a4a479e108ed86f7a28a010-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/43c656628a4a479e108ed86f7a28a010-Paper.pdf)]
    * Title: A Near-Optimal Algorithm for Debiasing Trained Machine Learning Models
    * Year: `2021`
    * Authors: Ibrahim M. Alabdulmohsin, Mario Lucic
    * Abstract: We present a scalable post-processing algorithm for debiasing trained models, including deep neural networks (DNNs), which we prove to be near-optimal by bounding its excess Bayes risk. We empirically validate its advantages on standard benchmark datasets across both classical algorithms as well as modern DNN architectures and demonstrate that it outperforms previous post-processing methods while performing on par with in-processing. In addition, we show that the proposed algorithm is particularly effective for models trained at scale where post-processing is a natural and practical choice.
count=3
* Finding Regions of Heterogeneity in Decision-Making via Expected Conditional Covariance
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/81930c54e08b6d26d9638dd2e4656dc1-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/81930c54e08b6d26d9638dd2e4656dc1-Paper.pdf)]
    * Title: Finding Regions of Heterogeneity in Decision-Making via Expected Conditional Covariance
    * Year: `2021`
    * Authors: Justin Lim, Christina X Ji, Michael Oberst, Saul Blecker, Leora Horwitz, David Sontag
    * Abstract: Individuals often make different decisions when faced with the same context, due to personal preferences and background. For instance, judges may vary in their leniency towards certain drug-related offenses, and doctors may vary in their preference for how to start treatment for certain types of patients. With these examples in mind, we present an algorithm for identifying types of contexts (e.g., types of cases or patients) with high inter-decision-maker disagreement. We formalize this as a causal inference problem, seeking a region where the assignment of decision-maker has a large causal effect on the decision. Our algorithm finds such a region by maximizing an empirical objective, and we give a generalization bound for its performance. In a semi-synthetic experiment, we show that our algorithm recovers the correct region of heterogeneity accurately compared to baselines. Finally, we apply our algorithm to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge.
count=3
* Risk-averse Heteroscedastic Bayesian Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/8f97d1d7e02158a83ceb2c14ff5372cd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/8f97d1d7e02158a83ceb2c14ff5372cd-Paper.pdf)]
    * Title: Risk-averse Heteroscedastic Bayesian Optimization
    * Year: `2021`
    * Authors: Anastasia Makarova, Ilnura Usmanova, Ilija Bogunovic, Andreas Krause
    * Abstract: Many black-box optimization tasks arising in high-stakes applications require risk-averse decisions. The standard Bayesian optimization (BO) paradigm, however, optimizes the expected value only. We generalize BO to trade mean and input-dependent variance of the objective, both of which we assume to be unknown a priori. In particular, we propose a novel risk-averse heteroscedastic Bayesian optimization algorithm (RAHBO) that aims to identify a solution with high return and low noise variance, while learning the noise distribution on the fly. To this end, we model both expectation and variance as (unknown) RKHS functions, and propose a novel risk-aware acquisition function. We bound the regret for our approach and provide a robust rule to report the final decision point for applications where only a single solution must be identified. We demonstrate the effectiveness of RAHBO on synthetic benchmark functions and hyperparameter tuning tasks.
count=3
* On Locality of Local Explanation Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/995665640dc319973d3173a74a03860c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/995665640dc319973d3173a74a03860c-Paper.pdf)]
    * Title: On Locality of Local Explanation Models
    * Year: `2021`
    * Authors: Sahra Ghalebikesabi, Lucile Ter-Minassian, Karla DiazOrdaz, Chris C Holmes
    * Abstract: Shapley values provide model agnostic feature attributions for model outcome at a particular instance by simulating feature absence under a global population distribution. The use of a global population can lead to potentially misleading results when local model behaviour is of interest. Hence we consider the formulation of neighbourhood reference distributions that improve the local interpretability of Shapley values. By doing so, we find that the Nadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as a self-normalised importance sampling estimator. Empirically, we observe that Neighbourhood Shapley values identify meaningful sparse feature relevance attributions that provide insight into local model behaviour, complimenting conventional Shapley analysis. They also increase on-manifold explainability and robustness to the construction of adversarial classifiers.
count=3
* SubTab: Subsetting Features of Tabular Data for Self-Supervised Representation Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/9c8661befae6dbcd08304dbf4dcaf0db-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/9c8661befae6dbcd08304dbf4dcaf0db-Paper.pdf)]
    * Title: SubTab: Subsetting Features of Tabular Data for Self-Supervised Representation Learning
    * Year: `2021`
    * Authors: Talip Ucar, Ehsan Hajiramezanali, Lindsay Edwards
    * Abstract: Self-supervised learning has been shown to be very effective in learning useful representations, and yet much of the success is achieved in data types such as images, audio, and text. The success is mainly enabled by taking advantage of spatial, temporal, or semantic structure in the data through augmentation. However, such structure may not exist in tabular datasets commonly used in fields such as healthcare, making it difficult to design an effective augmentation method, and hindering a similar progress in tabular data setting. In this paper, we introduce a new framework, Subsetting features of Tabular data (SubTab), that turns the task of learning from tabular data into a multi-view representation learning problem by dividing the input features to multiple subsets. We argue that reconstructing the data from the subset of its features rather than its corrupted version in an autoencoder setting can better capture its underlying latent representation. In this framework, the joint representation can be expressed as the aggregate of latent variables of the subsets at test time, which we refer to as collaborative inference. Our experiments show that the SubTab achieves the state of the art (SOTA) performance of 98.31% on MNIST in tabular setting, on par with CNN-based SOTA models, and surpasses existing baselines on three other real-world datasets by a significant margin.
count=3
* SurvITE: Learning Heterogeneous Treatment Effects from Time-to-Event Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/e0eacd983971634327ae1819ea8b6214-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/e0eacd983971634327ae1819ea8b6214-Paper.pdf)]
    * Title: SurvITE: Learning Heterogeneous Treatment Effects from Time-to-Event Data
    * Year: `2021`
    * Authors: Alicia Curth, Changhee Lee, Mihaela van der Schaar
    * Abstract: We study the problem of inferring heterogeneous treatment effects from time-to-event data. While both the related problems of (i) estimating treatment effects for binary or continuous outcomes and (ii) predicting survival outcomes have been well studied in the recent machine learning literature, their combination -- albeit of high practical relevance -- has received considerably less attention. With the ultimate goal of reliably estimating the effects of treatments on instantaneous risk and survival probabilities, we focus on the problem of learning (discrete-time) treatment-specific conditional hazard functions. We find that unique challenges arise in this context due to a variety of covariate shift issues that go beyond a mere combination of well-studied confounding and censoring biases. We theoretically analyse their effects by adapting recent generalization bounds from domain adaptation and treatment effect estimation to our setting and discuss implications for model design. We use the resulting insights to propose a novel deep learning method for treatment-specific hazard estimation based on balancing representations. We investigate performance across a range of experimental settings and empirically confirm that our method outperforms baselines by addressing covariate shifts from various sources.
count=3
* Estimating decision tree learnability with polylogarithmic sample complexity
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/439d8c975f26e5005dcdbf41b0d84161-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/439d8c975f26e5005dcdbf41b0d84161-Paper.pdf)]
    * Title: Estimating decision tree learnability with polylogarithmic sample complexity
    * Year: `2020`
    * Authors: Guy Blanc, Neha Gupta, Jane Lange, Li-Yang Tan
    * Abstract: We show that top-down decision tree learning heuristics (such as ID3, C4.5, and CART) are amenable to highly efficient {\sl learnability estimation}: for monotone target functions, the error of the decision tree hypothesis constructed by these heuristics can be estimated with {\sl polylogarithmically} many labeled examples, exponentially smaller than the number necessary to run these heuristics, and indeed, exponentially smaller than information-theoretic minimum required to learn a good decision tree. This adds to a small but growing list of fundamental learning algorithms that have been shown to be amenable to learnability estimation. En route to this result, we design and analyze sample-efficient {\sl minibatch} versions of top-down decision tree learning heuristics and show that they achieve the same provable guarantees as the full-batch versions. We further give ``active local'' versions of these heuristics: given a test point $x^\star$, we show how the label $T(x^\star)$ of the decision tree hypothesis $T$ can be computed with polylogarithmically many labeled examples, exponentially smaller than the number necessary to learn~$T$.
count=3
* Manifold structure in graph embeddings
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/8682cc30db9c025ecd3fee433f8ab54c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/8682cc30db9c025ecd3fee433f8ab54c-Paper.pdf)]
    * Title: Manifold structure in graph embeddings
    * Year: `2020`
    * Authors: Patrick Rubin-Delanchy
    * Abstract: Statistical analysis of a graph often starts with embedding, the process of representing its nodes as points in space. How to choose the embedding dimension is a nuanced decision in practice, but in theory a notion of true dimension is often available. In spectral embedding, this dimension may be very high. However, this paper shows that existing random graph models, including graphon and other latent position models, predict the data should live near a much lower-dimensional set. One may therefore circumvent the curse of dimensionality by employing methods which exploit hidden manifold structure.
count=3
* Assisted Learning: A Framework for Multi-Organization Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/a7b23e6eefbe6cf04b8e62a6f0915550-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/a7b23e6eefbe6cf04b8e62a6f0915550-Paper.pdf)]
    * Title: Assisted Learning: A Framework for Multi-Organization Learning
    * Year: `2020`
    * Authors: Xun Xian, Xinran Wang, Jie Ding, Reza Ghanadan
    * Abstract: In an increasing number of AI scenarios, collaborations among different organizations or agents (e.g., human and robots, mobile units) are often essential to accomplish an organization-specific mission. However, to avoid leaking useful and possibly proprietary information, organizations typically enforce stringent security constraints on sharing modeling algorithms and data, which significantly limits collaborations. In this work, we introduce the Assisted Learning framework for organizations to assist each other in supervised learning tasks without revealing any organization's algorithm, data, or even task. An organization seeks assistance by broadcasting task-specific but nonsensitive statistics and incorporating others' feedback in one or more iterations to eventually improve its predictive performance. Theoretical and experimental studies, including real-world medical benchmarks, show that Assisted Learning can often achieve near-oracle learning performance as if data and training processes were centralized.
count=3
* AdaTune: Adaptive Tensor Program Compilation Made Efficient
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/a9b7ba70783b617e9998dc4dd82eb3c5-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/a9b7ba70783b617e9998dc4dd82eb3c5-Paper.pdf)]
    * Title: AdaTune: Adaptive Tensor Program Compilation Made Efficient
    * Year: `2020`
    * Authors: Menghao Li, Minjia Zhang, Chi Wang, Mingqin Li
    * Abstract: Deep learning models are computationally intense, and implementations often have to be highly optimized by experts or hardware vendors to be usable in practice. The DL compiler, together with Learning to Compile have proven to be a powerful technique for optimizing tensor programs. However, a limitation of this approach is that it still suffers from unbearably long overall optimization time. In this paper, we present a new method, called AdaTune, that significantly reduces the optimization time of tensor programs for high-performance deep learning inference. In particular, we propose an adaptive evaluation method that statistically early terminates a costly hardware measurement without losing much accuracy. We further devise a surrogate model with uncertainty quantification that allows the optimization to adapt to hardware and model heterogeneity better. Finally, we introduce a contextual optimizer that provides adaptive control of the exploration and exploitation to improve the transformation space searching effectiveness. We evaluate and compare the levels of optimization obtained by a state-of-the-art DL compiler and AdaTune. The experiment results show that AdaTune obtains up to 115% higher GFLOPS than the baseline under the same optimization time budget. Furthermore, AdaTune provides 1.3--3.9X speedup in optimization time over the state-of-the-art to reach the same optimization quality for a range of models across different hardware architectures.
count=3
* Handling Missing Data with Graph Representation Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/dc36f18a9a0a776671d4879cae69b551-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/dc36f18a9a0a776671d4879cae69b551-Paper.pdf)]
    * Title: Handling Missing Data with Graph Representation Learning
    * Year: `2020`
    * Authors: Jiaxuan You, Xiaobai Ma, Yi Ding, Mykel J. Kochenderfer, Jure Leskovec
    * Abstract: Machine learning with missing data has been approached in many different ways, including feature imputation where missing feature values are estimated based on observed values and label prediction where downstream labels are learned directly from incomplete data. However, existing imputation models tend to have strong prior assumptions and cannot learn from downstream tasks, while models targeting label predictions often involve heuristics and can encounter scalability issues. Here we propose GRAPE, a framework for feature imputation as well as label prediction. GRAPE tackles the missing data problem using graph representation, where the observations and features are viewed as two types of nodes in a bipartite graph, and the observed feature values as edges. Under the GRAPE framework, the feature imputation is formulated as an edge-level prediction task and the label prediction as a node-level prediction task. These tasks are then solved with Graph Neural Networks. Experimental results on nine benchmark datasets show that GRAPE yields 20% lower mean absolute error for imputation tasks and 10% lower for label prediction tasks, compared with existing state-of-the-art methods.
count=3
* Reconsidering Generative Objectives For Counterfactual Reasoning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/f5cfbc876972bd0d031c8abc37344c28-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/f5cfbc876972bd0d031c8abc37344c28-Paper.pdf)]
    * Title: Reconsidering Generative Objectives For Counterfactual Reasoning
    * Year: `2020`
    * Authors: Danni Lu, Chenyang Tao, Junya Chen, Fan Li, Feng Guo, Lawrence Carin
    * Abstract: There has been recent interest in exploring generative goals for counterfactual reasoning, such as individualized treatment effect (ITE) estimation. However, existing solutions often fail to address issues that are unique to causal inference, such as covariate balancing and (infeasible) counterfactual validation. As a step towards more flexible, scalable and accurate ITE estimation, we present a novel generative Bayesian estimation framework that integrates representation learning, adversarial matching and causal estimation. By appealing to the Robinson decomposition, we derive a reformulated variational bound that explicitly targets the causal effect estimation rather than specific predictive goals. Our procedure acknowledges the uncertainties in representation and solves a Fenchel mini-max game to resolve the representation imbalance for better counterfactual generalization, justified by new theory. Further, the latent variable formulation employed enables robustness to unobservable latent confounders, extending the scope of its applicability. The utility of the proposed solution is demonstrated via an extensive set of tests against competing solutions, both under various simulation setups and to real-world datasets, with encouraging results reported.
count=3
* Gradient Information for Representation and Modeling
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/6d9c547cf146054a5a720606a7694467-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/6d9c547cf146054a5a720606a7694467-Paper.pdf)]
    * Title: Gradient Information for Representation and Modeling
    * Year: `2019`
    * Authors: Jie Ding, Robert Calderbank, Vahid Tarokh
    * Abstract: Motivated by Fisher divergence, in this paper we present a new set of information quantities which we refer to as gradient information. These measures serve as surrogates for classical information measures such as those based on logarithmic loss, Kullback-Leibler divergence, directed Shannon information, etc. in many data-processing scenarios of interest, and often provide significant computational advantage, improved stability and robustness. As an example, we apply these measures to the Chow-Liu tree algorithm, and demonstrate remarkable performance and significant computational reduction using both synthetic and real data.
count=3
* Selecting Optimal Decisions via Distributionally Robust Nearest-Neighbor Regression
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/8efb100a295c0c690931222ff4467bb8-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/8efb100a295c0c690931222ff4467bb8-Paper.pdf)]
    * Title: Selecting Optimal Decisions via Distributionally Robust Nearest-Neighbor Regression
    * Year: `2019`
    * Authors: Ruidi Chen, Ioannis Paschalidis
    * Abstract: This paper develops a prediction-based prescriptive model for optimal decision making that (i) predicts the outcome under each action using a robust nonlinear model, and (ii) adopts a randomized prescriptive policy determined by the predicted outcomes. The predictive model combines a new regularized regression technique, which was developed using Distributionally Robust Optimization (DRO) with an ambiguity set constructed from the Wasserstein metric, with the K-Nearest Neighbors (K-NN) regression, which helps to capture the nonlinearity embedded in the data. We show theoretical results that guarantee the out-of-sample performance of the predictive model, and prove the optimality of the randomized policy in terms of the expected true future outcome. We demonstrate the proposed methodology on a hypertension dataset, showing that our prescribed treatment leads to a larger reduction in the systolic blood pressure compared to a series of alternatives. A clinically meaningful threshold level used to activate the randomized policy is also derived under a sub-Gaussian assumption on the predicted outcome.
count=3
* Robustness Verification of Tree-based Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/cd9508fdaa5c1390e9cc329001cf1459-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/cd9508fdaa5c1390e9cc329001cf1459-Paper.pdf)]
    * Title: Robustness Verification of Tree-based Models
    * Year: `2019`
    * Authors: Hongge Chen, Huan Zhang, Si Si, Yang Li, Duane Boning, Cho-Jui Hsieh
    * Abstract: We study the robustness verification problem of tree based models, including random forest (RF) and gradient boosted decision tree (GBDT). Formal robustness verification of decision tree ensembles involves finding the exact minimal adversarial perturbation or a guaranteed lower bound of it. Existing approaches cast this verification problem into a mixed integer linear programming (MILP) problem, which finds the minimal adversarial distortion in exponential time so is impractical for large ensembles. Although this verification problem is NP-complete in general, we give a more precise complexity characterization. We show that there is a simple linear time algorithm for verifying a single tree, and for tree ensembles the verification problem can be cast as a max-clique problem on a multi-partite boxicity graph. For low dimensional problems when boxicity can be viewed as constant, this reformulation leads to a polynomial time algorithm. For general problems, by exploiting the boxicity of the graph, we devise an efficient verification algorithm that can give tight lower bounds on robustness of decision tree ensembles, and allows iterative improvement and any-time termination. On RF/GBDT models trained on a variety of datasets, we significantly outperform the lower bounds obtained by relaxing the MILP formulation into a linear program (LP), and are hundreds times faster than solving MILPs to get the exact minimal adversarial distortion. Our proposed method is capable of giving tight robustness verification bounds on large GBDTs with hundreds of deep trees.
count=3
* Multi-Layered Gradient Boosting Decision Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/39027dfad5138c9ca0c474d71db915c3-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/39027dfad5138c9ca0c474d71db915c3-Paper.pdf)]
    * Title: Multi-Layered Gradient Boosting Decision Trees
    * Year: `2018`
    * Authors: Ji Feng, Yang Yu, Zhi-Hua Zhou
    * Abstract: Multi-layered distributed representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are still the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical distributed representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive backpropagation nor differentiability. Experiments confirmed the effectiveness of the model in terms of performance and representation learning ability.
count=3
* Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/39e98420b5e98bfbdc8a619bef7b8f61-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/39e98420b5e98bfbdc8a619bef7b8f61-Paper.pdf)]
    * Title: Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions
    * Year: `2018`
    * Authors: Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, Joris M. Mooij
    * Abstract: An important goal common to domain adaptation and causal inference is to make accurate predictions when the distributions for the source (or training) domain(s) and target (or test) domain(s) differ. In many cases, these different distributions can be modeled as different contexts of a single underlying system, in which each distribution corresponds to a different perturbation of the system, or in causal terms, an intervention. We focus on a class of such causal domain adaptation problems, where data for one or more source domains are given, and the task is to predict the distribution of a certain target variable from measurements of other variables in one or more target domains. We propose an approach for solving these problems that exploits causal inference and does not rely on prior knowledge of the causal graph, the type of interventions or the intervention targets. We demonstrate our approach by evaluating a possible implementation on simulated and real world data.
count=3
* To Trust Or Not To Trust A Classifier
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/7180cffd6a8e829dacfc2a31b3f72ece-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/7180cffd6a8e829dacfc2a31b3f72ece-Paper.pdf)]
    * Title: To Trust Or Not To Trust A Classifier
    * Year: `2018`
    * Authors: Heinrich Jiang, Been Kim, Melody Guan, Maya Gupta
    * Abstract: Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the {\it trust score}, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.
count=3
* BRITS: Bidirectional Recurrent Imputation for Time Series
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/734e6bfcd358e25ac1db0a4241b95651-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/734e6bfcd358e25ac1db0a4241b95651-Paper.pdf)]
    * Title: BRITS: Bidirectional Recurrent Imputation for Time Series
    * Year: `2018`
    * Authors: Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, Yitan Li
    * Abstract: Time series are widely used as signals in many classification/regression tasks. It is ubiquitous that time series contains many missing values. Given multiple correlated time series data, how to fill in missing values and to predict their class labels? Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. In this paper, we propose BRITS, a novel method based on recurrent neural networks for missing value imputation in time series data. Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during the backpropagation. BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data. We evaluate our model on three real-world datasets, including an air quality dataset, a health-care data, and a localization data for human activity. Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression accuracies.
count=3
* REFUEL: Exploring Sparse Features in Deep Reinforcement Learning for Fast Disease Diagnosis
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/b5a1d925221b37e2e399f7b319038ba0-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/b5a1d925221b37e2e399f7b319038ba0-Paper.pdf)]
    * Title: REFUEL: Exploring Sparse Features in Deep Reinforcement Learning for Fast Disease Diagnosis
    * Year: `2018`
    * Authors: Yu-Shao Peng, Kai-Fu Tang, Hsuan-Tien Lin, Edward Chang
    * Abstract: This paper proposes REFUEL, a reinforcement learning method with two techniques: {\em reward shaping} and {\em feature rebuilding}, to improve the performance of online symptom checking for disease diagnosis. Reward shaping can guide the search of policy towards better directions. Feature rebuilding can guide the agent to learn correlations between features. Together, they can find symptom queries that can yield positive responses from a patient with high probability. Experimental results justify that the two techniques in REFUEL allows the symptom checker to identify the disease more rapidly and accurately.
count=3
* Label Distribution Learning Forests
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf)]
    * Title: Label Distribution Learning Forests
    * Year: `2017`
    * Authors: Wei Shen, KAI ZHAO, Yilu Guo, Alan L. Yuille
    * Abstract: Label distribution learning (LDL) is a general learning framework, which assigns to an instance a distribution over a set of labels rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning, e.g., to learn deep features in an end-to-end manner. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by a mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning. We define a distribution-based loss function for a forest, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on several LDL tasks and a computer vision application, showing significant improvements to the state-of-the-art LDL methods.
count=3
* Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf)]
    * Title: Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
    * Year: `2017`
    * Authors: Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell
    * Abstract: Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.
count=3
* A Communication-Efficient Parallel Algorithm for Decision Tree
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/10a5ab2db37feedfdeaab192ead4ac0e-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/10a5ab2db37feedfdeaab192ead4ac0e-Paper.pdf)]
    * Title: A Communication-Efficient Parallel Algorithm for Decision Tree
    * Year: `2016`
    * Authors: Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu
    * Abstract: Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called \emph{Parallel Voting Decision Tree (PV-Tree)}, to tackle this challenge. After partitioning the training data onto a number of (e.g., $M$) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-$k$ attributes are selected from each machine according to its local data. Then, the indices of these top attributes are aggregated by a server, and the globally top-$2k$ attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-$2k$ attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the tradeoff between accuracy and efficiency.
count=3
* Yggdrasil: An Optimized System for Training Deep Decision Trees at Scale
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/9fdb62f932adf55af2c0e09e55861964-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/9fdb62f932adf55af2c0e09e55861964-Paper.pdf)]
    * Title: Yggdrasil: An Optimized System for Training Deep Decision Trees at Scale
    * Year: `2016`
    * Authors: Firas Abuzaid, Joseph K. Bradley, Feynman T. Liang, Andrew Feng, Lee Yang, Matei Zaharia, Ameet S. Talwalkar
    * Abstract: Deep distributed decision trees and tree ensembles have grown in importance due to the need to model increasingly large datasets. However, PLANET, the standard distributed tree learning algorithm implemented in systems such as \xgboost and Spark MLlib, scales poorly as data dimensionality and tree depths grow. We present Yggdrasil, a new distributed tree learning method that outperforms existing methods by up to 24x. Unlike PLANET, Yggdrasil is based on vertical partitioning of the data (i.e., partitioning by feature), along with a set of optimized data structures to reduce the CPU and communication costs of training. Yggdrasil (1) trains directly on compressed data for compressible features and labels; (2) introduces efficient data structures for training on uncompressed data; and (3) minimizes communication between nodes by using sparse bitvectors. Moreover, while PLANET approximates split points through feature binning, Yggdrasil does not require binning, and we analytically characterize the impact of this approximation. We evaluate Yggdrasil against the MNIST 8M dataset and a high-dimensional dataset at Yahoo; for both, Yggdrasil is faster by up to an order of magnitude.
count=3
* Efficient Non-greedy Optimization of Decision Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/1579779b98ce9edb98dd85606f2c119d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/1579779b98ce9edb98dd85606f2c119d-Paper.pdf)]
    * Title: Efficient Non-greedy Optimization of Decision Trees
    * Year: `2015`
    * Authors: Mohammad Norouzi, Maxwell Collins, Matthew A. Johnson, David J. Fleet, Pushmeet Kohli
    * Abstract: Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. Computing the gradient of the proposed surrogate objective with respect to each training exemplar is O(d^2), where d is the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.
count=3
* Just-In-Time Learning for Fast and Flexible Inference
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/76dc611d6ebaafc66cc0879c71b5db5c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf)]
    * Title: Just-In-Time Learning for Fast and Flexible Inference
    * Year: `2014`
    * Authors: S. M. Ali Eslami, Daniel Tarlow, Pushmeet Kohli, John Winn
    * Abstract: Much of research in machine learning has centered around the search for inference algorithms that are both general-purpose and efficient. The problem is extremely challenging and general inference remains computationally expensive. We seek to address this problem by observing that in most specific applications of a model, we typically only need to perform a small subset of all possible inference computations. Motivated by this, we introduce just-in-time learning, a framework for fast and flexible inference that learns to speed up inference at run-time. Through a series of experiments, we show how this framework can allow us to combine the flexibility of sampling with the efficiency of deterministic message-passing.
count=3
* Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/9a4400501febb2a95e79248486a5f6d3-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/9a4400501febb2a95e79248486a5f6d3-Paper.pdf)]
    * Title: Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models
    * Year: `2014`
    * Authors: Yarin Gal, Mark van der Wilk, Carl Edward Rasmussen
    * Abstract: Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research. We introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algorithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting. We show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data.
count=2
* SleepVST: Sleep Staging from Near-Infrared Video Signals using Pre-Trained Transformers
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Carter_SleepVST_Sleep_Staging_from_Near-Infrared_Video_Signals_using_Pre-Trained_Transformers_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Carter_SleepVST_Sleep_Staging_from_Near-Infrared_Video_Signals_using_Pre-Trained_Transformers_CVPR_2024_paper.pdf)]
    * Title: SleepVST: Sleep Staging from Near-Infrared Video Signals using Pre-Trained Transformers
    * Year: `2024`
    * Authors: Jonathan F. Carter, João Jorge, Oliver Gibson, Lionel Tarassenko
    * Abstract: Advances in camera-based physiological monitoring have enabled the robust non-contact measurement of respiration and the cardiac pulse which are known to be indicative of the sleep stage. This has led to research into camera-based sleep monitoring as a promising alternative to "gold-standard" polysomnography which is cumbersome expensive to administer and hence unsuitable for longer-term clinical studies. In this paper we introduce SleepVST a transformer model which enables state-of-the-art performance in camera-based sleep stage classification (sleep staging). After pre-training on contact sensor data SleepVST outperforms existing methods for cardio-respiratory sleep staging on the SHHS and MESA datasets achieving total Cohen's kappa scores of 0.75 and 0.77 respectively. We then show that SleepVST can be successfully transferred to cardio-respiratory waveforms extracted from video enabling fully contact-free sleep staging. Using a video dataset of 50 nights we achieve a total accuracy of 78.8% and a Cohen's \kappa of 0.71 in four-class video-based sleep staging setting a new state-of-the-art in the domain.
count=2
* Towards Generalizable Tumor Synthesis
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Towards_Generalizable_Tumor_Synthesis_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Towards_Generalizable_Tumor_Synthesis_CVPR_2024_paper.pdf)]
    * Title: Towards Generalizable Tumor Synthesis
    * Year: `2024`
    * Authors: Qi Chen, Xiaoxi Chen, Haorui Song, Zhiwei Xiong, Alan Yuille, Chen Wei, Zongwei Zhou
    * Abstract: Tumor synthesis enables the creation of artificial tumors in medical images facilitating the training of AI models for tumor detection and segmentation. However success in tumor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and furthermore the resulting AI models being capable of detecting real tumors in images sourced from different domains (e.g. hospitals). This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical observation: early-stage tumors (< 2cm) tend to have similar imaging characteristics in computed tomography (CT) whether they originate in the liver pancreas or kidneys. We have ascertained that generative AI models e.g. Diffusion Models can create realistic tumors generalized to a range of organs even when trained on a limited number of tumor examples from only one organ. Moreover we have shown that AI models trained on these synthetic tumors can be generalized to detect and segment real tumors from CT volumes encompassing a broad spectrum of patient demographics imaging protocols and healthcare facilities.
count=2
* SD2Event:Self-supervised Learning of Dynamic Detectors and Contextual Descriptors for Event Cameras
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Gao_SD2EventSelf-supervised_Learning_of_Dynamic_Detectors_and_Contextual_Descriptors_for_Event_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_SD2EventSelf-supervised_Learning_of_Dynamic_Detectors_and_Contextual_Descriptors_for_Event_CVPR_2024_paper.pdf)]
    * Title: SD2Event:Self-supervised Learning of Dynamic Detectors and Contextual Descriptors for Event Cameras
    * Year: `2024`
    * Authors: Yuan Gao, Yuqing Zhu, Xinjun Li, Yimin Du, Tianzhu Zhang
    * Abstract: Event cameras offer many advantages over traditional frame-based cameras such as high dynamic range and low latency. Therefore event cameras are widely applied in diverse computer vision applications where event-based keypoint detection is a fundamental task. However achieving robust event-based keypoint detection remains challenging because the ground truth of event keypoints is difficult to obtain descriptors extracted by CNN usually lack discriminative ability in the presence of intense noise and fixed keypoint detectors are limited in detecting varied keypoint patterns. To address these challenges a novel event-based keypoint detection method is proposed by learning dynamic detectors and contextual descriptors in a self-supervised manner (SD2Event) including a contextual feature descriptor learning (CFDL) module and a dynamic keypoint detector learning (DKDL) module. The proposed SD2Event enjoys several merits. First the proposed CFDL module can model long-range contexts efficiently and effectively. Second the DKDL module generates dynamic keypoint detectors which can detect keypoints with diverse patterns across various event streams. Third the proposed self-supervised signals can guide the model's adaptation to event data. Extensive experimental results on three challenging benchmarks show that our proposed method significantly outperforms stateof-the-art event-based keypoint detection methods.
count=2
* Distilled Datamodel with Reverse Gradient Matching
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Ye_Distilled_Datamodel_with_Reverse_Gradient_Matching_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Ye_Distilled_Datamodel_with_Reverse_Gradient_Matching_CVPR_2024_paper.pdf)]
    * Title: Distilled Datamodel with Reverse Gradient Matching
    * Year: `2024`
    * Authors: Jingwen Ye, Ruonan Yu, Songhua Liu, Xinchao Wang
    * Abstract: The proliferation of large-scale AI models trained on extensive datasets has revolutionized machine learning. With these models taking on increasingly central roles in various applications the need to understand their behavior and enhance interpretability has become paramount. To investigate the impact of changes in training data on a pre-trained model a common approach is leave-one-out retraining. This entails systematically altering the training dataset by removing specific samples to observe resulting changes within the model. However retraining the model for each altered dataset presents a significant computational challenge given the need to perform this operation for every dataset variation. In this paper we introduce an efficient framework for assessing data impact comprising offline training and online evaluation stages. During the offline training phase we approximate the influence of training data on the target model through a distilled synset formulated as a reversed gradient matching problem. For online evaluation we expedite the leave-one-out process using the synset which is then utilized to compute the attribution matrix based on the evaluation objective. Experimental evaluations including training data attribution and assessments of data quality demonstrate that our proposed method achieves comparable model behavior evaluation while significantly speeding up the process compared to the direct retraining method.
count=2
* Continual Forgetting for Pre-trained Vision Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Continual_Forgetting_for_Pre-trained_Vision_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Continual_Forgetting_for_Pre-trained_Vision_Models_CVPR_2024_paper.pdf)]
    * Title: Continual Forgetting for Pre-trained Vision Models
    * Year: `2024`
    * Authors: Hongbo Zhao, Bolin Ni, Junsong Fan, Yuxi Wang, Yuntao Chen, Gaofeng Meng, Zhaoxiang Zhang
    * Abstract: For privacy and security concerns the need to erase unwanted information from pre-trained vision models is becoming evident nowadays. In real-world scenarios erasure requests originate at any time from both users and model owners. These requests usually form a sequence. Therefore under such a setting selective information is expected to be continuously removed from a pre-trained model while maintaining the rest. We define this problem as continual forgetting and identify two key challenges. (i) For unwanted knowledge efficient and effective deleting is crucial. (ii) For remaining knowledge the impact brought by the forgetting procedure should be minimal. To address them we propose Group Sparse LoRA (GS-LoRA). Specifically towards (i) we use LoRA modules to fine-tune the FFN layers in Transformer blocks for each forgetting task independently and towards (ii) a simple group sparse regularization is adopted enabling automatic selection of specific LoRA groups and zeroing out the others. GS-LoRA is effective parameter-efficient data-efficient and easy to implement. We conduct extensive experiments on face recognition object detection and image classification and demonstrate that GS-LoRA manages to forget specific classes with minimal impact on other classes. Codes will be released on https://github.com/bjzhb666/GS-LoRA.
count=2
* Accelerated Coordinate Encoding: Learning to Relocalize in Minutes Using RGB and Poses
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Brachmann_Accelerated_Coordinate_Encoding_Learning_to_Relocalize_in_Minutes_Using_RGB_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Brachmann_Accelerated_Coordinate_Encoding_Learning_to_Relocalize_in_Minutes_Using_RGB_CVPR_2023_paper.pdf)]
    * Title: Accelerated Coordinate Encoding: Learning to Relocalize in Minutes Using RGB and Poses
    * Year: `2023`
    * Authors: Eric Brachmann, Tommaso Cavallari, Victor Adrian Prisacariu
    * Abstract: Learning-based visual relocalizers exhibit leading pose accuracy, but require hours or days of training. Since training needs to happen on each new scene again, long training times make learning-based relocalization impractical for most applications, despite its promise of high accuracy. In this paper we show how such a system can actually achieve the same accuracy in less than 5 minutes. We start from the obvious: a relocalization network can be split in a scene-agnostic feature backbone, and a scene-specific prediction head. Less obvious: using an MLP prediction head allows us to optimize across thousands of view points simultaneously in each single training iteration. This leads to stable and extremely fast convergence. Furthermore, we substitute effective but slow end-to-end training using a robust pose solver with a curriculum over a reprojection loss. Our approach does not require privileged knowledge, such a depth maps or a 3D model, for speedy training. Overall, our approach is up to 300x faster in mapping than state-of-the-art scene coordinate regression, while keeping accuracy on par. Code is available: https://nianticlabs.github.io/ace
count=2
* DAA: A Delta Age AdaIN Operation for Age Estimation via Binary Code Transformer
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Chen_DAA_A_Delta_Age_AdaIN_Operation_for_Age_Estimation_via_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_DAA_A_Delta_Age_AdaIN_Operation_for_Age_Estimation_via_CVPR_2023_paper.pdf)]
    * Title: DAA: A Delta Age AdaIN Operation for Age Estimation via Binary Code Transformer
    * Year: `2023`
    * Authors: Ping Chen, Xingpeng Zhang, Ye Li, Ju Tao, Bin Xiao, Bing Wang, Zongjie Jiang
    * Abstract: Naked eye recognition of age is usually based on comparison with the age of others. However, this idea is ignored by computer tasks because it is difficult to obtain representative contrast images of each age. Inspired by the transfer learning, we designed the Delta Age AdaIN (DAA) operation to obtain the feature difference with each age, which obtains the style map of each age through the learned values representing the mean and standard deviation. We let the input of transfer learning as the binary code of age natural number to obtain continuous age feature information. The learned two groups of values in Binary code mapping are corresponding to the mean and standard deviation of the comparison ages. In summary, our method consists of four parts: FaceEncoder, DAA operation, Binary code mapping, and AgeDecoder modules. After getting the delta age via AgeDecoder, we take the average value of all comparison ages and delta ages as the predicted age. Compared with state-of-the-art methods, our method achieves better performance with fewer parameters on multiple facial age datasets. Code is available at https://github.com/redcping/Delta_Age_AdaIN
count=2
* A Data-Based Perspective on Transfer Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Jain_A_Data-Based_Perspective_on_Transfer_Learning_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_A_Data-Based_Perspective_on_Transfer_Learning_CVPR_2023_paper.pdf)]
    * Title: A Data-Based Perspective on Transfer Learning
    * Year: `2023`
    * Authors: Saachi Jain, Hadi Salman, Alaa Khaddaj, Eric Wong, Sung Min Park, Aleksander Mądry
    * Abstract: It is commonly believed that more pre-training data leads to better transfer learning performance. However, recent evidence suggests that removing data from the source dataset can actually help too. In this work, we present a framework for probing the impact of the source dataset's composition on transfer learning performance. Our framework facilitates new capabilities such as identifying transfer learning brittleness and detecting pathologies such as data-leakage and the presence of misleading examples in the source dataset. In particular, we demonstrate that removing detrimental datapoints identified by our framework improves transfer performance from ImageNet on a variety of transfer tasks.
count=2
* Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning With Multimodal Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2023_paper.pdf)]
    * Title: Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning With Multimodal Models
    * Year: `2023`
    * Authors: Zhiqiu Lin, Samuel Yu, Zhiyi Kuang, Deepak Pathak, Deva Ramanan
    * Abstract: The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better visual dog classifier by reading about dogs and listening to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for vision-language adaptation. Furthermore, we show that our approach can benefit existing methods such as prefix tuning and classifier ensembling. Finally, to explore other modalities beyond vision and language, we construct the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classification. We hope our success can inspire future works to embrace cross-modality for even broader domains and tasks.
count=2
* ViTs for SITS: Vision Transformers for Satellite Image Time Series
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Tarasiou_ViTs_for_SITS_Vision_Transformers_for_Satellite_Image_Time_Series_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Tarasiou_ViTs_for_SITS_Vision_Transformers_for_Satellite_Image_Time_Series_CVPR_2023_paper.pdf)]
    * Title: ViTs for SITS: Vision Transformers for Satellite Image Time Series
    * Year: `2023`
    * Authors: Michail Tarasiou, Erik Chavez, Stefanos Zafeiriou
    * Abstract: In this paper we introduce the Temporo-Spatial Vision Transformer (TSViT), a fully-attentional model for general Satellite Image Time Series (SITS) processing based on the Vision Transformer (ViT). TSViT splits a SITS record into non-overlapping patches in space and time which are tokenized and subsequently processed by a factorized temporo-spatial encoder. We argue, that in contrast to natural images, a temporal-then-spatial factorization is more intuitive for SITS processing and present experimental evidence for this claim. Additionally, we enhance the model's discriminative power by introducing two novel mechanisms for acquisition-time-specific temporal positional encodings and multiple learnable class tokens. The effect of all novel design choices is evaluated through an extensive ablation study. Our proposed architecture achieves state-of-the-art performance, surpassing previous approaches by a significant margin in three publicly available SITS semantic segmentation and classification datasets. All model, training and evaluation codes can be found at https://github.com/michaeltrs/DeepSatModels.
count=2
* A Loopback Network for Explainable Microvascular Invasion Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_A_Loopback_Network_for_Explainable_Microvascular_Invasion_Classification_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_A_Loopback_Network_for_Explainable_Microvascular_Invasion_Classification_CVPR_2023_paper.pdf)]
    * Title: A Loopback Network for Explainable Microvascular Invasion Classification
    * Year: `2023`
    * Authors: Shengxuming Zhang, Tianqi Shi, Yang Jiang, Xiuming Zhang, Jie Lei, Zunlei Feng, Mingli Song
    * Abstract: Microvascular invasion (MVI) is a critical factor for prognosis evaluation and cancer treatment. The current diagnosis of MVI relies on pathologists to manually find out cancerous cells from hundreds of blood vessels, which is time-consuming, tedious, and subjective. Recently, deep learning has achieved promising results in medical image analysis tasks. However, the unexplainability of black box models and the requirement of massive annotated samples limit the clinical application of deep learning based diagnostic methods. In this paper, aiming to develop an accurate, objective, and explainable diagnosis tool for MVI, we propose a Loopback Network (LoopNet) for classifying MVI efficiently. With the image-level category annotations of the collected Pathologic Vessel Image Dataset (PVID), LoopNet is devised to be composed binary classification branch and cell locating branch. The latter is devised to locate the area of cancerous cells, regular non-cancerous cells, and background. For healthy samples, the pseudo masks of cells supervise the cell locating branch to distinguish the area of regular non-cancerous cells and background. For each MVI sample, the cell locating branch predicts the mask of cancerous cells. Then the masked cancerous and non-cancerous areas of the same sample are inputted back to the binary classification branch separately. The loopback between two branches enables the category label to supervise the cell locating branch to learn the locating ability for cancerous areas. Experiment results show that the proposed LoopNet achieves 97.5% accuracy on MVI classification. Surprisingly, the proposed loopback mechanism not only enables LoopNet to predict the cancerous area but also facilitates the classification backbone to achieve better classification performance.
count=2
* Learning To Detect Scene Landmarks for Camera Localization
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Do_Learning_To_Detect_Scene_Landmarks_for_Camera_Localization_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Do_Learning_To_Detect_Scene_Landmarks_for_Camera_Localization_CVPR_2022_paper.pdf)]
    * Title: Learning To Detect Scene Landmarks for Camera Localization
    * Year: `2022`
    * Authors: Tien Do, Ondrej Miksik, Joseph DeGol, Hyun Soo Park, Sudipta N. Sinha
    * Abstract: Modern camera localization methods that use image retrieval, feature matching, and 3D structure-based pose estimation require long-term storage of numerous scene images or a vast amount of image features. This can make them unsuitable for resource constrained VR/AR devices and also raises serious privacy concerns. We present a new learned camera localization technique that eliminates the need to store features or a detailed 3D point cloud. Our key idea is to implicitly encode the appearance of a sparse yet salient set of 3D scene points into a convolutional neural network (CNN) that can detect these scene points in query images whenever they are visible. We refer to these points as scene landmarks. We also show that a CNN can be trained to regress bearing vectors for such landmarks even when they are not within the camera's field-of-view. We demonstrate that the predicted landmarks yield accurate pose estimates and that our method outperforms DSAC*, the state-of-the-art in learned localization. Furthermore, extending HLoc (an accurate method) by combining its correspondences with our predictions, boosts its accuracy even further.
count=2
* Localized Adversarial Domain Generalization
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Localized_Adversarial_Domain_Generalization_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Localized_Adversarial_Domain_Generalization_CVPR_2022_paper.pdf)]
    * Title: Localized Adversarial Domain Generalization
    * Year: `2022`
    * Authors: Wei Zhu, Le Lu, Jing Xiao, Mei Han, Jiebo Luo, Adam P. Harrison
    * Abstract: Deep learning methods can struggle to handle domain shifts not seen in training data, which can cause them to not generalize well to unseen domains. This has led to research attention on domain generalization (DG), which aims to the model's generalization ability to out-of-distribution. Adversarial domain generalization is a popular approach to DG, but conventional approaches (1) struggle to sufficiently align features so that local neighborhoods are mixed across domains; and (2) can suffer from feature space over collapse which can threaten generalization performance. To address these limitations, we propose localized adversarial domain generalization with space compactness maintenance (LADG) which constitutes two major contributions. First, we propose an adversarial localized classifier as the domain discriminator, along with a principled primary branch. This constructs a min-max game whereby the aim of the featurizer is to produce locally mixed domains. Second, we propose to use a coding-rate loss to alleviate feature space over collapse. We conduct comprehensive experiments on the Wilds DG benchmark to validate our approach, where LADG outperforms leading competitors on most datasets.
count=2
* Large-Scale Localization Datasets in Crowded Indoor Spaces
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Lee_Large-Scale_Localization_Datasets_in_Crowded_Indoor_Spaces_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Large-Scale_Localization_Datasets_in_Crowded_Indoor_Spaces_CVPR_2021_paper.pdf)]
    * Title: Large-Scale Localization Datasets in Crowded Indoor Spaces
    * Year: `2021`
    * Authors: Donghwan Lee, Soohyun Ryu, Suyong Yeon, Yonghan Lee, Deokhwa Kim, Cheolho Han, Yohann Cabon, Philippe Weinzaepfel, Nicolas Guerin, Gabriela Csurka, Martin Humenberger
    * Abstract: Estimating the precise location of a camera using visual localization enables interesting applications such as augmented reality or robot navigation. This is particularly useful in indoor environments where other localization technologies, such as GNSS, fail. Indoor spaces impose interesting challenges on visual localization algorithms: occlusions due to people, textureless surfaces, large viewpoint changes, low light, repetitive textures, etc. Existing indoor datasets are either comparably small or do only cover a subset of the mentioned challenges. In this paper, we introduce 5 new indoor datasets for visual localization in challenging real-world environments. They were captured in a large shopping mall and a large metro station in Seoul, South Korea, using a dedicated mapping platform consisting of 10 cameras and 2 laser scanners. In order to obtain accurate ground truth camera poses, we developed a robust LiDAR SLAM which provides initial poses that are then refined using a novel structure-from-motion based optimization. We present a benchmark of modern visual localization algorithms on these challenging datasets showing superior performance of structure-based methods using robust image features. The datasets are available at: https://naverlabs.com/datasets
count=2
* AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_AttentiveNAS_Improving_Neural_Architecture_Search_via_Attentive_Sampling_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_AttentiveNAS_Improving_Neural_Architecture_Search_via_Attentive_Sampling_CVPR_2021_paper.pdf)]
    * Title: AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling
    * Year: `2021`
    * Authors: Dilin Wang, Meng Li, Chengyue Gong, Vikas Chandra
    * Abstract: Neural architecture search (NAS) has shown great promise in designing state-of-the-art (SOTA) models that are both accurate and efficient. Recently, two-stage NAS, e.g. BigNAS, decouples the model training and searching process and achieves remarkable search efficiency and accuracy. Two-stage NAS requires sampling from the search space during training, which directly impacts the accuracy of the final searched models. While uniform sampling has been widely used for its simplicity, it is agnostic of the model performance Pareto front, which is the main focus in the search process, and thus, misses opportunities to further improve the model accuracy. In this work, we propose AttentiveNAS that focuses on improving the sampling strategy to achieve better performance Pareto. We also propose algorithms to efficiently and effectively identify the networks on the Pareto during training. Without extra re-training or post-processing, we can simultaneously obtain a large number of networks across a wide range of FLOPs. Our discovered model family, AttentiveNAS models, achieves top-1 accuracy from 77.3% to 80.7% on ImageNet, and outperforms SOTA models, including BigNAS, Once-for-All networks and FBNetV3. We also achieve ImageNet accuracy of 80.1% with only 491 MFLOPs. Our training code and pretrained models are available at https://github. com/facebookresearch/AttentiveNAS.
count=2
* Boosting Ensemble Accuracy by Revisiting Ensemble Diversity Metrics
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Wu_Boosting_Ensemble_Accuracy_by_Revisiting_Ensemble_Diversity_Metrics_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Boosting_Ensemble_Accuracy_by_Revisiting_Ensemble_Diversity_Metrics_CVPR_2021_paper.pdf)]
    * Title: Boosting Ensemble Accuracy by Revisiting Ensemble Diversity Metrics
    * Year: `2021`
    * Authors: Yanzhao Wu, Ling Liu, Zhongwei Xie, Ka-Ho Chow, Wenqi Wei
    * Abstract: Neural network ensembles are gaining popularity by harnessing the complementary wisdom of multiple base models. Ensemble teams with high diversity promote high failure independence, which is effective for boosting the overall ensemble accuracy. This paper provides an in-depth study on how to design and compute ensemble diversity, which can capture the complementary decision capacity of ensemble member models. We make three original contributions. First, we revisit the ensemble diversity metrics in the literature and analyze the inherent problems of poor correlation between ensemble diversity and ensemble accuracy, which leads to the low quality ensemble selection using such diversity metrics. Second, instead of computing diversity scores for ensemble teams of different sizes using the same criteria, we introduce focal model based ensemble diversity metrics, coined as FQ-diversity metrics. Our new metrics significantly improve the intrinsic correlation between high ensemble diversity and high ensemble accuracy. Third, we introduce a diversity fusion method, coined as the EQ-diversity metric, by integrating the top three most representative FQ-diversity metrics. Comprehensive experiments on two benchmark datasets (CIFAR-10 and ImageNet) show that our FQ and EQ diversity metrics are effective for selecting high diversity ensemble teams to boost overall ensemble accuracy.
count=2
* A Dual Iterative Refinement Method for Non-Rigid Shape Matching
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Xiang_A_Dual_Iterative_Refinement_Method_for_Non-Rigid_Shape_Matching_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Xiang_A_Dual_Iterative_Refinement_Method_for_Non-Rigid_Shape_Matching_CVPR_2021_paper.pdf)]
    * Title: A Dual Iterative Refinement Method for Non-Rigid Shape Matching
    * Year: `2021`
    * Authors: Rui Xiang, Rongjie Lai, Hongkai Zhao
    * Abstract: In this work, a robust and efficient dual iterative refinement (DIR) method is proposed for dense correspondence between two nearly isometric shapes. The key idea is to use dual information, such as spatial and spectral, or local and global features, in a complementary and effective way, and extract more accurate information from current iteration to use for the next iteration. In each DIR iteration, starting from current correspondence, a zoom-in process at each point is used to select well matched anchor pairs by a local mapping distortion criterion. These selected anchor pairs are then used to align spectral features (or other appropriate global features) whose dimension adaptively matches the capacity of the selected anchor pairs. Thanks to the effective combination of complementary information in a data-adaptive way, DIR is not only efficient but also robust to render accurate results within a few iterations. By choosing appropriate dual features, DIR has the flexibility to handle patch and partial matching as well. Extensive experiments on various data sets demonstrate the superiority of DIR over other state-of-the-art methods in terms of both accuracy and efficiency.
count=2
* Prototype Augmentation and Self-Supervision for Incremental Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Prototype_Augmentation_and_Self-Supervision_for_Incremental_Learning_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Prototype_Augmentation_and_Self-Supervision_for_Incremental_Learning_CVPR_2021_paper.pdf)]
    * Title: Prototype Augmentation and Self-Supervision for Incremental Learning
    * Year: `2021`
    * Authors: Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, Cheng-Lin Liu
    * Abstract: Despite the impressive performance in many individual tasks, deep neural networks suffer from catastrophic forgetting when learning new tasks incrementally. Recently, various incremental learning methods have been proposed, and some approaches achieved acceptable performance relying on stored data or complex generative models. However, storing data from previous tasks is limited by memory or privacy issues, and generative models are usually unstable and inefficient in training. In this paper, we propose a simple non-exemplar based method named PASS, to address the catastrophic forgetting problem in incremental learning. On the one hand, we propose to memorize one class-representative prototype for each old class and adopt prototype augmentation (protoAug) in the deep feature space to maintain the decision boundary of previous tasks. On the other hand, we employ self-supervised learning (SSL) to learn more generalizable and transferable features for other tasks, which demonstrates the effectiveness of SSL in incremental learning. Experimental results on benchmark datasets show that our approach significantly outperforms non-exemplar based methods, and achieves comparable performance compared to exemplar based approaches.
count=2
* Weakly-Supervised Domain Adaptation via GAN and Mesh Model for Estimating 3D Hand Poses Interacting Objects
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Baek_Weakly-Supervised_Domain_Adaptation_via_GAN_and_Mesh_Model_for_Estimating_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Baek_Weakly-Supervised_Domain_Adaptation_via_GAN_and_Mesh_Model_for_Estimating_CVPR_2020_paper.pdf)]
    * Title: Weakly-Supervised Domain Adaptation via GAN and Mesh Model for Estimating 3D Hand Poses Interacting Objects
    * Year: `2020`
    * Authors: Seungryul Baek,  Kwang In Kim,  Tae-Kyun Kim
    * Abstract: Despite recent successes in hand pose estimation, there yet remain challenges on RGB-based 3D hand pose estimation (HPE) under hand-object interaction (HOI) scenarios where severe occlusions and cluttered backgrounds exhibit. Recent RGB HOI benchmarks have been collected either in real or synthetic domain, however, the size of datasets is far from enough to deal with diverse objects combined with hand poses, and 3D pose annotations of real samples are lacking, especially for occluded cases. In this work, we propose a novel end-to-end trainable pipeline that adapts the hand-object domain to the single hand-only domain, while learning for HPE. The domain adaption occurs in image space via 2D pixel-level guidance by Generative Adversarial Network (GAN) and 3D mesh guidance by mesh renderer (MR). Via the domain adaption in image space, not only 3D HPE accuracy is improved, but also HOI input images are translated to segmented and de-occluded hand-only images. The proposed method takes advantages of both the guidances: GAN accurately aligns hands, while MR effectively fills in occluded pixels. The experiments using Dexter-Object, Ego-Dexter and HO3D datasets show that our method significantly outperforms state-of-the-arts trained by hand-only data and is comparable to those supervised by HOI data. Note our method is trained primarily by hand-only images with pose labels, and HOI images without pose labels.
count=2
* MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Bernard_MINA_Convex_Mixed-Integer_Programming_for_Non-Rigid_Shape_Alignment_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Bernard_MINA_Convex_Mixed-Integer_Programming_for_Non-Rigid_Shape_Alignment_CVPR_2020_paper.pdf)]
    * Title: MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment
    * Year: `2020`
    * Authors: Florian Bernard,  Zeeshan Khan Suri,  Christian Theobalt
    * Abstract: We present a convex mixed-integer programming formulation for non-rigid shape matching. To this end, we propose a novel shape deformation model based on an efficient low-dimensional discrete model, so that finding a globally optimal solution is tractable in (most) practical cases. Our approach combines several favourable properties, namely it is independent of the initialisation, it is much more efficient to solve to global optimality compared to analogous quadratic assignment problem formulations, and it is highly flexible in terms of the variants of matching problems it can handle. Experimentally we demonstrate that our approach outperforms existing methods for sparse shape matching, that it can be used for initialising dense shape matching methods, and we showcase its flexibility on several examples.
count=2
* Appearance Shock Grammar for Fast Medial Axis Extraction From Real Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Camaro_Appearance_Shock_Grammar_for_Fast_Medial_Axis_Extraction_From_Real_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Camaro_Appearance_Shock_Grammar_for_Fast_Medial_Axis_Extraction_From_Real_CVPR_2020_paper.pdf)]
    * Title: Appearance Shock Grammar for Fast Medial Axis Extraction From Real Images
    * Year: `2020`
    * Authors: Charles-Olivier Dufresne Camaro,  Morteza Rezanejad,  Stavros Tsogkas,  Kaleem Siddiqi,  Sven Dickinson
    * Abstract: We combine ideas from shock graph theory with more recent appearance-based methods for medial axis extraction from complex natural scenes, improving upon the present best unsupervised method, in terms of efficiency and performance. We make the following specific contributions: i) we extend the shock graph representation to the domain of real images, by generalizing the shock type definitions using local, appearance-based criteria; ii) we then use the rules of a Shock Grammar to guide our search for medial points, drastically reducing run time when compared to other methods, which exhaustively consider all points in the input image; iii) we remove the need for typical post-processing steps including thinning, non-maximum suppression, and grouping, by adhering to the Shock Grammar rules while deriving the medial axis solution; iv) finally, we raise some fundamental concerns with the evaluation scheme used in previous work and propose a more appropriate alternative for assessing the performance of medial axis extraction from scenes. Our experiments on the BMAX500 and SK-LARGE datasets demonstrate the effectiveness of our approach. We outperform the present state-of-the-art, excelling particularly in the high-precision regime, while running an order of magnitude faster and requiring no post-processing.
count=2
* Learning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Learning_Canonical_Shape_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Learning_Canonical_Shape_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2020_paper.pdf)]
    * Title: Learning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation
    * Year: `2020`
    * Authors: Dengsheng Chen,  Jun Li,  Zheng Wang,  Kai Xu
    * Abstract: We present a novel approach to category-level 6D object pose and size estimation. To tackle intra-class shape variations, we learn canonical shape space (CASS), a unified representation for a large variety of instances of a certain object category. In particular, CASS is modeled as the latent space of a deep generative model of canonical 3D shapes with normalized pose. We train a variational auto-encoder (VAE) for generating 3D point clouds in the canonical space from an RGBD image. The VAE is trained in a cross-category fashion, exploiting the publicly available large 3D shape repositories. Since the 3D point cloud is generated in normalized pose (with actual size), the encoder of the VAE learns view-factorized RGBD embedding. It maps an RGBD image in arbitrary view into a poseindependent 3D shape representation. Object pose is then estimated via contrasting it with a pose-dependent feature of the input RGBD extracted with a separate deep neural networks. We integrate the learning of CASS and pose and size estimation into an end-to-end trainable network, achieving the state-of-the-art performance.
count=2
* Hierarchical Scene Coordinate Classification and Regression for Visual Localization
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Hierarchical_Scene_Coordinate_Classification_and_Regression_for_Visual_Localization_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Hierarchical_Scene_Coordinate_Classification_and_Regression_for_Visual_Localization_CVPR_2020_paper.pdf)]
    * Title: Hierarchical Scene Coordinate Classification and Regression for Visual Localization
    * Year: `2020`
    * Authors: Xiaotian Li,  Shuzhe Wang,  Yi Zhao,  Jakob Verbeek,  Juho Kannala
    * Abstract: Visual localization is critical to many applications in computer vision and robotics. To address single-image RGB localization, state-of-the-art feature-based methods match local descriptors between a query image and a pre-built 3D model. Recently, deep neural networks have been exploited to regress the mapping between raw pixels and 3D coordinates in the scene, and thus the matching is implicitly performed by the forward pass through the network. However, in a large and ambiguous environment, learning such a regression task directly can be difficult for a single network. In this work, we present a new hierarchical scene coordinate network to predict pixel scene coordinates in a coarse-to-fine manner from a single RGB image. The network consists of a series of output layers, each of them conditioned on the previous ones. The final output layer predicts the 3D coordinates and the others produce progressively finer discrete location labels. The proposed method outperforms the baseline regression-only network and allows us to train compact models which scale robustly to large environments. It sets a new state-of-the-art for single-image RGB localization performance on the 7-Scenes, 12-Scenes, Cambridge Landmarks datasets, and three combined scenes. Moreover, for large-scale outdoor localization on the Aachen Day-Night dataset, we present a hybrid approach which outperforms existing scene coordinate regression methods, and reduces significantly the performance gap w.r.t. explicit feature matching methods.
count=2
* KFNet: Learning Temporal Camera Relocalization Using Kalman Filtering
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_KFNet_Learning_Temporal_Camera_Relocalization_Using_Kalman_Filtering_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_KFNet_Learning_Temporal_Camera_Relocalization_Using_Kalman_Filtering_CVPR_2020_paper.pdf)]
    * Title: KFNet: Learning Temporal Camera Relocalization Using Kalman Filtering
    * Year: `2020`
    * Authors: Lei Zhou,  Zixin Luo,  Tianwei Shen,  Jiahui Zhang,  Mingmin Zhen,  Yao Yao,  Tian Fang,  Long Quan
    * Abstract: Temporal camera relocalization estimates the pose with respect to each video frame in sequence, as opposed to one-shot relocalization which focuses on a still image. Even though the time dependency has been taken into account, current temporal relocalization methods still generally underperform the state-of-the-art one-shot approaches in terms of accuracy. In this work, we improve the temporal relocalization method by using a network architecture that incorporates Kalman filtering (KFNet) for online camera relocalization. In particular, KFNet extends the scene coordinate regression problem to the time domain in order to recursively establish 2D and 3D correspondences for the pose determination. The network architecture design and the loss formulation are based on Kalman filtering in the context of Bayesian learning. Extensive experiments on multiple relocalization benchmarks demonstrate the high accuracy of KFNet at the top of both one-shot and temporal relocalization approaches.
count=2
* Exploiting Edge Features for Graph Neural Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Gong_Exploiting_Edge_Features_for_Graph_Neural_Networks_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Gong_Exploiting_Edge_Features_for_Graph_Neural_Networks_CVPR_2019_paper.pdf)]
    * Title: Exploiting Edge Features for Graph Neural Networks
    * Year: `2019`
    * Authors: Liyu Gong,  Qiang Cheng
    * Abstract: Edge features contain important information about graphs. However, current state-of-the-art neural network models designed for graph learning, e.g., graph convolutional networks (GCN) and graph attention networks (GAT), inadequately utilize edge features, especially multi-dimensional edge features. In this paper, we build a new framework for a family of new graph neural network models that can more sufficiently exploit edge features, including those of undirected or multi-dimensional edges. The proposed framework can consolidate current graph neural network models, e.g., GCN and GAT. The proposed framework and new models have the following novelties: First, we propose to use doubly stochastic normalization of graph edge features instead of the commonly used row or symmetric normalization approaches used in current graph neural networks. Second, we construct new formulas for the operations in each individual layer so that they can handle multi-dimensional edge features. Third, for the proposed new framework, edge features are adaptive across network layers. As a result, our proposed new framework and new models are able to exploit a rich source of graph edge information. We apply our new models to graph node classification on several citation networks, whole graph classification, and regression on several molecular datasets. Compared with the current state-of-the-art methods, i.e., GCNs and GAT, our models obtain better performance, which testify to the importance of exploiting edge features in graph neural networks.
count=2
* Crowd Counting and Density Estimation by Trellis Encoder-Decoder Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Jiang_Crowd_Counting_and_Density_Estimation_by_Trellis_Encoder-Decoder_Networks_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Jiang_Crowd_Counting_and_Density_Estimation_by_Trellis_Encoder-Decoder_Networks_CVPR_2019_paper.pdf)]
    * Title: Crowd Counting and Density Estimation by Trellis Encoder-Decoder Networks
    * Year: `2019`
    * Authors: Xiaolong Jiang,  Zehao Xiao,  Baochang Zhang,  Xiantong Zhen,  Xianbin Cao,  David Doermann,  Ling Shao
    * Abstract: Crowd counting has recently attracted increasing interest in computer vision but remains a challenging problem. In this paper, we propose a trellis encoder-decoder network (TEDnet) for crowd counting, which focuses on generating high-quality density estimation maps. The major contributions are four-fold. First, we develop a new trellis architecture that incorporates multiple decoding paths to hierarchically aggregate features at different encoding stages, which improves the representative capability of convolutional features for large variations in objects. Second, we employ dense skip connections interleaved across paths to facilitate sufficient multi-scale feature fusions, which also helps TEDnet to absorb the supervision information. Third, we propose a new combinatorial loss to enforce similarities in local coherence and spatial correlation between maps. By distributedly imposing this combinatorial loss on intermediate outputs, TEDnet can improve the back-propagation process and alleviate the gradient vanishing problem. Finally, on four widely-used benchmarks, our TEDnet achieves the best overall performance in terms of both density map quality and counting accuracy, with an improvement up to 14% in MAE metric. These results validate the effectiveness of TEDnet for crowd counting.
count=2
* LAF-Net: Locally Adaptive Fusion Networks for Stereo Confidence Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Kim_LAF-Net_Locally_Adaptive_Fusion_Networks_for_Stereo_Confidence_Estimation_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_LAF-Net_Locally_Adaptive_Fusion_Networks_for_Stereo_Confidence_Estimation_CVPR_2019_paper.pdf)]
    * Title: LAF-Net: Locally Adaptive Fusion Networks for Stereo Confidence Estimation
    * Year: `2019`
    * Authors: Sunok Kim,  Seungryong Kim,  Dongbo Min,  Kwanghoon Sohn
    * Abstract: We present a novel method that estimates confidence map of an initial disparity by making full use of tri-modal input, including matching cost, disparity, and color image through deep networks. The proposed network, termed as Locally Adaptive Fusion Networks (LAF-Net), learns locally-varying attention and scale maps to fuse the tri-modal confidence features. The attention inference networks encode the importance of tri-modal confidence features and then concatenate them using the attention maps in an adaptive and dynamic fashion. This enables us to make an optimal fusion of the heterogeneous features, compared to a simple concatenation technique that is commonly used in conventional approaches. In addition, to encode the confidence features with locally-varying receptive fields, the scale inference networks learn the scale map and warp the fused confidence features through convolutional spatial transformer networks. Finally, the confidence map is progressively estimated in the recursive refinement networks to enforce a spatial context and local consistency. Experimental results show that this model outperforms the state-of-the-art methods on various benchmarks.
count=2
* ADCrowdNet: An Attention-Injective Deformable Convolutional Network for Crowd Understanding
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_ADCrowdNet_An_Attention-Injective_Deformable_Convolutional_Network_for_Crowd_Understanding_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_ADCrowdNet_An_Attention-Injective_Deformable_Convolutional_Network_for_Crowd_Understanding_CVPR_2019_paper.pdf)]
    * Title: ADCrowdNet: An Attention-Injective Deformable Convolutional Network for Crowd Understanding
    * Year: `2019`
    * Authors: Ning Liu,  Yongchao Long,  Changqing Zou,  Qun Niu,  Li Pan,  Hefeng Wu
    * Abstract: We propose an attention-injective deformable convolutional network called ADCrowdNet for crowd understanding that can address the accuracy degradation problem of highly congested noisy scenes. ADCrowdNet contains two concatenated networks. An attention-aware network called Attention Map Generator (AMG) first detects crowd regions in images and computes the congestion degree of these regions. Based on detected crowd regions and congestion priors, a multi-scale deformable network called Density Map Estimator (DME) then generates high-quality density maps. With the attention-aware training scheme and multi-scale deformable convolutional scheme, the proposed ADCrowdNet achieves the capability of being more effective to capture the crowd features and more resistant to various noises. We have evaluated our method on four popular crowd counting datasets (ShanghaiTech, UCF_CC_50, WorldEXPO'10, and UCSD) and an extra vehicle counting dataset TRANCOS, and our approach beats existing state-of-the-art approaches on all of these datasets.
count=2
* Context-Aware Crowd Counting
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Context-Aware_Crowd_Counting_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Context-Aware_Crowd_Counting_CVPR_2019_paper.pdf)]
    * Title: Context-Aware Crowd Counting
    * Year: `2019`
    * Authors: Weizhe Liu,  Mathieu Salzmann,  Pascal Fua
    * Abstract: State-of-the-art methods for counting people in crowded scenes rely on deep networks to estimate crowd density. They typically use the same filters over the whole image or over large image patches. Only then do they estimate local scale to compensate for perspective distortion. This is typically achieved by training an auxiliary classifier to select, for predefined image patches, the best kernel size among a limited set of choices. As such, these methods are not end-to-end trainable and restricted in the scope of context they can leverage. In this paper, we introduce an end-to-end trainable deep architecture that combines features obtained using multiple receptive field sizes and learns the importance of each such feature at each image location. In other words, our approach adaptively encodes the scale of the contextual information required to accurately predict crowd density. This yields an algorithm that outperforms state-of-the-art crowd counting methods, especially when perspective effects are strong.
count=2
* Connecting the Dots: Learning Representations for Active Monocular Depth Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Riegler_Connecting_the_Dots_Learning_Representations_for_Active_Monocular_Depth_Estimation_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Riegler_Connecting_the_Dots_Learning_Representations_for_Active_Monocular_Depth_Estimation_CVPR_2019_paper.pdf)]
    * Title: Connecting the Dots: Learning Representations for Active Monocular Depth Estimation
    * Year: `2019`
    * Authors: Gernot Riegler,  Yiyi Liao,  Simon Donne,  Vladlen Koltun,  Andreas Geiger
    * Abstract: We propose a technique for depth estimation with a monocular structured-light camera, i.e., a calibrated stereo set-up with one camera and one laser projector. Instead of formulating the depth estimation via a correspondence search problem, we show that a simple convolutional architecture is sufficient for high-quality disparity estimates in this setting. As accurate ground-truth is hard to obtain, we train our model in a self-supervised fashion with a combination of photometric and geometric losses. Further, we demonstrate that the projected pattern of the structured light sensor can be reliably separated from the ambient information. This can then be used to improve depth boundaries in a weakly supervised fashion by modeling the joint statistics of image and depth edges. The model trained in this fashion compares favorably to the state-of-the-art on challenging synthetic and real-world datasets. In addition, we contribute a novel simulator, which allows to benchmark active depth prediction algorithms in controlled conditions.
count=2
* Real-Time Self-Adaptive Deep Stereo
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Tonioni_Real-Time_Self-Adaptive_Deep_Stereo_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Tonioni_Real-Time_Self-Adaptive_Deep_Stereo_CVPR_2019_paper.pdf)]
    * Title: Real-Time Self-Adaptive Deep Stereo
    * Year: `2019`
    * Authors: Alessio Tonioni,  Fabio Tosi,  Matteo Poggi,  Stefano Mattoccia,  Luigi Di Stefano
    * Abstract: Deep convolutional neural networks trained end-to-end are the state-of-the-art methods to regress dense disparity maps from stereo pairs. These models, however, suffer from a notable decrease in accuracy when exposed to scenarios significantly different from the training set (e.g., real vs synthetic images, etc.). We argue that it is extremely unlikely to gather enough samples to achieve effective training/tuning in any target domain, thus making this setup impractical for many applications. Instead, we propose to perform unsupervised and continuous online adaptation of a deep stereo network, which allows for preserving its accuracy in any environment. However, this strategy is extremely computationally demanding and thus prevents real-time inference. We address this issue introducing a new lightweight, yet effective, deep stereo architecture, Modularly ADaptive Network(MADNet), and developing a Modular ADaptation (MAD) algorithm, which independently trains sub-portions of the network. By deploying MADNet together with MAD we introduce the first real-time self-adaptive deep stereo system enabling competitive performance on heterogeneous datasets. Our code is publicly available at https://github.com/CVLAB-Unibo/Real-time-self-adaptive-deep-stereo.
count=2
* Im2Flow: Motion Hallucination From Static Images for Action Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Gao_Im2Flow_Motion_Hallucination_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Im2Flow_Motion_Hallucination_CVPR_2018_paper.pdf)]
    * Title: Im2Flow: Motion Hallucination From Static Images for Action Recognition
    * Year: `2018`
    * Authors: Ruohan Gao, Bo Xiong, Kristen Grauman
    * Abstract: Existing methods to recognize actions in static images take the images at their face value, learning the appearances---objects, scenes, and body poses---that distinguish each action class. However, such models are deprived of the rich dynamic structure and motions that also define human activity. We propose an approach that hallucinates the unobserved future motion implied by a single snapshot to help static-image action recognition. The key idea is to learn a prior over short-term dynamics from thousands of unlabeled videos, infer the anticipated optical flow on novel static images, and then train discriminative models that exploit both streams of information. Our main contributions are twofold. First, we devise an encoder-decoder convolutional neural network and a novel optical flow encoding that can translate a static image into an accurate flow map. Second, we show the power of hallucinated flow for recognition, successfully transferring the learned motion into a standard two-stream network for activity recognition. On seven datasets, we demonstrate the power of the approach. It not only achieves state-of-the-art accuracy for dense optical flow prediction, but also consistently enhances recognition of actions and dynamic scenes.
count=2
* Guided Proofreading of Automatic Segmentations for Connectomics
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Haehn_Guided_Proofreading_of_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Haehn_Guided_Proofreading_of_CVPR_2018_paper.pdf)]
    * Title: Guided Proofreading of Automatic Segmentations for Connectomics
    * Year: `2018`
    * Authors: Daniel Haehn, Verena Kaynig, James Tompkin, Jeff W. Lichtman, Hanspeter Pfister
    * Abstract: Automatic cell image segmentation methods in connectomics produce merge and split errors, which require correction through proofreading. Previous research has identified the visual search for these errors as the bottleneck in interactive proofreading. To aid error correction, we develop two classifiers that automatically recommend candidate merges and splits to the user. These classifiers use a convolutional neural network (CNN) that has been trained with errors in automatic segmentations against expert-labeled ground truth. Our classifiers detect potentially-erroneous regions by considering a large context region around a segmentation boundary. Corrections can then be performed by a user with yes/no decisions, which reduces variation of information 7.5x faster than previous proofreading methods. We also present a fully-automatic mode that uses a probability threshold to make merge/split decisions. Extensive experiments using the automatic approach and comparing performance of novice and expert users demonstrate that our method performs favorably against state-of-the-art proofreading methods on different connectomics datasets.
count=2
* 4D Human Body Correspondences From Panoramic Depth Maps
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Li_4D_Human_Body_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_4D_Human_Body_CVPR_2018_paper.pdf)]
    * Title: 4D Human Body Correspondences From Panoramic Depth Maps
    * Year: `2018`
    * Authors: Zhong Li, Minye Wu, Wangyiteng Zhou, Jingyi Yu
    * Abstract: The availability of affordable 3D full body reconstruction systems has given rise to free-viewpoint video (FVV) of human avatars. Most existing solutions produce temporally uncorrelated point clouds or meshes with unknown point/vertex correspondences. Individually compressing each frame is ineffective and still yields to ultra-large data sizes. We present an end-to-end deep learning scheme to establish dense shape correspondences and subsequently compress the data. Our approach uses sparse set of "panoramic" depth maps or PDMs, each emulating an inward-viewing concentric mosaics (CM). We then develop a learning-based technique to learn pixel-wise feature descriptors on PDMs. The results are fed into an autoencoder-based network for compression. Comprehensive experiments demonstrate our solution is robust and effective on both public and our newly captured datasets.
count=2
* CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Li_CSRNet_Dilated_Convolutional_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_CSRNet_Dilated_Convolutional_CVPR_2018_paper.pdf)]
    * Title: CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes
    * Year: `2018`
    * Authors: Yuhong Li, Xiaofan Zhang, Deming Chen
    * Abstract: We propose a network for Congested Scene Recognition called CSRNet to provide a data-driven and deep learning method that can understand highly congested scenes and perform accurate count estimation as well as present high-quality density maps. The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations. CSRNet is an easy-trained model because of its pure convolutional structure. We demonstrate CSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the WorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art performance. In the ShanghaiTech Part_B dataset, CSRNet achieves 47.3% lower Mean Absolute Error (MAE) than the previous state-of-the-art method. We extend the targeted applications for counting other objects, such as the vehicle in TRANCOS dataset. Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-of-the-art approach.
count=2
* DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_DecideNet_Counting_Varying_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_DecideNet_Counting_Varying_CVPR_2018_paper.pdf)]
    * Title: DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation
    * Year: `2018`
    * Authors: Jiang Liu, Chenqiang Gao, Deyu Meng, Alexander G. Hauptmann
    * Abstract: In real-world crowd counting applications, the crowd densities vary greatly in spatial and temporal domains. A detection based counting method will estimate crowds accurately in low density scenes, while its reliability in congested areas is downgraded. A regression based approach, on the other hand, captures the general density information in crowded regions. Without knowing the location of each person, it tends to overestimate the count in low density areas. Thus, exclusively using either one of them is not sufficient to handle all kinds of scenes with varying densities. To address this issue, a novel end-to-end crowd counting framework, named DecideNet (DEteCtIon and Density Estimation Network) is proposed. It can adaptively decide the appropriate counting mode for different locations on the image based on its real density conditions. DecideNet starts with estimating the crowd density by generating detection and regression based density maps separately. To capture inevitable variation in densities, it incorporates an attention module, meant to adaptively assess the reliability of the two types of estimations. The final crowd counts are obtained with the guidance of the attention module to adopt suitable estimations from the two kinds of density maps. Experimental results show that our method achieves state-of-the-art performance on three challenging crowd counting datasets.
count=2
* Efficient Diverse Ensemble for Discriminative Co-Tracking
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Meshgi_Efficient_Diverse_Ensemble_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Meshgi_Efficient_Diverse_Ensemble_CVPR_2018_paper.pdf)]
    * Title: Efficient Diverse Ensemble for Discriminative Co-Tracking
    * Year: `2018`
    * Authors: Kourosh Meshgi, Shigeyuki Oba, Shin Ishii
    * Abstract: Ensemble discriminative tracking utilizes a committee of classifiers, to label data samples, which are in turn, used for retraining the tracker to localize the target using the collective knowledge of the committee. Committee members could vary in their features, memory update schemes, or training data, however, it is inevitable to have committee members that excessively agree because of large overlaps in their version space. To remove this redundancy and have an effective ensemble learning, it is critical for the committee to include consistent hypotheses that differ from one-another, covering the version space with minimum overlaps. In this study, we propose an online ensemble tracker that directly generates a diverse committee by generating an efficient set of artificial training. The artificial data is sampled from the empirical distribution of the samples taken from both target and background, whereas the process is governed by query-by-committee to shrink the overlap between classifiers. The experimental results demonstrate that the proposed scheme outperforms conventional ensemble trackers on public benchmarks.
count=2
* SeedNet: Automatic Seed Generation With Deep Reinforcement Learning for Robust Interactive Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Song_SeedNet_Automatic_Seed_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Song_SeedNet_Automatic_Seed_CVPR_2018_paper.pdf)]
    * Title: SeedNet: Automatic Seed Generation With Deep Reinforcement Learning for Robust Interactive Segmentation
    * Year: `2018`
    * Authors: Gwangmo Song, Heesoo Myeong, Kyoung Mu Lee
    * Abstract: In this paper, we propose an automatic seed generation technique with deep reinforcement learning to solve the interactive segmentation problem. One of the main issues of the interactive segmentation problem is robust and consistent object extraction with less human effort. Most of the existing algorithms highly depend on the distribution of inputs, which differs from one user to another and hence need sequential user interactions to achieve adequate performance. In our system, when a user first specifies a point on the desired object and a point in the background, a sequence of artificial user input is automatically generated for precisely segmenting the desired object. The proposed system allows the user to reduce the number of input significantly. This problem is difficult to cast as a supervised learning problem because it is not possible to define globally optimal user input at some stage of the interactive segmentation task. Hence, we formulate automatic seed generation problem as Markov Decision Process (MDP) and then optimize it by reinforcement learning with Deep Q-Network (DQN). We train our network on the MSRA10K dataset and show that the network achieves notable performance improvement from inaccurate initial segmentation on both seen and unseen datasets.
count=2
* VoxelNet
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.pdf)]
    * Title: VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection
    * Year: `2018`
    * Authors: Yin Zhou, Oncel Tuzel
    * Abstract: Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.
count=2
* Loss Max-Pooling for Semantic Image Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Bulo_Loss_Max-Pooling_for_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Bulo_Loss_Max-Pooling_for_CVPR_2017_paper.pdf)]
    * Title: Loss Max-Pooling for Semantic Image Segmentation
    * Year: `2017`
    * Authors: Samuel Rota Bulo, Gerhard Neuhold, Peter Kontschieder
    * Abstract: In this work we introduce a novel loss max-pooling concept for handling imbalanced training data distributions, applicable as alternative loss layer in the context of deep neural networks for semantic image segmentation tasks. Most real-world semantic segmentation datasets exhibit long tail distributions with few object categories comprising the majority of data and consequently biasing the classifiers towards them. Our method adaptively re-weights the contributions of each pixel based on their observed losses, targeting under-performing classification results as often encountered for under-represented object classes. Moreover, our approach goes beyond conventional cost-sensitive learning attempts through adaptive considerations that allow us to indirectly address both, inter- and intra-class imbalances. We provide a theoretical justification of our approach, complementary to experimental analyses on standard semantic segmentation datasets. In our experiments on the challenging Cityscapes and Pascal VOC 2012 segmentation benchmarks we find consistently improved results, demonstrating the efficacy of our approach.
count=2
* Dynamic Attention-Controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-Set Sample Weighting
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Feng_Dynamic_Attention-Controlled_Cascaded_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Feng_Dynamic_Attention-Controlled_Cascaded_CVPR_2017_paper.pdf)]
    * Title: Dynamic Attention-Controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-Set Sample Weighting
    * Year: `2017`
    * Authors: Zhen-Hua Feng, Josef Kittler, William Christmas, Patrik Huber, Xiao-Jun Wu
    * Abstract: We present a new Cascaded Shape Regression (CSR) architecture, namely Dynamic Attention-Controlled CSR (DAC-CSR), for robust facial landmark detection on unconstrained faces. Our DAC-CSR divides facial landmark detection into three cascaded sub-tasks: face bounding box refinement, general CSR and attention-controlled CSR. The first two stages refine initial face bounding boxes and output intermediate facial landmarks. Then, an online dynamic model selection method is used to choose appropriate domain-specific CSRs for further landmark refinement. The key innovation of our DAC-CSR is the fault-tolerant mechanism, using fuzzy set sample weighting, for attention-controlled domain-specific model training. Moreover, we advocate data augmentation with a simple but effective 2D profile face generator, and context-aware feature extraction for better facial feature representation. Experimental results obtained on challenging datasets demonstrate the merits of our DAC-CSR over the state-of-the-art methods.
count=2
* 3D Convolutional Neural Networks for Efficient and Robust Hand Pose Estimation From Single Depth Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Ge_3D_Convolutional_Neural_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Ge_3D_Convolutional_Neural_CVPR_2017_paper.pdf)]
    * Title: 3D Convolutional Neural Networks for Efficient and Robust Hand Pose Estimation From Single Depth Images
    * Year: `2017`
    * Authors: Liuhao Ge, Hui Liang, Junsong Yuan, Daniel Thalmann
    * Abstract: We propose a simple, yet effective approach for real-time hand pose estimation from single depth images using three-dimensional Convolutional Neural Networks (3D CNNs). Image based features extracted by 2D CNNs are not directly suitable for 3D hand pose estimation due to the lack of 3D spatial information. Our proposed 3D CNN taking a 3D volumetric representation of the hand depth image as input can capture the 3D spatial structure of the input and accurately regress full 3D hand pose in a single pass. In order to make the 3D CNN robust to variations in hand sizes and global orientations, we perform 3D data augmentation on the training data. Experiments show that our proposed 3D CNN based approach outperforms state-of-the-art methods on two challenging hand pose datasets, and is very efficient as our implementation runs at over 215 fps on a standard computer with a single GPU.
count=2
* BranchOut: Regularization for Online Ensemble Tracking With Convolutional Neural Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Han_BranchOut_Regularization_for_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Han_BranchOut_Regularization_for_CVPR_2017_paper.pdf)]
    * Title: BranchOut: Regularization for Online Ensemble Tracking With Convolutional Neural Networks
    * Year: `2017`
    * Authors: Bohyung Han, Jack Sim, Hartwig Adam
    * Abstract: We propose an extremely simple but effective regularization technique of convolutional neural networks (CNNs), referred to as BranchOut, for online ensemble tracking. Our algorithm employs a CNN for target representation, which has a common convolutional layers but has multiple branches of fully connected layers. For better regularization, a subset of branches in the CNN are selected randomly for online learning whenever target appearance models need to be updated. Each branch may have a different number of layers to maintain variable abstraction levels of target appearances. BranchOut with multi-level target representation allows us to learn robust target appearance models with diversity and handle various challenges in visual tracking problem effectively. The proposed algorithm is evaluated in standard tracking benchmarks and shows the state-of-the-art performance even without additional pretraining on external tracking sequences.
count=2
* Seeing Invisible Poses: Estimating 3D Body Pose From Egocentric Video
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Jiang_Seeing_Invisible_Poses_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Jiang_Seeing_Invisible_Poses_CVPR_2017_paper.pdf)]
    * Title: Seeing Invisible Poses: Estimating 3D Body Pose From Egocentric Video
    * Year: `2017`
    * Authors: Hao Jiang, Kristen Grauman
    * Abstract: Understanding the camera wearer's activity is central to egocentric vision, yet one key facet of that activity is inherently invisible to the camera--the wearer's body pose. Prior work focuses on estimating the pose of hands and arms when they come into view, but this 1) gives an incomplete view of the full body posture, and 2) prevents any pose estimate at all in many frames, since the hands are only visible in a fraction of daily life activities. We propose to infer the "invisible pose" of a person behind the egocentric camera. Given a single video, our efficient learning-based approach returns the full body 3D joint positions for each frame. Our method exploits cues from the dynamic motion signatures of the surrounding scene--which change predictably as a function of body pose--as well as static scene structures that reveal the viewpoint (e.g., sitting vs. standing). We further introduce a novel energy minimization scheme to infer the pose sequence. It uses soft predictions of the poses per time instant together with a non-parametric model of human pose dynamics over longer windows. Our method outperforms an array of possible alternatives, including typical deep learning approaches for direct pose regression from images.
count=2
* Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper.pdf)]
    * Title: Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution
    * Year: `2017`
    * Authors: Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang
    * Abstract: Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy.
count=2
* Deep Mixture of Linear Inverse Regressions Applied to Head-Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Lathuiliere_Deep_Mixture_of_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Lathuiliere_Deep_Mixture_of_CVPR_2017_paper.pdf)]
    * Title: Deep Mixture of Linear Inverse Regressions Applied to Head-Pose Estimation
    * Year: `2017`
    * Authors: Stephane Lathuiliere, Remi Juge, Pablo Mesejo, Rafael Munoz-Salinas, Radu Horaud
    * Abstract: Convolutional Neural Networks (ConvNets) have become the state-of-the-art for many classification and regression problems in computer vision. When it comes to regression, approaches such as measuring the Euclidean distance of target and predictions are often employed as output layer. In this paper, we propose the coupling of a Gaussian mixture of linear inverse regressions with a ConvNet, and we describe the methodological foundations and the associated algorithm to jointly train the deep network and the regression function. We test our model on the head-pose estimation problem. In this particular problem, we show that inverse regression outperforms regression models currently used by state-of-the-art computer vision methods. Our method does not require the incorporation of additional data, as it is often proposed in the literature, thus it is able to work well on relatively small training datasets. Finally, it outperforms state-of-the-art methods in head-pose estimation using a widely used head-pose dataset. To the best of our knowledge, we are the first to incorporate inverse regression into deep learning for computer vision applications.
count=2
* Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Monti_Geometric_Deep_Learning_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Monti_Geometric_Deep_Learning_CVPR_2017_paper.pdf)]
    * Title: Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs
    * Year: `2017`
    * Authors: Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, Michael M. Bronstein
    * Abstract: Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.
count=2
* Coarse-To-Fine Volumetric Prediction for Single-Image 3D Human Pose
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Pavlakos_Coarse-To-Fine_Volumetric_Prediction_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Pavlakos_Coarse-To-Fine_Volumetric_Prediction_CVPR_2017_paper.pdf)]
    * Title: Coarse-To-Fine Volumetric Prediction for Single-Image 3D Human Pose
    * Year: `2017`
    * Authors: Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas Daniilidis
    * Abstract: This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization and a subsequent optimization step to recover 3D pose. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a fine discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-fine prediction scheme. This step addresses the large dimensionality increase and enables iterative refinement and repeated processing of the image features. The proposed approach outperforms all state-of-the-art methods on standard benchmarks achieving a relative error reduction greater than 30% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our end-to-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images.
count=2
* Unambiguous Text Localization and Retrieval for Cluttered Scenes
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Rong_Unambiguous_Text_Localization_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Rong_Unambiguous_Text_Localization_CVPR_2017_paper.pdf)]
    * Title: Unambiguous Text Localization and Retrieval for Cluttered Scenes
    * Year: `2017`
    * Authors: Xuejian Rong, Chucai Yi, Yingli Tian
    * Abstract: Text instance as one category of self-described objects provides valuable information for understanding and describing cluttered scenes. In this paper, we explore the task of unambiguous text localization and retrieval, to accurately localize a specific targeted text instance in a cluttered image given a natural language description that refers to it. To address this issue, first a novel recurrent Dense Text Localization Network (DTLN) is proposed to sequentially decode the intermediate convolutional representations of a cluttered scene image into a set of distinct text instance detections. Our approach avoids repeated detections at multiple scales of the same text instance by recurrently memorizing previous detections, and effectively tackles crowded text instances in close proximity. Second, we propose a Context Reasoning Text Retrieval (CRTR) model, which jointly encodes text instances and their context information through a recurrent network, and ranks localized text bounding boxes by a scoring function of context compatibility. Quantitative evaluations on standard scene text localization benchmarks and a newly collected scene text retrieval dataset demonstrate the effectiveness and advantages of our models for both scene text localization and retrieval.
count=2
* A Generative Model for Depth-Based Robust 3D Facial Pose Tracking
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Sheng_A_Generative_Model_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Sheng_A_Generative_Model_CVPR_2017_paper.pdf)]
    * Title: A Generative Model for Depth-Based Robust 3D Facial Pose Tracking
    * Year: `2017`
    * Authors: Lu Sheng, Jianfei Cai, Tat-Jen Cham, Vladimir Pavlovic, King Ngi Ngan
    * Abstract: We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that unifies pose tracking and face model adaptation on-the-fly. Particularly, we propose a statistical 3D face model that owns the flexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face model's visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.
count=2
* Learning and Refining of Privileged Information-Based RNNs for Action Recognition From Depth Sequences
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Shi_Learning_and_Refining_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Shi_Learning_and_Refining_CVPR_2017_paper.pdf)]
    * Title: Learning and Refining of Privileged Information-Based RNNs for Action Recognition From Depth Sequences
    * Year: `2017`
    * Authors: Zhiyuan Shi, Tae-Kyun Kim
    * Abstract: Existing RNN-based approaches for action recognition from depth sequences require either skeleton joints or hand-crafted depth features as inputs. An end-to-end manner, mapping from raw depth maps to action classes, is non-trivial to design due to the fact that: 1) single channel map lacks texture thus weakens the discriminative power; 2) relatively small set of depth training data. To address these challenges, we propose to learn an RNN driven by privileged information (PI) in three-steps: An encoder is pre-trained to learn a joint embedding of depth appearance and PI (i.e. skeleton joints). The learned embedding layers are then tuned in the learning step, aiming to optimize the network by exploiting PI in a form of multi-task loss. However, exploiting PI as a secondary task provides little help to improve the performance of a primary task (i.e. classification) due to the gap between them. Finally, a bridging matrix is defined to connect two tasks by discovering latent PI in the refining step. Our PI-based classification loss maintains a consistency between latent PI and predicted distribution. The latent PI and network are iteratively estimated and updated in an expectation-maximization procedure. The proposed learning process provides greater discriminative power to model subtle depth difference, while helping avoid overfitting the scarcer training data. Our experiments show significant performance gains over state-of-the-art methods on three public benchmark datasets and our newly collected Blanket dataset.
count=2
* Deep Learning Human Mind for Automated Visual Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Spampinato_Deep_Learning_Human_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Spampinato_Deep_Learning_Human_CVPR_2017_paper.pdf)]
    * Title: Deep Learning Human Mind for Automated Visual Classification
    * Year: `2017`
    * Authors: Concetto Spampinato, Simone Palazzo, Isaak Kavasidis, Daniela Giordano, Nasim Souly, Mubarak Shah
    * Abstract: What if we could effectively read the mind and transfer human visual capabilities to computer vision methods? In this paper, we aim at addressing this question by developing the first visual object classifier driven by human brain signals. In particular, we employ EEG data evoked by visual object stimuli combined with Recurrent Neural Networks (RNN) to learn a discriminative brain activity manifold of visual categories in a reading the mind effort. Afterward, we transfer the learned capabilities to machines by training a Convolutional Neural Network (CNN)-based regressor to project images onto the learned manifold, thus allowing machines to employ human brain-based features for automated visual classification. We use a 128-channel EEG with active electrodes to record brain activity of several subjects while looking at images of 40 ImageNet object classes. The proposed RNN-based approach for discriminating object classes using brain signals reaches an average accuracy of about 83%, which greatly outperforms existing methods attempting to learn EEG visual object representations. As for automated object categorization, our human brain-driven approach obtains competitive performance, comparable to those achieved by powerful CNN models and it is also able to generalize over different visual datasets.
count=2
* Crossing Nets: Combining GANs and VAEs With a Shared Latent Space for Hand Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Wan_Crossing_Nets_Combining_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wan_Crossing_Nets_Combining_CVPR_2017_paper.pdf)]
    * Title: Crossing Nets: Combining GANs and VAEs With a Shared Latent Space for Hand Pose Estimation
    * Year: `2017`
    * Authors: Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao
    * Abstract: State-of-the-art methods for 3D hand pose estimation from depth images require large amounts of annotated training data. We propose modelling the statistical relationship of 3D hand poses and corresponding depth images using two deep generative models with a shared latent space. By design, our architecture allows for learning from unlabeled image data in a semi-supervised manner. Assuming a one-to-one mapping between a pose and a depth map, any given point in the shared latent space can be projected into both a hand pose or into a corresponding depth map. Regressing the hand pose can then be done by learning a discriminator to estimate the posterior of the latent pose given some depth map. To prevent over-fitting and to better exploit unlabeled depth maps, the generator and discriminator are trained jointly. At each iteration, the generator is updated with the back-propagated gradient from the discriminator to synthesize realistic depth maps of the articulated hand, while the discriminator benefits from an augmented training set of synthesized samples and unlabeled depth maps. The proposed discriminator network architecture is highly efficient and runs at 90fps on the CPU with accuracies comparable or better than state-of-art on 3 publicly available benchmarks.
count=2
* Deep Crisp Boundaries
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Deep_Crisp_Boundaries_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Deep_Crisp_Boundaries_CVPR_2017_paper.pdf)]
    * Title: Deep Crisp Boundaries
    * Year: `2017`
    * Authors: Yupei Wang, Xin Zhao, Kaiqi Huang
    * Abstract: Edge detection had made significant progress with the help of deep Convolutional Networks (ConvNet). ConvNet based edge detectors approached human level performance on standard benchmarks. We provide a systematical study of these detector outputs, and show that they failed to accurately localize edges, which can be adversarial for tasks that require crisp edge inputs. In addition, we propose a novel refinement architecture to address the challenging problem of learning a crisp edge detector using ConvNet. Our method leverages a top-down backward refinement pathway, and progressively increases the resolution of feature maps to generate crisp edges. Our results achieve promising performance on BSDS500, surpassing human accuracy when using standard criteria, and largely outperforming state-of-the-art methods when using more strict criteria. We further demonstrate the benefit of crisp edge maps for estimating optical flow and generating object proposals.
count=2
* Turning an Urban Scene Video Into a Cinemagraph
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Yan_Turning_an_Urban_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Yan_Turning_an_Urban_CVPR_2017_paper.pdf)]
    * Title: Turning an Urban Scene Video Into a Cinemagraph
    * Year: `2017`
    * Authors: Hang Yan, Yebin Liu, Yasutaka Furukawa
    * Abstract: This paper proposes an algorithm that turns a regular video capturing urban scenes into a high-quality endless animation, known as a Cinemagraph. The creation of a Cinemagraph usually requires a static camera in a carefully configured scene. The task becomes challenging for a regular video with a moving camera and objects. Our approach first warps an input video into the viewpoint of a reference camera. Based on the warped video, we propose effective temporal analysis algorithms to detect regions with static geometry and dynamic appearance, where geometric modeling is reliable and visually attractive animations can be created. Lastly, the algorithm applies a sequence of video processing techniques to produce a Cinemagraph movie. We have tested the proposed approach on numerous challenging real scenes. To our knowledge, this work is the first to automatically generate Cinemagraph animations from regular movies in the wild.
count=2
* Scale-Aware Alignment of Hierarchical Image Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Scale-Aware_Alignment_of_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Scale-Aware_Alignment_of_CVPR_2016_paper.pdf)]
    * Title: Scale-Aware Alignment of Hierarchical Image Segmentation
    * Year: `2016`
    * Authors: Yuhua Chen, Dengxin Dai, Jordi Pont-Tuset, Luc Van Gool
    * Abstract: Image segmentation is a key component in many computer vision systems, and it is recovering a prominent spot in the literature as methods improve and overcome their limitations. The outputs of most recent algorithms are in the form of a hierarchical segmentation, which provides segmentation at different scales in a single tree-like structure. Commonly, these hierarchical methods start from some low-level features, and are not aware of the scale information of the different regions in them. As such, one might need to work on many different levels of the hierarchy to find the objects in the scene. This work tries to modify the existing hierarchical algorithm by improving their alignment, that is, by trying to modify the depth of the regions in the tree to better couple depth and scale. To do so, we first train a regressor to predict the scale of regions using mid-level features. We then define the anchor slice as the set of regions that better balance between over-segmentation and under-segmentation. The output of our method is an improved hierarchy, re-aligned by the anchor slice. To demonstrate the power of our method, we perform comprehensive experiments, which show that our method, as a post-processing step, can significantly improve the quality of the hierarchical segmentation representations, and ease the usage of hierarchical image segmentation to high-level vision tasks such as object segmentation. We also prove that the improvement generalizes well across different algorithms and datasets, with a low computational cost.
count=2
* Visual Tracking Using Attention-Modulated Disintegration and Integration
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Choi_Visual_Tracking_Using_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Choi_Visual_Tracking_Using_CVPR_2016_paper.pdf)]
    * Title: Visual Tracking Using Attention-Modulated Disintegration and Integration
    * Year: `2016`
    * Authors: Jongwon Choi, Hyung Jin Chang, Jiyeoup Jeong, Yiannis Demiris, Jin Young Choi
    * Abstract: In this paper, we present a novel attention-modulated visual tracking algorithm that decomposes an object into multiple cognitive units, and trains multiple elementary trackers in order to modulate the distribution of attention according to various feature and kernel types. In the integration stage it recombines the units to memorize and recognize the target object effectively. With respect to the elementary trackers, we present a novel attentional feature-based correlation filter (AtCF) that focuses on distinctive attentional features. The effectiveness of the proposed algorithm is validated through experimental comparison with state-of-the-art methods on widely-used tracking benchmark datasets.
count=2
* Structured Regression Gradient Boosting
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Diego_Structured_Regression_Gradient_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Diego_Structured_Regression_Gradient_CVPR_2016_paper.pdf)]
    * Title: Structured Regression Gradient Boosting
    * Year: `2016`
    * Authors: Ferran Diego, Fred A. Hamprecht
    * Abstract: We propose a new way to train a structured output prediction model. More specifically, we train nonlinear data terms in a Gaussian Conditional Random Field (GCRF) by a generalized version of gradient boosting. The approach is evaluated on three challenging regression benchmarks: vessel detection, single image depth estimation and image inpainting. These experiments suggest that the proposed boosting framework matches or exceeds the state-of-the-art.
count=2
* Occlusion Boundary Detection via Deep Exploration of Context
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Fu_Occlusion_Boundary_Detection_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Fu_Occlusion_Boundary_Detection_CVPR_2016_paper.pdf)]
    * Title: Occlusion Boundary Detection via Deep Exploration of Context
    * Year: `2016`
    * Authors: Huan Fu, Chaohui Wang, Dacheng Tao, Michael J. Black
    * Abstract: Occlusion boundaries contain rich perceptual information about the underlying scene structure. They also provide important cues in many visual perception tasks such as scene understanding, object recognition, and segmentation. In this paper, we improve occlusion boundary detection via enhanced exploration of contextual information (e.g., local structural boundary patterns, observations from surrounding regions, and temporal context), and in doing so develop a novel approach based on convolutional neural networks (CNNs) and conditional random fields (CRFs). Experimental results demonstrate that our detector significantly outperforms the state-of-the-art (e.g., improving the F-measure from 0.62 to 0.71 on the commonly used CMU benchmark). Last but not least, we empirically assess the roles of several important components of the proposed detector, so as to validate the rationale behind this approach.
count=2
* Synthetic Data for Text Localisation in Natural Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Gupta_Synthetic_Data_for_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Gupta_Synthetic_Data_for_CVPR_2016_paper.pdf)]
    * Title: Synthetic Data for Text Localisation in Natural Images
    * Year: `2016`
    * Authors: Ankush Gupta, Andrea Vedaldi, Andrew Zisserman
    * Abstract: In this paper we introduce a new method for text detection in natural images. The method comprises two contributions: First, a fast and scalable engine to generate synthetic images of text in clutter. This engine overlays synthetic text to existing background images in a natural way, accounting for the local 3D scene geometry. Second, we use the synthetic images to train a Fully-Convolutional Regression Network (FCRN) which efficiently performs text detection and bounding-box regression at all locations and multiple scales in an image. We discuss the relation of FCRN to the recently-introduced YOLO detector, as well as other end-to-end object detection systems based on deep learning. The resulting detection network significantly out performs current methods for text detection in natural images, achieving an F-measure of 84.2% on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per second on a GPU.
count=2
* Pairwise Decomposition of Image Sequences for Active Multi-View Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Johns_Pairwise_Decomposition_of_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Johns_Pairwise_Decomposition_of_CVPR_2016_paper.pdf)]
    * Title: Pairwise Decomposition of Image Sequences for Active Multi-View Recognition
    * Year: `2016`
    * Authors: Edward Johns, Stefan Leutenegger, Andrew J. Davison
    * Abstract: A multi-view image sequence provides a much richer capacity for object recognition than from a single image. However, most existing solutions to multi-view recognition typically adopt hand-crafted, model-based geometric methods, which do not readily embrace recent trends in deep learning. We propose to bring Convolutional Neural Networks to generic multi-view recognition, by decomposing an image sequence into a set of image pairs, classifying each pair independently, and then learning an object classifier by weighting the contribution of each pair. This allows for recognition over arbitrary camera trajectories, without requiring explicit training over the potentially infinite number of camera paths and lengths. Building these pairwise relationships then naturally extends to the next-best-view problem in an active recognition framework. To achieve this, we train a second Convolutional Neural Network to map directly from an observed image to next viewpoint. Finally, we incorporate this into a trajectory optimisation task, whereby the best recognition confidence is sought for a given trajectory length. We present state-of-the-art results in both guided and unguided multi-view recognition on the ModelNet dataset, and show how our method can be used with depth images, greyscale images, or both.
count=2
* Consensus of Non-Rigid Reconstructions
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Lee_Consensus_of_Non-Rigid_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Lee_Consensus_of_Non-Rigid_CVPR_2016_paper.pdf)]
    * Title: Consensus of Non-Rigid Reconstructions
    * Year: `2016`
    * Authors: Minsik Lee, Jungchan Cho, Songhwai Oh
    * Abstract: Recently, there have been many progresses for the problem of non-rigid structure reconstruction based on 2D trajectories, but it is still challenging to deal with complex deformations or restricted view ranges. Promising alternatives are the piecewise reconstruction approaches, which divide trajectories into several local parts and stitch their individual reconstructions to produce an entire 3D structure. These methods show the state-of-the-art performance, however, most of them are specialized for relatively smooth surfaces and some are quite complicated. Meanwhile, it has been reported numerously in the field of pattern recognition that obtaining consensus from many weak hypotheses can give a strong, powerful result. Inspired by these reports, in this paper, we push the concept of part-based reconstruction to the limit: Instead of considering the parts as explicitly-divided local patches, we draw a large number of small random trajectory sets. From their individual reconstructions, we pull out a statistic of each 3D point to retrieve a strong reconstruction, of which the procedure can be expressed as a sparse l_1-norm minimization problem. In order to resolve the reflection ambiguity between weak (and possibly bad) reconstructions, we propose a novel optimization framework which only involves a single eigenvalue decomposition. The proposed method can be applied to any type of data and outperforms the existing methods for the benchmark sequences, even though it is composed of a few, simple steps. Furthermore, it is easily parallelizable, which is another advantage.
count=2
* Efficient Deep Learning for Stereo Matching
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Luo_Efficient_Deep_Learning_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Luo_Efficient_Deep_Learning_CVPR_2016_paper.pdf)]
    * Title: Efficient Deep Learning for Stereo Matching
    * Year: `2016`
    * Authors: Wenjie Luo, Alexander G. Schwing, Raquel Urtasun
    * Abstract: In the past year, convolutional neural networks have been shown to perform extremely well for stereo estimation. However, current architectures rely on siamese networks which exploit concatenation followed by further processing layers, requiring a minute of GPU computation per image pair. In contrast, in this paper we propose a matching network which is able to produce very accurate results in less than a second of GPU computation. Towards this goal, we exploit a product layer which simply computes the inner product between the two representations of a siamese architecture. We train our network by treating the problem as multi-class classification, where the classes are all possible disparities. This allows us to get calibrated scores, which result in much better matching performance when compared to existing approaches.
count=2
* What Players Do With the Ball: A Physically Constrained Interaction Modeling
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Maksai_What_Players_Do_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Maksai_What_Players_Do_CVPR_2016_paper.pdf)]
    * Title: What Players Do With the Ball: A Physically Constrained Interaction Modeling
    * Year: `2016`
    * Authors: Andrii Maksai, Xinchao Wang, Pascal Fua
    * Abstract: Tracking the ball is critical for video-based analysis of team sports. However, it is difficult, especially in low-resolution images, due to the small size of the ball, its speed that creates motion blur, and its often being occluded by players. In this paper, we propose a generic and principled approach to modeling the interaction between the ball and the players while also imposing appropriate physical constraints on the ball's trajectory. We show that our approach, formulated in terms of a Mixed Integer Program, is more robust and more accurate than several state-of-the-art approaches on real-life volleyball, basketball, and soccer sequences.
count=2
* Object Tracking via Dual Linear Structured SVM and Explicit Feature Map
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Ning_Object_Tracking_via_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Ning_Object_Tracking_via_CVPR_2016_paper.pdf)]
    * Title: Object Tracking via Dual Linear Structured SVM and Explicit Feature Map
    * Year: `2016`
    * Authors: Jifeng Ning, Jimei Yang, Shaojie Jiang, Lei Zhang, Ming-Hsuan Yang
    * Abstract: Structured support vector machine (SSVM) based methods has demonstrated encouraging performance in recent object tracking benchmarks. However, the complex and expensive optimization limits their deployment in real-world applications. In this paper, we present a simple yet efficient dual linear SSVM (DLSSVM) algorithm to enable fast learning and execution during tracking. By analyzing the dual variables, we propose a primal classifier update formula where the learning step size is computed in closed form. This online learning method significantly improves the robustness of the proposed linear SSVM with low computational cost. Second, we approximate the intersection kernel for feature representations with an explicit feature map to further improve tracking performance. Finally, we extend the proposed DLSSVM tracker in a multiscale manner to address the "drift" problem. Experimental results on large benchmark datasets with 50 and 100 video sequences show that the proposed DLSSVM tracking algorithm achieves state-of-the-art performance.
count=2
* DeepHand: Robust Hand Pose Estimation by Completing a Matrix Imputed With Deep Features
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Sinha_DeepHand_Robust_Hand_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Sinha_DeepHand_Robust_Hand_CVPR_2016_paper.pdf)]
    * Title: DeepHand: Robust Hand Pose Estimation by Completing a Matrix Imputed With Deep Features
    * Year: `2016`
    * Authors: Ayan Sinha, Chiho Choi, Karthik Ramani
    * Abstract: We propose DeepHand to estimate the 3D pose of a hand using depth data from commercial 3D sensors. We discriminatively train convolutional neural networks to output a low dimensional activation feature given a depth map. This activation feature vector is representative of the global or local joint angle parameters of a hand pose. We efficiently identify 'spatial' nearest neighbors to the activation feature, from a database of features corresponding to synthetic depth maps, and store some 'temporal' neighbors from previous frames. Our matrix completion algorithm uses these 'spatio-temporal' activation features and the corresponding known pose parameter values to to estimate the unknown pose parameters of the input feature vector. Our database of activation features supplements large viewpoint coverage and our hierarchical estimation of pose parameters is robust to occlusions. We show that our approach compares favorably to state-of-the-art methods while achieving real time performance (32 FPS) on a standard computer.
count=2
* Mnemonic Descent Method: A Recurrent Process Applied for End-To-End Face Alignment
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Trigeorgis_Mnemonic_Descent_Method_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Trigeorgis_Mnemonic_Descent_Method_CVPR_2016_paper.pdf)]
    * Title: Mnemonic Descent Method: A Recurrent Process Applied for End-To-End Face Alignment
    * Year: `2016`
    * Authors: George Trigeorgis, Patrick Snape, Mihalis A. Nicolaou, Epameinondas Antonakos, Stefanos Zafeiriou
    * Abstract: Cascaded regression has recently become the method of choice for solving non-linear least squares problems such as deformable image alignment. Given a sizeable training set, cascaded regression learns a set of generic rules that are sequentially applied to minimise the least squares problem. Despite the success of cascaded regression for problems such as face alignment and head pose estimation, there are several shortcomings arising in the strategies proposed thus far. Specifically, (a) the regressors are learnt independently, (b) the descent directions may cancel one another out and (c) handcrafted features (e.g., HoGs, SIFT etc.) are mainly used to drive the cascade, which may be sub-optimal for the task at hand. In this paper, we propose a combined and jointly trained convolutional recurrent neural network architecture that allows the training of an end-to-end to system that attempts to alleviate the aforementioned drawbacks. The recurrent module facilitates the joint optimisation of the regressors by assuming the cascades form a nonlinear dynamical system, in effect fully utilising the information between all cascade levels by introducing a memory unit that shares information across all levels. The convolutional module allows the network to extract features that are specialised for the task at hand and are experimentally shown to outperform hand-crafted features. We show that the application of the proposed architecture for the problem of face alignment results in a strong improvement over the current state-of-the-art.
count=2
* Towards Open World Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Bendale_Towards_Open_World_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Bendale_Towards_Open_World_2015_CVPR_paper.pdf)]
    * Title: Towards Open World Recognition
    * Year: `2015`
    * Authors: Abhijit Bendale, Terrance Boult
    * Abstract: With the of advent rich classification models and high computational power visual recognition systems have found many operational applications. Recognition in the real world poses multiple challenges that are not apparent in controlled lab environments. The datasets are dynamic and novel categories must be continuously detected and then added. At prediction time, a trained system has to deal with myriad unseen categories. Operational systems require minimum down time, even to learn. To handle these operational issues, we present the problem of Open World recognition and formally define it. We prove that thresholding sums of monotonically decreasing functions of distances in linearly transformed feature space can balance open space risk and empirical risk. Our theory extends existing algorithms for open world recognition. We present a protocol for evaluation of open world recognition systems. We present the Nearest Non-Outlier (NNO) algorithm which evolves model efficiently, adding object categories incrementally while detecting outliers and managing open space risk. We perform experiments on the ImageNet dataset with 1.2M+ images to validate the effectiveness of our method on large scale visual recognition tasks. NNO consistently yields superior results on open world recognition.
count=2
* Effective Learning-Based Illuminant Estimation Using Simple Features
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Cheng_Effective_Learning-Based_Illuminant_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Cheng_Effective_Learning-Based_Illuminant_2015_CVPR_paper.pdf)]
    * Title: Effective Learning-Based Illuminant Estimation Using Simple Features
    * Year: `2015`
    * Authors: Dongliang Cheng, Brian Price, Scott Cohen, Michael S. Brown
    * Abstract: Illumination estimation is the process of determining the chromaticity of the illumination in an imaged scene in order to remove undesirable color casts through white-balancing. While computational color constancy is a well-studied topic in computer vision, it remains challenging due to the ill-posed nature of the problem. One class of techniques relies on low-level statistical information in the image color distribution and works under various assumptions (e.g. Grey-World, White-Patch, etc). These methods have an advantage that they are simple and fast, but often do not perform well. More recent state-of-the-art methods employ learning-based techniques that produce better results, but often rely on complex features and have long evaluation and training times. In this paper, we present a learning-based method based on four simple color features and show how to use this with an ensemble of regression trees to estimate the illumination. We demonstrate that our approach is not only faster than existing learning-based methods in terms of both evaluation and training time, but also gives the best results reported to date on modern color constancy data sets.
count=2
* Finding Distractors In Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Fried_Finding_Distractors_In_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Fried_Finding_Distractors_In_2015_CVPR_paper.pdf)]
    * Title: Finding Distractors In Images
    * Year: `2015`
    * Authors: Ohad Fried, Eli Shechtman, Dan B. Goldman, Adam Finkelstein
    * Abstract: We propose a new computer vision task we call "distractor prediction." Distractors are the regions of an image that draw attention away from the main subjects and reduce the overall image quality. Removing distractors --- for example, using in-painting --- can improve the composition of an image. In this work we created two datasets of images with user annotations to identify the characteristics of distractors. We use these datasets to train an algorithm to predict distractor maps. Finally, we use our predictor to automatically enhance images.
count=2
* Understanding Classifier Errors by Examining Influential Neighbors
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Kabra_Understanding_Classifier_Errors_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Kabra_Understanding_Classifier_Errors_2015_CVPR_paper.pdf)]
    * Title: Understanding Classifier Errors by Examining Influential Neighbors
    * Year: `2015`
    * Authors: Mayank Kabra, Alice Robie, Kristin Branson
    * Abstract: Modern supervised learning algorithms can learn very accurate and complex discriminating functions. But when these classifiers fail, this complexity can also be a drawback because there is no easy, intuitive way to diagnose why they are failing and remedy the problem. This important question has received little attention. To address this problem, we propose a novel method to analyze and understand a classifier's errors. Our method centers around a measure of how much influence a training example has on the classifier's prediction for a test example. To understand why a classifier is mispredicting the label of a given test example, the user can find and review the most influential training examples that caused this misprediction, allowing them to focus their attention on relevant areas of the data space. This will aid the user in determining if and how the training data is inconsistently labeled or lacking in diversity, or if the feature representation is insufficient. As computing the influence of each training example is computationally impractical, we propose a novel distance metric to approximate influence for boosting classifiers that is fast enough to be used interactively. We also show several novel use paradigms of our distance metric. Through experiments, we show that it can be used to find incorrectly or inconsistently labeled training examples, to find specific areas of the data space that need more training data, and to gain insight into which features are missing from the current representation.
count=2
* Functional Correspondence by Matrix Completion
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Kovnatsky_Functional_Correspondence_by_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Kovnatsky_Functional_Correspondence_by_2015_CVPR_paper.pdf)]
    * Title: Functional Correspondence by Matrix Completion
    * Year: `2015`
    * Authors: Artiom Kovnatsky, Michael M. Bronstein, Xavier Bresson, Pierre Vandergheynst
    * Abstract: In this paper, we consider the problem of finding dense intrinsic correspondence between manifolds using the recently introduced functional framework. We pose the functional correspondence problem as matrix completion with manifold geometric structure and inducing functional localization with the L1 norm. We discuss efficient numerical procedures for the solution of our problem. Our method compares favorably to the accuracy of state-of-the-art correspondence algorithms on non-rigid shape matching benchmarks, and is especially advantageous in settings when only scarce data is available.
count=2
* DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Shen_DeepContour_A_Deep_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Shen_DeepContour_A_Deep_2015_CVPR_paper.pdf)]
    * Title: DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection
    * Year: `2015`
    * Authors: Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, Zhijiang Zhang
    * Abstract: Contour detection serves as the basis of a variety of computer vision tasks such as image segmentation and object recognition. The mainstream works to address this problem focus on designing engineered gradient features. In this work, we show that contour detection accuracy can be improved by instead making the use of the deep features learned from convolutional neural networks (CNNs). While rather than using the networks as a blackbox feature extractor, we customize the training strategy by partitioning contour (positive) data into subclasses and fitting each subclass by different model parameters. A new loss function, named positive-sharing loss, in which each subclass shares the loss for the whole positive class, is proposed to learn the parameters. Compared to the sofmax loss function, the proposed one, introduces an extra regularizer to emphasizes the losses for the positive and negative classes, which facilitates to explore more discriminative features. Our experimental results demonstrate that learned deep features can achieve top performance on Berkeley Segmentation Dataset and Benchmark (BSDS500) and obtain competitive cross dataset generalization result on the NYUD dataset.
count=2
* Shadow Optimization From Structured Deep Edge Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Shen_Shadow_Optimization_From_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Shen_Shadow_Optimization_From_2015_CVPR_paper.pdf)]
    * Title: Shadow Optimization From Structured Deep Edge Detection
    * Year: `2015`
    * Authors: Li Shen, Teck Wee Chua, Karianto Leman
    * Abstract: We present a novel learning-based framework for shadow detection from a single image. The local structure of shadow boundaries as well as the global interactions of the shadow and non-shadow regions remain largely unexploited by previous learning-based approaches. In this paper, we propose an efficient structured labelling framework for shadow detection from a single image. A convolutional Neural Networks framework is designed to capture the local structure information of shadow edge and to learn the most relevant features. We further propose and formulate a global shadow optimization framework which can model the complex global interactions over the shadow and light regions. Using the shadow edges detected by our proposed method, the shadow map can be solved by efficient least-square optimization. Our proposed framework is efficient and achieves state-of-the-art results on the major shadow benchmark databases collected under a variety of conditions.
count=2
* Integrating Parametric and Non-Parametric Models For Scene Labeling
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Shuai_Integrating_Parametric_and_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Shuai_Integrating_Parametric_and_2015_CVPR_paper.pdf)]
    * Title: Integrating Parametric and Non-Parametric Models For Scene Labeling
    * Year: `2015`
    * Authors: Bing Shuai, Gang Wang, Zhen Zuo, Bing Wang, Lifan Zhao
    * Abstract: We adopt Convolutional Neural Networks (CNN) as our parametric model to learn discriminative features and classifiers for local patch classification. As visually similar pixels are indistinguishable from local context, we alleviate such ambiguity by putting a global scene constraint. We estimate the global potential in a non-parametric framework. Furthermore, a large margin based CNN metric learning method is proposed for better global potential estimation. The final pixel class prediction is performed by integrating local and global beliefs. Even without any post-processing, we achieve state-of-the-art on SiftFlow and competitive results on Stanford Background benchmark.
count=2
* Fast and Robust Hand Tracking Using Detection-Guided Optimization
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Sridhar_Fast_and_Robust_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Sridhar_Fast_and_Robust_2015_CVPR_paper.pdf)]
    * Title: Fast and Robust Hand Tracking Using Detection-Guided Optimization
    * Year: `2015`
    * Authors: Srinath Sridhar, Franziska Mueller, Antti Oulasvirta, Christian Theobalt
    * Abstract: Markerless tracking of hands and fingers is a promising enabler for human-computer interaction. However, adoption has been limited because of tracking inaccuracies, incomplete coverage of motions, low framerate, complex camera setups, and high computational requirements. In this paper, we present a fast method for accurately tracking rapid and complex articulations of the hand using a single depth camera. Our algorithm uses a novel detection-guided optimization strategy that increases the robustness and speed of pose estimation. In the detection step, a randomized decision forest classifies pixels into parts of the hand. In the optimization step, a novel objective function combines the detected part labels and a Gaussian mixture representation of the depth to estimate a pose that best fits the depth. Our approach needs comparably less computational resources which makes it extremely fast (50 fps without GPU support). The approach also supports varying static, or moving, camera-to-scene arrangements. We show the benefits of our method by evaluating on public datasets and comparing against previous work.
count=2
* Layered RGBD Scene Flow Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Sun_Layered_RGBD_Scene_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Sun_Layered_RGBD_Scene_2015_CVPR_paper.pdf)]
    * Title: Layered RGBD Scene Flow Estimation
    * Year: `2015`
    * Authors: Deqing Sun, Erik B. Sudderth, Hanspeter Pfister
    * Abstract: As consumer depth sensors become widely available, estimating scene flow from RGBD sequences has received increasing attention. Although the depth information allows the recovery of 3D motion from a single view, it poses new challenges. In particular, depth boundaries are not well-aligned with RGB image edges and therefore not reliable cues to localize 2D motion boundaries. In addition, methods that extend the 2D optical flow formulation to 3D still produce large errors in occlusion regions. To better use depth for occlusion reasoning, we propose a layered RGBD scene flow method that jointly solves for the scene segmentation and the motion. Our key observation is that the noisy depth is sufficient to decide the depth ordering of layers, thereby avoiding a computational bottleneck for RGB layered methods. Furthermore, the depth enables us to estimate a per-layer 3D rigid motion to constrain the motion of each layer. Experimental results on both the Middlebury and real-world sequences demonstrate the effectiveness of the layered approach for RGBD scene flow estimation.
count=2
* Fast Action Proposals for Human Action Detection and Search
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Yu_Fast_Action_Proposals_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Yu_Fast_Action_Proposals_2015_CVPR_paper.pdf)]
    * Title: Fast Action Proposals for Human Action Detection and Search
    * Year: `2015`
    * Authors: Gang Yu, Junsong Yuan
    * Abstract: In this paper we target at generating generic action proposals in unconstrained videos. Each action proposal corresponds to a temporal series of spatial bounding boxes, i.e., a spatio-temporal video tube, which has a good potential to locate one human action. Assuming each action is performed by a human with meaningful motion, both appearance and motion cues are utilized to measure the actionness of the video tubes. After picking those spatiotemporal paths of high actionness scores, our action proposal generation is formulated as a maximum set coverage problem, where greedy search is performed to select a set of action proposals that can maximize the overall actionness score. Compared with existing action proposal approaches, our action proposals do not rely on video segmentation and can be generated in nearly real-time. Experimental results on two challenging datasets, MSRII and UCF 101, validate the superior performance of our action proposals as well as competitive results on action detection and search.
count=2
* Computing the Stereo Matching Cost With a Convolutional Neural Network
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Zbontar_Computing_the_Stereo_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Zbontar_Computing_the_Stereo_2015_CVPR_paper.pdf)]
    * Title: Computing the Stereo Matching Cost With a Convolutional Neural Network
    * Year: `2015`
    * Authors: Jure Zbontar, Yann LeCun
    * Abstract: We present a method for extracting depth information from a rectified image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset.
count=2
* Appearance-Based Gaze Estimation in the Wild
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Zhang_Appearance-Based_Gaze_Estimation_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Zhang_Appearance-Based_Gaze_Estimation_2015_CVPR_paper.pdf)]
    * Title: Appearance-Based Gaze Estimation in the Wild
    * Year: `2015`
    * Authors: Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling
    * Abstract: Appearance-based gaze estimation is believed to work well in real-world settings, but existing datasets have been collected under controlled laboratory conditions and methods have been not evaluated across multiple datasets. In this work we study appearance-based gaze estimation in the wild. We present the MPIIGaze dataset that contains 213,659 images we collected from 15 participants during natural everyday laptop use over more than three months. Our dataset is significantly more variable than existing ones with respect to appearance and illumination. We also present a method for in-the-wild appearance-based gaze estimation using multimodal convolutional neural networks that significantly outperforms state-of-the art methods in the most challenging cross-dataset evaluation. We present an extensive evaluation of several state-of-the-art image-based gaze estimation algorithms on three current datasets, including our own. This evaluation provides clear insights and allows us to identify key research challenges of gaze estimation in the wild.
count=2
* Supervised Descriptor Learning for Multi-Output Regression
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Zhen_Supervised_Descriptor_Learning_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Zhen_Supervised_Descriptor_Learning_2015_CVPR_paper.pdf)]
    * Title: Supervised Descriptor Learning for Multi-Output Regression
    * Year: `2015`
    * Authors: Xiantong Zhen, Zhijie Wang, Mengyang Yu, Shuo Li
    * Abstract: Descriptor learning has recently drawn increasing attention in computer vision, Existing algorithms are mainly developed for classification rather than for regression which however has recently emerged as a powerful tool to solve a broad range of problems, e.g., head pose estimation. In this paper, we propose a novel supervised descriptor learning (SDL) algorithm to establish a discriminative and compact feature representation for multi-output regression. By formulating as generalized low-rank approximations of matrices with a supervised manifold regularization (SMR), the SDL removes irrelevant and redundant information from raw features by transforming into a low-dimensional space under the supervision of multivariate targets. The obtained discriminative while compact descriptor largely reduces the variability and ambiguity in multi-output regression, and therefore enables more accurate and efficient multivariate estimation. We demonstrate the effectiveness of the proposed SDL algorithm on a representative multi-output regression task: head pose estimation using the benchmark Pointing'04 dataset. Experimental results show that the SDL can achieve high pose estimation accuracy and significantly outperforms state-of-the-art algorithms by an error reduction up to 27.5%. The proposed SDL algorithm provides a general descriptor learning framework in a supervised way for multi-output regression which can largely boost the performance of existing multi-output regression tasks.
count=2
* Multiscale Combinatorial Grouping
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Arbelaez_Multiscale_Combinatorial_Grouping_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Arbelaez_Multiscale_Combinatorial_Grouping_2014_CVPR_paper.pdf)]
    * Title: Multiscale Combinatorial Grouping
    * Year: `2014`
    * Authors: Pablo Arbelaez, Jordi Pont-Tuset, Jonathan T. Barron, Ferran Marques, Jitendra Malik
    * Abstract: We propose a unified approach for bottom-up hierarchical image segmentation and object candidate generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object candidates by exploring efficiently their combinatorial space. We conduct extensive experiments on both the BSDS500 and on the PASCAL 2012 segmentation datasets, showing that MCG produces state-of-the-art contours, hierarchical regions and object candidates.
count=2
* The Synthesizability of Texture Examples
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Dai_The_Synthesizability_of_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Dai_The_Synthesizability_of_2014_CVPR_paper.pdf)]
    * Title: The Synthesizability of Texture Examples
    * Year: `2014`
    * Authors: Dengxin Dai, Hayko Riemenschneider, Luc Van Gool
    * Abstract: Example-based texture synthesis (ETS) has been widely used to generate high quality textures of desired sizes from a small example. However, not all textures are equally well reproducible that way. We predict how synthesizable a particular texture is by ETS. We introduce a dataset (21,302 textures) of which all images have been annotated in terms of their synthesizability. We design a set of texture features, such as 'textureness', homogeneity, repetitiveness, and irregularity, and train a predictor using these features on the data collection. This work is the first attempt to quantify this image property, and we find that texture synthesizability can be learned and predicted. We use this insight to trim images to parts that are more synthesizable. Also we suggest which texture synthesis method is best suited to synthesise a given texture. Our approach can be seen as 'winner-uses-all': picking one method among several alternatives, ending up with an overall superior ETS method. Such strategy could also be considered for other vision tasks: rather than building an even stronger method, choose from existing methods based on some simple preprocessing.
count=2
* Randomized Max-Margin Compositions for Visual Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Eigenstetter_Randomized_Max-Margin_Compositions_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Eigenstetter_Randomized_Max-Margin_Compositions_2014_CVPR_paper.pdf)]
    * Title: Randomized Max-Margin Compositions for Visual Recognition
    * Year: `2014`
    * Authors: Angela Eigenstetter, Masato Takami, Bjorn Ommer
    * Abstract: A main theme in object detection are currently discriminative part-based models. The powerful model that combines all parts is then typically only feasible for few constituents, which are in turn iteratively trained to make them as strong as possible. We follow the opposite strategy by randomly sampling a large number of instance specific part classifiers. Due to their number, we cannot directly train a powerful classifier to combine all parts. Therefore, we randomly group them into fewer, overlapping compositions that are trained using a maximum-margin approach. In contrast to the common rationale of compositional approaches, we do not aim for semantically meaningful ensembles. Rather we seek randomized compositions that are discriminative and generalize over all instances of a category. Our approach not only localizes objects in cluttered scenes, but also explains them by parsing with compositions and their constituent parts. We conducted experiments on PASCAL VOC07, on the VOC10 evaluation server, and on the MITIndoor scene dataset. To the best of our knowledge, our randomized max-margin compositions (RM2C) are the currently best performing single class object detector using only HOG features. Moreover, the individual contributions of compositions and their parts are evaluated in separate experiments that demonstrate their potential.
count=2
* Filter Forests for Learning Data-Dependent Convolutional Kernels
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Fanello_Filter_Forests_for_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Fanello_Filter_Forests_for_2014_CVPR_paper.pdf)]
    * Title: Filter Forests for Learning Data-Dependent Convolutional Kernels
    * Year: `2014`
    * Authors: Sean Ryan Fanello, Cem Keskin, Pushmeet Kohli, Shahram Izadi, Jamie Shotton, Antonio Criminisi, Ugo Pattacini, Tim Paek
    * Abstract: We propose 'filter forests' (FF), an efficient new discriminative approach for predicting continuous variables given a signal and its context. FF can be used for general signal restoration tasks that can be tackled via convolutional filtering, where it attempts to learn the optimal filtering kernels to be applied to each data point. The model can learn both the size of the kernel and its values, conditioned on the observation and its spatial or temporal context. We show that FF compares favorably to both Markov random field based and recently proposed regression forest based approaches for labeling problems in terms of efficiency and accuracy. In particular, we demonstrate how FF can be used to learn optimal denoising filters for natural images as well as for other tasks such as depth image refinement, and 1D signal magnitude estimation. Numerous experiments and quantitative comparisons show that FFs achieve accuracy at par or superior to recent state of the art techniques, while being several orders of magnitude faster.
count=2
* One Millisecond Face Alignment with an Ensemble of Regression Trees
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Kazemi_One_Millisecond_Face_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Kazemi_One_Millisecond_Face_2014_CVPR_paper.pdf)]
    * Title: One Millisecond Face Alignment with an Ensemble of Regression Trees
    * Year: `2014`
    * Authors: Vahid Kazemi, Josephine Sullivan
    * Abstract: This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.
count=2
* Salient Region Detection via High-Dimensional Color Transform
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Kim_Salient_Region_Detection_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Kim_Salient_Region_Detection_2014_CVPR_paper.pdf)]
    * Title: Salient Region Detection via High-Dimensional Color Transform
    * Year: `2014`
    * Authors: Jiwhan Kim, Dongyoon Han, Yu-Wing Tai, Junmo Kim
    * Abstract: In this paper, we introduce a novel technique to automatically detect salient regions of an image via high-dimensional color transform. Our main idea is to represent a saliency map of an image as a linear combination of high-dimensional color space where salient regions and backgrounds can be distinctively separated. This is based on an observation that salient regions often have distinctive colors compared to the background in human perception, but human perception is often complicated and highly nonlinear. By mapping a low dimensional RGB color to a feature vector in a high-dimensional color space, we show that we can linearly separate the salient regions from the background by finding an optimal linear combination of color coefficients in the high-dimensional color space. Our high dimensional color space incorporates multiple color representations including RGB, CIELab, HSV and with gamma corrections to enrich its representative power. Our experimental results on three benchmark datasets show that our technique is effective, and it is computationally efficient in comparison to previous state-of-the-art techniques.
count=2
* Visual Tracking via Probability Continuous Outlier Model
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Wang_Visual_Tracking_via_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Wang_Visual_Tracking_via_2014_CVPR_paper.pdf)]
    * Title: Visual Tracking via Probability Continuous Outlier Model
    * Year: `2014`
    * Authors: Dong Wang, Huchuan Lu
    * Abstract: In this paper, we present a novel online visual tracking method based on linear representation. First, we present a novel probability continuous outlier model (PCOM) to depict the continuous outliers that occur in the linear representation model. In the proposed model, the element of the noisy observation sample can be either represented by a PCA subspace with small Guassian noise or treated as an arbitrary value with a uniform prior, in which the spatial consistency prior is exploited by using a binary Markov random field model. Then, we derive the objective function of the PCOM method, the solution of which can be iteratively obtained by the outlier-free least squares and standard max-flow/min-cut steps. Finally, based on the proposed PCOM method, we design an effective observation likelihood function and a simple update scheme for visual tracking. Both qualitative and quantitative evaluations demonstrate that our tracker achieves very favorable performance in terms of both accuracy and speed.
count=2
* Leveraging Hierarchical Parametric Networks for Skeletal Joints Based Action Segmentation and Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Wu_Leveraging_Hierarchical_Parametric_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Wu_Leveraging_Hierarchical_Parametric_2014_CVPR_paper.pdf)]
    * Title: Leveraging Hierarchical Parametric Networks for Skeletal Joints Based Action Segmentation and Recognition
    * Year: `2014`
    * Authors: Di Wu, Ling Shao
    * Abstract: Over the last few years, with the immense popularity of the Kinect, there has been renewed interest in developing methods for human gesture and action recognition from 3D skeletal data. A number of approaches have been proposed to extract representative features from 3D skeletal data, most commonly hard wired geometric or bio-inspired shape context features. We propose a hierarchial dynamic framework that first extracts high level skeletal joints features and then uses the learned representation for estimating emission probability to infer action sequences. Currently gaussian mixture models are the dominant technique for modeling the emission distribution of hidden Markov models. We show that better action recognition using skeletal features can be achieved by replacing gaussian mixture models by deep neural networks that contain many layers of features to predict probability distributions over states of hidden Markov models. The framework can be easily extended to include a ergodic state to segment and recognize actions simultaneously.
count=2
* Towards Multi-view and Partially-Occluded Face Alignment
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Xing_Towards_Multi-view_and_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Xing_Towards_Multi-view_and_2014_CVPR_paper.pdf)]
    * Title: Towards Multi-view and Partially-Occluded Face Alignment
    * Year: `2014`
    * Authors: Junliang Xing, Zhiheng Niu, Junshi Huang, Weiming Hu, Shuicheng Yan
    * Abstract: We present a robust model to locate facial landmarks under different views and possibly severe occlusions. To build reliable relationships between face appearance and shape with large view variations, we propose to formulate face alignment as an L1-induced Stagewise Relational Dictionary (SRD) learning problem. During each training stage, the SRD model learns a relational dictionary to capture consistent relationships between face appearance and shape, which are respectively modeled by the pose-indexed image features and the shape displacements for current estimated landmarks. During testing, the SRD model automatically selects a sparse set of the most related shape displacements for the testing face and uses them to refine its shape iteratively. To locate facial landmarks under occlusions, we further propose to learn an occlusion dictionary to model different kinds of partial face occlusions. By deploying the occlusion dictionary into the SRD model, the alignment performance for occluded faces can be further improved. Our algorithm is simple, effective, and easy to implement. Extensive experiments on two benchmark datasets and two newly built datasets have demonstrated its superior performances over the state-of-the-art methods, especially for faces with large view variations and/or occlusions.
count=2
* Fully-Connected CRFs with Non-Parametric Pairwise Potential
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Campbell_Fully-Connected_CRFs_with_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Campbell_Fully-Connected_CRFs_with_2013_CVPR_paper.pdf)]
    * Title: Fully-Connected CRFs with Non-Parametric Pairwise Potential
    * Year: `2013`
    * Authors: Neill D.F. Campbell, Kartic Subr, Jan Kautz
    * Abstract: Conditional Random Fields (CRFs) are used for diverse tasks, ranging from image denoising to object recognition. For images, they are commonly defined as a graph with nodes corresponding to individual pixels and pairwise links that connect nodes to their immediate neighbors. Recent work has shown that fully-connected CRFs, where each node is connected to every other node, can be solved efficiently under the restriction that the pairwise term is a Gaussian kernel over a Euclidean feature space. In this paper, we generalize the pairwise terms to a non-linear dissimilarity measure that is not required to be a distance metric. To this end, we propose a density estimation technique to derive conditional pairwise potentials in a nonparametric manner. We then use an efficient embedding technique to estimate an approximate Euclidean feature space for these potentials, in which the pairwise term can still be expressed as a Gaussian kernel. We demonstrate that the use of non-parametric models for the pairwise interactions, conditioned on the input data, greatly increases expressive power whilst maintaining efficient inference.
count=2
* Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Gupta_Perceptual_Organization_and_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Gupta_Perceptual_Organization_and_2013_CVPR_paper.pdf)]
    * Title: Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images
    * Year: `2013`
    * Authors: Saurabh Gupta, Pablo Arbelaez, Jitendra Malik
    * Abstract: We address the problems of contour detection, bottomup grouping and semantic segmentation using RGB-D data. We focus on the challenging setting of cluttered indoor scenes, and evaluate our approach on the recently introduced NYU-Depth V2 (NYUD2) dataset [27]. We propose algorithms for object boundary detection and hierarchical segmentation that generalize the gP b ucm approach of [2] by making effective use of depth information. We show that our system can label each contour with its type (depth, normal or albedo). We also propose a generic method for long-range amodal completion of surfaces and show its effectiveness in grouping. We then turn to the problem of semantic segmentation and propose a simple approach that classifies superpixels into the 40 dominant object categories in NYUD2. We use both generic and class-specific features to encode the appearance and geometry of objects. We also show how our approach can be used for scene classification, and how this contextual information in turn improves object recognition. In all of these tasks, we report significant improvements over the state-of-the-art.
count=2
* Pixel-Level Hand Detection in Ego-centric Videos
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Li_Pixel-Level_Hand_Detection_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Li_Pixel-Level_Hand_Detection_2013_CVPR_paper.pdf)]
    * Title: Pixel-Level Hand Detection in Ego-centric Videos
    * Year: `2013`
    * Authors: Cheng Li, Kris M. Kitani
    * Abstract: We address the task of pixel-level hand detection in the context of ego-centric cameras. Extracting hand regions in ego-centric videos is a critical step for understanding handobject manipulation and analyzing hand-eye coordination. However, in contrast to traditional applications of hand detection, such as gesture interfaces or sign-language recognition, ego-centric videos present new challenges such as rapid changes in illuminations, significant camera motion and complex hand-object manipulations. To quantify the challenges and performance in this new domain, we present a fully labeled indoor/outdoor ego-centric hand detection benchmark dataset containing over 200 million labeled pixels, which contains hand images taken under various illumination conditions. Using both our dataset and a publicly available ego-centric indoors dataset, we give extensive analysis of detection performance using a wide range of local appearance features. Our analysis highlights the effectiveness of sparse features and the importance of modeling global illumination. We propose a modeling strategy based on our findings and show that our model outperforms several baseline approaches.
count=2
* Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Shotton_Scene_Coordinate_Regression_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Shotton_Scene_Coordinate_Regression_2013_CVPR_paper.pdf)]
    * Title: Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images
    * Year: `2013`
    * Authors: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon
    * Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel's correspondence to 3D points in the scene's world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.
count=2
* Deep Convolutional Network Cascade for Facial Point Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Sun_Deep_Convolutional_Network_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Sun_Deep_Convolutional_Network_2013_CVPR_paper.pdf)]
    * Title: Deep Convolutional Network Cascade for Facial Point Detection
    * Year: `2013`
    * Authors: Yi Sun, Xiaogang Wang, Xiaoou Tang
    * Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability 1 .
count=2
* Supervised Descent Method and Its Applications to Face Alignment
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Xiong_Supervised_Descent_Method_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Xiong_Supervised_Descent_Method_2013_CVPR_paper.pdf)]
    * Title: Supervised Descent Method and Its Applications to Face Alignment
    * Year: `2013`
    * Authors: Xuehan Xiong, Fernando De la Torre
    * Abstract: Many computer vision problems (e.g., camera calibration, image alignment, structure from motion) are solved through a nonlinear optimization method. It is generally accepted that 2 nd order descent methods are the most robust, fast and reliable approaches for nonlinear optimization of a general smooth function. However, in the context of computer vision, 2 nd order descent methods have two main drawbacks: (1) The function might not be analytically differentiable and numerical approximations are impractical. (2) The Hessian might be large and not positive definite. To address these issues, this paper proposes a Supervised Descent Method (SDM) for minimizing a Non-linear Least Squares (NLS) function. During training, the SDM learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing, SDM minimizes the NLS objective using the learned descent directions without computing the Jacobian nor the Hessian. We illustrate the benefits of our approach in synthetic and real examples, and show how SDM achieves state-ofthe-art performance in the problem of facial feature detection. The code is available at www.humansensing.cs. cmu.edu/intraface.
count=2
* GrowCLIP: Data-Aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-Training
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Deng_GrowCLIP_Data-Aware_Automatic_Model_Growing_for_Large-scale_Contrastive_Language-Image_Pre-Training_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_GrowCLIP_Data-Aware_Automatic_Model_Growing_for_Large-scale_Contrastive_Language-Image_Pre-Training_ICCV_2023_paper.pdf)]
    * Title: GrowCLIP: Data-Aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-Training
    * Year: `2023`
    * Authors: Xinchi Deng, Han Shi, Runhui Huang, Changlin Li, Hang Xu, Jianhua Han, James Kwok, Shen Zhao, Wei Zhang, Xiaodan Liang
    * Abstract: Cross-modal pre-training has shown impressive performance on a wide range of downstream tasks, benefiting from massive image-text pairs collected from the Internet. In practice, online data are growing constantly, highlighting the importance of the ability of pre-trained model to learn from data that is continuously growing. Existing works on cross-modal pre-training mainly focus on training a network with fixed architecture. However, it is impractical to limit the model capacity when considering the continuously growing nature of pre-training data in real-world applications. On the other hand, it is important to utilize the knowledge in current model to obtain efficient training and better performance. To address the above issues, in this paper, we propose GrowCLIP, a data-driven automatic model growing algorithm for contrastive language-image pre-training with continuous image-text pairs as input. Specially, we adopt a dynamic growth space and seek out the optimal architecture at each growth step to adapt to online learning scenarios. And the shared encoder is proposed in our growth space to enhance the degree of cross-modal fusion. Besides, we explore the effect of growth in different dimensions, which could provide future references for the design of cross-modal model architecture. Finally, we employ parameter inheriting with momentum (PIM) to maintain the previous knowledge and address the issue of local minimum dilemma. Compared with the existing methods, GrowCLIP improve 2.3% average top-1 accuracy on zero-shot image classification of 9 downstream tasks. As for zero-shot image retrieval, GrowCLIP can improve 1.2% for top-1 image-to-text recall on Flickr30K dataset.
count=2
* MUter: Machine Unlearning on Adversarially Trained Models
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Liu_MUter_Machine_Unlearning_on_Adversarially_Trained_Models_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_MUter_Machine_Unlearning_on_Adversarially_Trained_Models_ICCV_2023_paper.pdf)]
    * Title: MUter: Machine Unlearning on Adversarially Trained Models
    * Year: `2023`
    * Authors: Junxu Liu, Mingsheng Xue, Jian Lou, Xiaoyu Zhang, Li Xiong, Zhan Qin
    * Abstract: Machine unlearning is an emerging task of removing the influence of selected training datapoints from a trained model upon data deletion requests, which echoes the widely enforced data regulations mandating the Right to be Forgotten. Many unlearning methods have been proposed recently, achieving significant efficiency gains over the naive baseline of retraining from scratch. However, existing methods focus exclusively on unlearning from standard training models and do not apply to adversarial training models (ATMs) despite their popularity as effective defenses against adversarial examples. During adversarial training, the training data are involved in not only an outer loop for minimizing the training loss, but also an inner loop for generating the adversarial perturbation. Such bi-level optimization greatly complicates the influence measure for the data to be deleted and renders the unlearning more challenging than standard model training with single-level optimization. This paper proposes a new approach called MUter for unlearning from ATMs. We derive a closed-form unlearning step underpinned by a total Hessian-related data influence measure, while existing methods can mis-capture the data influence associated with the indirect Hessian part. We further alleviate the computational cost by introducing a series of approximations and conversions to avoid the most computationally demanding parts of Hessian inversions. The efficiency and effectiveness of MUter have been validated through experiments on four datasets using both linear and neural network models.
count=2
* Gramian Attention Heads are Strong yet Efficient Vision Learners
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Ryu_Gramian_Attention_Heads_are_Strong_yet_Efficient_Vision_Learners_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Ryu_Gramian_Attention_Heads_are_Strong_yet_Efficient_Vision_Learners_ICCV_2023_paper.pdf)]
    * Title: Gramian Attention Heads are Strong yet Efficient Vision Learners
    * Year: `2023`
    * Authors: Jongbin Ryu, Dongyoon Han, Jongwoo Lim
    * Abstract: We introduce a novel architecture design that enhances expressiveness by incorporating multiple head classifiers (i.e., classification heads) instead of relying on channel expansion or additional building blocks. Our approach employs attention-based aggregation, utilizing pairwise feature similarity to enhance multiple lightweight heads with minimal resource overhead. We compute the Gramian matrices to reinforce class tokens in an attention layer for each head. This enables the heads to learn more discriminative representations, enhancing their aggregation capabilities. Furthermore, we propose a learning algorithm that encourages heads to complement each other by reducing correlation for aggregation. Our models eventually surpass state-of-the-art CNNs and ViTs regarding the accuracy-throughput trade-off on ImageNet-1K and deliver remarkable performance across various downstream tasks, such as COCO object instance segmentation, ADE20k semantic segmentation, and fine-grained visual classification datasets. The effectiveness of our framework is substantiated by practical experimental results and further underpinned by generalization error bound. We release the code publicly at: https://github.com/Lab-LVM/imagenet-models.
count=2
* End-to-End Detection and Pose Estimation of Two Interacting Hands
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Kim_End-to-End_Detection_and_Pose_Estimation_of_Two_Interacting_Hands_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_End-to-End_Detection_and_Pose_Estimation_of_Two_Interacting_Hands_ICCV_2021_paper.pdf)]
    * Title: End-to-End Detection and Pose Estimation of Two Interacting Hands
    * Year: `2021`
    * Authors: Dong Uk Kim, Kwang In Kim, Seungryul Baek
    * Abstract: Three dimensional hand pose estimation has reached a level of maturity, enabling real-world applications for single-hand cases. However, accurate estimation of the pose of two closely interacting hands still remains a challenge as in this case, one hand often occludes the other. We present a new algorithm that accurately estimates hand poses in such a challenging scenario. The crux of our algorithm lies in a framework that jointly trains the estimators of interacting hands, leveraging their inter-dependence. Further, we employ a GAN-type discriminator of interacting hand pose that helps avoid physically implausible configurations, e.g intersecting fingers, and exploit the visibility of joints to improve intermediate 2D pose estimation. We incorporate them into a single model that learns to detect hands and estimate their pose based on a unified criterion of pose estimation accuracy. To our knowledge, this is the first attempt to build an end-to-end network that detects and estimates the pose of two closely interacting hands (as well as single hands). In the experiments with three datasets representing challenging real-world scenarios, our algorithm demonstrated significant and consistent performance improvements over state-of-the-arts.
count=2
* Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Larsson_Fine-Grained_Segmentation_Networks_Self-Supervised_Segmentation_for_Improved_Long-Term_Visual_Localization_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Larsson_Fine-Grained_Segmentation_Networks_Self-Supervised_Segmentation_for_Improved_Long-Term_Visual_Localization_ICCV_2019_paper.pdf)]
    * Title: Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization
    * Year: `2019`
    * Authors: Mans Larsson,  Erik Stenborg,  Carl Toft,  Lars Hammarstrand,  Torsten Sattler,  Fredrik Kahl
    * Abstract: Long-term visual localization is the problem of estimating the camera pose of a given query image in a scene whose appearance changes over time. It is an important problem in practice that is, for example, encountered in autonomous driving. In order to gain robustness to such changes, long-term localization approaches often use segmantic segmentations as an invariant scene representation, as the semantic meaning of each scene part should not be affected by seasonal and other changes. However, these representations are typically not very discriminative due to the very limited number of available classes. In this paper, we propose a novel neural network, the Fine-Grained Segmentation Network (FGSN), that can be used to provide image segmentations with a larger number of labels and can be trained in a self-supervised fashion. In addition, we show how FGSNs can be trained to output consistent labels across seasonal changes. We show through extensive experiments that integrating the fine-grained segmentations produced by our FGSNs into existing localization algorithms leads to substantial improvements in localization performance.
count=2
* CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_CDPN_Coordinates-Based_Disentangled_Pose_Network_for_Real-Time_RGB-Based_6-DoF_Object_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_CDPN_Coordinates-Based_Disentangled_Pose_Network_for_Real-Time_RGB-Based_6-DoF_Object_ICCV_2019_paper.pdf)]
    * Title: CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation
    * Year: `2019`
    * Authors: Zhigang Li,  Gu Wang,  Xiangyang Ji
    * Abstract: 6-DoF object pose estimation from a single RGB image is a fundamental and long-standing problem in computer vision. Current leading approaches solve it by training deep networks to either regress both rotation and translation from image directly or to construct 2D-3D correspondences and further solve them via PnP indirectly. We argue that rotation and translation should be treated differently for their significant difference. In this work, we propose a novel 6-DoF pose estimation approach: Coordinates-based Disentangled Pose Network (CDPN), which disentangles the pose to predict rotation and translation separately to achieve highly accurate and robust pose estimation. Our method is flexible, efficient, highly accurate and can deal with texture-less and occluded objects. Extensive experiments on LINEMOD and Occlusion datasets are conducted and demonstrate the superiority of our approach. Concretely, our approach significantly exceeds the state-of-the- art RGB-based methods on commonly used metrics.
count=2
* DeepVCP: An End-to-End Deep Neural Network for Point Cloud Registration
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Lu_DeepVCP_An_End-to-End_Deep_Neural_Network_for_Point_Cloud_Registration_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_DeepVCP_An_End-to-End_Deep_Neural_Network_for_Point_Cloud_Registration_ICCV_2019_paper.pdf)]
    * Title: DeepVCP: An End-to-End Deep Neural Network for Point Cloud Registration
    * Year: `2019`
    * Authors: Weixin Lu,  Guowei Wan,  Yao Zhou,  Xiangyu Fu,  Pengfei Yuan,  Shiyu Song
    * Abstract: We present DeepVCP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the interference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable registration accuracy and runtime efficiency to the state-of-the-art geometry-based methods, but with higher robustness to inaccurate initial poses. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method make it attractive to the substantial applications relying on the point cloud registration task.
count=2
* Unsupervised Deep Learning for Structured Shape Matching
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Roufosse_Unsupervised_Deep_Learning_for_Structured_Shape_Matching_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Roufosse_Unsupervised_Deep_Learning_for_Structured_Shape_Matching_ICCV_2019_paper.pdf)]
    * Title: Unsupervised Deep Learning for Structured Shape Matching
    * Year: `2019`
    * Authors: Jean-Michel Roufosse,  Abhishek Sharma,  Maks Ovsjanikov
    * Abstract: We present a novel method for computing correspondences across 3D shapes using unsupervised learning. Our method computes a non-linear transformation of given descriptor functions, while optimizing for global structural properties of the resulting maps, such as their bijectivity or approximate isometry. To this end, we use the functional maps framework, and build upon the recent FMNet architecture for descriptor learning. Unlike that approach, however, we show that learning can be done in a purely unsupervised setting, without having access to any ground truth correspondences. This results in a very general shape matching method that we call SURFMNet for Spectral Unsupervised FMNet, and which can be used to establish correspondences within 3D shape collections without any prior information. We demonstrate on a wide range of challenging benchmarks, that our approach leads to state-of-the-art results compared to the existing unsupervised methods and achieves results that are comparable even to the supervised learning techniques. Moreover, our framework is an order of magnitude faster, and does not rely on geodesic distance computation or expensive post-processing.
count=2
* Is This the Right Place? Geometric-Semantic Pose Verification for Indoor Visual Localization
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Taira_Is_This_the_Right_Place_Geometric-Semantic_Pose_Verification_for_Indoor_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Taira_Is_This_the_Right_Place_Geometric-Semantic_Pose_Verification_for_Indoor_ICCV_2019_paper.pdf)]
    * Title: Is This the Right Place? Geometric-Semantic Pose Verification for Indoor Visual Localization
    * Year: `2019`
    * Authors: Hajime Taira,  Ignacio Rocco,  Jiri Sedlar,  Masatoshi Okutomi,  Josef Sivic,  Tomas Pajdla,  Torsten Sattler,  Akihiko Torii
    * Abstract: Visual localization in large and complex indoor scenes, dominated by weakly textured rooms and repeating geometric patterns, is a challenging problem with high practical relevance for applications such as Augmented Reality and robotics. To handle the ambiguities arising in this scenario, a common strategy is, first, to generate multiple estimates for the camera pose from which a given query image was taken. The pose with the largest geometric consistency with the query image, e.g., in the form of an inlier count, is then selected in a second stage. While a significant amount of research has concentrated on the first stage, there has been considerably less work on the second stage. In this paper, we thus focus on pose verification. We show that combining different modalities, namely appearance, geometry, and semantics, considerably boosts pose verification and consequently pose accuracy. We develop multiple hand-crafted as well as a trainable approach to join into the geometric-semantic verification and show significant improvements over state-of-the-art on a very challenging indoor dataset.
count=2
* Multi-Stage Pathological Image Classification Using Semantic Segmentation
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Takahama_Multi-Stage_Pathological_Image_Classification_Using_Semantic_Segmentation_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Takahama_Multi-Stage_Pathological_Image_Classification_Using_Semantic_Segmentation_ICCV_2019_paper.pdf)]
    * Title: Multi-Stage Pathological Image Classification Using Semantic Segmentation
    * Year: `2019`
    * Authors: Shusuke Takahama,  Yusuke Kurose,  Yusuke Mukuta,  Hiroyuki Abe,  Masashi Fukayama,  Akihiko Yoshizawa,  Masanobu Kitagawa,  Tatsuya Harada
    * Abstract: Histopathological image analysis is an essential process for the discovery of diseases such as cancer. However, it is challenging to train CNN on whole slide images (WSIs) of gigapixel resolution considering the available memory capacity. Most of the previous works divide high resolution WSIs into small image patches and separately input them into the model to classify it as a tumor or a normal tissue. However, patch-based classification uses only patch-scale local information but ignores the relationship between neighboring patches. If we consider the relationship of neighboring patches and global features, we can improve the classification performance. In this paper, we propose a new model structure combining the patch-based classification model and whole slide-scale segmentation model in order to improve the prediction performance of automatic pathological diagnosis. We extract patch features from the classification model and input them into the segmentation model to obtain a whole slide tumor probability heatmap. The classification model considers patch-scale local features, and the segmentation model can take global information into account. We also propose a new optimization method that retains gradient information and trains the model partially for end-to-end learning with limited GPU memory capacity. We apply our method to the tumor/normal prediction on WSIs and the classification performance is improved compared with the conventional patch-based method.
count=2
* Learn to Scale: Generating Multipolar Normalized Density Maps for Crowd Counting
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Learn_to_Scale_Generating_Multipolar_Normalized_Density_Maps_for_Crowd_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Learn_to_Scale_Generating_Multipolar_Normalized_Density_Maps_for_Crowd_ICCV_2019_paper.pdf)]
    * Title: Learn to Scale: Generating Multipolar Normalized Density Maps for Crowd Counting
    * Year: `2019`
    * Authors: Chenfeng Xu,  Kai Qiu,  Jianlong Fu,  Song Bai,  Yongchao Xu,  Xiang Bai
    * Abstract: Dense crowd counting aims to predict thousands of human instances from an image, by calculating integrals of a density map over image pixels. Existing approaches mainly suffer from the extreme density variations. Such density pattern shift poses challenges even for multi-scale model ensembling. In this paper, we propose a simple yet effective approach to tackle this problem. First, a patch-level density map is extracted by a density estimation model and further grouped into several density levels which are determined over full datasets. Second, each patch density map is automatically normalized by an online center learning strategy with a multipolar center loss. Such a design can significantly condense the density distribution into several clusters, and enable that the density variance can be learned by a single model. Extensive experiments demonstrate the superiority of the proposed method. Our work outperforms the state-of-the-art by 4.2%, 14.3%, 27.1% and 20.1% in MAE, on the ShanghaiTech Part A, ShanghaiTech Part B, UCF_CC_50 and UCF-QNRF datasets, respectively.
count=2
* Deep TextSpotter: An End-To-End Trainable Scene Text Localization and Recognition Framework
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Busta_Deep_TextSpotter_An_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Busta_Deep_TextSpotter_An_ICCV_2017_paper.pdf)]
    * Title: Deep TextSpotter: An End-To-End Trainable Scene Text Localization and Recognition Framework
    * Year: `2017`
    * Authors: Michal Busta, Lukas Neumann, Jiri Matas
    * Abstract: A method for scene text localization and recognition is proposed. The novelties include: training of both text detection and recognition in a single end-to-end pass, the structure of the recognition CNN and the geometry of its input layer that preserves the aspect of the text and adapts its resolution to the data. The proposed method achieves state-of-the-art accuracy in the end-to-end text recognition on two standard datasets - ICDAR 2013 and ICDAR 2015, whilst being an order of magnitude faster than competing methods - the whole pipeline runs at 10 frames per second on an NVidia K80 GPU.
count=2
* Look, Perceive and Segment: Finding the Salient Objects in Images via Two-Stream Fixation-Semantic CNNs
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Chen_Look_Perceive_and_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Look_Perceive_and_ICCV_2017_paper.pdf)]
    * Title: Look, Perceive and Segment: Finding the Salient Objects in Images via Two-Stream Fixation-Semantic CNNs
    * Year: `2017`
    * Authors: Xiaowu Chen, Anlin Zheng, Jia Li, Feng Lu
    * Abstract: Recently, CNN-based models have achieved remarkable success in image-based salient object detection (SOD). In these models, a key issue is to find a proper network architecture that best fits for the task of SOD. Toward this end, this paper proposes two-stream fixation-semantic CNNs, whose architecture is inspired by the fact that salient objects in complex images can be unambiguously annotated by selecting the pre-segmented semantic objects that receive the highest fixation density in eye-tracking experiments. In the two-stream CNNs, a fixation stream is pre-trained on eye-tracking data whose architecture well fits for the task of fixation prediction, and a semantic stream is pre-trained on images with semantic tags that has a proper architecture for semantic perception. By fusing these two streams into an inception-segmentation module and jointly fine-tuning them on images with manually annotated salient objects, the proposed networks show impressive performance in segmenting salient objects. Experimental results show that our approach outperforms 10 state-of-the-art models (5 deep, 5 non-deep) on 4 datasets.
count=2
* A Two Stream Siamese Convolutional Neural Network for Person Re-Identification
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Chung_A_Two_Stream_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Chung_A_Two_Stream_ICCV_2017_paper.pdf)]
    * Title: A Two Stream Siamese Convolutional Neural Network for Person Re-Identification
    * Year: `2017`
    * Authors: Dahjung Chung, Khalid Tahboub, Edward J. Delp
    * Abstract: Person re-identification is an important task in video surveillance systems. It can be formally defined as establishing the correspondence between images of a person taken from different cameras at different times. In this pa- per, we present a two stream convolutional neural network where each stream is a Siamese network. This architecture can learn spatial and temporal information separately. We also propose a weighted two stream training objective function which combines the Siamese cost of the spatial and temporal streams with the objective of predicting a person's identity. We evaluate our proposed method on the publicly available PRID2011 and iLIDS-VID datasets and demonstrate the efficacy of our proposed method. On average, the top rank matching accuracy is 4% higher than the accuracy achieved by the cross-view quadratic discriminant analysis used in combination with the hierarchical Gaussian descriptor (GOG+XQDA), and 5% higher than the recurrent neural network method.
count=2
* Factorized Bilinear Models for Image Recognition
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Li_Factorized_Bilinear_Models_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Li_Factorized_Bilinear_Models_ICCV_2017_paper.pdf)]
    * Title: Factorized Bilinear Models for Image Recognition
    * Year: `2017`
    * Authors: Yanghao Li, Naiyan Wang, Jiaying Liu, Xiaodi Hou
    * Abstract: Although Deep Convolutional Neural Networks (CNNs) have liberated their power in various computer vision tasks, the most important components of CNN, convolutional layers and fully connected layers, are still limited to linear transformations. In this paper, we propose a novel Factorized Bilinear (FB) layer to model the pairwise feature interactions by considering the quadratic terms in the transformations. Compared with existing methods that tried to incorporate complex non-linearity structures into CNNs, the factorized parameterization makes our FB layer only require a linear increase of parameters and affordable computational cost. To further reduce the risk of overfitting of the FB layer, a specific remedy called DropFactor is devised during the training process. We also analyze the connection between FB layer and some existing models, and show FB layer is a generalization to them. Finally, we validate the effectiveness of FB layer on several widely adopted datasets including CIFAR-10, CIFAR-100 and ImageNet, and demonstrate superior results compared with various state-of-the-art deep models.
count=2
* Generating High-Quality Crowd Density Maps Using Contextual Pyramid CNNs
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Sindagi_Generating_High-Quality_Crowd_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Sindagi_Generating_High-Quality_Crowd_ICCV_2017_paper.pdf)]
    * Title: Generating High-Quality Crowd Density Maps Using Contextual Pyramid CNNs
    * Year: `2017`
    * Authors: Vishwanath A. Sindagi, Vishal M. Patel
    * Abstract: We present a novel method called Contextual Pyramid CNN (CP-CNN) for generating high-quality crowd density and count estimation by explicitly incorporating global and local contextual information of crowd images. The proposed CP-CNN consists of four modules: Global Context Estimator (GCE), Local Context Estimator (LCE), Density Map Estimator (DME) and a Fusion-CNN (F-CNN). GCE is a VGG-16 based CNN that encodes global context and it is trained to classify input images into different density classes, whereas LCE is another CNN that encodes local context information and it is trained to perform patch-wise classification of input images into different density classes. DME is a multi-column architecture-based CNN that aims to generate high-dimensional feature maps from the input image which are fused with the contextual information estimated by GCE and LCE using F-CNN. To generate high resolution and high-quality density maps, F-CNN uses a set of convolutional and fractionally-strided convolutional layers and it is trained along with the DME in an end-to-end fashion using a combination of adversarial loss and pixel-level Euclidean loss. Extensive experiments on highly challenging datasets show that the proposed method achieves significant improvements over the state-of-the-art methods.
count=2
* Real-Time Monocular Pose Estimation of 3D Objects Using Temporally Consistent Local Color Histograms
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Tjaden_Real-Time_Monocular_Pose_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Tjaden_Real-Time_Monocular_Pose_ICCV_2017_paper.pdf)]
    * Title: Real-Time Monocular Pose Estimation of 3D Objects Using Temporally Consistent Local Color Histograms
    * Year: `2017`
    * Authors: Henning Tjaden, Ulrich Schwanecke, Elmar Schomer
    * Abstract: We present a novel approach to 6DOF pose estimation and segmentation of rigid 3D objects using a single monocular RGB camera based on temporally consistent, local color histograms. We show that this approach outperforms previous methods in cases of cluttered backgrounds, heterogenous objects, and occlusions. The proposed histograms can be used as statistical object descriptors within a template matching strategy for pose recovery after temporary tracking loss e. g. caused by massive occlusion or if the object leaves the camera's field of view. The descriptors can be trained online within a couple of seconds moving a handheld object in front of a camera. During the training stage, our approach is already capable to recover from accidental tracking loss. We demonstrate the performance of our method in comparison to the state of the art in different challenging experiments including a popular public data set.
count=2
* Image-Based Localization Using LSTMs for Structured Feature Correlation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Walch_Image-Based_Localization_Using_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Walch_Image-Based_Localization_Using_ICCV_2017_paper.pdf)]
    * Title: Image-Based Localization Using LSTMs for Structured Feature Correlation
    * Year: `2017`
    * Authors: Florian Walch, Caner Hazirbas, Laura Leal-Taixe, Torsten Sattler, Sebastian Hilsenbeck, Daniel Cremers
    * Abstract: In this work we propose a new CNN+LSTM architecture for camera pose regression for indoor and outdoor scenes. CNNs allow us to learn suitable feature representations for localization that are robust against motion blur and illumination changes. We make use of LSTM units on the CNN output, which play the role of a structured dimensionality reduction on the feature vector, leading to drastic improvements in localization performance. We provide extensive quantitative comparison of CNN-based and SIFT-based localization methods, showing the weaknesses and strengths of each. Furthermore, we present a new large-scale indoor dataset with accurate ground truth from a laser scanner. Experimental results on both indoor and outdoor public datasets show our method outperforms existing deep architectures, and can localize images in hard conditions, e.g., in the presence of mostly textureless surfaces, where classic SIFT-based methods fail.
count=2
* Saliency Pattern Detection by Ranking Structured Trees
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Saliency_Pattern_Detection_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Saliency_Pattern_Detection_ICCV_2017_paper.pdf)]
    * Title: Saliency Pattern Detection by Ranking Structured Trees
    * Year: `2017`
    * Authors: Lei Zhu, Haibin Ling, Jin Wu, Huiping Deng, Jin Liu
    * Abstract: In this paper we propose a new salient object detection method via structured label prediction. By learning appearance features in rectangular regions, our structural region representation encodes the local saliency distribution with a matrix of binary labels. We show that the linear combination of structured labels can well model the saliency distribution in local regions. Representing region saliency with structured labels has two advantages: 1) it connects the label assignment of all enclosed pixels, which produces a smooth saliency prediction; and 2) regular-shaped nature of structured labels enables well definition of traditional cues such as regional properties and center surround contrast, and these cues help to build meaningful and informative saliency measures. To measure the consistency between a structured label and the corresponding saliency distribution, we further propose an adaptive label ranking algorithm using proposals that are generated by a CNN model. Finally, we introduce a K-NN enhanced graph representation for saliency propagation, which is more favorable for our task than the widely-used adjacent-graph-based ones. Experimental results demonstrate the effectiveness of our proposed method on six popular benchmarks compared with state-of-the-art approaches.
count=2
* Weakly-Supervised Structured Output Learning With Flexible and Latent Graphs Using High-Order Loss Functions
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Carneiro_Weakly-Supervised_Structured_Output_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Carneiro_Weakly-Supervised_Structured_Output_ICCV_2015_paper.pdf)]
    * Title: Weakly-Supervised Structured Output Learning With Flexible and Latent Graphs Using High-Order Loss Functions
    * Year: `2015`
    * Authors: Gustavo Carneiro, Tingying Peng, Christine Bayer, Nassir Navab
    * Abstract: We introduce two new structured output models that use a latent graph, which is flexible in terms of the number of nodes and structure, where the training process minimises a high-order loss function using a weakly annotated training set. These models are developed in the context of microscopy imaging of malignant tumours, where the estimation of the number and proportion of classes of microcirculatory supply units (MCSU) is important in the assessment of the efficacy of common cancer treatments (an MCSU is a region of the tumour tissue supplied by a microvessel). The proposed methodologies take as input multimodal microscopy images of a tumour, and estimate the number and proportion of MCSU classes. This estimation is facilitated by the use of an underlying latent graph (not present in the manual annotations), where each MCSU is represented by a node in this graph, labelled with the MCSU class and image location. The training process uses the manual weak annotations available, consisting of the number of MCSU classes per training image, where the training objective is the minimisation of a high-order loss function based on the norm of the error between the manual and estimated annotations. One of the models proposed is based on a new flexible latent structure support vector machine (FLSSVM) and the other is based on a deep convolutional neural network (DCNN) model. Using a dataset of 89 weakly annotated pairs of multimodal images from eight tumours, we show that the quantitative results from DCNN are superior, but the qualitative results from FLSSVM are better and both display high correlation values regarding the number and proportion of MCSU classes compared to the manual annotations.
count=2
* Learning to Rank Based on Subsequences
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Fernando_Learning_to_Rank_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Fernando_Learning_to_Rank_ICCV_2015_paper.pdf)]
    * Title: Learning to Rank Based on Subsequences
    * Year: `2015`
    * Authors: Basura Fernando, Efstratios Gavves, Damien Muselet, Tinne Tuytelaars
    * Abstract: We present a supervised learning to rank algorithm that effectively orders images by exploiting the structure in image sequences. Most often in the supervised learning to rank literature, ranking is approached either by analysing pairs of images or by optimizing a list-wise surrogate loss function on full sequences. In this work we propose MidRank, which learns from moderately sized sub-sequences instead. These sub-sequences contain useful structural ranking information that leads to better learnability during training and better generalization during testing. By exploiting sub-sequences, the proposed MidRank improves ranking accuracy considerably on an extensive array of image ranking applications and datasets.
count=2
* Learning to Boost Filamentary Structure Segmentation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Gu_Learning_to_Boost_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Gu_Learning_to_Boost_ICCV_2015_paper.pdf)]
    * Title: Learning to Boost Filamentary Structure Segmentation
    * Year: `2015`
    * Authors: Lin Gu, Li Cheng
    * Abstract: The challenging problem of filamentary structure segmentation has a broad range of applications in biological and medical fields. A critical yet challenging issue remains on how to detect and restore the small filamentary fragments from backgrounds: The small fragments are of diverse shapes and appearances, meanwhile the backgrounds could be cluttered and ambiguous. Focusing on this issue, this paper proposes an iterative two-step learning-based approach to boost the performance based on a base segmenter arbitrarily chosen from a number of existing segmenters: We start with an initial partial segmentation where the filamentary structure obtained is of high confidence based on this existing segmenter. We also define a scanning horizon as epsilon balls centred around the partial segmentation result. Step one of our approach centers on a data-driven latent classification tree model to detect the filamentary fragments. This model is learned via a training process, where a large number of distinct local figure/background separation scenarios are established and geometrically organized into a tree structure. Step two spatially restores the isolated fragments back to the current partial segmentation, which is accomplished by means of completion fields and matting. Both steps are then alternated with the growth of partial segmentation result, until the input image space is entirely explored. Our approach is rather generic and can be easily augmented to a wide range of existing supervised/unsupervised segmenters to produce an improved result. This has been empirically verified on specific filamentary structure segmentation tasks: retinal blood vessel segmentation as well as neuronal segmentations, where noticeable improvement has been shown over the original state-of-the-arts.
count=2
* Introducing Geometry in Active Learning for Image Segmentation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Konyushkova_Introducing_Geometry_in_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Konyushkova_Introducing_Geometry_in_ICCV_2015_paper.pdf)]
    * Title: Introducing Geometry in Active Learning for Image Segmentation
    * Year: `2015`
    * Authors: Ksenia Konyushkova, Raphael Sznitman, Pascal Fua
    * Abstract: We propose an Active Learning approach to training a segmentation classifier that exploits geometric priors to streamline the annotation process in 3D image volumes. To this end, we use these priors not only to select voxels most in need of annotation but to guarantee that they lie on 2D planar patch, which makes it much easier to annotate than if they were randomly distributed in the volume. A simplified version of this approach is effective in natural 2D images. We evaluated our approach on Electron Microscopy and Magnetic Resonance image volumes, as well as on natural images. Comparing our approach against several accepted baselines demonstrates a marked performance increase.
count=2
* 3D Hand Pose Estimation Using Randomized Decision Forest With Segmentation Index Points
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Li_3D_Hand_Pose_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Li_3D_Hand_Pose_ICCV_2015_paper.pdf)]
    * Title: 3D Hand Pose Estimation Using Randomized Decision Forest With Segmentation Index Points
    * Year: `2015`
    * Authors: Peiyi Li, Haibin Ling, Xi Li, Chunyuan Liao
    * Abstract: In this paper, we propose a real-time 3D hand pose estimation algorithm using the randomized decision forest framework. Our algorithm takes a depth image as input and generates a set of skeletal joints as output. Previous decision forest-based methods often give labels to all points in a point cloud at a very early stage and vote for the joint locations. By contrast, our algorithm only tracks a set of more flexible virtual landmark points, named segmentation index points (SIPs), before reaching the final decision at a leaf node. Roughly speaking, a SIP represents the centroid of a subset of skeletal joints, which are to be located at the leaves of the branch expanded from the SIP. Inspired by recent latent regression forest-based hand pose estimation framework (Tang et al. 2014), we integrate SIP into the framework with several important improvements: First, we devise a new forest growing strategy, whose decision is made using a randomized feature guided by SIPs. Second, we speed-up the training procedure since only SIPs, not the skeletal joints, are estimated at non-leaf nodes. Third, the experimental results on public benchmark datasets show clearly the advantage of the proposed algorithm over previous state-of-the-art methods, and our algorithm runs at 55.5 fps on a normal CPU without parallelism.
count=2
* Video Segmentation With Just a Few Strokes
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Nagaraja_Video_Segmentation_With_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Nagaraja_Video_Segmentation_With_ICCV_2015_paper.pdf)]
    * Title: Video Segmentation With Just a Few Strokes
    * Year: `2015`
    * Authors: Naveen Shankar Nagaraja, Frank R. Schmidt, Thomas Brox
    * Abstract: As the use of videos is becoming more popular in computer vision, the need for annotated video datasets increases. Such datasets are required either as training data or simply as ground truth for benchmark datasets. A particular challenge in video segmentation is due to disocclusions, which hamper frame-to-frame propagation, in conjunction with non-moving objects. We show that a combination of motion from point trajectories, as known from motion segmentation, along with minimal supervision can largely help solve this problem. Moreover, we integrate a new constraint that enforces consistency of the color distribution in successive frames. We quantify user interaction effort with respect to segmentation quality on challenging ego motion videos. We compare our approach to a diverse set of algorithms in terms of user effort and in terms of performance on common video segmentation benchmarks.
count=2
* Hierarchical Higher-Order Regression Forest Fields: An Application to 3D Indoor Scene Labelling
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Pham_Hierarchical_Higher-Order_Regression_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Pham_Hierarchical_Higher-Order_Regression_ICCV_2015_paper.pdf)]
    * Title: Hierarchical Higher-Order Regression Forest Fields: An Application to 3D Indoor Scene Labelling
    * Year: `2015`
    * Authors: Trung T. Pham, Ian Reid, Yasir Latif, Stephen Gould
    * Abstract: This paper addresses the problem of semantic segmentation of 3D indoor scenes reconstructed from RGB-D images.Traditionally label prediction for 3D points is tackled by employing graphical models that capture scene features and complex relations between different class labels. However, the existing work is restricted to pairwise conditional random fields, which are insufficient when encoding rich scene context. In this work we propose models with higher-order potentials to describe complex relational information from the 3D scenes. Specifically, we relax the labelling problem to a regression, and generalize the higher-order associative P n Potts model to a new family of arbitrary higher-order models based on regression forests. We show that these models, like the robust P n models, can still be decomposed into the sum of pairwise terms by introducing auxiliary variables. Moreover, our proposed higher-order models also permit extension to hierarchical random fields, which allows for the integration of scene context and features computed at different scales. Our potential functions are constructed based on regression forests encoding Gaussian densities that admit efficient inference. The parameters of our model are learned from training data using a structured learning approach. Results on two datasets show clear improvements over current state-of-the-art methods.
count=2
* 3D-Assisted Feature Synthesis for Novel Views of an Object
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Su_3D-Assisted_Feature_Synthesis_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Su_3D-Assisted_Feature_Synthesis_ICCV_2015_paper.pdf)]
    * Title: 3D-Assisted Feature Synthesis for Novel Views of an Object
    * Year: `2015`
    * Authors: Hao Su, Fan Wang, Eric Yi, Leonidas J. Guibas
    * Abstract: Comparing two images from different views has been a long-standing challenging problem in computer vision, as visual features are not stable under large view point changes. In this paper, given a single input image of an object, we synthesize its features for other views, leveraging an existing modestly-sized 3D model collection of related but not identical objects.To accomplish this, we study the relationship of image patches between different views of the same object, seeking what we call surrogate patches -- patches in one view whose feature content predicts well the features of a patch in another view. Based upon these surrogate relationships, we can create feature sets for all views of the latent object on a per patch basis, providing us an augmented multi-view representation of the object. We provide theoretical and empirical analysis of the feature synthesis process, and evaluate the augmented features in fine-grained image retrieval/recognition and instance retrieval tasks. Experimental results show that our synthesized features do enable view-independent comparison between images and perform significantly better than other traditional approaches in this respect.
count=2
* Unsupervised Cross-Modal Synthesis of Subject-Specific Scans
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Vemulapalli_Unsupervised_Cross-Modal_Synthesis_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Vemulapalli_Unsupervised_Cross-Modal_Synthesis_ICCV_2015_paper.pdf)]
    * Title: Unsupervised Cross-Modal Synthesis of Subject-Specific Scans
    * Year: `2015`
    * Authors: Raviteja Vemulapalli, Hien Van Nguyen, Shaohua Kevin Zhou
    * Abstract: Recently, cross-modal synthesis of subject-specific scans has been receiving significant attention from the medical imaging community. Though various synthesis approaches have been introduced in the recent past, most of them are either tailored to a specific application or proposed for the supervised setting, i.e., they assume the availability of training data from the same set of subjects in both source and target modalities. But, collecting multiple scans from each subject is undesirable. Hence, to address this issue, we propose a general unsupervised cross-modal medical image synthesis approach that works without paired training data. Given a source modality image of a subject, we first generate multiple target modality candidate values for each voxel independently using cross-modal nearest neighbor search. Then, we select the best candidate values jointly for all the voxels by simultaneously maximizing a global mutual information cost function and a local spatial consistency cost function. Finally, we use coupled sparse representation for further refinement of synthesized images. Our experiments on generating T1-MRI brain scans from T2-MRI and vice versa demonstrate that the synthesis capability of the proposed unsupervised approach is comparable to various state-of-the-art supervised approaches in the literature.
count=2
* Dense Optical Flow Prediction From a Static Image
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Walker_Dense_Optical_Flow_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Walker_Dense_Optical_Flow_ICCV_2015_paper.pdf)]
    * Title: Dense Optical Flow Prediction From a Static Image
    * Year: `2015`
    * Authors: Jacob Walker, Abhinav Gupta, Martial Hebert
    * Abstract: Given a scene, what is going to move, and in what direction will it move? Such a question could be considered a non-semantic form of action prediction. In this work, we present a convolutional neural network (CNN) based approach for motion prediction. Given a static image, this CNN predicts the future motion of each and every pixel in the image in terms of optical flow. Our CNN model leverages the data in tens of thousands of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene. Because our CNN model makes no assumptions about the underlying scene, it can predict future optical flow on a diverse set of scenarios. We outperform all previous approaches by large margins.
count=2
* Randomized Ensemble Tracking
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Bai_Randomized_Ensemble_Tracking_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Bai_Randomized_Ensemble_Tracking_2013_ICCV_paper.pdf)]
    * Title: Randomized Ensemble Tracking
    * Year: `2013`
    * Authors: Qinxun Bai, Zheng Wu, Stan Sclaroff, Margrit Betke, Camille Monnier
    * Abstract: We propose a randomized ensemble algorithm to model the time-varying appearance of an object for visual tracking. In contrast with previous online methods for updating classifier ensembles in tracking-by-detection, the weight vector that combines weak classifiers is treated as a random variable and the posterior distribution for the weight vector is estimated in a Bayesian manner. In essence, the weight vector is treated as a distribution that reflects the confidence among the weak classifiers used to construct and adapt the classifier ensemble. The resulting formulation models the time-varying discriminative ability among weak classifiers so that the ensembled strong classifier can adapt to the varying appearance, backgrounds, and occlusions. The formulation is tested in a tracking-by-detection implementation. Experiments on 28 challenging benchmark videos demonstrate that the proposed method can achieve results comparable to and often better than those of stateof-the-art approaches.
count=2
* POP: Person Re-identification Post-rank Optimisation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Liu_POP_Person_Re-identification_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Liu_POP_Person_Re-identification_2013_ICCV_paper.pdf)]
    * Title: POP: Person Re-identification Post-rank Optimisation
    * Year: `2013`
    * Authors: Chunxiao Liu, Chen Change Loy, Shaogang Gong, Guijin Wang
    * Abstract: Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either "one-shot" or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user's searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just "one shot" feedback optimisation, by as much as over 30% performance improvement for rank 1 reidentification on the VIPeR and i-LIDS datasets.
count=2
* Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Shankar_Semantic_Transform_Weakly_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Shankar_Semantic_Transform_Weakly_2013_ICCV_paper.pdf)]
    * Title: Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes
    * Year: `2013`
    * Authors: Sukrit Shankar, Joan Lasenby, Roberto Cipolla
    * Abstract: Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks [19, 23]. However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impractical for realworld applications. In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. Such a semantic space is found for every attribute category. To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. This problem is ideally NP-hard, and we thus propose a constrained search tree formulation for the same. Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets.
count=2
* Efficient Hand Pose Estimation from a Single Depth Image
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Xu_Efficient_Hand_Pose_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Xu_Efficient_Hand_Pose_2013_ICCV_paper.pdf)]
    * Title: Efficient Hand Pose Estimation from a Single Depth Image
    * Year: `2013`
    * Authors: Chi Xu, Li Cheng
    * Abstract: We tackle the practical problem of hand pose estimation from a single noisy depth image. A dedicated three-step pipeline is proposed: Initial estimation step provides an initial estimation of the hand in-plane orientation and 3D location; Candidate generation step produces a set of 3D pose candidate from the Hough voting space with the help of the rotational invariant depth features; Verification step delivers the final 3D hand pose as the solution to an optimization problem. We analyze the depth noises, and suggest tips to minimize their negative impacts on the overall performance. Our approach is able to work with Kinecttype noisy depth images, and reliably produces pose estimations of general motions efficiently (12 frames per second). Extensive experiments are conducted to qualitatively and quantitatively evaluate the performance with respect to the state-of-the-art methods that have access to additional RGB images. Our approach is shown to deliver on par or even better results.
count=2
* Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Zhang_Deformable_Part_Descriptors_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Zhang_Deformable_Part_Descriptors_2013_ICCV_paper.pdf)]
    * Title: Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction
    * Year: `2013`
    * Authors: Ning Zhang, Ryan Farrell, Forrest Iandola, Trevor Darrell
    * Abstract: Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements over state-of-art algorithms.
count=2
* Learning CRFs for Image Parsing with Adaptive Subgradient Descent
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Zhang_Learning_CRFs_for_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Zhang_Learning_CRFs_for_2013_ICCV_paper.pdf)]
    * Title: Learning CRFs for Image Parsing with Adaptive Subgradient Descent
    * Year: `2013`
    * Authors: Honghui Zhang, Jingdong Wang, Ping Tan, Jinglu Wang, Long Quan
    * Abstract: We propose an adaptive subgradient descent method to efficiently learn the parameters of CRF models for image parsing. To balance the learning efficiency and performance of the learned CRF models, the parameter learning is iteratively carried out by solving a convex optimization problem in each iteration, which integrates a proximal term to preserve the previously learned information and the large margin preference to distinguish bad labeling and the ground truth labeling. A solution of subgradient descent updating form is derived for the convex optimization problem, with an adaptively determined updating step-size. Besides, to deal with partially labeled training data, we propose a new objective constraint modeling both the labeled and unlabeled parts in the partially labeled training data for the parameter learning of CRF models. The superior learning efficiency of the proposed method is verified by the experiment results on two public datasets. We also demonstrate the powerfulness of our method for handling partially labeled training data.
count=2
* Synthcity: a benchmark framework for diverse use cases of tabular synthetic data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/09723c9f291f6056fd1885081859c186-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/09723c9f291f6056fd1885081859c186-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Synthcity: a benchmark framework for diverse use cases of tabular synthetic data
    * Year: `2023`
    * Authors: Zhaozhi Qian, Rob Davis, Mihaela van der Schaar
    * Abstract: Accessible high-quality data is the bread and butter of machine learning research, and the demand for data has exploded as larger and more advanced ML models are built across different domains. Yet, real data often contain sensitive information, are subject to various biases, and are costly to acquire, which compromise their quality and accessibility. Synthetic data have thus emerged as a complement to, sometimes even a replacement for, real data for ML training. However, the landscape of synthetic data research has been fragmented due to the diverse range of data modalities, such as tabular, time series, and images, and the wide array of use cases, including privacy preservation, fairness considerations, and data augmentation. This fragmentation poses practical challenges when comparing and selecting synthetic data generators in for different problem settings. To this end, we develop Synthcity, an open-source Python library that allows researchers and practitioners to perform one-click benchmarking of synthetic data generators across data modalities and use cases. Beyond benchmarking, Synthcity serves as a centralized toolkit for accessing cutting-edge data generators. In addition, Synthcity’s flexible plug-in style API makes it easy to incorporate additional data generators into the framework. Using examples of tabular data generation and data augmentation, we illustrate the general applicability of Synthcity, and the insight one can obtain.
count=2
* Learning Robust Statistics for Simulation-based Inference under Model Misspecification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/16c5b4102a6b6eb061e502ce6736ad8a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/16c5b4102a6b6eb061e502ce6736ad8a-Paper-Conference.pdf)]
    * Title: Learning Robust Statistics for Simulation-based Inference under Model Misspecification
    * Year: `2023`
    * Authors: Daolang Huang, Ayush Bharti, Amauri Souza, Luigi Acerbi, Samuel Kaski
    * Abstract: Simulation-based inference (SBI) methods such as approximate Bayesian computation (ABC), synthetic likelihood, and neural posterior estimation (NPE) rely on simulating statistics to infer parameters of intractable likelihood models. However, such methods are known to yield untrustworthy and misleading inference outcomes under model misspecification, thus hindering their widespread applicability. In this work, we propose the first general approach to handle model misspecification that works across different classes of SBI methods. Leveraging the fact that the choice of statistics determines the degree of misspecification in SBI, we introduce a regularized loss function that penalizes those statistics that increase the mismatch between the data and the model. Taking NPE and ABC as use cases, we demonstrate the superior performance of our method on high-dimensional time-series models that are artificially misspecified. We also apply our method to real data from the field of radio propagation where the model is known to be misspecified. We show empirically that the method yields robust inference in misspecified scenarios, whilst still being accurate when the model is well-specified.
count=2
* Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/3322a9a72a1707de14badd5e552ff466-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/3322a9a72a1707de14badd5e552ff466-Paper-Conference.pdf)]
    * Title: Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations
    * Year: `2023`
    * Authors: Hammaad Adam, Fan Yin, Huibin Hu, Neil Tenenholtz, Lorin Crawford, Lester Mackey, Allison Koenecke
    * Abstract: Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests.
count=2
* Beyond Normal: On the Evaluation of Mutual Information Estimators
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/36b80eae70ff629d667f210e13497edf-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/36b80eae70ff629d667f210e13497edf-Paper-Conference.pdf)]
    * Title: Beyond Normal: On the Evaluation of Mutual Information Estimators
    * Year: `2023`
    * Authors: Paweł Czyż, Frederic Grabowski, Julia Vogt, Niko Beerenwinkel, Alexander Marx
    * Abstract: Mutual information is a general statistical dependency measure which has found applications in representation learning, causality, domain generalization and computational biology. However, mutual information estimators are typically evaluated on simple families of probability distributions, namely multivariate normal distribution and selected distributions with one-dimensional random variables. In this paper, we show how to construct a diverse family of distributions with known ground-truth mutual information and propose a language-independent benchmarking platform for mutual information estimators. We discuss the general applicability and limitations of classical and neural estimators in settings involving high dimensions, sparse interactions, long-tailed distributions, and high mutual information. Finally, we provide guidelines for practitioners on how to select appropriate estimator adapted to the difficulty of problem considered and issues one needs to consider when applying an estimator to a new data set.
count=2
* Predict-then-Calibrate: A New Perspective of Robust Contextual LP
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/397271e11322fae8ba7f827c50ca8d9b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/397271e11322fae8ba7f827c50ca8d9b-Paper-Conference.pdf)]
    * Title: Predict-then-Calibrate: A New Perspective of Robust Contextual LP
    * Year: `2023`
    * Authors: Chunlin Sun, Linyu Liu, Xiaocheng Li
    * Abstract: Contextual optimization, also known as predict-then-optimize or prescriptive analytics, considers an optimization problem with the presence of covariates (context or side information). The goal is to learn a prediction model (from the training data) that predicts the objective function from the covariates, and then in the test phase, solve the optimization problem with the covariates but without the observation of the objective function. In this paper, we consider a risk-sensitive version of the problem and propose a generic algorithm design paradigm called predict-then-calibrate. The idea is to first develop a prediction model without concern for the downstream risk profile or robustness guarantee, and then utilize calibration (or recalibration) methods to quantify the uncertainty of the prediction. While the existing methods suffer from either a restricted choice of the prediction model or strong assumptions on the underlying data, we show the disentangling of the prediction model and the calibration/uncertainty quantification has several advantages. First, it imposes no restriction on the prediction model and thus fully unleashes the potential of off-the-shelf machine learning methods. Second, the derivation of the risk and robustness guarantee can be made independent of the choice of the prediction model through a data-splitting idea. Third, our paradigm of predict-then-calibrate applies to both (risk-sensitive) robust and (risk-neutral) distributionally robust optimization (DRO) formulations. Theoretically, it gives new generalization bounds for the contextual LP problem and sheds light on the existing results of DRO for contextual LP. Numerical experiments further reinforce the advantage of the predict-then-calibrate paradigm in that an improvement on either the prediction model or the calibration model will lead to a better final performance.
count=2
* BioMassters: A Benchmark Dataset for Forest Biomass Estimation using Multi-modal Satellite Time-series
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/40daf2a00278c4bea1b26cd4c8a654f8-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/40daf2a00278c4bea1b26cd4c8a654f8-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: BioMassters: A Benchmark Dataset for Forest Biomass Estimation using Multi-modal Satellite Time-series
    * Year: `2023`
    * Authors: Andrea Nascetti, Ritu Yadav, Kirill Brodt, Qixun Qu, Hongwei Fan, Yuri Shendryk, Isha Shah, Christine Chung
    * Abstract: Above Ground Biomass is an important variable as forests play a crucial role in mitigating climate change as they act as an efficient, natural and cost-effective carbon sink. Traditional field and airborne LiDAR measurements have been proven to provide reliable estimations of forest biomass. Nevertheless, the use of these techniques at a large scale can be challenging and expensive. Satellite data have been widely used as a valuable tool in estimating biomass on a global scale. However, the full potential of dense multi-modal satellite time series data, in combination with modern deep learning approaches, has yet to be fully explored. The aim of the "BioMassters" data challenge and benchmark dataset is to investigate the potential of multi-modal satellite data (Sentinel-1 SAR and Sentinel-2 MSI) to estimate forest biomass at a large scale using the Finnish Forest Centre's open forest and nature airborne LiDAR data as a reference. The performance of the top three baseline models shows the potential of deep learning to produce accurate and higher-resolution biomass maps. Our benchmark dataset is publically available at https://huggingface.co/datasets/nascetti-a/BioMassters (doi:10.57967/hf/1009) and the implementation of the top three winning models are available at https://github.com/drivendataorg/the-biomassters.
count=2
* Data-Driven Network Neuroscience: On Data Collection and Benchmark
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/44e3a3115ca26e5127851acd0cedd0d9-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/44e3a3115ca26e5127851acd0cedd0d9-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Data-Driven Network Neuroscience: On Data Collection and Benchmark
    * Year: `2023`
    * Authors: Jiaxing Xu, Yunhan Yang, David Huang, Sophi Shilpa Gururajapathy, Yiping Ke, Miao Qiao, Alan Wang, Haribalan Kumar, Josh McGeown, Eryn Kwon
    * Abstract: This paper presents a comprehensive and quality collection of functional human brain network data for potential research in the intersection of neuroscience, machine learning, and graph analytics. Anatomical and functional MRI images have been used to understand the functional connectivity of the human brain and are particularly important in identifying underlying neurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism. Recently, the study of the brain in the form of brain networks using machine learning and graph analytics has become increasingly popular, especially to predict the early onset of these conditions. A brain network, represented as a graph, retains rich structural and positional information that traditional examination methods are unable to capture. However, the lack of publicly accessible brain network data prevents researchers from data-driven explorations. One of the main difficulties lies in the complicated domain-specific preprocessing steps and the exhaustive computation required to convert the data from MRI images into brain networks. We bridge this gap by collecting a large amount of MRI images from public databases and a private source, working with domain experts to make sensible design choices, and preprocessing the MRI images to produce a collection of brain network datasets. The datasets originate from 6 different sources, cover 4 brain conditions, and consist of a total of 2,702 subjects. We test our graph datasets on 12 machine learning models to provide baselines and validate the data quality on a recent graph analysis model. To lower the barrier to entry and promote the research in this interdisciplinary field, we release our brain network data and complete preprocessing details including codes at https://doi.org/10.17608/k6.auckland.21397377 and https://github.com/brainnetuoa/datadrivennetwork_neuroscience.
count=2
* VeriX: Towards Verified Explainability of Deep Neural Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/46907c2ff9fafd618095161d76461842-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/46907c2ff9fafd618095161d76461842-Paper-Conference.pdf)]
    * Title: VeriX: Towards Verified Explainability of Deep Neural Networks
    * Year: `2023`
    * Authors: Min Wu, Haoze Wu, Clark Barrett
    * Abstract: We present VeriX (Verified eXplainability), a system for producing optimal robust explanations and generating counterfactuals along decision boundaries of machine learning models. We build such explanations and counterfactuals iteratively using constraint solving techniques and a heuristic based on feature-level sensitivity ranking. We evaluate our method on image recognition benchmarks and a real-world scenario of autonomous aircraft taxiing.
count=2
* REFINE: A Fine-Grained Medication Recommendation System Using Deep Learning and Personalized Drug Interaction Modeling
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/4b7439a4ab0b8e4bcb4e2412c6a10a58-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/4b7439a4ab0b8e4bcb4e2412c6a10a58-Paper-Conference.pdf)]
    * Title: REFINE: A Fine-Grained Medication Recommendation System Using Deep Learning and Personalized Drug Interaction Modeling
    * Year: `2023`
    * Authors: Suman Bhoi, Mong Li Lee, Wynne Hsu, Ngiap Chuan Tan
    * Abstract: Patients with co-morbidities often require multiple medications to manage their conditions. However, existing medication recommendation systems only offer class-level medications and regard all interactions among drugs to have the same level of severity. This limits their ability to provide personalized and safe recommendations tailored to individual needs. In this work, we introduce a deep learning-based fine-grained medication recommendation system called REFINE, which is designed to improve treatment outcomes and minimize adverse drug interactions. In order to better characterize patients’ health conditions, we model the trend in medication dosage titrations and lab test responses, and adapt the vision transformer to obtain effective patient representations. We also model drug interaction severity levels as weighted graphs to learn safe drug combinations and design a balanced loss function to avoid overly conservative recommendations and miss medications that might be needed for certain conditions. Extensive experiments on two real-world datasets show that REFINE outperforms state-of-the-art techniques.
count=2
* Anonymous Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/70899a5d74f83317c78f1a7d413d1baa-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/70899a5d74f83317c78f1a7d413d1baa-Paper-Conference.pdf)]
    * Title: Anonymous Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization
    * Year: `2023`
    * Authors: Adel Javanmard, Vahab Mirrokni
    * Abstract: While personalized recommendations systems have become increasingly popular, ensuring user data protection remains a top concern in the development of these learning systems. A common approach to enhancing privacy involves training models using anonymous data rather than individual data. In this paper, we explore a natural technique called "look-alike clustering", which involves replacing sensitive features of individuals with the cluster's average values. We provide a precise analysis of how training models using anonymous cluster centers affects their generalization capabilities. We focus on an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis is based on the Convex Gaussian Minimax Theorem (CGMT) and allows us to theoretically understand the role of different model components on the generalization error. In addition, we demonstrate that in certain high-dimensional regimes, training over anonymous cluster centers acts as a regularization and improves generalization error of the trained models. Finally, we corroborate our asymptotic theory with finite-sample numerical experiments where we observe a perfect match when the sample size is only of order of a few hundreds.
count=2
* Estimating and Controlling for Equalized Odds via Sensitive Attribute Predictors
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/752820c79b4ebb72809014bdfdedd603-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/752820c79b4ebb72809014bdfdedd603-Paper-Conference.pdf)]
    * Title: Estimating and Controlling for Equalized Odds via Sensitive Attribute Predictors
    * Year: `2023`
    * Authors: Beepul Bharti, Paul Yi, Jeremias Sulam
    * Abstract: As the use of machine learning models in real world high-stakes decision settings continues to grow, it is highly important that we are able to audit and control for any potential fairness violations these models may exhibit towards certain groups. To do so, one naturally requires access to sensitive attributes, such as demographics, biological sex, or other potentially sensitive features that determine group membership. Unfortunately, in many settings, this information is often unavailable. In this work we study the well known equalized odds (EOD) definition of fairness. In a setting without sensitive attributes, we first provide tight and computable upper bounds for the EOD violation of a predictor. These bounds precisely reflect the worst possible EOD violation. Second, we demonstrate how one can provably control the worst-case EOD by a new post-processing correction method. Our results characterize when directly controlling for EOD with respect to the predicted sensitive attributes is -- and when is not -- optimal when it comes to controlling worst-case EOD. Our results hold under assumptions that are milder than previous works, and we illustrate these results with experiments on synthetic and real datasets.
count=2
* CoDrug: Conformal Drug Property Prediction with Density Estimation under Covariate Shift
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/7691484a7a35d5e2742279c1d926b778-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/7691484a7a35d5e2742279c1d926b778-Paper-Conference.pdf)]
    * Title: CoDrug: Conformal Drug Property Prediction with Density Estimation under Covariate Shift
    * Year: `2023`
    * Authors: Siddhartha Laghuvarapu, Zhen Lin, Jimeng Sun
    * Abstract: In drug discovery, it is vital to confirm the predictions of pharmaceutical properties from computational models using costly wet-lab experiments. Hence, obtaining reliable uncertainty estimates is crucial for prioritizing drug molecules for subsequent experimental validation. Conformal Prediction (CP) is a promising tool for creating such prediction sets for molecular properties with a coverage guarantee. However, the exchangeability assumption of CP is often challenged with covariate shift in drug discovery tasks: Most datasets contain limited labeled data, which may not be representative of the vast chemical space from which molecules are drawn. To address this limitation, we propose a method called CoDrug that employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying for distribution shift. In extensive experiments involving realistic distribution drifts in various small-molecule drug discovery tasks, we demonstrate the ability of CoDrug to provide valid prediction sets and its utility in addressing the distribution shift arising from de novo drug design models. On average, using CoDrug can reduce the coverage gap by over 35% when compared to conformal prediction sets not adjusted for covariate shift.
count=2
* “Why Not Looking backward?” A Robust Two-Step Method to Automatically Terminate Bayesian Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/870c1e0589822bf37590b84984c345c4-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/870c1e0589822bf37590b84984c345c4-Paper-Conference.pdf)]
    * Title: “Why Not Looking backward?” A Robust Two-Step Method to Automatically Terminate Bayesian Optimization
    * Year: `2023`
    * Authors: Shuang Li, Ke Li, Wei Li
    * Abstract: Bayesian Optimization (BO) is a powerful method for tackling expensive black-box optimization problems. As a sequential model-based optimization strategy, BO iteratively explores promising solutions until a predetermined budget, either iterations or time, is exhausted. The decision on when to terminate BO significantly influences both the quality of solutions and its computational efficiency. In this paper, we propose a simple, yet theoretically grounded, two-step method for automatically terminating BO. Our core concept is to proactively identify if the search is within a convex region by examining previously observed samples. BO is halted once the local regret within this convex region falls below a predetermined threshold. To enhance numerical stability, we propose an approximation method for calculating the termination indicator by solving a bilevel optimization problem. We conduct extensive empirical studies on diverse benchmark problems, including synthetic functions, reinforcement learning, and hyperparameter optimization. Experimental results demonstrate that our proposed method saves up to $\approx 80\%$ computational budget yet is with an order of magnitude smaller performance degradation, comparing against the other peer methods. In addition, our proposed termination method is robust in terms of the setting of its termination criterion.
count=2
* Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/8797d13e5998acfab387d4bf0a5b9b00-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/8797d13e5998acfab387d4bf0a5b9b00-Paper-Conference.pdf)]
    * Title: Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions
    * Year: `2023`
    * Authors: Dongjie Wang, Meng Xiao, Min Wu, pengfei wang, Yuanchun Zhou, Yanjie Fu
    * Abstract: Feature transformation aims to generate new pattern-discriminative feature space from original features to improve downstream machine learning (ML) task performances. However, the discrete search space for the optimal feature explosively grows on the basis of combinations of features and operations from low-order forms to high-order forms. Existing methods, such as exhaustive search, expansion reduction, evolutionary algorithms, reinforcement learning, and iterative greedy, suffer from large search space. Overly emphasizing efficiency in algorithm design usually sacrifice stability or robustness. To fundamentally fill this gap, we reformulate discrete feature transformation as a continuous space optimization task and develop an embedding-optimization-reconstruction framework. This framework includes four steps: 1) reinforcement-enhanced data preparation, aiming to prepare high-quality transformation-accuracy training data; 2) feature transformation operation sequence embedding, intending to encapsulate the knowledge of prepared training data within a continuous space; 3) gradient-steered optimal embedding search, dedicating to uncover potentially superior embeddings within the learned space; 4) transformation operation sequence reconstruction, striving to reproduce the feature transformation solution to pinpoint the optimal feature space. Finally, extensive experiments and case studies are performed to demonstrate the effectiveness and robustness of the proposed method. The code and data are publicly accessible https://www.dropbox.com/sh/imh8ckui7va3k5u/AACulQegVx0MuywYyoCqSdVPa?dl=0.
count=2
* Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/8b4add8b0aa8749d80a34ca5d941c355-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/8b4add8b0aa8749d80a34ca5d941c355-Paper-Conference.pdf)]
    * Title: Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks
    * Year: `2023`
    * Authors: Jimmy Di, Jack Douglas, Jayadev Acharya, Gautam Kamath, Ayush Sekhari
    * Abstract: We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a poisoned dataset. We demonstrate efficacy of our attack when unlearning is performed via retraining from scratch, the idealized setting of machine unlearning which other efficient methods attempt to emulate, as well as against the approximate unlearning approach of Graves et al. (2021).
count=2
* Conformal Meta-learners for Predictive Inference of Individual Treatment Effects
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/94ab02a30b0e4a692a42ccd0b4c55399-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/94ab02a30b0e4a692a42ccd0b4c55399-Paper-Conference.pdf)]
    * Title: Conformal Meta-learners for Predictive Inference of Individual Treatment Effects
    * Year: `2023`
    * Authors: Ahmed M. Alaa, Zaid Ahmad, Mark van der Laan
    * Abstract: We investigate the problem of machine learning-based (ML) predictive inference on individual treatment effects (ITEs). Previous work has focused primarily on developing ML-based “meta-learners” that can provide point estimates of the conditional average treatment effect (CATE)—these are model-agnostic approaches for combining intermediate nuisance estimates to produce estimates of CATE. In this paper, we develop conformal meta-learners, a general framework for issuing predictive intervals for ITEs by applying the standard conformal prediction (CP) procedure on top of CATE meta-learners. We focus on a broad class of meta-learners based on two-stage pseudo-outcome regression and develop a stochastic ordering framework to study their validity. We show that inference with conformal meta-learners is marginally valid if their (pseudo-outcome) conformity scores stochastically dominate “oracle” conformity scores evaluated on the unobserved ITEs. Additionally, we prove that commonly used CATE meta-learners, such as the doubly-robust learner, satisfy a model- and distribution-free stochastic (or convex) dominance condition, making their conformal inferences valid for practically-relevant levels of target coverage. Whereas existing procedures conduct inference on nuisance parameters (i.e., potential outcomes) via weighted CP, conformal meta-learners enable direct inference on the target parameter (ITE). Numerical experiments show that conformal meta-learners provide valid intervals with competitive efficiency while retaining the favorable point estimation properties of CATE meta-learners.
count=2
* Sequential Predictive Two-Sample and Independence Testing
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/a72b207734d6112f6b47447e46be40e9-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/a72b207734d6112f6b47447e46be40e9-Paper-Conference.pdf)]
    * Title: Sequential Predictive Two-Sample and Independence Testing
    * Year: `2023`
    * Authors: Aleksandr Podkopaev, Aaditya Ramdas
    * Abstract: We study the problems of sequential nonparametric two-sample and independence testing. Sequential tests process data online and allow using observed data to decide whether to stop and reject the null hypothesis or to collect more data, while maintaining type I error control. We build upon the principle of (nonparametric) testing by betting, where a gambler places bets on future observations and their wealth measures evidence against the null hypothesis. While recently developed kernel-based betting strategies often work well on simple distributions, selecting a suitable kernel for high-dimensional or structured data, such as images, is often nontrivial. To address this drawback, we design prediction-based betting strategies that rely on the following fact: if a sequentially updated predictor starts to consistently determine (a) which distribution an instance is drawn from, or (b) whether an instance is drawn from the joint distribution or the product of the marginal distributions (the latter produced by external randomization), it provides evidence against the two-sample or independence nulls respectively. We empirically demonstrate the superiority of our tests over kernel-based approaches under structured settings. Our tests can be applied beyond the case of independent and identically distributed data, remaining valid and powerful even when the data distribution drifts over time.
count=2
* Adapting Fairness Interventions to Missing Values
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/ba0ad9d1e0c737800b2340b9cd68c208-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/ba0ad9d1e0c737800b2340b9cd68c208-Paper-Conference.pdf)]
    * Title: Adapting Fairness Interventions to Missing Values
    * Year: `2023`
    * Authors: Raymond Feng, Flavio Calmon, Hao Wang
    * Abstract: Missing values in real-world data pose a significant and unique challenge to algorithmic fairness. Different demographic groups may be unequally affected by missing data, and the standard procedure for handling missing values where first data is imputed, then the imputed data is used for classification—a procedure referred to as "impute-then-classify"—can exacerbate discrimination. In this paper, we analyze how missing values affect algorithmic fairness. We first prove that training a classifier from imputed data can significantly worsen the achievable values of group fairness and average accuracy. This is because imputing data results in the loss of the missing pattern of the data, which often conveys information about the predictive label. We present scalable and adaptive algorithms for fair classification with missing values. These algorithms can be combined with any preexisting fairness-intervention algorithm to handle all possible missing patterns while preserving information encoded within the missing patterns. Numerical experiments with state-of-the-art fairness interventions demonstrate that our adaptive algorithms consistently achieve higher fairness and accuracy than impute-then-classify across different datasets.
count=2
* SSL4EO-L: Datasets and Foundation Models for Landsat Imagery
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/bbf7ee04e2aefec136ecf60e346c2e61-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/bbf7ee04e2aefec136ecf60e346c2e61-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: SSL4EO-L: Datasets and Foundation Models for Landsat Imagery
    * Year: `2023`
    * Authors: Adam Stewart, Nils Lehmann, Isaac Corley, Yi Wang, Yi-Chia Chang, Nassim Ait Ait Ali Braham, Shradha Sehgal, Caleb Robinson, Arindam Banerjee
    * Abstract: The Landsat program is the longest-running Earth observation program in history, with 50+ years of data acquisition by 8 satellites. The multispectral imagery captured by sensors onboard these satellites is critical for a wide range of scientific fields. Despite the increasing popularity of deep learning and remote sensing, the majority of researchers still use decision trees and random forests for Landsat image analysis due to the prevalence of small labeled datasets and lack of foundation models. In this paper, we introduce SSL4EO-L, the first ever dataset designed for Self-Supervised Learning for Earth Observation for the Landsat family of satellites (including 3 sensors and 2 product levels) and the largest Landsat dataset in history (5M image patches). Additionally, we modernize and re-release the L7 Irish and L8 Biome cloud detection datasets, and introduce the first ML benchmark datasets for Landsats 4–5 TM and Landsat 7 ETM+ SR. Finally, we pre-train the first foundation models for Landsat imagery using SSL4EO-L and evaluate their performance on multiple semantic segmentation tasks. All datasets and model weights are available via the TorchGeo library, making reproducibility and experimentation easy, and enabling scientific advancements in the burgeoning field of remote sensing for a multitude of downstream applications.
count=2
* Certified Minimax Unlearning with Generalization Rates and Deletion Capacity
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/c69465280855cfe25d566e359da140c1-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/c69465280855cfe25d566e359da140c1-Paper-Conference.pdf)]
    * Title: Certified Minimax Unlearning with Generalization Rates and Deletion Capacity
    * Year: `2023`
    * Authors: Jiaqi Liu, Jian Lou, Zhan Qin, Kui Ren
    * Abstract: We study the problem of $(\epsilon,\delta)$-certified machine unlearning for minimax models. Most of the existing works focus on unlearning from standard statistical learning models that have a single variable and their unlearning steps hinge on the direct Hessian-based conventional Newton update. We develop a new $(\epsilon,\delta)$-certified machine unlearning algorithm for minimax models. It proposes a minimax unlearning step consisting of a total Hessian-based complete Newton update and the Gaussian mechanism borrowed from differential privacy. To obtain the unlearning certification, our method injects calibrated Gaussian noises by carefully analyzing the ''sensitivity'' of the minimax unlearning step (i.e., the closeness between the minimax unlearning variables and the retraining-from-scratch variables). We derive the generalization rates in terms of population strong and weak primal-dual risk for three different cases of loss functions, i.e., (strongly-)convex-(strongly-)concave losses. We also provide the deletion capacity to guarantee that a desired population risk can be maintained as long as the number of deleted samples does not exceed the derived amount. With training samples $n$ and model dimension $d$, it yields the order $\mathcal O(n/d^{1/4})$, which shows a strict gap over the baseline method of differentially private minimax learning that has $\mathcal O(n/d^{1/2})$. In addition, our rates of generalization and deletion capacity match the state-of-the-art rates derived previously for standard statistical learning models.
count=2
* Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/cb2266111eadcfa2c02187ace64e2183-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/cb2266111eadcfa2c02187ace64e2183-Paper-Conference.pdf)]
    * Title: Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks
    * Year: `2023`
    * Authors: Ziyi Huang, Henry Lam, Haofeng Zhang
    * Abstract: Uncertainty quantification (UQ) is important for reliability assessment and enhancement of machine learning models. In deep learning, uncertainties arise not only from data, but also from the training procedure that often injects substantial noises and biases. These hinder the attainment of statistical guarantees and, moreover, impose computational challenges on UQ due to the need for repeated network retraining. Building upon the recent neural tangent kernel theory, we create statistically guaranteed schemes to principally \emph{characterize}, and \emph{remove}, the uncertainty of over-parameterized neural networks with very low computation effort. In particular, our approach, based on what we call a procedural-noise-correcting (PNC) predictor, removes the procedural uncertainty by using only \emph{one} auxiliary network that is trained on a suitably labeled dataset, instead of many retrained networks employed in deep ensembles. Moreover, by combining our PNC predictor with suitable light-computation resampling methods, we build several approaches to construct asymptotically exact-coverage confidence intervals using as low as four trained networks without additional overheads.
count=2
* Meta-in-context learning in large language models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/cda04d7ea67ea1376bf8c6962d8541e0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/cda04d7ea67ea1376bf8c6962d8541e0-Paper-Conference.pdf)]
    * Title: Meta-in-context learning in large language models
    * Year: `2023`
    * Authors: Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matt Botvinick, Jane Wang, Eric Schulz
    * Abstract: Large language models have shown tremendous performance in a variety of tasks. In-context learning -- the ability to improve at a task after being provided with a number of demonstrations -- is seen as one of the main contributors to their success. In the present paper, we demonstrate that the in-context learning abilities of large language models can be recursively improved via in-context learning itself. We coin this phenomenon meta-in-context learning. Looking at two idealized domains, a one-dimensional regression task and a two-armed bandit task, we show that meta-in-context learning adaptively reshapes a large language model's priors over expected tasks. Furthermore, we find that meta-in-context learning modifies the in-context learning strategies of such models. Finally, we broaden the scope of our investigation to encompass two diverse benchmarks: one focusing on real-world regression problems and the other encompassing multiple NLP tasks. In both cases, we observe competitive performance comparable to that of traditional learning algorithms. Taken together, our work improves our understanding of in-context learning and paves the way toward adapting large language models to the environment they are applied purely through meta-in-context learning rather than traditional finetuning.
count=2
* Derandomized novelty detection with FDR control via conformal e-values
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/cec8ad7715d0d13899d5d7d31970f527-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/cec8ad7715d0d13899d5d7d31970f527-Paper-Conference.pdf)]
    * Title: Derandomized novelty detection with FDR control via conformal e-values
    * Year: `2023`
    * Authors: Meshi Bashari, Amir Epstein, Yaniv Romano, Matteo Sesia
    * Abstract: Conformal inference provides a general distribution-free method to rigorously calibrate the output of any machine learning algorithm for novelty detection. While this approach has many strengths, it has the limitation of being randomized, in the sense that it may lead to different results when analyzing twice the same data and this can hinder the interpretation of any findings. We propose to make conformal inferences more stable by leveraging suitable conformal e-values instead of p-values to quantify statistical significance. This solution allows the evidence gathered from multiple analyses of the same data to be aggregated effectively while provably controlling the false discovery rate. Further, we show that the proposed method can reduce randomness without much loss of power compared to standard conformal inference, partly thanks to an innovative way of weighting conformal e-values based on additional side information carefully extracted from the same data. Simulations with synthetic and real data confirm this solution can be effective at eliminating random noise in the inferences obtained with state-of-the-art alternative techniques, sometimes also leading to higher power.
count=2
* Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/cf42f133f355e0e07a8957b508b26a1b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/cf42f133f355e0e07a8957b508b26a1b-Paper-Conference.pdf)]
    * Title: Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms
    * Year: `2023`
    * Authors: Dheeraj Baby, Saurabh Garg, Tzu-Ching Yen, Sivaraman Balakrishnan, Zachary Lipton, Yu-Xiang Wang
    * Abstract: This paper focuses on supervised and unsupervised online label shift,where the class marginals $Q(y)$ variesbut the class-conditionals $Q(x|y)$ remain invariant. In the unsupervised setting, our goal is to adapt a learner, trained on some offline labeled data, to changing label distributions given unlabeled online data. In the supervised setting, we must both learn a classifier and adapt to the dynamically evolving class marginals given only labeled online data. We develop novel algorithms that reduce the adaptation problem to online regression and guarantee optimal dynamic regret without any prior knowledge of the extent of drift in the label distribution. Our solution is based on bootstrapping the estimates of *online regression oracles* that track the drifting proportions. Experiments across numerous simulated and real-world online label shift scenarios demonstrate the superior performance of our proposed approaches, often achieving 1-3% improvement in accuracy while being sample and computationally efficient. Code is publicly available at https://github.com/Anon-djiwh/OnlineLabelShift
count=2
* Evaluating Self-Supervised Learning for Molecular Graph Embeddings
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d6dc15cc2442a40904e704d624d1fbe8-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d6dc15cc2442a40904e704d624d1fbe8-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Evaluating Self-Supervised Learning for Molecular Graph Embeddings
    * Year: `2023`
    * Authors: Hanchen Wang, Jean Kaddour, Shengchao Liu, Jian Tang, Joan Lasenby, Qi Liu
    * Abstract: Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiring embeddings without expert labelling, a capability that carries profound implications for molecular graphs due to the staggering number of potential molecules and the high cost of obtaining labels. However, GSSL methods are designed not for optimisation within a specific domain but rather for transferability across a variety of downstream tasks. This broad applicability complicates their evaluation. Addressing this challenge, we present "Molecular Graph Representation Evaluation" (MOLGRAPHEVAL), generating detailed profiles of molecular graph embeddings with interpretable and diversified attributes. MOLGRAPHEVAL offers a suite of probing tasks grouped into three categories: (i) generic graph, (ii) molecular substructure, and (iii) embedding space properties. By leveraging MOLGRAPHEVAL to benchmark existing GSSL methods against both current downstream datasets and our suite of tasks, we uncover significant inconsistencies between inferences drawn solely from existing datasets and those derived from more nuanced probing. These findings suggest that current evaluation methodologies fail to capture the entirety of the landscape.
count=2
* Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d862f7f5445255090de13b825b880d59-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d862f7f5445255090de13b825b880d59-Paper-Conference.pdf)]
    * Title: Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models
    * Year: `2023`
    * Authors: Naoki Egami, Musashi Hinck, Brandon Stewart, Hanying Wei
    * Abstract: In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. One increasingly common way to annotate documents cheaply at scale is through large language models (LLMs). However, like other scalable ways of producing annotations, such surrogate labels are often imperfect and biased. We present a new algorithm for using imperfect annotation surrogates for downstream statistical analyses while guaranteeing statistical properties—like asymptotic unbiasedness and proper uncertainty quantification—which are fundamental to CSS research. We show that direct use of surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80-90\%. To address this, we build on debiased machine learning to propose the design-based supervised learning (DSL) estimator. DSL employs a doubly-robust procedure to combine surrogate labels with a smaller number of high-quality, gold-standard labels. Our approach guarantees valid inference for downstream statistical analyses, even when surrogates are arbitrarily biased and without requiring stringent assumptions, by controlling the probability of sampling documents for gold-standard labeling. Both our theoretical analysis and experimental results show that DSL provides valid statistical inference while achieving root mean squared errors comparable to existing alternatives that focus only on prediction without inferential guarantees.
count=2
* Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/dbc4b67c6430c22460623186c3d3fdc2-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/dbc4b67c6430c22460623186c3d3fdc2-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization
    * Year: `2023`
    * Authors: Kamil Dreczkowski, Antoine Grosnit, Haitham Bou Ammar
    * Abstract: This paper introduces a modular framework for Mixed-variable and Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking and standardized evaluation in the field. Current MCBO papers often introduce non-diverse or non-standard benchmarks to evaluate their methods, impeding the proper assessment of different MCBO primitives and their combinations. Additionally, papers introducing a solution for a single MCBO primitive often omit benchmarking against baselines that utilize the same methods for the remaining primitives. This omission is primarily due to the significant implementation overhead involved, resulting in a lack of controlled assessments and an inability to showcase the merits of a contribution effectively.To overcome these challenges, our proposed framework enables an effortless combination of Bayesian Optimization components, and provides a diverse set of synthetic and real-world benchmarking tasks. Leveraging this flexibility, we implement 47 novel MCBO algorithms and benchmark them against seven existing MCBO solvers and five standard black-box optimization algorithms on ten tasks, conducting over 4000 experiments. Our findings reveal a superior combination of MCBO primitives outperforming existing approaches and illustrate the significance of model fit and the use of a trust region. We make our MCBO library available under the MIT license at \url{https://github.com/huawei-noah/HEBO/tree/master/MCBO}.
count=2
* BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/e150e6d0a1e5214740c39c6e4503ba7a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/e150e6d0a1e5214740c39c6e4503ba7a-Paper-Conference.pdf)]
    * Title: BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis
    * Year: `2023`
    * Authors: Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, Weiyao Lin
    * Abstract: Bases have become an integral part of modern deep learning-based models for time series forecasting due to their ability to act as feature extractors or future references. To be effective, a basis must be tailored to the specific set of time series data and exhibit distinct correlation with each time series within the set. However, current state-of-the-art methods are limited in their ability to satisfy both of these requirements simultaneously. To address this challenge, we propose BasisFormer, an end-to-end time series forecasting architecture that leverages learnable and interpretable bases. This architecture comprises three components: First, we acquire bases through adaptive self-supervised learning, which treats the historical and future sections of the time series as two distinct views and employs contrastive learning. Next, we design a Coef module that calculates the similarity coefficients between the time series and bases in the historical view via bidirectional cross-attention. Finally, we present a Forecast module that selects and consolidates the bases in the future view based on the similarity coefficients, resulting in accurate future predictions. Through extensive experiments on six datasets, we demonstrate that BasisFormer outperforms previous state-of-the-art methods by 11.04% and 15.78% respectively for univariate and multivariate forecasting tasks. Code isavailable at: https://github.com/nzl5116190/Basisformer.
count=2
* When Do Neural Nets Outperform Boosted Trees on Tabular Data?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/f06d5ebd4ff40b40dd97e30cee632123-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/f06d5ebd4ff40b40dd97e30cee632123-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: When Do Neural Nets Outperform Boosted Trees on Tabular Data?
    * Year: `2023`
    * Authors: Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, Micah Goldblum, Colin White
    * Abstract: Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and question the importance of this debate. To this end, we conduct the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs. Next, we analyze dozens of metafeatures to determine what \emph{properties} of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities. Our insights act as a guide for practitioners to determine which techniques may work best on their dataset. Finally, with the goal of accelerating tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 'hardest' of the datasets we study. Our benchmark suite, codebase, and all raw results are available at https://github.com/naszilla/tabzilla.
count=2
* M$^2$Hub: Unlocking the Potential of Machine Learning for Materials Discovery
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/f43380ca3f86cd989f3269583c3c8b55-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/f43380ca3f86cd989f3269583c3c8b55-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: M$^2$Hub: Unlocking the Potential of Machine Learning for Materials Discovery
    * Year: `2023`
    * Authors: Yuanqi Du, Yingheng Wang, Yining Huang, Jianan Canal Li, Yanqiao Zhu, Tian Xie, Chenru Duan, John Gregoire, Carla P. Gomes
    * Abstract: We introduce M$^2$Hub, a toolkit for advancing machine learning in materials discovery. Machine learning has achieved remarkable progress in modeling molecular structures, especially biomolecules for drug discovery. However, the development of machine learning approaches for modeling materials structures lag behind, which is partly due to the lack of an integrated platform that enables access to diverse tasks for materials discovery. To bridge this gap, M$^2$Hub will enable easy access to materials discovery tasks, datasets, machine learning methods, evaluations, and benchmark results that cover the entire workflow. Specifically, the first release of M$^2$Hub focuses on three key stages in materials discovery: virtual screening, inverse design, and molecular simulation, including 9 datasets that covers 6 types of materials with 56 tasks across 8 types of material properties. We further provide 2 synthetic datasets for the purpose of generative tasks on materials. In addition to random data splits, we also provide 3 additional data partitions to reflect the real-world materials discovery scenarios. State-of-the-art machine learning methods (including those are suitable for materials structures but never compared in the literature) are benchmarked on representative tasks. Our codes and library are publicly available at \url{https://github.com/yuanqidu/M2Hub}.
count=2
* DivBO: Diversity-aware CASH for Ensemble Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/13b2f88be223cd2b4d6be67b56e02fa8-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/13b2f88be223cd2b4d6be67b56e02fa8-Paper-Conference.pdf)]
    * Title: DivBO: Diversity-aware CASH for Ensemble Learning
    * Year: `2022`
    * Authors: Yu Shen, Yupeng Lu, Yang Li, Yaofeng Tu, Wentao Zhang, Bin CUI
    * Abstract: The Combined Algorithm Selection and Hyperparameters optimization (CASH) problem is one of the fundamental problems in Automated Machine Learning (AutoML). Motivated by the success of ensemble learning, recent AutoML systems build post-hoc ensembles to output the final predictions instead of using the best single learner. However, while most CASH methods focus on searching for a single learner with the best performance, they neglect the diversity among base learners (i.e., they may suggest similar configurations to previously evaluated ones), which is also a crucial consideration when building an ensemble. To tackle this issue and further enhance the ensemble performance, we propose DivBO, a diversity-aware framework to inject explicit search of diversity into the CASH problems. In the framework, we propose to use a diversity surrogate to predict the pair-wise diversity of two unseen configurations. Furthermore, we introduce a temporary pool and a weighted acquisition function to guide the search of both performance and diversity based on Bayesian optimization. Empirical results on 15 public datasets show that DivBO achieves the best average ranks (1.82 and 1.73) on both validation and test errors among 10 compared methods, including post-hoc designs in recent AutoML systems and state-of-the-art baselines for ensemble learning on CASH problems.
count=2
* Conformal Frequency Estimation with Sketched Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/2b2011a7d5396faf5899863d896a3c24-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/2b2011a7d5396faf5899863d896a3c24-Paper-Conference.pdf)]
    * Title: Conformal Frequency Estimation with Sketched Data
    * Year: `2022`
    * Authors: Matteo Sesia, Stefano Favaro
    * Abstract: A flexible conformal inference method is developed to construct confidence intervals for the frequencies of queried objects in very large data sets, based on a much smaller sketch of those data. The approach is data-adaptive and requires no knowledge of the data distribution or of the details of the sketching algorithm; instead, it constructs provably valid frequentist confidence intervals under the sole assumption of data exchangeability. Although our solution is broadly applicable, this paper focuses on applications involving the count-min sketch algorithm and a non-linear variation thereof. The performance is compared to that of frequentist and Bayesian alternatives through simulations and experiments with data sets of SARS-CoV-2 DNA sequences and classic English literature.
count=2
* Subgroup Robustness Grows On Trees: An Empirical Baseline Investigation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/408cf1a1d9ff35d5fea7075565dbf434-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/408cf1a1d9ff35d5fea7075565dbf434-Paper-Conference.pdf)]
    * Title: Subgroup Robustness Grows On Trees: An Empirical Baseline Investigation
    * Year: `2022`
    * Authors: Josh Gardner, Zoran Popovic, Ludwig Schmidt
    * Abstract: Researchers have proposed many methods for fair and robust machine learning, but comprehensive empirical evaluation of their subgroup robustness is lacking. In this work, we address this gap in the context of tabular data, where sensitive subgroups are clearly-defined, real-world fairness problems abound, and prior works often do not compare to state-of-the-art tree-based models as baselines. We conduct an empirical comparison of several previously-proposed methods for fair and robust learning alongside state-of-the-art tree-based methods and other baselines. Via experiments with more than $340{,}000$ model configurations on eight datasets, we show that tree-based methods have strong subgroup robustness, even when compared to robustness- and fairness-enhancing methods. Moreover, the best tree-based models tend to show good performance over a range of metrics, while robust or group-fair models can show brittleness, with significant performance differences across different metrics for a fixed model. We also demonstrate that tree-based models show less sensitivity to hyperparameter configurations, and are less costly to train. Our work suggests that tree-based ensemble models make an effective baseline for tabular data, and are a sensible default when subgroup robustness is desired. See https://github.com/jpgard/subgroup-robustness-grows-on-trees for code to reproduce our experiments and detailed experimental results.
count=2
* Hierarchical Lattice Layer for Partially Monotone Neural Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/47ed62021460f2e9bba7be3e74260090-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/47ed62021460f2e9bba7be3e74260090-Paper-Conference.pdf)]
    * Title: Hierarchical Lattice Layer for Partially Monotone Neural Networks
    * Year: `2022`
    * Authors: Hiroki Yanagisawa, Kohei Miyaguchi, Takayuki Katsuki
    * Abstract: Partially monotone regression is a regression analysis in which the target values are monotonically increasing with respect to a subset of input features. The TensorFlow Lattice library is one of the standard machine learning libraries for partially monotone regression. It consists of several neural network layers, and its core component is the lattice layer. One of the problems of the lattice layer is that it requires the projected gradient descent algorithm with many constraints to train it. Another problem is that it cannot receive a high-dimensional input vector due to the memory consumption. We propose a novel neural network layer, the hierarchical lattice layer (HLL), as an extension of the lattice layer so that we can use a standard stochastic gradient descent algorithm to train HLL while satisfying monotonicity constraints and so that it can receive a high-dimensional input vector. Our experiments demonstrate that HLL did not sacrifice its prediction performance on real datasets compared with the lattice layer.
count=2
* Bridge the Gap Between Architecture Spaces via A Cross-Domain Predictor
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/572aaddf9ff774f7c1cf3d0c81c7185b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/572aaddf9ff774f7c1cf3d0c81c7185b-Paper-Conference.pdf)]
    * Title: Bridge the Gap Between Architecture Spaces via A Cross-Domain Predictor
    * Year: `2022`
    * Authors: Yuqiao Liu, Yehui Tang, Zeqiong Lv, Yunhe Wang, Yanan Sun
    * Abstract: Neural Architecture Search (NAS) can automatically design promising neural architectures without artificial experience. Though it achieves great success, prohibitively high search cost is required to find a high-performance architecture, which blocks its practical implementation. Neural predictor can directly evaluate the performance of neural networks based on their architectures and thereby save much budget. However, existing neural predictors require substantial annotated architectures trained from scratch, which still consume many computational resources. To solve this issue, we propose a Cross-Domain Predictor (CDP), which is trained based on the existing NAS benchmark datasets (e.g., NAS-Bench-101), but can be used to find high-performance architectures in large-scale search spaces. Particularly, we propose a progressive subspace adaptation strategy to address the domain discrepancy between the source architecture space and the target space. Considering the large difference between two architecture spaces, an assistant space is developed to smooth the transfer process. Compared with existing NAS methods, the proposed CDP is much more efficient. For example, CDP only requires the search cost of 0.1 GPU Days to find architectures with 76.9% top-1 accuracy on ImageNet and 97.51% on CIFAR-10.
count=2
* Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/5771d9f214b75be6ff20f63bba315644-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/5771d9f214b75be6ff20f63bba315644-Paper-Conference.pdf)]
    * Title: Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization
    * Year: `2022`
    * Authors: Zijie Zhang, Yang Zhou, Xin Zhao, Tianshi Che, Lingjuan Lyu
    * Abstract: The right to be forgotten calls for efficient machine unlearning techniques that make trained machine learning models forget a cohort of data. The combination of training and unlearning operations in traditional machine unlearning methods often leads to the expensive computational cost on large-scale data. This paper presents a prompt certified machine unlearning algorithm, PCMU, which executes one-time operation of simultaneous training and unlearning in advance for a series of machine unlearning requests, without the knowledge of the removed/forgotten data. First, we establish a connection between randomized smoothing for certified robustness on classification and randomized smoothing for certified machine unlearning on gradient quantization. Second, we propose a prompt certified machine unlearning model based on randomized data smoothing and gradient quantization. We theoretically derive the certified radius R regarding the data change before and after data removals and the certified budget of data removals about R. Last but not least, we present another practical framework of randomized gradient smoothing and quantization, due to the dilemma of producing high confidence certificates in the first framework. We theoretically demonstrate the certified radius R' regarding the gradient change, the correlation between two types of certified radii, and the certified budget of data removals about R'.
count=2
* Learning Individualized Treatment Rules with Many Treatments: A Supervised Clustering Approach Using Adaptive Fusion
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/663865ea167425c6c562cb0b6bcf76c7-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/663865ea167425c6c562cb0b6bcf76c7-Paper-Conference.pdf)]
    * Title: Learning Individualized Treatment Rules with Many Treatments: A Supervised Clustering Approach Using Adaptive Fusion
    * Year: `2022`
    * Authors: Haixu Ma, Donglin Zeng, Yufeng Liu
    * Abstract: Learning an optimal Individualized Treatment Rule (ITR) is a very important problem in precision medicine. This paper is concerned with the challenge when the number of treatment arms is large, and some groups of treatments in the large treatment space may work similarly for the patients. Motivated by the recent development of supervised clustering, we propose a novel adaptive fusion based method to cluster the treatments with similar treatment effects together and estimate the optimal ITR simultaneously through a single convex optimization. The problem is formulated as balancing \textit{loss}$+$\textit{penalty} terms with a tuning parameter, which allows the entire solution path of the treatment clustering process to be clearly visualized hierarchically. For computation, we propose an efficient algorithm based on accelerated proximal gradient and further conduct a novel group-lasso based algorithm for variable selection to boost the performance. Moreover, we demonstrate the theoretical guarantee of recovering the underlying true clustering structure of the treatments for our method. Finally, we demonstrate the superior performance of our method via both simulations and a real data application on cancer treatment, which may assist the decision making process for doctors.
count=2
* What's the Harm? Sharp Bounds on the Fraction Negatively Affected by Treatment
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/666cccc6376058e251315b4de7e085b9-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/666cccc6376058e251315b4de7e085b9-Paper-Conference.pdf)]
    * Title: What's the Harm? Sharp Bounds on the Fraction Negatively Affected by Treatment
    * Year: `2022`
    * Authors: Nathan Kallus
    * Abstract: The fundamental problem of causal inference -- that we never observe counterfactuals -- prevents us from identifying how many might be negatively affected by a proposed intervention. If, in an A/B test, half of users click (or buy, or watch, or renew, etc.), whether exposed to the standard experience A or a new one B, hypothetically it could be because the change affects no one, because the change positively affects half the user population to go from no-click to click while negatively affecting the other half, or something in between. While unknowable, this impact is clearly of material importance to the decision to implement a change or not, whether due to fairness, long-term, systemic, or operational considerations. We therefore derive the tightest-possible (i.e., sharp) bounds on the fraction negatively affected (and other related estimands) given data with only factual observations, whether experimental or observational. Naturally, the more we can stratify individuals by observable covariates, the tighter the sharp bounds. Since these bounds involve unknown functions that must be learned from data, we develop a robust inference algorithm that is efficient almost regardless of how and how fast these functions are learned, remains consistent when some are mislearned, and still gives valid conservative bounds when most are mislearned. Our methodology altogether therefore strongly supports credible conclusions: it avoids spuriously point-identifying this unknowable impact, focusing on the best bounds instead, and it permits exceedingly robust inference on these. We demonstrate our method in simulation studies and in a case study of career counseling for the unemployed.
count=2
* Optimal Binary Classification Beyond Accuracy
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/7412b6288499d68769430839b98ff1c1-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/7412b6288499d68769430839b98ff1c1-Paper-Conference.pdf)]
    * Title: Optimal Binary Classification Beyond Accuracy
    * Year: `2022`
    * Authors: Shashank Singh, Justin T. Khim
    * Abstract: The vast majority of statistical theory on binary classification characterizes performance in terms of accuracy. However, accuracy is known in many cases to poorly reflect the practical consequences of classification error, most famously in imbalanced binary classification, where data are dominated by samples from one of two classes. The first part of this paper derives a novel generalization of the Bayes-optimal classifier from accuracy to any performance metric computed from the confusion matrix. Specifically, this result (a) demonstrates that stochastic classifiers sometimes outperform the best possible deterministic classifier and (b) removes an empirically unverifiable absolute continuity assumption that is poorly understood but pervades existing results. We then demonstrate how to use this generalized Bayes classifier to obtain regret bounds in terms of the error of estimating regression functions under uniform loss. Finally, we use these results to develop some of the first finite-sample statistical guarantees specific to imbalanced binary classification. Specifically, we demonstrate that optimal classification performance depends on properties of class imbalance, such as a novel notion called Uniform Class Imbalance, that have not previously been formalized. We further illustrate these contributions numerically in the case of $k$-nearest neighbor classification.
count=2
* Probabilistic Missing Value Imputation for Mixed Categorical and Ordered Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/8a7e7f5ed2aee24e98d65b5efdde8e1f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/8a7e7f5ed2aee24e98d65b5efdde8e1f-Paper-Conference.pdf)]
    * Title: Probabilistic Missing Value Imputation for Mixed Categorical and Ordered Data
    * Year: `2022`
    * Authors: Yuxuan Zhao, Alex Townsend, Madeleine Udell
    * Abstract: Many real-world datasets contain missing entries and mixed data types including categorical and ordered (e.g. continuous and ordinal) variables. Imputing the missing entries is necessary, since many data analysis pipelines require complete data, but challenging especially for mixed data. This paper proposes a probabilistic imputation method using an extended Gaussian copula model that supports both single and multiple imputation. The method models mixed categorical and ordered data using a latent Gaussian distribution. The unordered characteristics of categorical variables is explicitly modeled using the argmax operator. The method makes no assumptions on the data marginals nor does it require tuning any hyperparameters. Experimental results on synthetic and real datasets show that imputation with the extended Gaussian copula outperforms the current state-of-the-art for both categorical and ordered variables in mixed data.
count=2
* Para-CFlows: $C^k$-universal diffeomorphism approximators as superior neural surrogates
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/b9523d484af624986c2e0c630ac44ecb-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/b9523d484af624986c2e0c630ac44ecb-Paper-Conference.pdf)]
    * Title: Para-CFlows: $C^k$-universal diffeomorphism approximators as superior neural surrogates
    * Year: `2022`
    * Authors: Junlong Lyu, Zhitang Chen, Chang Feng, Wenjing Cun, Shengyu Zhu, Yanhui Geng, ZHIJIE XU, Chen Yongwei
    * Abstract: Invertible neural networks based on Coupling Flows (CFlows) have various applications such as image synthesis and data compression. The approximation universality for CFlows is of paramount importance to ensure the model expressiveness. In this paper, we prove that CFlows}can approximate any diffeomorphism in $C^k$-norm if its layers can approximate certain single-coordinate transforms. Specifically, we derive that a composition of affine coupling layers and invertible linear transforms achieves this universality. Furthermore, in parametric cases where the diffeomorphism depends on some extra parameters, we prove the corresponding approximation theorems for parametric coupling flows named Para-CFlows. In practice, we apply Para-CFlows as a neural surrogate model in contextual Bayesian optimization tasks, to demonstrate its superiority over other neural surrogate models in terms of optimization performance and gradient approximations.
count=2
* Pitfalls of Epistemic Uncertainty Quantification through Loss Minimisation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/bc1d640f841f752c689aae20b31198c1-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/bc1d640f841f752c689aae20b31198c1-Paper-Conference.pdf)]
    * Title: Pitfalls of Epistemic Uncertainty Quantification through Loss Minimisation
    * Year: `2022`
    * Authors: Viktor Bengs, Eyke Hüllermeier, Willem Waegeman
    * Abstract: Uncertainty quantification has received increasing attention in machine learning in the recent past. In particular, a distinction between aleatoric and epistemic uncertainty has been found useful in this regard. The latter refers to the learner's (lack of) knowledge and appears to be especially difficult to measure and quantify. In this paper, we analyse a recent proposal based on the idea of a second-order learner, which yields predictions in the form of distributions over probability distributions. While standard (first-order) learners can be trained to predict accurate probabilities, namely by minimising suitable loss functions on sample data, we show that loss minimisation does not work for second-order predictors: The loss functions proposed for inducing such predictors do not incentivise the learner to represent its epistemic uncertainty in a faithful way.
count=2
* ADBench: Anomaly Detection Benchmark
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/cf93972b116ca5268827d575f2cc226b-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/cf93972b116ca5268827d575f2cc226b-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: ADBench: Anomaly Detection Benchmark
    * Year: `2022`
    * Authors: Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, Yue Zhao
    * Abstract: Given a long list of anomaly detection algorithms developed in the last few decades, how do they perform with regard to (i) varying levels of supervision, (ii) different types of anomalies, and (iii) noisy and corrupted data? In this work, we answer these key questions by conducting (to our best knowledge) the most comprehensive anomaly detection benchmark with 30 algorithms on 57 benchmark datasets, named ADBench. Our extensive experiments (98,436 in total) identify meaningful insights into the role of supervision and anomaly types, and unlock future directions for researchers in algorithm selection and design. With ADBench, researchers can easily conduct comprehensive and fair evaluations for newly proposed methods on the datasets (including our contributed ones from natural language and computer vision domains) against the existing baselines. To foster accessibility and reproducibility, we fully open-source ADBench and the corresponding results.
count=2
* Deep Ensembles Work, But Are They Necessary?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/da18c47118a2d09926346f33bebde9f4-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/da18c47118a2d09926346f33bebde9f4-Paper-Conference.pdf)]
    * Title: Deep Ensembles Work, But Are They Necessary?
    * Year: `2022`
    * Authors: Taiga Abe, Estefany Kelly Buchanan, Geoff Pleiss, Richard Zemel, John P. Cunningham
    * Abstract: Ensembling neural networks is an effective way to increase accuracy, and can often match the performance of individual larger models. This observation poses a natural question: given the choice between a deep ensemble and a single neural network with similar accuracy, is one preferable over the other? Recent work suggests that deep ensembles may offer distinct benefits beyond predictive power: namely, uncertainty quantification and robustness to dataset shift. In this work, we demonstrate limitations to these purported benefits, and show that a single (but larger) neural network can replicate these qualities. First, we show that ensemble diversity, by any metric, does not meaningfully contribute to an ensemble's ability to detect out-of-distribution (OOD) data, but is instead highly correlated with the relative improvement of a single larger model. Second, we show that the OOD performance afforded by ensembles is strongly determined by their in-distribution (InD) performance, and - in this sense - is not indicative of any "effective robustness." While deep ensembles are a practical way to achieve improvements to predictive power, uncertainty quantification, and robustness, our results show that these improvements can be replicated by a (larger) single model.
count=2
* Fuzzy Learning Machine
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/ee26c68c6d62b7d8333815264aa28577-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/ee26c68c6d62b7d8333815264aa28577-Paper-Conference.pdf)]
    * Title: Fuzzy Learning Machine
    * Year: `2022`
    * Authors: Junbiao Cui, Jiye Liang
    * Abstract: Classification is one of the most important problems in machine learning and the nature of it is concept cognition. So far, dozens of different classifiers have been designed. Although their working mechanisms vary widely, few of them fully consider concept cognition. In this paper, a new learning machine, fuzzy learning machine (FLM), is proposed from the perspective of concept cognition. Inspired by cognitive science, its working mechanism is of strong interpretability. At the same time, FLM roots in set theory and fuzzy set theory, so FLM has a solid mathematical foundation. The systematic experimental results on a large number of data sets show that FLM can achieve excellent performance, even with the simple implementation.
count=2
* Off-Policy Evaluation with Policy-Dependent Optimization Response
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/f074a994e062146561db9cdc63999efa-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/f074a994e062146561db9cdc63999efa-Paper-Conference.pdf)]
    * Title: Off-Policy Evaluation with Policy-Dependent Optimization Response
    * Year: `2022`
    * Authors: Wenshuo Guo, Michael Jordan, Angela Zhou
    * Abstract: The intersection of causal inference and machine learning for decision-making is rapidly expanding, but the default decision criterion remains an average of individual causal outcomes across a population. In practice, various operational restrictions ensure that a decision-maker's utility is not realized as an average but rather as an output of a downstream decision-making problem (such as matching, assignment, network flow, minimizing predictive risk). In this work, we develop a new framework for off-policy evaluation with policy-dependent linear optimization responses: causal outcomes introduce stochasticity in objective function coefficients. Under this framework, a decision-maker's utility depends on the policy-dependent optimization, which introduces a fundamental challenge of optimization bias even for the case of policy evaluation. We construct unbiased estimators for the policy-dependent estimand by a perturbation method, and discuss asymptotic variance properties for a set of adjusted plug-in estimators. Lastly, attaining unbiased policy evaluation allows for policy optimization: we provide a general algorithm for optimizing causal interventions. We corroborate our theoretical results with numerical simulations.
count=2
* Neurosymbolic Deep Generative Models for Sequence Data with Relational Constraints
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/f13ceb1b94145aad0e54186373cc86d7-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/f13ceb1b94145aad0e54186373cc86d7-Paper-Conference.pdf)]
    * Title: Neurosymbolic Deep Generative Models for Sequence Data with Relational Constraints
    * Year: `2022`
    * Authors: Halley Young, Maxwell Du, Osbert Bastani
    * Abstract: There has been significant recent progress designing deep generative models that generate realistic sequence data such as text or music. Nevertheless, it remains difficult to incorporate high-level structure to guide the generative process, and many such models perform well on local coherence, but less so on global coherence. We propose a novel approach for incorporating global structure in the form of relational constraints between different subcomponents of an example (e.g., lines of a poem or measures of music). Our generative model has two parts: (i) one model to generate a realistic set of relational constraints, and (ii) a second model to generate realistic data satisfying these constraints. For model (i), we propose a constrained optimization algorithm that infers the relational constraints present in the training data, and then learn a generative model based on the resulting constraint data. In our experiments, we show that our approach significantly improves over state-of-the-art in terms of capturing high-level structure in the data, while performing comparably or better in terms of low-level structure. We also show that using constrained optimization for part (ii) as well leads to increased controllability with little decrease in quality compared to pure learning-based models.
count=2
* PhysGNN: A Physics--Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image--Guided Neurosurgery
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/f200119a40846e508954abcd61f5f3fd-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/f200119a40846e508954abcd61f5f3fd-Paper-Conference.pdf)]
    * Title: PhysGNN: A Physics--Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image--Guided Neurosurgery
    * Year: `2022`
    * Authors: Yasmin Salehi, Dennis Giannacopoulos
    * Abstract: Correctly capturing intraoperative brain shift in image-guided neurosurgical procedures is a critical task for aligning preoperative data with intraoperative geometry for ensuring accurate surgical navigation. While the finite element method (FEM) is a proven technique to effectively approximate soft tissue deformation through biomechanical formulations, their degree of success boils down to a trade-off between accuracy and speed. To circumvent this problem, the most recent works in this domain have proposed leveraging data-driven models obtained by training various machine learning algorithms---e.g., random forests, artificial neural networks (ANNs)---with the results of finite element analysis (FEA) to speed up tissue deformation approximations by prediction. These methods, however, do not account for the structure of the finite element (FE) mesh during training that provides information on node connectivities as well as the distance between them, which can aid with approximating tissue deformation based on the proximity of force load points with the rest of the mesh nodes. Therefore, this work proposes a novel framework, PhysGNN, a data-driven model that approximates the solution of the FEM by leveraging graph neural networks (GNNs), which are capable of accounting for the mesh structural information and inductive learning over unstructured grids and complex topological structures. Empirically, we demonstrate that the proposed architecture, PhysGNN, promises accurate and fast soft tissue deformation approximations, and is competitive with the state-of-the-art (SOTA) algorithms while promising enhanced computational feasibility, therefore suitable for neurosurgical settings.
count=2
* Convex Polytope Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/285a25c17f351708754cdb6d56f3962e-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/285a25c17f351708754cdb6d56f3962e-Paper.pdf)]
    * Title: Convex Polytope Trees
    * Year: `2021`
    * Authors: Mohammadreza Armandpour, Ali Sadeghian, Mingyuan Zhou
    * Abstract: A decision tree is commonly restricted to use a single hyperplane to split the covariate space at each of its internal nodes. It often requires a large number of nodes to achieve high accuracy. In this paper, we propose convex polytope trees (CPT) to expand the family of decision trees by an interpretable generalization of their decision boundary. The splitting function at each node of CPT is based on the logical disjunction of a community of differently weighted probabilistic linear decision-makers, which also geometrically corresponds to a convex polytope in the covariate space. We use a nonparametric Bayesian prior at each node to infer the community's size, encouraging simpler decision boundaries by shrinking the number of polytope facets. We develop a greedy method to efficiently construct CPT and scalable end-to-end training algorithms for the tree parameters when the tree structure is given. We empirically demonstrate the efficiency of CPT over existing state-of-the-art decision trees in several real-world classification and regression tasks from diverse domains.
count=2
* Conformal Prediction using Conditional Histograms
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/31b3b31a1c2f8a370206f111127c0dbd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/31b3b31a1c2f8a370206f111127c0dbd-Paper.pdf)]
    * Title: Conformal Prediction using Conditional Histograms
    * Year: `2021`
    * Authors: Matteo Sesia, Yaniv Romano
    * Abstract: This paper develops a conformal method to compute prediction intervals for non-parametric regression that can automatically adapt to skewed data. Leveraging black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, it translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the black-box model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives, including conformalized quantile regression and other distributional conformal prediction approaches.
count=2
* Machine Learning for Variance Reduction in Online Experiments
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/488b084119a1c7a4950f00706ec7ea16-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/488b084119a1c7a4950f00706ec7ea16-Paper.pdf)]
    * Title: Machine Learning for Variance Reduction in Online Experiments
    * Year: `2021`
    * Authors: Yongyi Guo, Dominic Coey, Mikael Konutgan, Wenting Li, Chris Schoener, Matt Goldman
    * Abstract: We consider the problem of variance reduction in randomized controlled trials, through the use of covariates correlated with the outcome but independent of the treatment. We propose a machine learning regression-adjusted treatment effect estimator, which we call MLRATE. MLRATE uses machine learning predictors of the outcome to reduce estimator variance. It employs cross-fitting to avoid overfitting biases, and we prove consistency and asymptotic normality under general conditions. MLRATE is robust to poor predictions from the machine learning step: if the predictions are uncorrelated with the outcomes, the estimator performs asymptotically no worse than the standard difference-in-means estimator, while if predictions are highly correlated with outcomes, the efficiency gains are large. In A/A tests, for a set of 48 outcome metrics commonly monitored in Facebook experiments, the estimator has over $70\%$ lower variance than the simple difference-in-means estimator, and about $19\%$ lower variance than the common univariate procedure which adjusts only for pre-experiment values of the outcome.
count=2
* Multiple Descent: Design Your Own Generalization Curve
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/4ae67a7dd7e491f8fb6f9ea0cf25dfdb-Paper.pdf)]
    * Title: Multiple Descent: Design Your Own Generalization Curve
    * Year: `2021`
    * Authors: Lin Chen, Yifei Min, Mikhail Belkin, Amin Karbasi
    * Abstract: This paper explores the generalization loss of linear regression in variably parameterized families of models, both under-parameterized and over-parameterized. We show that the generalization curve can have an arbitrary number of peaks, and moreover, the locations of those peaks can be explicitly controlled. Our results highlight the fact that both the classical U-shaped generalization curve and the recently observed double descent curve are not intrinsic properties of the model family. Instead, their emergence is due to the interaction between the properties of the data and the inductive biases of learning algorithms.
count=2
* Reliable Post hoc Explanations: Modeling Uncertainty in Explainability
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/4e246a381baf2ce038b3b0f82c7d6fb4-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/4e246a381baf2ce038b3b0f82c7d6fb4-Paper.pdf)]
    * Title: Reliable Post hoc Explanations: Modeling Uncertainty in Explainability
    * Year: `2021`
    * Authors: Dylan Slack, Anna Hilgard, Sameer Singh, Himabindu Lakkaraju
    * Abstract: As black box explanations are increasingly being employed to establish model credibility in high stakes settings, it is important to ensure that these explanations are accurate and reliable. However, prior work demonstrates that explanations generated by state-of-the-art techniques are inconsistent, unstable, and provide very little insight into their correctness and reliability. In addition, these methods are also computationally inefficient, and require significant hyper-parameter tuning. In this paper, we address the aforementioned challenges by developing a novel Bayesian framework for generating local explanations along with their associated uncertainty. We instantiate this framework to obtain Bayesian versions of LIME and KernelSHAP which output credible intervals for the feature importances, capturing the associated uncertainty. The resulting explanations not only enable us to make concrete inferences about their quality (e.g., there is a 95% chance that the feature importance lies within the given range), but are also highly consistent and stable. We carry out a detailed theoretical analysis that leverages the aforementioned uncertainty to estimate how many perturbations to sample, and how to sample for faster convergence. This work makes the first attempt at addressing several critical issues with popular explanation methods in one shot, thereby generating consistent, stable, and reliable explanations with guarantees in a computationally efficient manner. Experimental evaluation with multiple real world datasets and user studies demonstrate that the efficacy of the proposed framework.
count=2
* SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/5a9542c773018268fc6271f7afeea969-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/5a9542c773018268fc6271f7afeea969-Paper.pdf)]
    * Title: SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning
    * Year: `2021`
    * Authors: Sungmin Cha, beomyoung kim, YoungJoon Yoo, Taesup Moon
    * Abstract: We consider a class-incremental semantic segmentation (CISS) problem. While some recently proposed algorithms utilized variants of knowledge distillation (KD) technique to tackle the problem, they only partially addressed the key additional challenges in CISS that causes the catastrophic forgetting; \textit{i.e.}, the semantic drift of the background class and multi-label prediction issue. To better address these challenges, we propose a new method, dubbed as SSUL-M (Semantic Segmentation with Unknown Label with Memory), by carefully combining several techniques tailored for semantic segmentation. More specifically, we make three main contributions; (1) modeling \textit{unknown} class within the background class to help learning future classes (help plasticity), (2) \textit{freezing} backbone network and past classifiers with binary cross-entropy loss and pseudo-labeling to overcome catastrophic forgetting (help stability), and (3) utilizing \textit{tiny exemplar memory} for the first time in CISS to improve \textit{both} plasticity and stability. As a result, we show our method achieves significantly better performance than the recent state-of-the-art baselines on the standard benchmark datasets. Furthermore, we justify our contributions with thorough and extensive ablation analyses and discuss different natures of the CISS problem compared to the standard class-incremental learning for classification. The official code is available at https://github.com/clovaai/SSUL.
count=2
* Characterizing the risk of fairwashing
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/7caf5e22ea3eb8175ab518429c8589a4-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/7caf5e22ea3eb8175ab518429c8589a4-Paper.pdf)]
    * Title: Characterizing the risk of fairwashing
    * Year: `2021`
    * Authors: Ulrich Aïvodji, Hiromi Arai, Sébastien Gambs, Satoshi Hara
    * Abstract: Fairwashing refers to the risk that an unfair black-box model can be explained by a fairer model through post-hoc explanation manipulation. In this paper, we investigate the capability of fairwashing attacks by analyzing their fidelity-unfairness trade-offs. In particular, we show that fairwashed explanation models can generalize beyond the suing group (i.e., data points that are being explained), meaning that a fairwashed explainer can be used to rationalize subsequent unfair decisions of a black-box model. We also demonstrate that fairwashing attacks can transfer across black-box models, meaning that other black-box models can perform fairwashing without explicitly using their predictions. This generalization and transferability of fairwashing attacks imply that their detection will be difficult in practice. Finally, we propose an approach to quantify the risk of fairwashing, which is based on the computation of the range of the unfairness of high-fidelity explainers.
count=2
* Probability Paths and the Structure of Predictions over Time
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/7f53f8c6c730af6aeb52e66eb74d8507-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/7f53f8c6c730af6aeb52e66eb74d8507-Paper.pdf)]
    * Title: Probability Paths and the Structure of Predictions over Time
    * Year: `2021`
    * Authors: Zhiyuan Jerry Lin, Hao Sheng, Sharad Goel
    * Abstract: In settings ranging from weather forecasts to political prognostications to financial projections, probability estimates of future binary outcomes often evolve over time. For example, the estimated likelihood of rain on a specific day changes by the hour as new information becomes available. Given a collection of such probability paths, we introduce a Bayesian framework -- which we call the Gaussian latent information martingale, or GLIM -- for modeling the structure of dynamic predictions over time. Suppose, for example, that the likelihood of rain in a week is 50%, and consider two hypothetical scenarios. In the first, one expects the forecast to be equally likely to become either 25% or 75% tomorrow; in the second, one expects the forecast to stay constant for the next several days. A time-sensitive decision-maker might select a course of action immediately in the latter scenario, but may postpone their decision in the former, knowing that new information is imminent. We model these trajectories by assuming predictions update according to a latent process of information flow, which is inferred from historical data. In contrast to general methods for time series analysis, this approach preserves important properties of probability paths such as the martingale structure and appropriate amount of volatility and better quantifies future uncertainties around probability paths. We show that GLIM outperforms three popular baseline methods, producing better estimated posterior probability path distributions measured by three different metrics. By elucidating the dynamic structure of predictions over time, we hope to help individuals make more informed choices.
count=2
* Deep Extended Hazard Models for Survival Analysis
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/7f6caf1f0ba788cd7953d817724c2b6e-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/7f6caf1f0ba788cd7953d817724c2b6e-Paper.pdf)]
    * Title: Deep Extended Hazard Models for Survival Analysis
    * Year: `2021`
    * Authors: Qixian Zhong, Jonas W. Mueller, Jane-Ling Wang
    * Abstract: Unlike standard prediction tasks, survival analysis requires modeling right censored data, which must be treated with care. While deep neural networks excel in traditional supervised learning, it remains unclear how to best utilize these models in survival analysis. A key question asks which data-generating assumptions of traditional survival models should be retained and which should be made more flexible via the function-approximating capabilities of neural networks. Rather than estimating the survival function targeted by most existing methods, we introduce a Deep Extended Hazard (DeepEH) model to provide a flexible and general framework for deep survival analysis. The extended hazard model includes the conventional Cox proportional hazards and accelerated failure time models as special cases, so DeepEH subsumes the popular Deep Cox proportional hazard (DeepSurv) and Deep Accelerated Failure Time (DeepAFT) models. We additionally provide theoretical support for the proposed DeepEH model by establishing consistency and convergence rate of the survival function estimator, which underscore the attractive feature that deep learning is able to detect low-dimensional structure of data in high-dimensional space. Numerical experiments also provide evidence that the proposed methods outperform existing statistical and deep learning approaches to survival analysis.
count=2
* Discerning Decision-Making Process of Deep Neural Networks with Hierarchical Voting Transformation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/8f1fa0193ca2b5d2fa0695827d8270e9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/8f1fa0193ca2b5d2fa0695827d8270e9-Paper.pdf)]
    * Title: Discerning Decision-Making Process of Deep Neural Networks with Hierarchical Voting Transformation
    * Year: `2021`
    * Authors: Ying Sun, Hengshu Zhu, Chuan Qin, Fuzhen Zhuang, Qing He, Hui Xiong
    * Abstract: Neural network based deep learning techniques have shown great success for numerous applications. While it is expected to understand their intrinsic decision-making processes, these deep neural networks often work in a black-box way. To this end, in this paper, we aim to discern the decision-making processes of neural networks through a hierarchical voting strategy by developing an explainable deep learning model, namely Voting Transformation-based Explainable Neural Network (VOTEN). Specifically, instead of relying on massive feature combinations, VOTEN creatively models expressive single-valued voting functions between explicitly modeled latent concepts to achieve high fitting ability. Along this line, we first theoretically analyze the major components of VOTEN and prove the relationship and advantages of VOTEN compared with Multi-Layer Perceptron (MLP), the basic structure of deep neural networks. Moreover, we design efficient algorithms to improve the model usability by explicitly showing the decision processes of VOTEN. Finally, extensive experiments on multiple real-world datasets clearly validate the performances and explainability of VOTEN.
count=2
* Multi-Step Budgeted Bayesian Optimization with Unknown Evaluation Costs
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/a8ecbabae151abacba7dbde04f761c37-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/a8ecbabae151abacba7dbde04f761c37-Paper.pdf)]
    * Title: Multi-Step Budgeted Bayesian Optimization with Unknown Evaluation Costs
    * Year: `2021`
    * Authors: Raul Astudillo, Daniel Jiang, Maximilian Balandat, Eytan Bakshy, Peter Frazier
    * Abstract: Bayesian optimization (BO) is a sample-efficient approach to optimizing costly-to-evaluate black-box functions. Most BO methods ignore how evaluation costs may vary over the optimization domain. However, these costs can be highly heterogeneous and are often unknown in advance in many practical settings, such as hyperparameter tuning of machine learning algorithms or physics-based simulation optimization. Moreover, those few existing methods that acknowledge cost heterogeneity do not naturally accommodate a budget constraint on the total evaluation cost. This combination of unknown costs and a budget constraint introduces a new dimension to the exploration-exploitation trade-off, where learning about the cost incurs a cost itself. Existing methods do not reason about the various trade-offs of this problem in a principled way, leading often to poor performance. We formalize this claim by proving that the expected improvement and the expected improvement per unit of cost, arguably the two most widely used acquisition functions in practice, can be arbitrarily inferior with respect to the optimal non-myopic policy. To overcome the shortcomings of existing approaches, we propose the budgeted multi-step expected improvement, a non-myopic acquisition function that generalizes classical expected improvement to the setting of heterogeneous and unknown evaluation costs. We show that our acquisition function outperforms existing methods in a variety of synthetic and real problems.
count=2
* Causal Effect Inference for Structured Treatments
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/d02e9bdc27a894e882fa0c9055c99722-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/d02e9bdc27a894e882fa0c9055c99722-Paper.pdf)]
    * Title: Causal Effect Inference for Structured Treatments
    * Year: `2021`
    * Authors: Jean Kaddour, Yuchen Zhu, Qi Liu, Matt J. Kusner, Ricardo Silva
    * Abstract: We address the estimation of conditional average treatment effects (CATEs) for structured treatments (e.g., graphs, images, texts). Given a weak condition on the effect, we propose the generalized Robinson decomposition, which (i) isolates the causal estimand (reducing regularization bias), (ii) allows one to plug in arbitrary models for learning, and (iii) possesses a quasi-oracle convergence guarantee under mild assumptions. In experiments with small-world and molecular graphs we demonstrate that our approach outperforms prior work in CATE estimation.
count=2
* How Powerful are Performance Predictors in Neural Architecture Search?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/ef575e8837d065a1683c022d2077d342-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/ef575e8837d065a1683c022d2077d342-Paper.pdf)]
    * Title: How Powerful are Performance Predictors in Neural Architecture Search?
    * Year: `2021`
    * Authors: Colin White, Arber Zela, Robin Ru, Yang Liu, Frank Hutter
    * Abstract: Early methods in the rapidly developing field of neural architecture search (NAS) required fully training thousands of neural networks. To reduce this extreme computational cost, dozens of techniques have since been proposed to predict the final performance of neural architectures. Despite the success of such performance prediction methods, it is not well-understood how different families of techniques compare to one another, due to the lack of an agreed-upon evaluation metric and optimization for different constraints on the initialization time and query time. In this work, we give the first large-scale study of performance predictors by analyzing 31 techniques ranging from learning curve extrapolation, to weight-sharing, to supervised learning, to zero-cost proxies. We test a number of correlation- and rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based NAS frameworks. Our results act as recommendations for the best predictors to use in different settings, and we show that certain families of predictors can be combined to achieve even better predictive power, opening up promising research directions. We release our code, featuring a library of 31 performance predictors.
count=2
* Algorithmic recourse under imperfect causal knowledge: a probabilistic approach
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/02a3c7fb3f489288ae6942498498db20-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/02a3c7fb3f489288ae6942498498db20-Paper.pdf)]
    * Title: Algorithmic recourse under imperfect causal knowledge: a probabilistic approach
    * Year: `2020`
    * Authors: Amir-Hossein Karimi, Julius von Kügelgen, Bernhard Schölkopf, Isabel Valera
    * Abstract: Recent work has discussed the limitations of counterfactual explanations to recommend actions for algorithmic recourse, and argued for the need of taking causal relationships between features into consideration. Unfortunately, in practice, the true underlying structural causal model is generally unknown. In this work, we first show that it is impossible to guarantee recourse without access to the true structural equations. To address this limitation, we propose two probabilistic approaches to select optimal actions that achieve recourse with high probability given limited causal knowledge (e.g., only the causal graph). The first captures uncertainty over structural equations under additive Gaussian noise, and uses Bayesian model averaging to estimate the counterfactual distribution. The second removes any assumptions on the structural equations by instead computing the average effect of recourse actions on individuals similar to the person who seeks recourse, leading to a novel subpopulation-based interventional notion of recourse. We then derive a gradient-based procedure for selecting optimal recourse actions, and empirically show that the proposed approaches lead to more reliable recommendations under imperfect causal knowledge than non-probabilistic baselines.
count=2
* Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/0d770c496aa3da6d2c3f2bd19e7b9d6b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/0d770c496aa3da6d2c3f2bd19e7b9d6b-Paper.pdf)]
    * Title: Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability
    * Year: `2020`
    * Authors: Christopher Frye, Colin Rowat, Ilya Feige
    * Abstract: Explaining AI systems is fundamental both to the development of high performing models and to the trust placed in them by their users. The Shapley framework for explainability has strength in its general applicability combined with its precise, rigorous foundation: it provides a common, model-agnostic language for AI explainability and uniquely satisfies a set of intuitive mathematical axioms. However, Shapley values are too restrictive in one significant regard: they ignore all causal structure in the data. We introduce a less restrictive framework, Asymmetric Shapley values (ASVs), which are rigorously founded on a set of axioms, applicable to any AI system, and can flexibly incorporate any causal structure known to be respected by the data. We demonstrate that ASVs can (i) improve model explanations by incorporating causal information, (ii) provide an unambiguous test for unfair discrimination in model predictions, (iii) enable sequentially incremental explanations in time-series models, and (iv) support feature-selection studies without the need for model retraining.
count=2
* Robust Recursive Partitioning for Heterogeneous Treatment Effects with Uncertainty Quantification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/1819020b02e926785cf3be594d957696-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/1819020b02e926785cf3be594d957696-Paper.pdf)]
    * Title: Robust Recursive Partitioning for Heterogeneous Treatment Effects with Uncertainty Quantification
    * Year: `2020`
    * Authors: Hyun-Suk Lee, Yao Zhang, William Zame, Cong Shen, Jang-Won Lee, Mihaela van der Schaar
    * Abstract: Subgroup analysis of treatment effects plays an important role in applications from medicine to public policy to recommender systems. It allows physicians (for example) to identify groups of patients for whom a given drug or treatment is likely to be effective and groups of patients for which it is not. Most of the current methods of subgroup analysis begin with a particular algorithm for estimating individualized treatment effects (ITE) and identify subgroups by maximizing the difference across subgroups of the average treatment effect in each subgroup. These approaches have several weaknesses: they rely on a particular algorithm for estimating ITE, they ignore (in)homogeneity within identified subgroups, and they do not produce good confidence estimates. This paper develops a new method for subgroup analysis, R2P, that addresses all these weaknesses. R2P uses an arbitrary, exogenously prescribed algorithm for estimating ITE and quantifies the uncertainty of the ITE estimation, using a construction that is more robust than other methods. Experiments using synthetic and semi-synthetic datasets (based on real data) demonstrate that R2P constructs partitions that are simultaneously more homogeneous within groups and more heterogeneous across groups than the partitions produced by other methods. Moreover, because R2P can employ any ITE estimator, it also produces much narrower confidence intervals with a prescribed coverage guarantee than other methods.
count=2
* Feature Importance Ranking for Deep Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/36ac8e558ac7690b6f44e2cb5ef93322-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf)]
    * Title: Feature Importance Ranking for Deep Learning
    * Year: `2020`
    * Authors: Maksymilian Wojtas, Ke Chen
    * Abstract: Feature importance ranking has become a powerful tool for explainable AI. However, its nature of combinatorial optimization poses a great challenge for deep learning. In this paper, we propose a novel dual-net architecture consisting of operator and selector for discovery of an optimal feature subset of a fixed size and ranking the importance of those features in the optimal subset simultaneously. During learning, the operator is trained for a supervised learning task via optimal feature subset candidates generated by the selector that learns predicting the learning performance of the operator working on different optimal subset candidates. We develop an alternate learning algorithm that trains two nets jointly and incorporates a stochastic local search procedure into learning to address the combinatorial optimization challenge. In deployment, the selector generates an optimal feature subset and ranks feature importance, while the operator makes predictions based on the optimal subset for test data. A thorough evaluation on synthetic, benchmark and real data sets suggests that our approach outperforms several state-of-the-art feature importance ranking and supervised feature selection methods. (Our source code is available: https://github.com/maksym33/FeatureImportanceDL)
count=2
* Bayesian Probabilistic Numerical Integration with Tree-Based Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/3fe94a002317b5f9259f82690aeea4cd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/3fe94a002317b5f9259f82690aeea4cd-Paper.pdf)]
    * Title: Bayesian Probabilistic Numerical Integration with Tree-Based Models
    * Year: `2020`
    * Authors: Harrison Zhu, Xing Liu, Ruya Kang, Zhichao Shen, Seth Flaxman, Francois-Xavier Briol
    * Abstract: Bayesian quadrature (BQ) is a method for solving numerical integration problems in a Bayesian manner, which allows users to quantify their uncertainty about the solution. The standard approach to BQ is based on a Gaussian process (GP) approximation of the integrand. As a result, BQ is inherently limited to cases where GP approximations can be done in an efficient manner, thus often prohibiting very high-dimensional or non-smooth target functions. This paper proposes to tackle this issue with a new Bayesian numerical integration algorithm based on Bayesian Additive Regression Trees (BART) priors, which we call BART-Int. BART priors are easy to tune and well-suited for discontinuous functions. We demonstrate that they also lend themselves naturally to a sequential design setting and that explicit convergence rates can be obtained in a variety of settings. The advantages and disadvantages of this new methodology are highlighted on a set of benchmark tests including the Genz functions, on a rare-event simulation problem and on a Bayesian survey design problem.
count=2
* Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Evolvability
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/5f8b73c0d4b1bf60dd7173b660b87c29-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/5f8b73c0d4b1bf60dd7173b660b87c29-Paper.pdf)]
    * Title: Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Evolvability
    * Year: `2020`
    * Authors: Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau
    * Abstract: In this paper, we revisit the problem of distribution-independently learning halfspaces under Massart noise with rate $\eta$. Recent work resolved a long-standing problem in this model of efficiently learning to error $\eta + \epsilon$ for any $\epsilon > 0$, by giving an improper learner that partitions space into $\text{poly}(d,1/\epsilon)$ regions. Here we give a much simpler algorithm and settle a number of outstanding open questions: (1) We give the first \emph{proper} learner for Massart halfspaces that achieves $\eta + \epsilon$. (2) Based on (1), we develop a blackbox knowledge distillation procedure to convert an arbitrarily complex classifier to an equally good proper classifier. (3) By leveraging a simple but overlooked connection to \emph{evolvability}, we show any SQ algorithm requires super-polynomially many queries to achieve $\mathsf{OPT} + \epsilon$. We then zoom out to study generalized linear models and give an efficient algorithm for learning under a challenging new corruption model generalizing Massart noise. Finally we study our algorithm for learning halfspaces under Massart noise empirically and find that it exhibits some appealing fairness properties as a byproduct of its strong provable robustness guarantees.
count=2
* Provably Efficient Neural Estimation of Structural Equation Models: An Adversarial Approach
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/65a99bb7a3115fdede20da98b08a370f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/65a99bb7a3115fdede20da98b08a370f-Paper.pdf)]
    * Title: Provably Efficient Neural Estimation of Structural Equation Models: An Adversarial Approach
    * Year: `2020`
    * Authors: Luofeng Liao, You-Lin Chen, Zhuoran Yang, Bo Dai, Mladen Kolar, Zhaoran Wang
    * Abstract: Structural equation models (SEMs) are widely used in sciences, ranging from economics to psychology, to uncover causal relationships underlying a complex system under consideration and estimate structural parameters of interest. We study estimation in a class of generalized SEMs where the object of interest is defined as the solution to a linear operator equation. We formulate the linear operator equation as a min-max game, where both players are parameterized by neural networks (NNs), and learn the parameters of these neural networks using the stochastic gradient descent. We consider both 2-layer and multi-layer NNs with ReLU activation functions and prove global convergence in an overparametrized regime, where the number of neurons is diverging. The results are established using techniques from online learning and local linearization of NNs, and improve in several aspects the current state-of-the-art. For the first time we provide a tractable estimation procedure for SEMs based on NNs with provable convergence and without the need for sample splitting.
count=2
* Sense and Sensitivity Analysis: Simple Post-Hoc Analysis of Bias Due to Unobserved Confounding
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/7d265aa7147bd3913fb84c7963a209d1-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/7d265aa7147bd3913fb84c7963a209d1-Paper.pdf)]
    * Title: Sense and Sensitivity Analysis: Simple Post-Hoc Analysis of Bias Due to Unobserved Confounding
    * Year: `2020`
    * Authors: Victor Veitch, Anisha Zaveri
    * Abstract: It is a truth universally acknowledged that an observed association without known mechanism must be in want of a causal estimate. Causal estimates from observational data will be biased in the presence of ‘unobserved confounding’. However, we might hope that the influence of unobserved confounders is weak relative to a ‘large’ estimated effect. The purpose of this paper is to develop Austen plots, a sensitivity analysis tool to aid such judgments by making it easier to reason about potential bias induced by unobserved confounding. We formalize confounding strength in terms of how strongly the unobserved confounding influences treatment assignment and outcome. For a target level of bias, an Austen plot shows the minimum values of treatment and outcome influence required to induce that level of bias. Austen plots generalize the classic sensitivity analysis approach of Imbens [Imb03]. Critically, Austen plots allow any approach for modeling the observed data. We illustrate the tool by assessing biases for several real causal inference problems, using a variety of machine learning approaches for the initial data analysis. Code, demo data, and a tutorial are available at github.com/anishazaveri/austen_plots.
count=2
* VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/7d97667a3e056acab9aaf653807b4a03-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/7d97667a3e056acab9aaf653807b4a03-Paper.pdf)]
    * Title: VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain
    * Year: `2020`
    * Authors: Jinsung Yoon, Yao Zhang, James Jordon, Mihaela van der Schaar
    * Abstract: Self- and semi-supervised learning frameworks have made significant progress in training machine learning models with limited labeled data in image and language domains. These methods heavily rely on the unique structure in the domain datasets (such as spatial relationships in images or semantic relationships in language). They are not adaptable to general tabular data which does not have the same explicit structure as image and language data. In this paper, we fill this gap by proposing novel self- and semi-supervised learning frameworks for tabular data, which we refer to collectively as VIME (Value Imputation and Mask Estimation). We create a novel pretext task of estimating mask vectors from corrupted tabular data in addition to the reconstruction pretext task for self-supervised learning. We also introduce a novel tabular data augmentation method for self- and semi-supervised learning frameworks. In experiments, we evaluate the proposed framework in multiple tabular datasets from various application domains, such as genomics and clinical data. VIME exceeds state-of-the-art performance in comparison to the existing baseline methods.
count=2
* Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/8ee7730e97c67473a424ccfeff49ab20-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/8ee7730e97c67473a424ccfeff49ab20-Paper.pdf)]
    * Title: Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses
    * Year: `2020`
    * Authors: Kaivalya Rawal, Himabindu Lakkaraju
    * Abstract: As predictive models are increasingly being deployed in high-stakes decision-making, there has been a lot of interest in developing algorithms which can provide recourses to affected individuals. While developing such tools is important, it is even more critical to analyze and interpret a predictive model, and vet it thoroughly to ensure that the recourses it offers are meaningful and non-discriminatory before it is deployed in the real world. To this end, we propose a novel model agnostic framework called Actionable Recourse Summaries (AReS) to construct global counterfactual explanations which provide an interpretable and accurate summary of recourses for the entire population. We formulate a novel objective which simultaneously optimizes for correctness of the recourses and interpretability of the explanations, while minimizing overall recourse costs across the entire population. More specifically, our objective enables us to learn, with optimality guarantees on recourse correctness, a small number of compact rule sets each of which capture recourses for well defined subpopulations within the data. We also demonstrate theoretically that several of the prior approaches proposed to generate recourses for individuals are special cases of our framework. Experimental evaluation with real world datasets and user studies demonstrate that our framework can provide decision makers with a comprehensive overview of recourses corresponding to any black box model, and consequently help detect undesirable model biases and discrimination.
count=2
* Instance-wise Feature Grouping
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/9b10a919ddeb07e103dc05ff523afe38-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/9b10a919ddeb07e103dc05ff523afe38-Paper.pdf)]
    * Title: Instance-wise Feature Grouping
    * Year: `2020`
    * Authors: Aria Masoomi, Chieh Wu, Tingting Zhao, Zifeng Wang, Peter Castaldi, Jennifer Dy
    * Abstract: In many learning problems, the domain scientist is often interested in discovering the groups of features that are redundant and are important for classification. Moreover, the features that belong to each group, and the important feature groups may vary per sample. But what do we mean by feature redundancy? In this paper, we formally define two types of redundancies using information theory: \textit{Representation} and \textit{Relevant redundancies}. We leverage these redundancies to design a formulation for instance-wise feature group discovery and reveal a theoretical guideline to help discover the appropriate number of groups. We approximate mutual information via a variational lower bound and learn the feature group and selector indicators with Gumbel-Softmax in optimizing our formulation. Experiments on synthetic data validate our theoretical claims. Experiments on MNIST, Fashion MNIST, and gene expression datasets show that our method discovers feature groups with high classification accuracies.
count=2
* Distributionally Robust Local Non-parametric Conditional Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/adf854f418fc96fb01ad92a2ed2fc35c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/adf854f418fc96fb01ad92a2ed2fc35c-Paper.pdf)]
    * Title: Distributionally Robust Local Non-parametric Conditional Estimation
    * Year: `2020`
    * Authors: Viet Anh Nguyen, Fan Zhang, Jose Blanchet, Erick Delage, Yinyu  Ye
    * Abstract: Conditional estimation given specific covariate values (i.e., local conditional estimation or functional estimation) is ubiquitously useful with applications in engineering, social and natural sciences. Existing data-driven non-parametric estimators mostly focus on structured homogeneous data (e.g., weakly independently and stationary data), thus they are sensitive to adversarial noise and may perform poorly under a low sample size. To alleviate these issues, we propose a new distributionally robust estimator that generates non-parametric local estimates by minimizing the worst-case conditional expected loss over all adversarial distributions in a Wasserstein ambiguity set. We show that despite being generally intractable, the local estimator can be efficiently found via convex optimization under broadly applicable settings, and it is robust to the corruption and heterogeneity of the data. Various experiments show the competitive performance of this new class of estimator.
count=2
* The Diversified Ensemble Neural Network
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/b86e8d03fe992d1b0e19656875ee557c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/b86e8d03fe992d1b0e19656875ee557c-Paper.pdf)]
    * Title: The Diversified Ensemble Neural Network
    * Year: `2020`
    * Authors: Shaofeng Zhang, Meng Liu, Junchi Yan
    * Abstract: Ensemble is a general way of improving the accuracy and stability of learning models, especially for the generalization ability on small datasets. Compared with tree-based methods, relatively less works have been devoted to an in-depth study on effective ensemble design for neural networks. In this paper, we propose a principled ensemble technique by constructing the so-called diversified ensemble layer to combine multiple networks as individual modules. We theoretically show that each individual model in our ensemble layer corresponds to weights in the ensemble layer optimized in different directions. Meanwhile, the devised ensemble layer can be readily integrated into popular neural architectures, including CNNs, RNNs, and GCNs. Extensive experiments are conducted on public tabular datasets, images, and texts. By adopting weight sharing approach, the results show our method can notably improve the accuracy and stability of the original neural networks with ignorable extra time and space overhead.
count=2
* Understanding Global Feature Contributions With Additive Importance Measures
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/c7bf0b7c1a86d5eb3be2c722cf2cf746-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/c7bf0b7c1a86d5eb3be2c722cf2cf746-Paper.pdf)]
    * Title: Understanding Global Feature Contributions With Additive Importance Measures
    * Year: `2020`
    * Authors: Ian Covert, Scott M. Lundberg, Su-In Lee
    * Abstract: Understanding the inner workings of complex machine learning models is a long-standing problem and most recent research has focused on local interpretability. To assess the role of individual input features in a global sense, we explore the perspective of defining feature importance through the predictive power associated with each feature. We introduce two notions of predictive power (model-based and universal) and formalize this approach with a framework of additive importance measures, which unifies numerous methods in the literature. We then propose SAGE, a model-agnostic method that quantifies predictive power while accounting for feature interactions. Our experiments show that SAGE can be calculated efficiently and that it assigns more accurate importance values than other methods.
count=2
* Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/1e0feeaff84a19bf3936e693311fa66d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/1e0feeaff84a19bf3936e693311fa66d-Paper.pdf)]
    * Title: Deep Multi-State Dynamic Recurrent Neural Networks Operating on Wavelet Based Neural Features for Robust Brain Machine Interfaces
    * Year: `2019`
    * Authors: Benyamin Allahgholizadeh Haghi, Spencer Kellis, Sahil Shah, Maitreyi Ashok, Luke Bashford, Daniel Kramer, Brian Lee, Charles Liu, Richard Andersen, Azita Emami
    * Abstract: We present a new deep multi-state Dynamic Recurrent Neural Network (DRNN) architecture for Brain Machine Interface (BMI) applications. Our DRNN is used to predict Cartesian representation of a computer cursor movement kinematics from open-loop neural data recorded from the posterior parietal cortex (PPC) of a human subject in a BMI system. We design the algorithm to achieve a reasonable trade-off between performance and robustness, and we constrain memory usage in favor of future hardware implementation. We feed the predictions of the network back to the input to improve prediction performance and robustness. We apply a scheduled sampling approach to the model in order to solve a statistical distribution mismatch between the ground truth and predictions. Additionally, we configure a small DRNN to operate with a short history of input, reducing the required buffering of input data and number of memory accesses. This configuration lowers the expected power consumption in a neural network accelerator. Operating on wavelet-based neural features, we show that the average performance of DRNN surpasses other state-of-the-art methods in the literature on both single- and multi-day data recorded over 43 days. Results show that multi-state DRNN has the potential to model the nonlinear relationships between the neural data and kinematics for robust BMIs.
count=2
* Covariate-Powered Empirical Bayes Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/48f7d3043bc03e6c48a6f0ebc0f258a8-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/48f7d3043bc03e6c48a6f0ebc0f258a8-Paper.pdf)]
    * Title: Covariate-Powered Empirical Bayes Estimation
    * Year: `2019`
    * Authors: Nikolaos Ignatiadis, Stefan Wager
    * Abstract: We study methods for simultaneous analysis of many noisy experiments in the presence of rich covariate information. The goal of the analyst is to optimally estimate the true effect underlying each experiment. Both the noisy experimental results and the auxiliary covariates are useful for this purpose, but neither data source on its own captures all the information available to the analyst. In this paper, we propose a flexible plug-in empirical Bayes estimator that synthesizes both sources of information and may leverage any black-box predictive model. We show that our approach is within a constant factor of minimax for a simple data-generating model. Furthermore, we establish robust convergence guarantees for our method that hold under considerable generality, and exhibit promising empirical performance on both real and simulated data.
count=2
* Hyperparameter Learning via Distributional Transfer
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/6754e06e46dfa419d5afe3c9781cecad-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/6754e06e46dfa419d5afe3c9781cecad-Paper.pdf)]
    * Title: Hyperparameter Learning via Distributional Transfer
    * Year: `2019`
    * Authors: Ho Chung Law, Peilin Zhao, Leung Sing Chan, Junzhou Huang, Dino Sejdinovic
    * Abstract: Bayesian optimisation is a popular technique for hyperparameter learning but typically requires initial exploration even in cases where similar prior tasks have been solved. We propose to transfer information across tasks using learnt representations of training datasets used in those tasks. This results in a joint Gaussian process model on hyperparameters and data representations. Representations make use of the framework of distribution embeddings into reproducing kernel Hilbert spaces. The developed method has a faster convergence compared to existing baselines, in some cases requiring only a few evaluations of the target objective.
count=2
* Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/6ea3f1874b188558fafbab78e8c3a968-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/6ea3f1874b188558fafbab78e8c3a968-Paper.pdf)]
    * Title: Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning
    * Year: `2019`
    * Authors: Valerio Perrone, Huibin Shen, Matthias W. Seeger, Cedric Archambeau, Rodolphe Jenatton
    * Abstract: Bayesian optimization (BO) is a successful methodology to optimize black-box functions that are expensive to evaluate. While traditional methods optimize each black-box function in isolation, there has been recent interest in speeding up BO by transferring knowledge across multiple related black-box functions. In this work, we introduce a method to automatically design the BO search space by relying on evaluations of previous black-box functions. We depart from the common practice of defining a set of arbitrary search ranges a priori by considering search space geometries that are learnt from historical data. This simple, yet effective strategy can be used to endow many existing BO methods with transfer learning properties. Despite its simplicity, we show that our approach considerably boosts BO by reducing the size of the search space, thus accelerating the optimization of a variety of black-box optimization problems. In particular, the proposed approach combined with random search results in a parameter-free, easy-to-implement, robust hyperparameter optimization strategy. We hope it will constitute a natural baseline for further research attempting to warm-start BO.
count=2
* Partially Encrypted Deep Learning using Functional Encryption
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/9d28de8ff9bb6a3fa41fddfdc28f3bc1-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/9d28de8ff9bb6a3fa41fddfdc28f3bc1-Paper.pdf)]
    * Title: Partially Encrypted Deep Learning using Functional Encryption
    * Year: `2019`
    * Authors: Théo Ryffel, David Pointcheval, Francis Bach, Edouard Dufour-Sans, Romain Gay
    * Abstract: Machine learning on encrypted data has received a lot of attention thanks to recent breakthroughs in homomorphic encryption and secure multi-party computation. It allows outsourcing computation to untrusted servers without sacrificing privacy of sensitive data. We propose a practical framework to perform partially encrypted and privacy-preserving predictions which combines adversarial training and functional encryption. We first present a new functional encryption scheme to efficiently compute quadratic functions so that the data owner controls what can be computed but is not involved in the calculation: it provides a decryption key which allows one to learn a specific function evaluation of some encrypted data. We then show how to use it in machine learning to partially encrypt neural networks with quadratic activation functions at evaluation time and we provide a thorough analysis of the information leaks based on indistinguishability of data items of the same label. Last, since several encryption schemes cannot deal with the last thresholding operation used for classification, we propose a training method to prevent selected sensitive features from leaking which adversarially optimizes the network against an adversary trying to identify these features. This is of great interest for several existing works using partially encrypted machine learning as it comes with almost no cost on the model's accuracy and significantly improves data privacy.
count=2
* Kalman Filter, Sensor Fusion, and Constrained Regression: Equivalences and Insights
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/b522259710151f8cc7870b970b4e0930-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/b522259710151f8cc7870b970b4e0930-Paper.pdf)]
    * Title: Kalman Filter, Sensor Fusion, and Constrained Regression: Equivalences and Insights
    * Year: `2019`
    * Authors: Maria Jahja, David Farrow, Roni Rosenfeld, Ryan J. Tibshirani
    * Abstract: The Kalman filter (KF) is one of the most widely used tools for data assimilation and sequential estimation. In this work, we show that the state estimates from the KF in a standard linear dynamical system setting are equivalent to those given by the KF in a transformed system, with infinite process noise (i.e., a ``flat prior'') and an augmented measurement space. This reformulation---which we refer to as augmented measurement sensor fusion (SF)---is conceptually interesting, because the transformed system here is seemingly static (as there is effectively no process model), but we can still capture the state dynamics inherent to the KF by folding the process model into the measurement space. Further, this reformulation of the KF turns out to be useful in settings in which past states are observed eventually (at some lag). Here, when the measurement noise covariance is estimated by the empirical covariance, we show that the state predictions from SF are equivalent to those from a regression of past states on past measurements, subject to particular linear constraints (reflecting the relationships encoded in the measurement map). This allows us to port standard ideas (say, regularization methods) in regression over to dynamical systems. For example, we can posit multiple candidate process models, fold all of them into the measurement model, transform to the regression perspective, and apply $\ell_1$ penalization to perform process model selection. We give various empirical demonstrations, and focus on an application to nowcasting the weekly incidence of influenza in the US.
count=2
* Are Disentangled Representations Helpful for Abstract Visual Reasoning?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/bc3c4a6331a8a9950945a1aa8c95ab8a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/bc3c4a6331a8a9950945a1aa8c95ab8a-Paper.pdf)]
    * Title: Are Disentangled Representations Helpful for Abstract Visual Reasoning?
    * Year: `2019`
    * Authors: Sjoerd van Steenkiste, Francesco Locatello, Jürgen Schmidhuber, Olivier Bachem
    * Abstract: A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples.
count=2
* Making AI Forget You: Data Deletion in Machine Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/cb79f8fa58b91d3af6c9c991f63962d3-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/cb79f8fa58b91d3af6c9c991f63962d3-Paper.pdf)]
    * Title: Making AI Forget You: Data Deletion in Machine Learning
    * Year: `2019`
    * Authors: Antonio Ginart, Melody Guan, Gregory Valiant, James Y. Zou
    * Abstract: Intense recent discussions have focused on how to provide individuals with control over when their data can and cannot be used --- the EU’s Right To Be Forgotten regulation is an example of this effort. In this paper we initiate a framework studying what to do when it is no longer permissible to deploy models derivative from specific user data. In particular, we formulate the problem of efficiently deleting individual data points from trained machine learning models. For many standard ML models, the only way to completely remove an individual's data is to retrain the whole model from scratch on the remaining data, which is often not computationally practical. We investigate algorithmic principles that enable efficient data deletion in ML. For the specific setting of $k$-means clustering, we propose two provably deletion efficient algorithms which achieve an average of over $100\times$ improvement in deletion efficiency across 6 datasets, while producing clusters of comparable statistical quality to a canonical $k$-means++ baseline.
count=2
* Conditional Independence Testing using Generative Adversarial Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/dc87c13749315c7217cdc4ac692e704c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/dc87c13749315c7217cdc4ac692e704c-Paper.pdf)]
    * Title: Conditional Independence Testing using Generative Adversarial Networks
    * Year: `2019`
    * Authors: Alexis Bellot, Mihaela van der Schaar
    * Abstract: We consider the hypothesis testing problem of detecting conditional dependence, with a focus on high-dimensional feature spaces. Our contribution is a new test statistic based on samples from a generative adversarial network designed to approximate directly a conditional distribution that encodes the null hypothesis, in a manner that maximizes power (the rate of true negatives). We show that such an approach requires only that density approximation be viable in order to ensure that we control type I error (the rate of false positives); in particular, no assumptions need to be made on the form of the distributions or feature dependencies. Using synthetic simulations with high-dimensional data we demonstrate significant gains in power over competing methods. In addition, we illustrate the use of our test to discover causal markers of disease in genetic data.
count=2
* The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/fc2c7c47b918d0c2d792a719dfb602ef-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/fc2c7c47b918d0c2d792a719dfb602ef-Paper.pdf)]
    * Title: The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers
    * Year: `2019`
    * Authors: Alex Lu, Amy Lu, Wiebke Schormann, Marzyeh Ghassemi, David Andrews, Alan Moses
    * Abstract: Understanding if classifiers generalize to out-of-sample datasets is a central problem in machine learning. Microscopy images provide a standardized way to measure the generalization capacity of image classifiers, as we can image the same classes of objects under increasingly divergent, but controlled factors of variation. We created a public dataset of 132,209 images of mouse cells, COOS-7 (Cells Out Of Sample 7-Class). COOS-7 provides a classification setting where four test datasets have increasing degrees of covariate shift: some images are random subsets of the training data, while others are from experiments reproduced months later and imaged by different instruments. We benchmarked a range of classification models using different representations, including transferred neural network features, end-to-end classification with a supervised deep CNN, and features from a self-supervised CNN. While most classifiers perform well on test datasets similar to the training dataset, all classifiers failed to generalize their performance to datasets with greater covariate shifts. These baselines highlight the challenges of covariate shifts in image data, and establish metrics for improving the generalization capacity of image classifiers.
count=2
* Learning SMaLL Predictors
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/03b2ceb73723f8b53cd533e4fba898ee-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/03b2ceb73723f8b53cd533e4fba898ee-Paper.pdf)]
    * Title: Learning SMaLL Predictors
    * Year: `2018`
    * Authors: Vikas Garg, Ofer Dekel, Lin Xiao
    * Abstract: We introduce a new framework for learning in severely resource-constrained settings. Our technique delicately amalgamates the representational richness of multiple linear predictors with the sparsity of Boolean relaxations, and thereby yields classifiers that are compact, interpretable, and accurate. We provide a rigorous formalism of the learning problem, and establish fast convergence of the ensuing algorithm via relaxation to a minimax saddle point objective. We supplement the theoretical foundations of our work with an extensive empirical evaluation.
count=2
* On Binary Classification in Extreme Regions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/0ebcc77dc72360d0eb8e9504c78d38bd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/0ebcc77dc72360d0eb8e9504c78d38bd-Paper.pdf)]
    * Title: On Binary Classification in Extreme Regions
    * Year: `2018`
    * Authors: Hamid JALALZAI, Stephan Clémençon, Anne Sabourin
    * Abstract: In pattern recognition, a random label Y is to be predicted based upon observing a random vector X valued in $\mathbb{R}^d$ with d>1 by means of a classification rule with minimum probability of error. In a wide variety of applications, ranging from finance/insurance to environmental sciences through teletraffic data analysis for instance, extreme (i.e. very large) observations X are of crucial importance, while contributing in a negligible manner to the (empirical) error however, simply because of their rarity. As a consequence, empirical risk minimizers generally perform very poorly in extreme regions. It is the purpose of this paper to develop a general framework for classification in the extremes. Precisely, under non-parametric heavy-tail assumptions for the class distributions, we prove that a natural and asymptotic notion of risk, accounting for predictive performance in extreme regions of the input space, can be defined and show that minimizers of an empirical version of a non-asymptotic approximant of this dedicated risk, based on a fraction of the largest observations, lead to classification rules with good generalization capacity, by means of maximal deviation inequalities in low probability regions. Beyond theoretical results, numerical experiments are presented in order to illustrate the relevance of the approach developed.
count=2
* Scalable Hyperparameter Transfer Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/14c879f3f5d8ed93a09f6090d77c2cc3-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/14c879f3f5d8ed93a09f6090d77c2cc3-Paper.pdf)]
    * Title: Scalable Hyperparameter Transfer Learning
    * Year: `2018`
    * Authors: Valerio Perrone, Rodolphe Jenatton, Matthias W. Seeger, Cedric Archambeau
    * Abstract: Bayesian optimization (BO) is a model-based approach for gradient-free black-box function optimization, such as hyperparameter optimization. Typically, BO relies on conventional Gaussian process (GP) regression, whose algorithmic complexity is cubic in the number of evaluations. As a result, GP-based BO cannot leverage large numbers of past function evaluations, for example, to warm-start related BO runs. We propose a multi-task adaptive Bayesian linear regression model for transfer learning in BO, whose complexity is linear in the function evaluations: one Bayesian linear regression model is associated to each black-box function optimization problem (or task), while transfer learning is achieved by coupling the models through a shared deep neural net. Experiments show that the neural net learns a representation suitable for warm-starting the black-box optimization problems and that BO runs can be accelerated when the target black-box function (e.g., validation loss) is learned together with other related signals (e.g., training loss). The proposed method was found to be at least one order of magnitude faster that methods recently published in the literature.
count=2
* Uplift Modeling from Separate Labels
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/198dd5fb9c43b2d29a548f8c77e85cf9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/198dd5fb9c43b2d29a548f8c77e85cf9-Paper.pdf)]
    * Title: Uplift Modeling from Separate Labels
    * Year: `2018`
    * Authors: Ikko Yamane, Florian Yger, Jamal Atif, Masashi Sugiyama
    * Abstract: Uplift modeling is aimed at estimating the incremental impact of an action on an individual's behavior, which is useful in various application domains such as targeted marketing (advertisement campaigns) and personalized medicine (medical treatments). Conventional methods of uplift modeling require every instance to be jointly equipped with two types of labels: the taken action and its outcome. However, obtaining two labels for each instance at the same time is difficult or expensive in many real-world problems. In this paper, we propose a novel method of uplift modeling that is applicable to a more practical setting where only one type of labels is available for each instance. We show a mean squared error bound for the proposed estimator and demonstrate its effectiveness through experiments.
count=2
* Diverse Ensemble Evolution: Curriculum Data-Model Marriage
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/3070e6addcd702cb58de5d7897bfdae1-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/3070e6addcd702cb58de5d7897bfdae1-Paper.pdf)]
    * Title: Diverse Ensemble Evolution: Curriculum Data-Model Marriage
    * Year: `2018`
    * Authors: Tianyi Zhou, Shengjie Wang, Jeff A. Bilmes
    * Abstract: We study a new method (``Diverse Ensemble Evolution (DivE$^2$)'') to train an ensemble of machine learning models that assigns data to models at each training epoch based on each model's current expertise and an intra- and inter-model diversity reward. DivE$^2$ schedules, over the course of training epochs, the relative importance of these characteristics; it starts by selecting easy samples for each model, and then gradually adjusts towards the models having specialized and complementary expertise on subsets of the training data, thereby encouraging high accuracy of the ensemble. We utilize an intra-model diversity term on data assigned to each model, and an inter-model diversity term on data assigned to pairs of models, to penalize both within-model and cross-model redundancy. We formulate the data-model marriage problem as a generalized bipartite matching, represented as submodular maximization subject to two matroid constraints. DivE$^2$ solves a sequence of continuous-combinatorial optimizations with slowly varying objectives and constraints. The combinatorial part handles the data-model marriage while the continuous part updates model parameters based on the assignments. In experiments, DivE$^2$ outperforms other ensemble training methods under a variety of model aggregation techniques, while also maintaining competitive efficiency.
count=2
* Multi-value Rule Sets for Interpretable Classification with Feature-Efficient Representations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/32bbf7b2bc4ed14eb1e9c2580056a989-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/32bbf7b2bc4ed14eb1e9c2580056a989-Paper.pdf)]
    * Title: Multi-value Rule Sets for Interpretable Classification with Feature-Efficient Representations
    * Year: `2018`
    * Authors: Tong Wang
    * Abstract: We present the Multi-value Rule Set (MRS) for interpretable classification with feature efficient presentations. Compared to rule sets built from single-value rules, MRS adopts a more generalized form of association rules that allows multiple values in a condition. Rules of this form are more concise than classical single-value rules in capturing and describing patterns in data. Our formulation also pursues a higher efficiency of feature utilization, which reduces possible cost in data collection and storage. We propose a Bayesian framework for formulating an MRS model and develop an efficient inference method for learning a maximum a posteriori, incorporating theoretically grounded bounds to iteratively reduce the search space and improve the search efficiency. Experiments on synthetic and real-world data demonstrate that MRS models have significantly smaller complexity and fewer features than baseline models while being competitive in predictive accuracy.
count=2
* Processing of missing data by neural networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/411ae1bf081d1674ca6091f8c59a266f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/411ae1bf081d1674ca6091f8c59a266f-Paper.pdf)]
    * Title: Processing of missing data by neural networks
    * Year: `2018`
    * Authors: Marek Śmieja, Łukasz Struski, Jacek Tabor, Bartosz Zieliński, Przemysław Spurek
    * Abstract: We propose a general, theoretically justified mechanism for processing missing data by neural networks. Our idea is to replace typical neuron's response in the first hidden layer by its expected value. This approach can be applied for various types of networks at minimal cost in their modification. Moreover, in contrast to recent approaches, it does not require complete data for training. Experimental results performed on different types of architectures show that our method gives better results than typical imputation strategies and other methods dedicated for incomplete data.
count=2
* Boolean Decision Rules via Column Generation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/743394beff4b1282ba735e5e3723ed74-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/743394beff4b1282ba735e5e3723ed74-Paper.pdf)]
    * Title: Boolean Decision Rules via Column Generation
    * Year: `2018`
    * Authors: Sanjeeb Dash, Oktay Gunluk, Dennis Wei
    * Abstract: This paper considers the learning of Boolean rules in either disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) or conjunctive normal form (CNF, AND-of-ORs) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. Column generation (CG) is used to efficiently search over an exponential number of candidate clauses (conjunctions or disjunctions) without the need for heuristic rule mining. This approach also bounds the gap between the selected rule set and the best possible rule set on the training data. To handle large datasets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 8 out of 16 datasets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurate.
count=2
* Representation Balancing MDPs for Off-policy Policy Evaluation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/980ecd059122ce2e50136bda65c25e07-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/980ecd059122ce2e50136bda65c25e07-Paper.pdf)]
    * Title: Representation Balancing MDPs for Off-policy Policy Evaluation
    * Year: `2018`
    * Authors: Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo A. Faisal, Finale Doshi-Velez, Emma Brunskill
    * Abstract: We study the problem of off-policy policy evaluation (OPPE) in RL. In contrast to prior work, we consider how to estimate both the individual policy value and average policy value accurately. We draw inspiration from recent work in causal reasoning, and propose a new finite sample generalization error bound for value estimates from MDP models. Using this upper bound as an objective, we develop a learning algorithm of an MDP model with a balanced representation, and show that our approach can yield substantially lower MSE in common synthetic benchmarks and a HIV treatment simulation domain.
count=2
* Predictive Approximate Bayesian Computation via Saddle Points
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/a14185bf0c82b3369f86efb3cac5ad28-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/a14185bf0c82b3369f86efb3cac5ad28-Paper.pdf)]
    * Title: Predictive Approximate Bayesian Computation via Saddle Points
    * Year: `2018`
    * Authors: Yingxiang Yang, Bo Dai, Negar Kiyavash, Niao He
    * Abstract: Approximate Bayesian computation (ABC) is an important methodology for Bayesian inference when the likelihood function is intractable. Sampling-based ABC algorithms such as rejection- and K2-ABC are inefficient when the parameters have high dimensions, while the regression-based algorithms such as K- and DR-ABC are hard to scale. In this paper, we introduce an optimization-based ABC framework that addresses these deficiencies. Leveraging a generative model for posterior and joint distribution matching, we show that ABC can be framed as saddle point problems, whose objectives can be accessed directly with samples. We present the predictive ABC algorithm (P-ABC), and provide a probabilistically approximately correct (PAC) bound that guarantees its learning consistency. Numerical experiment shows that P-ABC outperforms both K2- and DR-ABC significantly.
count=2
* Representation Learning for Treatment Effect Estimation from Observational Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/a50abba8132a77191791390c3eb19fe7-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/a50abba8132a77191791390c3eb19fe7-Paper.pdf)]
    * Title: Representation Learning for Treatment Effect Estimation from Observational Data
    * Year: `2018`
    * Authors: Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, Aidong Zhang
    * Abstract: Estimating individual treatment effect (ITE) is a challenging problem in causal inference, due to the missing counterfactuals and the selection bias. Existing ITE estimation methods mainly focus on balancing the distributions of control and treated groups, but ignore the local similarity information that is helpful. In this paper, we propose a local similarity preserved individual treatment effect (SITE) estimation method based on deep representation learning. SITE preserves local similarity and balances data distributions simultaneously, by focusing on several hard samples in each mini-batch. Experimental results on synthetic and three real-world datasets demonstrate the advantages of the proposed SITE method, compared with the state-of-the-art ITE estimation methods.
count=2
* A Structured Prediction Approach for Label Ranking
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/b3dd760eb02d2e669c604f6b2f1e803f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/b3dd760eb02d2e669c604f6b2f1e803f-Paper.pdf)]
    * Title: A Structured Prediction Approach for Label Ranking
    * Year: `2018`
    * Authors: Anna Korba, Alexandre Garcia, Florence d'Alché-Buc
    * Abstract: We propose to solve a label ranking problem as a structured output regression task. In this view, we adopt a least square surrogate loss approach that solves a supervised learning problem in two steps: a regression step in a well-chosen feature space and a pre-image (or decoding) step. We use specific feature maps/embeddings for ranking data, which convert any ranking/permutation into a vector representation. These embeddings are all well-tailored for our approach, either by resulting in consistent estimators, or by solving trivially the pre-image problem which is often the bottleneck in structured prediction. Their extension to the case of incomplete or partial rankings is also discussed. Finally, we provide empirical results on synthetic and real-world datasets showing the relevance of our method.
count=2
* Deep Attentive Tracking via Reciprocative Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/c32d9bf27a3da7ec8163957080c8628e-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/c32d9bf27a3da7ec8163957080c8628e-Paper.pdf)]
    * Title: Deep Attentive Tracking via Reciprocative Learning
    * Year: `2018`
    * Authors: Shi Pu, Yibing Song, Chao Ma, Honggang Zhang, Ming-Hsuan Yang
    * Abstract: Visual attention, derived from cognitive neuroscience, facilitates human perception on the most pertinent subset of the sensory data. Recently, significant efforts have been made to exploit attention schemes to advance computer vision systems. For visual tracking, it is often challenging to track target objects undergoing large appearance changes. Attention maps facilitate visual tracking by selectively paying attention to temporal robust features. Existing tracking-by-detection approaches mainly use additional attention modules to generate feature weights as the classifiers are not equipped with such mechanisms. In this paper, we propose a reciprocative learning algorithm to exploit visual attention for training deep classifiers. The proposed algorithm consists of feed-forward and backward operations to generate attention maps, which serve as regularization terms coupled with the original classification loss function for training. The deep classifier learns to attend to the regions of target objects robust to appearance changes. Extensive experiments on large-scale benchmark datasets show that the proposed attentive tracking method performs favorably against the state-of-the-art approaches.
count=2
* Active Learning for Non-Parametric Regression Using Purely Random Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/dc4c44f624d600aa568390f1f1104aa0-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/dc4c44f624d600aa568390f1f1104aa0-Paper.pdf)]
    * Title: Active Learning for Non-Parametric Regression Using Purely Random Trees
    * Year: `2018`
    * Authors: Jack Goetz, Ambuj Tewari, Paul Zimmerman
    * Abstract: Active learning is the task of using labelled data to select additional points to label, with the goal of fitting the most accurate model with a fixed budget of labelled points. In binary classification active learning is known to produce faster rates than passive learning for a broad range of settings. However in regression restrictive structure and tailored methods were previously needed to obtain theoretically superior performance. In this paper we propose an intuitive tree based active learning algorithm for non-parametric regression with provable improvement over random sampling. When implemented with Mondrian Trees our algorithm is tuning parameter free, consistent and minimax optimal for Lipschitz functions.
count=2
* Introspective Classification with Convolutional Nets
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/11b921ef080f7736089c757404650e40-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/11b921ef080f7736089c757404650e40-Paper.pdf)]
    * Title: Introspective Classification with Convolutional Nets
    * Year: `2017`
    * Authors: Long Jin, Justin Lazarow, Zhuowen Tu
    * Abstract: We propose introspective convolutional networks (ICN) that emphasize the importance of having convolutional neural networks empowered with generative capabilities. We employ a reclassification-by-synthesis algorithm to perform training using a formulation stemmed from the Bayes theory. Our ICN tries to iteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by improving the classification. The single CNN classifier learned is at the same time generative --- being able to directly synthesize new samples within its own discriminative model. We conduct experiments on benchmark datasets including MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures, and observe improved classification results.
count=2
* Maximum Margin Interval Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/2288f691b58edecadcc9a8691762b4fd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/2288f691b58edecadcc9a8691762b4fd-Paper.pdf)]
    * Title: Maximum Margin Interval Trees
    * Year: `2017`
    * Authors: Alexandre Drouin, Toby Hocking, Francois Laviolette
    * Abstract: Learning a regression function using censored or interval-valued output data is an important problem in fields such as genomics and medicine. The goal is to learn a real-valued prediction function, and the training output labels indicate an interval of possible values. Whereas most existing algorithms for this task are linear models, in this paper we investigate learning nonlinear tree models. We propose to learn a tree by minimizing a margin-based discriminative objective function, and we provide a dynamic programming algorithm for computing the optimal solution in log-linear time. We show empirically that this algorithm achieves state-of-the-art speed and prediction accuracy in a benchmark of several data sets.
count=2
* Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/2bb232c0b13c774965ef8558f0fbd615-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/2bb232c0b13c774965ef8558f0fbd615-Paper.pdf)]
    * Title: Robust Hypothesis Test for Nonlinear Effect with Gaussian Processes
    * Year: `2017`
    * Authors: Jeremiah Liu, Brent Coull
    * Abstract: This work constructs a hypothesis test for detecting whether an data-generating function $h: \real^p \rightarrow \real$ belongs to a specific reproducing kernel Hilbert space $\mathcal{H}_0$, where the structure of $\mathcal{H}_0$ is only partially known. Utilizing the theory of reproducing kernels, we reduce this hypothesis to a simple one-sided score test for a scalar parameter, develop a testing procedure that is robust against the mis-specification of kernel functions, and also propose an ensemble-based estimator for the null model to guarantee test performance in small samples. To demonstrate the utility of the proposed method, we apply our test to the problem of detecting nonlinear interaction between groups of continuous features. We evaluate the finite-sample performance of our test under different data-generating functions and estimation strategies for the null model. Our results revealed interesting connection between notions in machine learning (model underfit/overfit) and those in statistical inference (i.e. Type I error/power of hypothesis test), and also highlighted unexpected consequences of common model estimating strategies (e.g. estimating kernel hyperparameters using maximum likelihood estimation) on model inference.
count=2
* Deep Learning with Topological Signatures
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/883e881bb4d22a7add958f2d6b052c9f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/883e881bb4d22a7add958f2d6b052c9f-Paper.pdf)]
    * Title: Deep Learning with Topological Signatures
    * Year: `2017`
    * Authors: Christoph Hofer, Roland Kwitt, Marc Niethammer, Andreas Uhl
    * Abstract: Inferring topological and geometrical information from data can offer an alternative perspective in machine learning problems. Methods from topological data analysis, e.g., persistent homology, enable us to obtain such information, typically in the form of summary representations of topological features. However, such topological signatures often come with an unusual structure (e.g., multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations, they suffer from being agnostic to the target learning task. In contrast, we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classification experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and, in case of the latter, we even outperform the state-of-the-art by a large margin.
count=2
* Learning Active Learning from Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Paper.pdf)]
    * Title: Learning Active Learning from Data
    * Year: `2017`
    * Authors: Ksenia Konyushkova, Raphael Sznitman, Pascal Fua
    * Abstract: In this paper, we suggest a novel data-driven approach to active learning (AL). The key idea is to train a regressor that predicts the expected error reduction for a candidate sample in a particular learning state. By formulating the query selection procedure as a regression problem we are not restricted to working with existing AL heuristics; instead, we learn strategies based on experience from previous AL outcomes. We show that a strategy can be learnt either from simple synthetic 2D datasets or from a subset of domain-specific data. Our method yields strategies that work well on real data from a wide range of domains.
count=2
* Optimized Pre-Processing for Discrimination Prevention
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/9a49a25d845a483fae4be7e341368e36-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf)]
    * Title: Optimized Pre-Processing for Discrimination Prevention
    * Year: `2017`
    * Authors: Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, Kush R. Varshney
    * Abstract: Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.
count=2
* Random Projection Filter Bank for Time Series Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/ca3ec598002d2e7662e2ef4bdd58278b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/ca3ec598002d2e7662e2ef4bdd58278b-Paper.pdf)]
    * Title: Random Projection Filter Bank for Time Series Data
    * Year: `2017`
    * Authors: Amir-massoud Farahmand, Sepideh Pourazarm, Daniel Nikovski
    * Abstract: We propose Random Projection Filter Bank (RPFB) as a generic and simple approach to extract features from time series data. RPFB is a set of randomly generated stable autoregressive filters that are convolved with the input time series to generate the features. These features can be used by any conventional machine learning algorithm for solving tasks such as time series prediction, classification with time series data, etc. Different filters in RPFB extract different aspects of the time series, and together they provide a reasonably good summary of the time series. RPFB is easy to implement, fast to compute, and parallelizable. We provide an error upper bound indicating that RPFB provides a reasonable approximation to a class of dynamical systems. The empirical results in a series of synthetic and real-world problems show that RPFB is an effective method to extract features from time series.
count=2
* Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/d594b1a945b5d645e59e21f88bd2d83b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/d594b1a945b5d645e59e21f88bd2d83b-Paper.pdf)]
    * Title: Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin
    * Year: `2017`
    * Authors: Ritambhara Singh, Jack Lanchantin, Arshdeep Sekhon, Yanjun Qi
    * Abstract: The past decade has seen a revolution in genomic technologies that enabled a flood of genome-wide profiling of chromatin marks. Recent literature tried to understand gene regulation by predicting gene expression from large-scale chromatin measurements. Two fundamental challenges exist for such learning tasks: (1) genome-wide chromatin signals are spatially structured, high-dimensional and highly modular; and (2) the core aim is to understand what are the relevant factors and how they work together. Previous studies either failed to model complex dependencies among input signals or relied on separate feature analysis to explain the decisions. This paper presents an attention-based deep learning approach; AttentiveChrome, that uses a unified architecture to model and to interpret dependencies among chromatin factors for controlling gene regulation. AttentiveChrome uses a hierarchy of multiple Long Short-Term Memory (LSTM) modules to encode the input signals and to model how various chromatin marks cooperate automatically. AttentiveChrome trains two levels of attention jointly with the target prediction, enabling it to attend differentially to relevant marks and to locate important positions per mark. We evaluate the model across 56 different cell types (tasks) in human. Not only is the proposed architecture more accurate, but its attention scores also provide a better interpretation than state-of-the-art feature visualization methods such as saliency map.
count=2
* Multi-way Interacting Regression via Factorization Machines
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/fcdf25d6e191893e705819b177cddea0-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/fcdf25d6e191893e705819b177cddea0-Paper.pdf)]
    * Title: Multi-way Interacting Regression via Factorization Machines
    * Year: `2017`
    * Authors: Mikhail Yurochkin, XuanLong Nguyen, nikolaos Vasiloglou
    * Abstract: We propose a Bayesian regression method that accounts for multi-way interactions of arbitrary orders among the predictor variables. Our model makes use of a factorization mechanism for representing the regression coefficients of interactions among the predictors, while the interaction selection is guided by a prior distribution on random hypergraphs, a construction which generalizes the Finite Feature Model. We present a posterior inference algorithm based on Gibbs sampling, and establish posterior consistency of our regression model. Our method is evaluated with extensive experiments on simulated data and demonstrated to be able to identify meaningful interactions in applications in genetics and retail demand forecasting.
count=2
* Combinatorial Energy Learning for Image Segmentation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/31857b449c407203749ae32dd0e7d64a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/31857b449c407203749ae32dd0e7d64a-Paper.pdf)]
    * Title: Combinatorial Energy Learning for Image Segmentation
    * Year: `2016`
    * Authors: Jeremy B. Maitin-Shepard, Viren Jain, Michal Januszewski, Peter Li, Pieter Abbeel
    * Abstract: We introduce a new machine learning approach for image segmentation that uses a neural network to model the conditional energy of a segmentation given an image. Our approach, combinatorial energy learning for image segmentation (CELIS) places a particular emphasis on modeling the inherent combinatorial nature of dense image segmentation problems. We propose efficient algorithms for learning deep neural networks to model the energy function, and for local optimization of this energy in the space of supervoxel agglomerations. We extensively evaluate our method on a publicly available 3-D microscopy dataset with 25 billion voxels of ground truth data. On an 11 billion voxel test set, we find that our method improves volumetric reconstruction accuracy by more than 20% as compared to two state-of-the-art baseline methods: graph-based segmentation of the output of a 3-D convolutional neural network trained to predict boundaries, as well as a random forest classifier trained to agglomerate supervoxels that were generated by a 3-D convolutional neural network.
count=2
* Local Similarity-Aware Deep Feature Embedding
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/556f391937dfd4398cbac35e050a2177-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/556f391937dfd4398cbac35e050a2177-Paper.pdf)]
    * Title: Local Similarity-Aware Deep Feature Embedding
    * Year: `2016`
    * Authors: Chen Huang, Chen Change Loy, Xiaoou Tang
    * Abstract: Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.
count=2
* Only H is left: Near-tight Episodic PAC RL
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/5d616dd38211ebb5d6ec52986674b6e4-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/5d616dd38211ebb5d6ec52986674b6e4-Paper.pdf)]
    * Title: Only H is left: Near-tight Episodic PAC RL
    * Year: `2016`
    * Authors: 
    * Abstract: In many applications such as advertisement placement or automated dialog systems, an intelligent system optimizes performance over a sequence of interactions with each user. Such tasks often involve many states and potentially time-dependent transition dynamics, and can be modeled well as episodic Markov decision processes (MDPs). In this paper, we present a PAC algorithm for reinforcement learning in episodic finite MDPs with time-dependent transitions that acts epsilon-optimal in all but O(S A H^3 / epsilon^2 log(1 / delta)) episodes. Our algorithm has a polynomial computational complexity, and our sample complexity bound accounts for the fact that we may only be able to approximately solve the internal planning problems. In addition, our PAC sample complexity bound has only linear dependency on the number of states S and actions A and strictly improves previous bounds with S^2 dependency in this setting. Compared against other methods for infinite horizon reinforcement learning with linear state space sample complexity our method has much lower dependency on the (effective) horizon. Indeed, our bound is optimal up to a factor of H.
count=2
* An ensemble diversity approach to supervised binary hashing
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/67f7fb873eaf29526a11a9b7ac33bfac-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/67f7fb873eaf29526a11a9b7ac33bfac-Paper.pdf)]
    * Title: An ensemble diversity approach to supervised binary hashing
    * Year: `2016`
    * Authors: Miguel A. Carreira-Perpinan, Ramin Raziperchikolaei
    * Abstract: Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.
count=2
* Launch and Iterate: Reducing Prediction Churn
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/dc5c768b5dc76a084531934b34601977-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/dc5c768b5dc76a084531934b34601977-Paper.pdf)]
    * Title: Launch and Iterate: Reducing Prediction Churn
    * Year: `2016`
    * Authors: Mahdi Milani Fard, Quentin Cormier, Kevin Canini, Maya Gupta
    * Abstract: Practical applications of machine learning often involve successive training iterations with changes to features and training examples. Ideally, changes in the output of any new model should only be improvements (wins) over the previous iteration, but in practice the predictions may change neutrally for many examples, resulting in extra net-zero wins and losses, referred to as unnecessary churn. These changes in the predictions are problematic for usability for some applications, and make it harder and more expensive to measure if a change is statistically significant positive. In this paper, we formulate the problem and present a stabilization operator to regularize a classifier towards a previous classifier. We use a Markov chain Monte Carlo stabilization operator to produce a model with more consistent predictions without adversely affecting accuracy. We investigate the properties of the proposal with theoretical analysis. Experiments on benchmark datasets for different classification algorithms demonstrate the method and the resulting reduction in churn.
count=2
* Parallelizing MCMC with Random Partition Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf)]
    * Title: Parallelizing MCMC with Random Partition Trees
    * Year: `2015`
    * Authors: Xiangyu Wang, Fangjian Guo, Katherine A. Heller, David B. Dunson
    * Abstract: The modern scale of data has brought new challenges to Bayesian inference. In particular, conventional MCMC algorithms are computationally very expensive for large data sets. A promising approach to solve this problem is embarrassingly parallel MCMC (EP-MCMC), which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset. The subset posterior draws are then aggregated via some combining rules to obtain the final approximation. Existing EP-MCMC algorithms are limited by approximation accuracy and difficulty in resampling. In this article, we propose a new EP-MCMC algorithm PART that solves these problems. The new algorithm applies random partition trees to combine the subset posterior draws, which is distribution-free, easy to resample from and can adapt to multiple scales. We provide theoretical justification and extensive experiments illustrating empirical performance.
count=2
* Efficient Learning by Directed Acyclic Graph For Resource Constrained Prediction
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/c0a271bc0ecb776a094786474322cb82-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/c0a271bc0ecb776a094786474322cb82-Paper.pdf)]
    * Title: Efficient Learning by Directed Acyclic Graph For Resource Constrained Prediction
    * Year: `2015`
    * Authors: Joseph Wang, Kirill Trapeznikov, Venkatesh Saligrama
    * Abstract: We study the problem of reducing test-time acquisition costs in classification systems. Our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction. We model our system as a directed acyclic graph (DAG) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements. This problem can be naturally posed as an empirical risk minimization over training data. Rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes, we propose an efficient algorithm motivated by dynamic programming. We learn node policies in the DAG by reducing the global objective to a series of cost sensitive learning problems. Our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture. In addition, we present an extension to map other budgeted learning problems with large number of sensors to our DAG architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors.
count=2
* Recognizing retinal ganglion cells in the dark
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/fe70c36866add1572a8e2b96bfede7bf-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/fe70c36866add1572a8e2b96bfede7bf-Paper.pdf)]
    * Title: Recognizing retinal ganglion cells in the dark
    * Year: `2015`
    * Authors: Emile Richard, Georges A. Goetz, E.J. Chichilnisky
    * Abstract: Many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs, and send their outputs to distinct targets. Therefore, a key step in understanding neural systems is to reliably distinguish cell types. An important example is the retina, for which present-day techniques for identifying cell types are accurate, but very labor-intensive. Here, we develop automated classifiers for functional identification of retinal ganglion cells, the output neurons of the retina, based solely on recorded voltage patterns on a large scale array. We use per-cell classifiers based on features extracted from electrophysiological images (spatiotemporal voltage waveforms) and interspike intervals (autocorrelations). These classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina, but fail in achieving the same accuracy in predicting cell polarities (ON vs. OFF). We then show how to use indicators of functional coupling within populations of ganglion cells (cross-correlation) to infer cell polarities with a matrix completion algorithm. This can result in accurate, fully automated methods for cell type classification.
count=2
* Multiscale Dictionary Learning for Estimating Conditional Distributions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf)]
    * Title: Multiscale Dictionary Learning for Estimating Conditional Distributions
    * Year: `2013`
    * Authors: Francesca Petralia, Joshua T. Vogelstein, David B. Dunson
    * Abstract: Nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem. It is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional. We propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space. A fast graph partitioning algorithm is applied to obtain the tree decomposition, with Bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner. The algorithm scales efficiently to approximately one million features. State of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features.
count=2
* Learning from Limited Demonstrations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/hash/fd5c905bcd8c3348ad1b35d7231ee2b1-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/file/fd5c905bcd8c3348ad1b35d7231ee2b1-Paper.pdf)]
    * Title: Learning from Limited Demonstrations
    * Year: `2013`
    * Authors: Beomjoon Kim, Amir-massoud Farahmand, Joelle Pineau, Doina Precup
    * Abstract: We propose an approach to learning from demonstration (LfD) which leverages expert data, even if the expert examples are very few or inaccurate. We achieve this by integrating LfD in an approximate policy iteration algorithm. The key idea of our approach is that expert examples are used to generate linear constraints on the optimization, in a similar fashion to large-margin classification. We prove an upper bound on the true Bellman error of the approximation computed by the algorithm at each iteration. We show empirically that the algorithm outperforms both pure policy iteration, as well as DAgger (a state-of-art LfD algorithm) and supervised learning in a variety of scenarios, including when very few and/or imperfect demonstrations are available. Our experiments include simulations as well as a real robotic navigation task.
count=2
* Convolutional-Recursive Deep Learning for 3D Object Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/3eae62bba9ddf64f69d49dc48e2dd214-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/3eae62bba9ddf64f69d49dc48e2dd214-Paper.pdf)]
    * Title: Convolutional-Recursive Deep Learning for 3D Object Classification
    * Year: `2012`
    * Authors: Richard Socher, Brody Huval, Bharath Bath, Christopher D. Manning, Andrew Ng
    * Abstract: Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We in- troduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns low-level translationally invariant features which are then given as inputs to multiple, ﬁxed-tree RNNs in order to compose higher order fea- tures. RNNs can be seen as combining convolution and pooling into one efﬁcient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during train- ing and testing than comparable architectures such as two-layer CNNs.
count=2
* Unsupervised Template Learning for Fine-Grained Object Recognition
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/a8f8f60264024dca151f164729b76c0b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/a8f8f60264024dca151f164729b76c0b-Paper.pdf)]
    * Title: Unsupervised Template Learning for Fine-Grained Object Recognition
    * Year: `2012`
    * Authors: Shulin Yang, Liefeng Bo, Jue Wang, Linda Shapiro
    * Abstract: Fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts. We suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classification. Learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms.
count=2
* Unsupervised Structure Discovery for Semantic Analysis of Audio
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/c6e19e830859f2cb9f7c8f8cacb8d2a6-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/c6e19e830859f2cb9f7c8f8cacb8d2a6-Paper.pdf)]
    * Title: Unsupervised Structure Discovery for Semantic Analysis of Audio
    * Year: `2012`
    * Authors: Sourish Chaudhuri, Bhiksha Raj
    * Abstract: Approaches to audio classification and retrieval tasks largely rely on detection-based discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has 2 layers with the first being generic sound units with no clear semantic associations, while the second layer attempts to find patterns over the generic sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report significant improvements over standard baselines.
count=2
* Multiplicative Forests for Continuous-Time Processes
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/cb70ab375662576bd1ac5aaf16b3fca4-Paper.pdf)]
    * Title: Multiplicative Forests for Continuous-Time Processes
    * Year: `2012`
    * Authors: Jeremy Weiss, Sriraam Natarajan, David Page
    * Abstract: Learning temporal dependencies between variables over continuous time is an important and challenging task. Continuous-time Bayesian networks effectively model such processes but are limited by the number of conditional intensity matrices, which grows exponentially in the number of parents per variable. We develop a partition-based representation using regression trees and forests whose parameter spaces grow linearly in the number of node splits. Using a multiplicative assumption we show how to update the forest likelihood in closed form, producing efficient model updates. Our results show multiplicative forests can be learned from few temporal trajectories with large gains in performance and scalability.
count=1
* DiG-IN: Diffusion Guidance for Investigating Networks - Uncovering Classifier Differences Neuron Visualisations and Visual Counterfactual Explanations
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Augustin_DiG-IN_Diffusion_Guidance_for_Investigating_Networks_-_Uncovering_Classifier_Differences_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Augustin_DiG-IN_Diffusion_Guidance_for_Investigating_Networks_-_Uncovering_Classifier_Differences_CVPR_2024_paper.pdf)]
    * Title: DiG-IN: Diffusion Guidance for Investigating Networks - Uncovering Classifier Differences Neuron Visualisations and Visual Counterfactual Explanations
    * Year: `2024`
    * Authors: Maximilian Augustin, Yannic Neuhaus, Matthias Hein
    * Abstract: While deep learning has led to huge progress in complex image classification tasks like ImageNet unexpected failure modes e.g. via spurious features call into question how reliably these classifiers work in the wild. Furthermore for safety-critical tasks the black-box nature of their decisions is problematic and explanations or at least methods which make decisions plausible are needed urgently. In this paper we address these problems by generating images that optimize a classifier-derived objective using a framework for guided image generation. We analyze the decisions of image classifiers by visual counterfactual explanations (VCEs) detection of systematic mistakes by analyzing images where classifiers maximally disagree and visualization of neurons and spurious features. In this way we validate existing observations e.g. the shape bias of adversarially robust models as well as novel failure modes e.g. systematic errors of zero-shot CLIP classifiers. Moreover our VCEs outperform previous work while being more versatile.
count=1
* BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Bai_BadCLIP_Trigger-Aware_Prompt_Learning_for_Backdoor_Attacks_on_CLIP_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Bai_BadCLIP_Trigger-Aware_Prompt_Learning_for_Backdoor_Attacks_on_CLIP_CVPR_2024_paper.pdf)]
    * Title: BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP
    * Year: `2024`
    * Authors: Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, Wei Liu
    * Abstract: Contrastive Vision-Language Pre-training known as CLIP has shown promising effectiveness in addressing downstream image recognition tasks. However recent works revealed that the CLIP model can be implanted with a downstream-oriented backdoor. On downstream tasks one victim model performs well on clean samples but predicts a specific target class whenever a specific trigger is present. For injecting a backdoor existing attacks depend on a large amount of additional data to maliciously fine-tune the entire pre-trained CLIP model which makes them inapplicable to data-limited scenarios. In this work motivated by the recent success of learnable prompts we address this problem by injecting a backdoor into the CLIP model in the prompt learning stage. Our method named BadCLIP is built on a novel and effective mechanism in backdoor attacks on CLIP i.e. influencing both the image and text encoders with the trigger. It consists of a learnable trigger applied to images and a trigger-aware context generator such that the trigger can change text features via trigger-aware prompts resulting in a powerful and generalizable attack. Extensive experiments conducted on 11 datasets verify that the clean accuracy of BadCLIP is similar to those of advanced prompt learning methods and the attack success rate is higher than 99% in most cases. BadCLIP is also generalizable to unseen classes and shows a strong generalization capability under cross-dataset and cross-domain settings. The code is available at https://github.com/jiawangbai/BadCLIP.
count=1
* Active Prompt Learning in Vision Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Bang_Active_Prompt_Learning_in_Vision_Language_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Bang_Active_Prompt_Learning_in_Vision_Language_Models_CVPR_2024_paper.pdf)]
    * Title: Active Prompt Learning in Vision Language Models
    * Year: `2024`
    * Authors: Jihwan Bang, Sumyeong Ahn, Jae-Gil Lee
    * Abstract: Pre-trained Vision Language Models (VLMs) have demonstrated notable progress in various zero-shot tasks such as classification and retrieval. Despite their performance because improving performance on new tasks requires task-specific knowledge their adaptation is essential. While labels are needed for the adaptation acquiring them is typically expensive. To overcome this challenge active learning a method of achieving a high performance by obtaining labels for a small number of samples from experts has been studied. Active learning primarily focuses on selecting unlabeled samples for labeling and leveraging them to train models. In this study we pose the question "how can the pre-trained VLMs be adapted under the active learning framework?" In response to this inquiry we observe that (1) simply applying a conventional active learning framework to pre-trained VLMs even may degrade performance compared to random selection because of the class imbalance in labeling candidates and (2) the knowledge of VLMs can provide hints for achieving the balance before labeling. Based on these observations we devise a novel active learning framework for VLMs denoted as PCB. To assess the effectiveness of our approach we conduct experiments on seven different real-world datasets and the results demonstrate that PCB surpasses conventional active learning and random sampling methods.
count=1
* FFF: Fixing Flawed Foundations in Contrastive Pre-Training Results in Very Strong Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Bulat_FFF_Fixing_Flawed_Foundations_in_Contrastive_Pre-Training_Results_in_Very_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Bulat_FFF_Fixing_Flawed_Foundations_in_Contrastive_Pre-Training_Results_in_Very_CVPR_2024_paper.pdf)]
    * Title: FFF: Fixing Flawed Foundations in Contrastive Pre-Training Results in Very Strong Vision-Language Models
    * Year: `2024`
    * Authors: Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos
    * Abstract: Despite noise and caption quality having been acknowledged as important factors impacting vision-language contrastive pre-training in this paper we show that the full potential of improving the training process by addressing such issues is yet to be realized. Specifically we firstly study and analyze two issues affecting training: incorrect assignment of negative pairs and low caption quality and diversity. Then we devise effective solutions for addressing both problems which essentially require training with multiple true positive pairs. Finally we propose training with sigmoid loss to address such a requirement. We show very large gains over the current state-of-the-art for both image recognition ( +6% on average over 11 datasets) and image retrieval ( +19% on Flickr30k and +15% on MSCOCO).
count=1
* A Generative Approach for Wikipedia-Scale Visual Entity Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Caron_A_Generative_Approach_for_Wikipedia-Scale_Visual_Entity_Recognition_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Caron_A_Generative_Approach_for_Wikipedia-Scale_Visual_Entity_Recognition_CVPR_2024_paper.pdf)]
    * Title: A Generative Approach for Wikipedia-Scale Visual Entity Recognition
    * Year: `2024`
    * Authors: Mathilde Caron, Ahmet Iscen, Alireza Fathi, Cordelia Schmid
    * Abstract: In this paper we address web-scale visual entity recognition specifically the task of mapping a given query image to one of the 6 million existing entities in Wikipedia. One way of approaching a problem of such scale is using dual encoder models (e.g. CLIP) where all the entity names and query images are embedded into a unified space paving the way for an approximate kNN search. Alternatively it is also possible to re-purpose a captioning model to directly generate the entity names for a given image. In contrast we introduce a novel Generative Entity Recognition (GER) framework which given an input image learns to auto-regressively decode a semantic and discriminative "code" identifying the target entity. Our experiments demonstrate the efficacy of this GER paradigm showcasing state-of-the-art performance on the challenging OVEN benchmark. GER surpasses strong captioning dual-encoder visual matching and hierarchical classification baselines affirming its advantage in tackling the complexities of web-scale recognition.
count=1
* Characteristics Matching Based Hash Codes Generation for Efficient Fine-grained Image Retrieval
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Characteristics_Matching_Based_Hash_Codes_Generation_for_Efficient_Fine-grained_Image_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Characteristics_Matching_Based_Hash_Codes_Generation_for_Efficient_Fine-grained_Image_CVPR_2024_paper.pdf)]
    * Title: Characteristics Matching Based Hash Codes Generation for Efficient Fine-grained Image Retrieval
    * Year: `2024`
    * Authors: Zhen-Duo Chen, Li-Jun Zhao, Zi-Chao Zhang, Xin Luo, Xin-Shun Xu
    * Abstract: The rapidly growing scale of data in practice poses demands on the efficiency of retrieval models. However for fine-grained image retrieval task there are inherent contradictions in the design of hashing based efficient models. Firstly the limited information embedding capacity of low-dimensional binary hash codes coupled with the detailed information required to describe fine-grained categories results in a contradiction in feature learning. Secondly there is also a contradiction between the complexity of fine-grained feature extraction models and retrieval efficiency. To address these issues in this paper we propose the characteristics matching based hash codes generation method. Coupled with the cross-layer semantic information transfer module and the multi-region feature embedding module the proposed method can generate hash codes that effectively capture fine-grained differences among samples while ensuring efficient inference. Extensive experiments on widely used datasets demonstrate that our method can significantly outperform state-of-the-art methods.
count=1
* Towards Calibrated Multi-label Deep Neural Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_Towards_Calibrated_Multi-label_Deep_Neural_Networks_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_Towards_Calibrated_Multi-label_Deep_Neural_Networks_CVPR_2024_paper.pdf)]
    * Title: Towards Calibrated Multi-label Deep Neural Networks
    * Year: `2024`
    * Authors: Jiacheng Cheng, Nuno Vasconcelos
    * Abstract: The problem of calibrating deep neural networks (DNNs) for multi-label learning is considered. It is well-known that DNNs trained by cross-entropy for single-label or one-hot classification are poorly calibrated. Many calibration techniques have been proposed to address the problem. However little attention has been paid to the calibration of multi-label DNNs. In this literature the focus has been on improving labeling accuracy in the face of severe dataset unbalance. This is addressed by the introduction of asymmetric losses which have became very popular. However these losses do not induce well calibrated classifiers. In this work we first provide a theoretical explanation for this poor calibration performance by showing that these loses losses lack the strictly proper property a necessary condition for accurate probability estimation. To overcome this problem we propose a new Strictly Proper Asymmetric (SPA) loss. This is complemented by a Label Pair Regularizer (LPR) that increases the number of calibration constraints introduced per training example. The effectiveness of both contributions is validated by extensive experiments on various multi-label datasets. The resulting training method is shown to significantly decrease the calibration error while maintaining state-of-the-art accuracy.
count=1
* Classes Are Not Equal: An Empirical Study on Image Recognition Fairness
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Cui_Classes_Are_Not_Equal_An_Empirical_Study_on_Image_Recognition_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Cui_Classes_Are_Not_Equal_An_Empirical_Study_on_Image_Recognition_CVPR_2024_paper.pdf)]
    * Title: Classes Are Not Equal: An Empirical Study on Image Recognition Fairness
    * Year: `2024`
    * Authors: Jiequan Cui, Beier Zhu, Xin Wen, Xiaojuan Qi, Bei Yu, Hanwang Zhang
    * Abstract: In this paper we present an empirical study on image recognition unfairness i.e. extreme class accuracy disparity on balanced data like ImageNet. We demonstrate that classes are not equal and unfairness is prevalent for image classification models across various datasets network architectures and model capacities. Moreover several intriguing properties of fairness are identified. First the unfairness lies in problematic representation rather than classifier bias distinguished from long-tailed recognition. Second with the proposed concept of Model Prediction Bias we investigate the origins of problematic representation during training optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization thus leading to their poor accuracy. Further we conclude that data augmentation and representation learning algorithms improve overall performance by promoting fairness to some degree in image classification.
count=1
* Scaling Laws of Synthetic Images for Model Training ... for Now
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Fan_Scaling_Laws_of_Synthetic_Images_for_Model_Training_..._for_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_Scaling_Laws_of_Synthetic_Images_for_Model_Training_..._for_CVPR_2024_paper.pdf)]
    * Title: Scaling Laws of Synthetic Images for Model Training ... for Now
    * Year: `2024`
    * Authors: Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, Yonglong Tian
    * Abstract: Recent significant advances in text-to-image models unlock the possibility of training vision systems using synthetic images potentially overcoming the difficulty of collecting curated data at scale. It is unclear however how these models behave at scale as more synthetic data is added to the training set. In this paper we study the scaling laws of synthetic images generated by state of the art text-to-image models for the training of supervised models: image classifiers with label supervision and CLIP with language supervision. We identify several factors including text prompts classifier-free guidance scale and types of text-to-image models that significantly affect scaling behavior. After tuning these factors we observe that synthetic images demonstrate a scaling trend similar to but slightly less effective than real images in CLIP training while they significantly underperform in scaling when training supervised image classifiers. Our analysis indicates that the main reason for this underperformance is the inability of off-the-shelf text-to-image models to generate certain concepts a limitation that significantly impairs the training of image classifiers. Our findings also suggest that scaling synthetic data can be particularly effective in scenarios such as: (1) when there is a limited supply of real images for a supervised problem (e.g. fewer than 0.5 million images in ImageNet) (2) when the evaluation dataset diverges significantly from the training data indicating the out-of-distribution scenario or (3) when synthetic data is used in conjunction with real images as demonstrated in the training of CLIP models.
count=1
* Embracing Unimodal Aleatoric Uncertainty for Robust Multimodal Fusion
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Gao_Embracing_Unimodal_Aleatoric_Uncertainty_for_Robust_Multimodal_Fusion_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_Embracing_Unimodal_Aleatoric_Uncertainty_for_Robust_Multimodal_Fusion_CVPR_2024_paper.pdf)]
    * Title: Embracing Unimodal Aleatoric Uncertainty for Robust Multimodal Fusion
    * Year: `2024`
    * Authors: Zixian Gao, Xun Jiang, Xing Xu, Fumin Shen, Yujie Li, Heng Tao Shen
    * Abstract: As a fundamental problem in multimodal learning multimodal fusion aims to compensate for the inherent limitations of a single modality. One challenge of multimodal fusion is that the unimodal data in their unique embedding space mostly contains potential noise which leads to corrupted cross-modal interactions. However in this paper we show that the potential noise in unimodal data could be well quantified and further employed to enhance more stable unimodal embeddings via contrastive learning. Specifically we propose a novel generic and robust multimodal fusion strategy termed Embracing Aleatoric Uncertainty (EAU) which is simple and can be applied to kinds of modalities. It consists of two key steps: (1) the Stable Unimodal Feature Augmentation (SUFA) that learns a stable unimodal representation by incorporating the aleatoric uncertainty into self-supervised contrastive learning. (2) Robust Multimodal Feature Integration (RMFI) leveraging an information-theoretic strategy to learn a robust compact joint representation. We evaluate our proposed EAU method on five multimodal datasets where the video RGB image text audio and depth image are involved. Extensive experiments demonstrate the EAU method is more noise-resistant than existing multimodal fusion strategies and establishes new state-of-the-art on several benchmarks.
count=1
* Scaling Laws for Data Filtering-- Data Curation cannot be Compute Agnostic
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Goyal_Scaling_Laws_for_Data_Filtering--_Data_Curation_cannot_be_Compute_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Goyal_Scaling_Laws_for_Data_Filtering--_Data_Curation_cannot_be_Compute_CVPR_2024_paper.pdf)]
    * Title: Scaling Laws for Data Filtering-- Data Curation cannot be Compute Agnostic
    * Year: `2024`
    * Authors: Sachin Goyal, Pratyush Maini, Zachary C. Lipton, Aditi Raghunathan, J. Zico Kolter
    * Abstract: Vision-language models (VLMs) are trained for thousands of GPU hours on carefully selected subsets of massive web scrapes. For instance the LAION public dataset retained only about 10 percent of the total crawled data. In recent times data curation has gained prominence with several works developing strategies to retain high-quality subsets of raw scraped data. However these strategies are typically developed agnostic to the available compute for training. In this paper we demonstrate that making filtering decisions independent of training compute is often suboptimal: well-curated data rapidly loses its utility when repeated eventually decreasing below the utility of unseen but lower-quality data. While past research in neural scaling laws has considered web data to be homogenous real data is not. Our work bridges this important gap in the literature by developing scaling laws that characterize the differing utility of various data subsets and accounting for how this diminishes for a data point at its nth repetition. Our key message is that data curation can not be agnostic of the total compute a model will be trained for. Even without ever jointly training on multiple data buckets our scaling laws enable us to estimate model performance under this dynamic trade-off between quality and repetition. This allows us to curate the best possible pool for achieving top performance on Datacomp at various compute budgets carving out a pareto-frontier for data curation.
count=1
* NICE: Neurogenesis Inspired Contextual Encoding for Replay-free Class Incremental Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Gurbuz_NICE_Neurogenesis_Inspired_Contextual_Encoding_for_Replay-free_Class_Incremental_Learning_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Gurbuz_NICE_Neurogenesis_Inspired_Contextual_Encoding_for_Replay-free_Class_Incremental_Learning_CVPR_2024_paper.pdf)]
    * Title: NICE: Neurogenesis Inspired Contextual Encoding for Replay-free Class Incremental Learning
    * Year: `2024`
    * Authors: Mustafa Burak Gurbuz, Jean Michael Moorman, Constantine Dovrolis
    * Abstract: Deep neural networks (DNNs) struggle to learn in dynamic settings because they mainly rely on static datasets. Continual learning (CL) aims to overcome this limitation by enabling DNNs to incrementally accumulate knowledge. A widely adopted scenario in CL is class-incremental learning (CIL) where DNNs are required to sequentially learn more classes. Among the various strategies in CL replay methods which revisit previous classes stand out as the only effective ones in CIL. Other strategies such as architectural modifications to segregate information across weights and protect them from change are ineffective in CIL. This is because they need additional information during testing to select the correct network parts to use. In this paper we propose NICE Neurogenesis Inspired Contextual Encoding a replay-free architectural method inspired by adult neurogenesis in the hippocampus. NICE groups neurons in the DNN based on different maturation stages and infers which neurons to use during testing without any additional signal. Through extensive experiments across 6 datasets and 3 architectures we show that NICE performs on par with or often outperforms replay methods. We also make the case that neurons exhibit highly distinctive activation patterns for the classes in which they specialize enabling us to determine when they should be used. The code is available at https://github.com/BurakGurbuz97/NICE.
count=1
* Anchor-based Robust Finetuning of Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Han_Anchor-based_Robust_Finetuning_of_Vision-Language_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Han_Anchor-based_Robust_Finetuning_of_Vision-Language_Models_CVPR_2024_paper.pdf)]
    * Title: Anchor-based Robust Finetuning of Vision-Language Models
    * Year: `2024`
    * Authors: Jinwei Han, Zhiwen Lin, Zhongyisun Sun, Yingguo Gao, Ke Yan, Shouhong Ding, Yuan Gao, Gui-Song Xia
    * Abstract: We aim at finetuning a vision-language model without hurting its out-of-distribution (OOD) generalization. We address two types of OOD generalization i.e. i) domain shift such as natural to sketch images and ii) zero-shot capability to recognize the category that was not contained in the finetune data. Arguably the diminished OOD generalization after finetuning stems from the excessively simplified finetuning target which only provides the class information such as "a photo of a [CLASS]". This is distinct from the process in that CLIP was pretrained where there is abundant text supervision with rich semantic information. Therefore we propose to compensate for the finetune process using auxiliary supervision with rich semantic information which acts as anchors to preserve the OOD generalization. Specifically two types of anchors are elaborated in our methods including i) text-compensated anchor which uses the images from the finetune set but enriches the text supervision from a pretrained captioner ii) image-text-pair anchor which is retrieved from the dataset similar to pretraining data of CLIP according to the downstream task associating with the original CLIP text with rich semantics. Those anchors are utilized as auxiliary semantic information to maintain the original feature space of CLIP thereby preserving the OOD generalization capabilities. Comprehensive experiments demonstrate that our method achieves in-distribution performance akin to conventional finetuning while attaining new state-of-the-art results on domain shift and zero-shot learning benchmarks.
count=1
* Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Han_Boosting_Self-Supervision_for_Single-View_Scene_Completion_via_Knowledge_Distillation_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Han_Boosting_Self-Supervision_for_Single-View_Scene_Completion_via_Knowledge_Distillation_CVPR_2024_paper.pdf)]
    * Title: Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation
    * Year: `2024`
    * Authors: Keonhee Han, Dominik Muhle, Felix Wimbauer, Daniel Cremers
    * Abstract: Inferring scene geometry from images via Structure from Motion is a long-standing and fundamental problem in computer vision. While classical approaches and more recently depth map predictions only focus on the visible parts of a scene the task of scene completion aims to reason about geometry even in occluded regions. With the popularity of NeRF implicit representations also became popular for scene completion by predicting so-called density fields. Unlike explicit approaches e.g. voxel-based methods density fields also allow for accurate depth prediction and novel-view synthesis via image-based rendering. In this work we propose to fuse the scene reconstruction from multiple images and distill this knowledge into a more accurate single-view scene reconstruction. To this end we propose MVBTS to fuse density fields from multiple posed images trained fully self-supervised only from image data. Using knowledge distillation we use MVBTS to train a single-view scene completion network via direct supervision called KDBTS. It achieves state-of-the-art performance on occupancy prediction especially in occluded regions.
count=1
* Gradient Reweighting: Towards Imbalanced Class-Incremental Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/He_Gradient_Reweighting_Towards_Imbalanced_Class-Incremental_Learning_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/He_Gradient_Reweighting_Towards_Imbalanced_Class-Incremental_Learning_CVPR_2024_paper.pdf)]
    * Title: Gradient Reweighting: Towards Imbalanced Class-Incremental Learning
    * Year: `2024`
    * Authors: Jiangpeng He
    * Abstract: Class-Incremental Learning (CIL) trains a model to continually recognize new classes from non-stationary data while retaining learned knowledge. A major challenge of CIL arises when applying to real-world data characterized by non-uniform distribution which introduces a dual imbalance problem involving (i) disparities between stored exemplars of old tasks and new class data (inter-phase imbalance) and (ii) severe class imbalances within each individual task (intra-phase imbalance). We show that this dual imbalance issue causes skewed gradient updates with biased weights in FC layers thus inducing over/under-fitting and catastrophic forgetting in CIL. Our method addresses it by reweighting the gradients towards balanced optimization and unbiased classifier learning. Additionally we observe imbalanced forgetting where paradoxically the instance-rich classes suffer higher performance degradation during CIL due to a larger amount of training data becoming unavailable in subsequent learning phases. To tackle this we further introduce a distribution-aware knowledge distillation loss to mitigate forgetting by aligning output logits proportionally with the distribution of lost training data. We validate our method on CIFAR-100 ImageNetSubset and Food101 across various evaluation protocols and demonstrate consistent improvements compared to existing works showing great potential to apply CIL in real-world scenarios with enhanced robustness and effectiveness.
count=1
* An Asymmetric Augmented Self-Supervised Learning Method for Unsupervised Fine-Grained Image Hashing
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Hu_An_Asymmetric_Augmented_Self-Supervised_Learning_Method_for_Unsupervised_Fine-Grained_Image_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_An_Asymmetric_Augmented_Self-Supervised_Learning_Method_for_Unsupervised_Fine-Grained_Image_CVPR_2024_paper.pdf)]
    * Title: An Asymmetric Augmented Self-Supervised Learning Method for Unsupervised Fine-Grained Image Hashing
    * Year: `2024`
    * Authors: Feiran Hu, Chenlin Zhang, Jiangliang Guo, Xiu-Shen Wei, Lin Zhao, Anqi Xu, Lingyan Gao
    * Abstract: Unsupervised fine-grained image hashing aims to learn compact binary hash codes in unsupervised settings addressing challenges posed by large-scale datasets and dependence on supervision. In this paper we first identify a granularity gap between generic and fine-grained datasets for unsupervised hashing methods highlighting the inadequacy of conventional self-supervised learning for fine-grained visual objects. To bridge this gap we propose the Asymmetric Augmented Self-Supervised Learning (A^2-SSL) method comprising three modules. The asymmetric augmented SSL module employs suitable augmentation strategies for positive/negative views preventing fine-grained category confusion inherent in conventional SSL. Part-oriented dense contrastive learning utilizes the Fisher Vector framework to capture and model fine-grained object parts enhancing unsupervised representations through part-level dense contrastive learning. Self-consistent hash code learning introduces a reconstruction task aligned with the self-consistency principle guiding the model to emphasize comprehensive features particularly fine-grained patterns. Experimental results on five benchmark datasets demonstrate the superiority of A^2-SSL over existing methods affirming its efficacy in unsupervised fine-grained image hashing.
count=1
* LEAD: Exploring Logit Space Evolution for Model Selection
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Hu_LEAD_Exploring_Logit_Space_Evolution_for_Model_Selection_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_LEAD_Exploring_Logit_Space_Evolution_for_Model_Selection_CVPR_2024_paper.pdf)]
    * Title: LEAD: Exploring Logit Space Evolution for Model Selection
    * Year: `2024`
    * Authors: Zixuan Hu, Xiaotong Li, Shixiang Tang, Jun Liu, Yichun Hu, Ling-Yu Duan
    * Abstract: The remarkable success of "pretrain-then-finetune" paradigm has led to a proliferation of available pre-trained models for vision tasks. This surge presents a significant challenge in efficiently choosing the most suitable pre-trained models for downstream tasks. The critical aspect of this challenge lies in effectively predicting the model transferability by considering the underlying fine-tuning dynamics. Existing methods often model fine-tuning dynamics in feature space with linear transformations which do not precisely align with the fine-tuning objective and fail to grasp the essential nonlinearity from optimization. To this end we present LEAD a finetuning-aligned approach based on the network output of logits. LEAD proposes a theoretical framework to model the optimization process and derives an ordinary differential equation (ODE) to depict the nonlinear evolution toward the final logit state. Additionally we design a class-aware decomposition method to consider the varying evolution dynamics across classes and further ensure practical applicability. Integrating the closely aligned optimization objective and nonlinear modeling capabilities derived from the differential equation our method offers a concise solution to effectively bridge the optimization gap in a single step bypassing the lengthy fine-tuning process. The comprehensive experiments on 24 supervised and self-supervised pre-trained models across 10 downstream datasets demonstrate impressive performances and showcase its broad adaptability even in low-data scenarios.
count=1
* LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Huang_LP_A_Surprisingly_Strong_Linear_Probe_for_Few-Shot_CLIP_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_LP_A_Surprisingly_Strong_Linear_Probe_for_Few-Shot_CLIP_CVPR_2024_paper.pdf)]
    * Title: LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP
    * Year: `2024`
    * Authors: Yunshi Huang, Fereshteh Shakeri, Jose Dolz, Malik Boudiaf, Houda Bahig, Ismail Ben Ayed
    * Abstract: In a recent strongly emergent literature on few-shot CLIP adaptation Linear Probe (LP) has been often reported as a weak baseline. This has motivated intensive research building convoluted prompt learning or feature adaptation strategies. In this work we propose and examine from convex-optimization perspectives a generalization of the standard LP baseline in which the linear classifier weights are learnable functions of the text embedding with class-wise multipliers blending image and text knowledge. As our objective function depends on two types of variables i.e. the class visual prototypes and the learnable blending parameters we propose a computationally efficient block coordinate Majorize-Minimize (MM) descent algorithm. In our full-batch MM optimizer which we coin LP++ step sizes are implicit unlike standard gradient descent practices where learning rates are intensively searched over validation sets. By examining the mathematical properties of our loss (e.g. Lipschitz gradient continuity) we build majorizing functions yielding data-driven learning rates and derive approximations of the loss's minima which provide data-informed initialization of the variables. Our image-language objective function along with these non-trivial optimization insights and ingredients yields surprisingly highly competitive few-shot CLIP performances. Furthermore LP++ operates in black-box relaxes intensive validation searches for the optimization hyper-parameters and runs orders-of-magnitudes faster than state-of-the-art few-shot CLIP adaptation methods. Our code is available at: https://github.com/FereshteShakeri/FewShot-CLIP-Strong-Baseline.git.
count=1
* A Noisy Elephant in the Room: Is Your Out-of-Distribution Detector Robust to Label Noise?
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Humblot-Renaux_A_Noisy_Elephant_in_the_Room_Is_Your_Out-of-Distribution_Detector_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Humblot-Renaux_A_Noisy_Elephant_in_the_Room_Is_Your_Out-of-Distribution_Detector_CVPR_2024_paper.pdf)]
    * Title: A Noisy Elephant in the Room: Is Your Out-of-Distribution Detector Robust to Label Noise?
    * Year: `2024`
    * Authors: Galadrielle Humblot-Renaux, Sergio Escalera, Thomas B. Moeslund
    * Abstract: The ability to detect unfamiliar or unexpected images is essential for safe deployment of computer vision systems. In the context of classification the task of detecting images outside of a model's training domain is known as out-of-distribution (OOD) detection. While there has been a growing research interest in developing post-hoc OOD detection methods there has been comparably little discussion around how these methods perform when the underlying classifier is not trained on a clean carefully curated dataset. In this work we take a closer look at 20 state-of-the-art OOD detection methods in the (more realistic) scenario where the labels used to train the underlying classifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive experiments across different datasets noise types & levels architectures and checkpointing strategies provide insights into the effect of class label noise on OOD detection and show that poor separation between incorrectly classified ID samples vs. OOD samples is an overlooked yet important limitation of existing methods. Code: https://github.com/glhr/ood-labelnoise
count=1
* Video ReCap: Recursive Captioning of Hour-Long Videos
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Islam_Video_ReCap_Recursive_Captioning_of_Hour-Long_Videos_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Islam_Video_ReCap_Recursive_Captioning_of_Hour-Long_Videos_CVPR_2024_paper.pdf)]
    * Title: Video ReCap: Recursive Captioning of Hour-Long Videos
    * Year: `2024`
    * Authors: Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius
    * Abstract: Most video captioning models are designed to process short video clips of few seconds and output text describing low-level visual concepts (e.g. objects scenes atomic actions). However most real-world videos last for minutes or hours and have a complex hierarchical structure spanning different temporal granularities. We propose Video ReCap a recursive video captioning model that can process video inputs of dramatically different lengths (from 1 second to 2 hours) and output video captions at multiple hierarchy levels. The recursive video-language architecture exploits the synergy between different video hierarchies and can process hour-long videos efficiently. We utilize a curriculum learning training scheme to learn the hierarchical structure of videos starting from clip-level captions describing atomic actions then focusing on segment-level descriptions and concluding with generating summaries for hour-long videos. Furthermore we introduce Ego4D-HCap dataset by augmenting Ego4D with 8267 manually collected long-range video summaries. Our recursive model can flexibly generate captions at different hierarchy levels while also being useful for other complex video understanding tasks such as VideoQA on EgoSchema. Data code and models are publicly available at https://sites.google.com/view/vidrecap.
count=1
* Efficient Hyperparameter Optimization with Adaptive Fidelity Identification
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_Efficient_Hyperparameter_Optimization_with_Adaptive_Fidelity_Identification_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_Efficient_Hyperparameter_Optimization_with_Adaptive_Fidelity_Identification_CVPR_2024_paper.pdf)]
    * Title: Efficient Hyperparameter Optimization with Adaptive Fidelity Identification
    * Year: `2024`
    * Authors: Jiantong Jiang, Zeyi Wen, Atif Mansoor, Ajmal Mian
    * Abstract: Hyperparameter Optimization and Neural Architecture Search are powerful in attaining state-of-the-art machine learning models with Bayesian Optimization (BO) standing out as a mainstream method. Extending BO into the multi-fidelity setting has been an emerging research topic in this field but faces the challenge of determining an appropriate fidelity for each hyperparameter configuration to fit the surrogate model. To tackle the challenge we propose a multi-fidelity BO method named FastBO which excels in adaptively deciding the fidelity for each configuration and providing strong performance while ensuring efficient resource usage. These advantages are achieved through our proposed techniques based on the concepts of efficient point and saturation point for each configuration which can be obtained from the empirical learning curve of the configuration estimated from early observations. Extensive experiments demonstrate FastBO's superior anytime performance and efficiency in identifying high-quality configurations and architectures. We also show that our method provides a way to extend any single-fidelity method to the multi-fidelity setting highlighting the wide applicability of our approach.
count=1
* Efficient Test-Time Adaptation of Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Karmanov_Efficient_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Karmanov_Efficient_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2024_paper.pdf)]
    * Title: Efficient Test-Time Adaptation of Vision-Language Models
    * Year: `2024`
    * Authors: Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, Eric Xing
    * Abstract: Test-time adaptation with pre-trained vision-language models has attracted increasing attention for tackling distribution shifts during the test time. Though prior studies have achieved very promising performance they involve intensive computation which is severely unaligned with test-time adaptation. We design TDA a training-free dynamic adapter that enables effective and efficient test-time adaptation with vision-language models. TDA works with a lightweight key-value cache that maintains a dynamic queue with few-shot pseudo labels as values and the corresponding test-sample features as keys. Leveraging the key-value cache TDA allows adapting to test data gradually via progressive pseudo label refinement which is super-efficient without incurring any backpropagation. In addition we introduce negative pseudo labeling that alleviates the adverse impact of pseudo label noises by assigning pseudo labels to certain negative classes when the model is uncertain about its pseudo label predictions. Extensive experiments over two benchmarks demonstrate TDA's superior effectiveness and efficiency as compared with the state-of-the-art. The code has been released in https://kdiaaa.github.io/tda/.
count=1
* Global and Local Prompts Cooperation via Optimal Transport for Federated Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Global_and_Local_Prompts_Cooperation_via_Optimal_Transport_for_Federated_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Global_and_Local_Prompts_Cooperation_via_Optimal_Transport_for_Federated_CVPR_2024_paper.pdf)]
    * Title: Global and Local Prompts Cooperation via Optimal Transport for Federated Learning
    * Year: `2024`
    * Authors: Hongxia Li, Wei Huang, Jingya Wang, Ye Shi
    * Abstract: Prompt learning in pretrained visual-language models has shown remarkable flexibility across various downstream tasks. Leveraging its inherent lightweight nature recent research attempted to integrate the powerful pretrained models into federated learning frameworks to simultaneously reduce communication costs and promote local training on insufficient data. Despite these efforts current federated prompt learning methods lack specialized designs to systematically address severe data heterogeneities e.g. data distribution with both label and feature shifts involved. To address this challenge we present Federated Prompts Cooperation via Optimal Transport (FedOTP) which introduces efficient collaborative prompt learning strategies to capture diverse category traits on a per-client basis. Specifically for each client we learn a global prompt to extract consensus knowledge among clients and a local prompt to capture client-specific category characteristics. Unbalanced Optimal Transport is then employed to align local visual features with these prompts striking a balance between global consensus and local personalization. By relaxing one of the equality constraints FedOTP enables prompts to focus solely on core image patch regions. Extensive experiments on datasets with various types of heterogeneities have demonstrated that our FedOTP outperforms the state-of-the-art methods.
count=1
* OMG-Seg: Is One Model Good Enough For All Segmentation?
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Li_OMG-Seg_Is_One_Model_Good_Enough_For_All_Segmentation_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_OMG-Seg_Is_One_Model_Good_Enough_For_All_Segmentation_CVPR_2024_paper.pdf)]
    * Title: OMG-Seg: Is One Model Good Enough For All Segmentation?
    * Year: `2024`
    * Authors: Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, Chen Change Loy
    * Abstract: In this work we address various segmentation tasks each traditionally tackled by distinct or partially unified models. We propose OMG-Seg One Model that is Good enough to efficiently and effectively handle all the segmentation tasks including image semantic instance and panoptic segmentation as well as their video counterparts open vocabulary settings prompt-driven interactive segmentation like SAM and video object segmentation. To our knowledge this is the first model to handle all these tasks in one model and achieve satisfactory performance. We show that OMG-Seg a transformer-based encoder-decoder architecture with task-specific queries and outputs can support over ten distinct segmentation tasks and yet significantly reduce computational and parameter overhead across various tasks and datasets. We rigorously evaluate the inter-task influences and correlations during co-training. Code and models are available at https://github.com/lxtGH/OMG-Seg.
count=1
* One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Li_One_Prompt_Word_is_Enough_to_Boost_Adversarial_Robustness_for_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_One_Prompt_Word_is_Enough_to_Boost_Adversarial_Robustness_for_CVPR_2024_paper.pdf)]
    * Title: One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models
    * Year: `2024`
    * Authors: Lin Li, Haoyan Guan, Jianing Qiu, Michael Spratling
    * Abstract: Large pre-trained Vision-Language Models (VLMs) like CLIP despite having remarkable generalization ability are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method named Adversarial Prompt Tuning (APT) is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly by simply adding one learned word to the prompts APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases in our most effective setting to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.
count=1
* Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy Tradeoff for Out-of-Distribution Few-shot Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Liao_Descriptor_and_Word_Soups_Overcoming_the_Parameter_Efficiency_Accuracy_Tradeoff_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Liao_Descriptor_and_Word_Soups_Overcoming_the_Parameter_Efficiency_Accuracy_Tradeoff_CVPR_2024_paper.pdf)]
    * Title: Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy Tradeoff for Out-of-Distribution Few-shot Learning
    * Year: `2024`
    * Authors: Christopher Liao, Theodoros Tsiligkaridis, Brian Kulis
    * Abstract: Over the past year a large body of multimodal research has emerged around zero-shot evaluation using GPT descriptors. These studies boost the zero-shot accuracy of pretrained VL models with an ensemble of label-specific text generated by GPT. A recent study WaffleCLIP demonstrated that similar zero-shot accuracy can be achieved with an ensemble of random descriptors. However both zero-shot methods are un-trainable and consequently sub-optimal when some few-shot out-of-distribution (OOD) training data is available. Inspired by these prior works we present two more flexible methods called descriptor and word soups which do not require an LLM at test time and can leverage training data to increase OOD target accuracy. Descriptor soup greedily selects a small set of textual descriptors using generic few-shot training data then calculates robust class embeddings using the selected descriptors. Word soup greedily assembles a chain of words in a similar manner. Compared to existing few-shot soft prompt tuning methods word soup requires fewer parameters by construction and less GPU memory since it does not require backpropagation. Both soups outperform current published few-shot methods even when combined with SoTA zero-shot methods on cross-dataset and domain generalization benchmarks. Compared with SoTA prompt and descriptor ensembling methods such as ProDA and WaffleCLIP word soup achieves higher OOD accuracy with fewer ensemble members. Please checkout our code: https://github.com/Chris210634/word_soups
count=1
* MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Lin_MoPE-CLIP_Structured_Pruning_for_Efficient_Vision-Language_Models_with_Module-wise_Pruning_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_MoPE-CLIP_Structured_Pruning_for_Efficient_Vision-Language_Models_with_Module-wise_Pruning_CVPR_2024_paper.pdf)]
    * Title: MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric
    * Year: `2024`
    * Authors: Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei, Zhenan Sun
    * Abstract: Vision-language pre-trained models have achieved impressive performance on various downstream tasks. However their large model sizes hinder their utilization on platforms with limited computational resources. We find that directly using smaller pre-trained models and applying magnitude-based pruning on CLIP models leads to inflexibility and inferior performance. Recent efforts for VLP compression either adopt uni-modal compression metrics resulting in limited performance or involve costly mask-search processes with learnable masks. In this paper we first propose the Module-wise Pruning Error (MoPE) metric accurately assessing CLIP module importance by performance decline on cross-modal tasks. Using the MoPE metric we introduce a unified pruning framework applicable to both pre-training and task-specific fine-tuning compression stages. For pre-training MoPE-CLIP effectively leverages knowledge from the teacher model significantly reducing pre-training costs while maintaining strong zero-shot capabilities. For fine-tuning consecutive pruning from width to depth yields highly competitive task-specific models. Extensive experiments in two stages demonstrate the effectiveness of the MoPE metric and MoPE-CLIP outperforms previous state-of-the-art VLP compression methods.
count=1
* Language Models as Black-Box Optimizers for Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Language_Models_as_Black-Box_Optimizers_for_Vision-Language_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Language_Models_as_Black-Box_Optimizers_for_Vision-Language_Models_CVPR_2024_paper.pdf)]
    * Title: Language Models as Black-Box Optimizers for Vision-Language Models
    * Year: `2024`
    * Authors: Shihong Liu, Samuel Yu, Zhiqiu Lin, Deepak Pathak, Deva Ramanan
    * Abstract: Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities on downstream tasks when fine-tuned with minimal data. However many VLMs rely on proprietary data and are not open-source which restricts the use of white-box approaches for fine-tuning. As such we aim to develop a black-box approach to optimize VLMs through natural language prompts thereby avoiding the need to access model parameters feature embeddings or even output logits. We propose employing chat-based LLMs to search for the best text prompt for VLMs. Specifically we adopt an automatic "hill-climbing" procedure that converges to an effective prompt by evaluating the performance of current prompts and asking LLMs to refine them based on textual feedback all within a conversational process without human-in-the-loop. In a challenging 1-shot image classification setup our simple approach surpasses the white-box continuous prompting method (CoOp) by an average of 1.5% across 11 datasets including ImageNet. Our approach also outperforms both human-engineered and LLM-generated prompts. We highlight the advantage of conversational feedback that incorporates both positive and negative prompts suggesting that LLMs can utilize the implicit "gradient" direction in textual feedback for a more efficient search. In addition we find that the text prompts generated through our strategy are not only more interpretable but also transfer well across different VLM architectures in a black-box manner. Lastly we demonstrate our framework on a state-of-the-art black-box VLM (DALL-E 3) for text-to-image optimization.
count=1
* Improved Self-Training for Test-Time Adaptation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Ma_Improved_Self-Training_for_Test-Time_Adaptation_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Improved_Self-Training_for_Test-Time_Adaptation_CVPR_2024_paper.pdf)]
    * Title: Improved Self-Training for Test-Time Adaptation
    * Year: `2024`
    * Authors: Jing Ma
    * Abstract: Test-time adaptation (TTA) is a technique to improve the performance of a pre-trained source model on a target distribution without using any labeled data. However existing self-trained TTA methods often face the challenges of unreliable pseudo-labels and unstable model optimization. In this paper we propose an Improved Self-Training (IST) approach which addresses these challenges by enhancing the pseudo-label quality and stabilizing the adaptation process. Specifically we use a simple augmentation strategy to generate multiple views of each test sample and construct a graph structure to correct the pseudo-labels based on the similarity of the latent features. Moreover we adopt a parameter moving average scheme to smooth the model updates and prevent catastrophic forgetting. Instead of using a model with fixed label space we explore the adaptability of the foundation model CLIP to various downstream tasks at test time. Extensive experiments on various benchmarks show that IST can achieve significant and consistent improvements over the existing TTA methods in classification detection and segmentation tasks.
count=1
* OVMR: Open-Vocabulary Recognition with Multi-Modal References
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Ma_OVMR_Open-Vocabulary_Recognition_with_Multi-Modal_References_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_OVMR_Open-Vocabulary_Recognition_with_Multi-Modal_References_CVPR_2024_paper.pdf)]
    * Title: OVMR: Open-Vocabulary Recognition with Multi-Modal References
    * Year: `2024`
    * Authors: Zehong Ma, Shiliang Zhang, Longhui Wei, Qi Tian
    * Abstract: The challenge of open-vocabulary recognition lies in the model has no clue of new categories it is applied to. Existing works have proposed different methods to embed category cues into the model e.g. through few-shot fine-tuning providing category names or textual descriptions to Vision-Language Models. Fine-tuning is time-consuming and degrades the generalization capability. Textual descriptions could be ambiguous and fail to depict visual details. This paper tackles open-vocabulary recognition from a different perspective by referring to multi-modal clues composed of textual descriptions and exemplar images. Our method named OVMR adopts two innovative components to pursue a more robust category cues embedding. A multi-modal classifier is first generated by dynamically complementing textual descriptions with image exemplars. A preference-based refinement module is hence applied to fuse uni-modal and multi-modal classifiers with the aim to alleviate issues of low-quality exemplar images or textual descriptions. The proposed OVMR is a plug-and-play module and works well with exemplar images randomly crawled from the Internet. Extensive experiments have demonstrated the promising performance of OVMR e.g. it outperforms existing methods across various scenarios and setups. Codes are publicly available at \href https://github.com/Zehong-Ma/OVMR https://github.com/Zehong-Ma/OVMR .
count=1
* Transductive Zero-Shot and Few-Shot CLIP
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Martin_Transductive_Zero-Shot_and_Few-Shot_CLIP_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Martin_Transductive_Zero-Shot_and_Few-Shot_CLIP_CVPR_2024_paper.pdf)]
    * Title: Transductive Zero-Shot and Few-Shot CLIP
    * Year: `2024`
    * Authors: Ségolène Martin, Yunshi Huang, Fereshteh Shakeri, Jean-Christophe Pesquet, Ismail Ben Ayed
    * Abstract: Transductive inference has been widely investigated in few-shot image classification but completely overlooked in the recent fast growing literature on adapting vision-langage models like CLIP. This paper addresses the transductive zero-shot and few-shot CLIP classification challenge in which inference is performed jointly across a mini-batch of unlabeled query samples rather than treating each instance independently. This paper addresses the transductive zero-shot and few-shot CLIP classification challenge in which inference is performed jointly across a mini-batch of unlabeled query samples rather than treating each instance independently. We initially construct informative vision-text probability features leading to a classification problem on the unit simplex set. Inspired by Expectation-Maximization (EM) our optimization-based classifying objective models the data probability distribution for each class using a Dirichlet law. The minimization problem is then tackled with a novel block Majorization-Minimization algorithm which simultaneously estimates the distribution parameters and class assignments. Extensivenumerical experiments on 11 datasets underscore the benefits and efficacy of our batch inference approach. On zero-shot tasks with test batches of 75 samples our approach yields near 20% improvement in ImageNet accuracy over CLIP's zero-shot performance. Additionally we outperform state-of-the-art methods in the few-shot setting. Code is available at https://github.com/SegoleneMartin/transductive-CLIP.
count=1
* Bayesian Exploration of Pre-trained Models for Low-shot Image Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Miao_Bayesian_Exploration_of_Pre-trained_Models_for_Low-shot_Image_Classification_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Miao_Bayesian_Exploration_of_Pre-trained_Models_for_Low-shot_Image_Classification_CVPR_2024_paper.pdf)]
    * Title: Bayesian Exploration of Pre-trained Models for Low-shot Image Classification
    * Year: `2024`
    * Authors: Yibo Miao, Yu Lei, Feng Zhou, Zhijie Deng
    * Abstract: Low-shot image classification is a fundamental task in computer vision and the emergence of large-scale vision-language models such as CLIP has greatly advanced the forefront of research in this field. However most existing CLIP-based methods lack the flexibility to effectively incorporate other pre-trained models that encompass knowledge distinct from CLIP. To bridge the gap this work proposes a simple and effective probabilistic model ensemble framework based on Gaussian processes which have previously demonstrated remarkable efficacy in processing small data. We achieve the integration of prior knowledge by specifying the mean function with CLIP and the kernel function with an ensemble of deep kernels built upon various pre-trained models. By regressing the classification label directly our framework enables analytical inference straightforward uncertainty quantification and principled hyper-parameter tuning. Through extensive experiments on standard benchmarks we demonstrate that our method consistently outperforms competitive ensemble baselines regarding predictive performance. Additionally we assess the robustness of our method and the quality of the yielded uncertainty estimates on out-of-distribution datasets. We also illustrate that our method despite relying on label regression still enjoys superior model calibration compared to most deterministic baselines.
count=1
* Building Optimal Neural Architectures using Interpretable Knowledge
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Mills_Building_Optimal_Neural_Architectures_using_Interpretable_Knowledge_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Mills_Building_Optimal_Neural_Architectures_using_Interpretable_Knowledge_CVPR_2024_paper.pdf)]
    * Title: Building Optimal Neural Architectures using Interpretable Knowledge
    * Year: `2024`
    * Authors: Keith G. Mills, Fred X. Han, Mohammad Salameh, Shengyao Lu, Chunhua Zhou, Jiao He, Fengyu Sun, Di Niu
    * Abstract: Neural Architecture Search is a costly practice. The fact that a search space can span a vast number of design choices with each architecture evaluation taking nontrivial overhead makes it hard for an algorithm to sufficiently explore candidate networks. In this paper we propose AutoBuild a scheme which learns to align the latent embeddings of operations and architecture modules with the ground-truth performance of the architectures they appear in. By doing so AutoBuild is capable of assigning interpretable importance scores to architecture modules such as individual operation features and larger macro operation sequences such that high-performance neural networks can be constructed without any need for search. Through experiments performed on state-of-the-art image classification segmentation and Stable Diffusion models we show that by mining a relatively small set of evaluated architectures AutoBuild can learn to build high-quality architectures directly or help to reduce search space to focus on relevant areas finding better architectures that outperform both the original labeled ones and ones found by search baselines. Code available at https://github.com/Ascend-Research/AutoBuild
count=1
* OpticalDR: A Deep Optical Imaging Model for Privacy-Protective Depression Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Pan_OpticalDR_A_Deep_Optical_Imaging_Model_for_Privacy-Protective_Depression_Recognition_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Pan_OpticalDR_A_Deep_Optical_Imaging_Model_for_Privacy-Protective_Depression_Recognition_CVPR_2024_paper.pdf)]
    * Title: OpticalDR: A Deep Optical Imaging Model for Privacy-Protective Depression Recognition
    * Year: `2024`
    * Authors: Yuchen Pan, Junjun Jiang, Kui Jiang, Zhihao Wu, Keyuan Yu, Xianming Liu
    * Abstract: Depression Recognition (DR) poses a considerable challenge especially in the context of the growing concerns surrounding privacy. Traditional automatic diagnosis of DR technology necessitates the use of facial images undoubtedly expose the patient identity features and poses privacy risks. In order to mitigate the potential risks associated with the inappropriate disclosure of patient facial images we design a new imaging system to erase the identity information of captured facial images while retain disease-relevant features. It is irreversible for identity information recovery while preserving essential disease-related characteristics necessary for accurate DR. More specifically we try to record a de-identified facial image (erasing the identifiable features as much as possible) by a learnable lens which is optimized in conjunction with the following DR task as well as a range of face analysis related auxiliary tasks in an end-to-end manner. These aforementioned strategies form our final Optical deep Depression Recognition network (OpticalDR). Experiments on CelebA AVEC 2013 and AVEC 2014 datasets demonstrate that our OpticalDR has achieved state-of-the-art privacy protection performance with an average AUC of 0.51 on popular facial recognition models and competitive results for DR with MAE/RMSE of 7.53/8.48 on AVEC 2013 and 7.89/8.82 on AVEC 2014 respectively. Code is available at https://github.com/divertingPan/OpticalDR.
count=1
* The Neglected Tails in Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Parashar_The_Neglected_Tails_in_Vision-Language_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Parashar_The_Neglected_Tails_in_Vision-Language_Models_CVPR_2024_paper.pdf)]
    * Title: The Neglected Tails in Vision-Language Models
    * Year: `2024`
    * Authors: Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, Shu Kong
    * Abstract: Vision-language models (VLMs) excel in zero-shot recognition but their performance varies greatly across different visual concepts. For example although CLIP achieves impressive accuracy on ImageNet (60-80%) its performance drops below 10% for more than ten concepts like night snake presumably due to their limited presence in the pretraining data. However measuring the frequency of concepts in VLMs' large-scale datasets is challenging. We address this by using large language models (LLMs) to count the number of pretraining texts that contain synonyms of these concepts. Our analysis confirms that popular datasets such as LAION exhibit a long-tailed concept distribution yielding biased performance in VLMs. We also find that downstream applications of VLMs including visual chatbots (e.g. GPT-4V) and text-to-image models (e.g. Stable Diffusion) often fail to recognize or generate images of rare concepts identified by our method. To mitigate the imbalanced performance of zero-shot VLMs we propose REtrieval-Augmented Learning (REAL). First instead of prompting VLMs using the original class names REAL uses their most frequent synonyms found in pretraining texts. This simple change already outperforms costly human-engineered and LLM-enriched prompts over nine benchmark datasets. Second REAL trains a linear classifier on a small yet balanced set of pretraining data retrieved using concept synonyms. REAL surpasses the previous zero-shot SOTA using 400x less storage and 10000x less training time!
count=1
* Prompt Learning via Meta-Regularization
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Park_Prompt_Learning_via_Meta-Regularization_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Park_Prompt_Learning_via_Meta-Regularization_CVPR_2024_paper.pdf)]
    * Title: Prompt Learning via Meta-Regularization
    * Year: `2024`
    * Authors: Jinyoung Park, Juyeon Ko, Hyunwoo J. Kim
    * Abstract: Pre-trained vision-language models have shown impressive success on various computer vision tasks with their zero-shot generalizability. Recently prompt learning approaches have been explored to efficiently and effectively adapt the vision-language models to a variety of downstream tasks. However most existing prompt learning methods suffer from task overfitting since the general knowledge of the pre-trained vision language models is forgotten while the prompts are finetuned on a small data set from a specific target task. To address this issue we propose a Prompt Meta-Regularization (ProMetaR) to improve the generalizability of prompt learning for vision-language models. Specifically ProMetaR meta-learns both the regularizer and the soft prompts to harness the task-specific knowledge from the downstream tasks and task-agnostic general knowledge from the vision-language models. Further ProMetaR augments the task to generate multiple virtual tasks to alleviate the meta-overfitting. In addition we provide the analysis to comprehend how ProMetaR improves the generalizability of prompt tuning in the perspective of the gradient alignment. Our extensive experiments demonstrate that our ProMetaR improves the generalizability of conventional prompt learning methods under base-to-base/base-to-new and domain generalization settings. The code of ProMetaR is available at https://github.com/mlvlab/ProMetaR.
count=1
* Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Saha_Improved_Zero-Shot_Classification_by_Adapting_VLMs_with_Text_Descriptions_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Saha_Improved_Zero-Shot_Classification_by_Adapting_VLMs_with_Text_Descriptions_CVPR_2024_paper.pdf)]
    * Title: Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions
    * Year: `2024`
    * Authors: Oindrila Saha, Grant Van Horn, Subhransu Maji
    * Abstract: The zero-shot performance of existing vision-language models (VLMs) such as CLIP is limited by the availability of large-scale aligned image and text datasets in specific domains. In this work we leverage two complementary sources of information -- descriptions of categories generated by large language models (LLMs) and abundant fine-grained image classification datasets -- to improve the zero-shot classification performance of VLMs across fine-grained domains. On the technical side we develop methods to train VLMs with this "bag-level" image-text supervision. We find that simply using these attributes at test-time does not improve performance but our training strategy for example on the iNaturalist dataset leads to an average improvement of 4-5% in zero-shot classification accuracy for novel categories of birds and flowers. Similar improvements are observed in domains where a subset of the categories was used to fine-tune the model. By prompting LLMs in various ways we generate descriptions that capture visual appearance habitat and geographic regions and pair them with existing attributes such as the taxonomic structure of the categories. We systematically evaluate their ability to improve zero-shot categorization in natural domains. Our findings suggest that geographic priors can be just as effective and are complementary to visual appearance. Our method also outperforms prior work on prompt-based tuning of VLMs. We release the benchmark consisting of 14 datasets at https://github.com/cvl-umass/AdaptCLIPZS which will contribute to future research in zero-shot recognition.
count=1
* Incremental Residual Concept Bottleneck Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Shang_Incremental_Residual_Concept_Bottleneck_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Shang_Incremental_Residual_Concept_Bottleneck_Models_CVPR_2024_paper.pdf)]
    * Title: Incremental Residual Concept Bottleneck Models
    * Year: `2024`
    * Authors: Chenming Shang, Shiji Zhou, Hengyuan Zhang, Xinzhe Ni, Yujiu Yang, Yuwang Wang
    * Abstract: Concept Bottleneck Models (CBMs) map the black-box visual representations extracted by deep neural networks onto a set of interpretable concepts and use the concepts to make predictions enhancing the transparency of the decision-making process. Multimodal pre-trained models can match visual representations with textual concept embeddings allowing for obtaining the interpretable concept bottleneck without the expertise concept annotations. Recent research has focused on the concept bank establishment and the high-quality concept selection. However it is challenging to construct a comprehensive concept bank through humans or large language models which severely limits the performance of CBMs. In this work we propose the Incremental Residual Concept Bottleneck Model (Res-CBM) to address the challenge of concept completeness. Specifically the residual concept bottleneck model employs a set of optimizable vectors to complete missing concepts then the incremental concept discovery module converts the complemented vectors with unclear meanings into potential concepts in the candidate concept bank. Our approach can be applied to any user-defined concept bank as a post-hoc processing method to enhance the performance of any CBMs. Furthermore to measure the descriptive efficiency of CBMs the Concept Utilization Efficiency (CUE) metric is proposed. Experiments show that the Res-CBM outperforms the current state-of-the-art methods in terms of both accuracy and efficiency and achieves comparable performance to black-box models across multiple datasets.
count=1
* DeIL: Direct-and-Inverse CLIP for Open-World Few-Shot Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Shao_DeIL_Direct-and-Inverse_CLIP_for_Open-World_Few-Shot_Learning_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Shao_DeIL_Direct-and-Inverse_CLIP_for_Open-World_Few-Shot_Learning_CVPR_2024_paper.pdf)]
    * Title: DeIL: Direct-and-Inverse CLIP for Open-World Few-Shot Learning
    * Year: `2024`
    * Authors: Shuai Shao, Yu Bai, Yan Wang, Baodi Liu, Yicong Zhou
    * Abstract: Open-World Few-Shot Learning (OFSL) is a critical field of research concentrating on the precise identification of target samples in environments with scarce data and unreliable labels thus possessing substantial practical significance. Recently the evolution of foundation models like CLIP has revealed their strong capacity for representation even in settings with restricted resources and data. This development has led to a significant shift in focus transitioning from the traditional method of "building models from scratch" to a strategy centered on "efficiently utilizing the capabilities of foundation models to extract relevant prior knowledge tailored for OFSL and apply it judiciously". Amidst this backdrop we unveil the Direct-and-Inverse CLIP (DeIL) an innovative method leveraging our proposed "Direct-and-Inverse" concept to activate CLIP-based methods for addressing OFSL. This concept transforms conventional single-step classification into a nuanced two-stage process: initially filtering out less probable categories followed by accurately determining the specific category of samples. DeIL comprises two key components: a pre-trainer (frozen) for data denoising and an adapter (tunable) for achieving precise final classification. In experiments DeIL achieves SOTA performance on 11 datasets.
count=1
* A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Silva-Rodriguez_A_Closer_Look_at_the_Few-Shot_Adaptation_of_Large_Vision-Language_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Silva-Rodriguez_A_Closer_Look_at_the_Few-Shot_Adaptation_of_Large_Vision-Language_CVPR_2024_paper.pdf)]
    * Title: A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models
    * Year: `2024`
    * Authors: Julio Silva-Rodríguez, Sina Hajimiri, Ismail Ben Ayed, Jose Dolz
    * Abstract: Efficient transfer learning (ETL) is receiving increasing attention to adapt large pre-trained language-vision models on downstream tasks with a few labeled samples. While significant progress has been made we reveal that state-of-the-art ETL approaches exhibit strong performance only in narrowly-defined experimental setups and with a careful adjustment of hyperparameters based on a large corpus of labeled samples. In particular we make two interesting and surprising empirical observations. First to outperform a simple Linear Probing baseline these methods require to optimize their hyper-parameters on each target task. And second they typically underperform --sometimes dramatically-- standard zero-shot predictions in the presence of distributional drifts. Motivated by the unrealistic assumptions made in the existing literature i.e. access to a large validation set and case-specific grid-search for optimal hyperparameters we propose a novel approach that meets the requirements of real-world scenarios. More concretely we introduce a CLass-Adaptive linear Probe (CLAP) objective whose balancing term is optimized via an adaptation of the general Augmented Lagrangian method tailored to this context. We comprehensively evaluate CLAP on a broad span of datasets and scenarios demonstrating that it consistently outperforms SoTA approaches while yet being a much more efficient alternative.
count=1
* Label Propagation for Zero-shot Classification with Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Stojni_Label_Propagation_for_Zero-shot_Classification_with_Vision-Language_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Stojni_Label_Propagation_for_Zero-shot_Classification_with_Vision-Language_Models_CVPR_2024_paper.pdf)]
    * Title: Label Propagation for Zero-shot Classification with Vision-Language Models
    * Year: `2024`
    * Authors: Vladan Stojni?, Yannis Kalantidis, Giorgos Tolias
    * Abstract: Vision-Language Models (VLMs) have demonstrated impressive performance on zero-shot classification i.e. classification when provided merely with a list of class names. In this paper we tackle the case of zero-shot classification in the presence of unlabeled data. We leverage the graph structure of the unlabeled data and introduce ZLaP a method based on label propagation (LP) that utilizes geodesic distances for classification. We tailor LP to graphs containing both text and image features and further propose an efficient method for performing inductive inference based on a dual solution and a sparsification step. We perform extensive experiments to evaluate the effectiveness of our method on 14 common datasets and show that ZLaP outperforms the latest related works. Code: https://github.com/vladan-stojnic/ZLaP
count=1
* AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Tang_AMU-Tuning_Effective_Logit_Bias_for_CLIP-based_Few-shot_Learning_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Tang_AMU-Tuning_Effective_Logit_Bias_for_CLIP-based_Few-shot_Learning_CVPR_2024_paper.pdf)]
    * Title: AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning
    * Year: `2024`
    * Authors: Yuwei Tang, Zhenyi Lin, Qilong Wang, Pengfei Zhu, Qinghua Hu
    * Abstract: Recently pre-trained vision-language models (e.g. CLIP) have shown great potential in few-shot learning and attracted a lot of research interest. Although efforts have been made to improve few-shot ability of CLIP key factors on the effectiveness of existing methods have not been well studied limiting further exploration of CLIP's potential in few-shot learning. In this paper we first introduce a unified formulation to analyze CLIP-based few-shot learning methods from a perspective of logit bias which encourages us to learn an effective logit bias for further improving performance of CLIP-based few-shot learning methods. To this end we disassemble three key components involved in computation of logit bias (i.e. logit features logit predictor and logit fusion) and empirically analyze the effect on performance of few-shot classification. Based on analysis of key components this paper proposes a novel AMU-Tuning method to learn effective logit bias for CLIP-based few-shot classification. Specifically our AMU-Tuning predicts logit bias by exploiting the appropriate Auxiliary features which are fed into an efficient feature-initialized linear classifier with Multi-branch training. Finally an Uncertainty-based fusion is developed to incorporate logit bias into CLIP for few-shot classification. The experiments are conducted on several widely used benchmarks and the results show AMU-Tuning clearly outperforms its counterparts while achieving state-of-the-art performance of CLIP-based few-shot learning without bells and whistles.
count=1
* ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Tian_ArGue_Attribute-Guided_Prompt_Tuning_for_Vision-Language_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Tian_ArGue_Attribute-Guided_Prompt_Tuning_for_Vision-Language_Models_CVPR_2024_paper.pdf)]
    * Title: ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models
    * Year: `2024`
    * Authors: Xinyu Tian, Shu Zou, Zhaoyuan Yang, Jing Zhang
    * Abstract: Although soft prompt tuning is effective in efficiently adapting Vision-Language (V&L) models for downstream tasks it shows limitations in dealing with distribution shifts. We address this issue with Attribute-Guided Prompt Tuning (ArGue) making three key contributions. 1) In contrast to the conventional approach of directly appending soft prompts preceding class names we align the model with primitive visual attributes generated by Large Language Models (LLMs). We posit that a model's ability to express high confidence in these attributes signifies its capacity to discern the correct class rationales. 2) We introduce attribute sampling to eliminate disadvantageous attributes thus only semantically meaningful attributes are preserved. 3) We propose negative prompting explicitly enumerating class-agnostic attributes to activate spurious correlations and encourage the model to generate highly orthogonal probability distributions in relation to these negative features. In experiments our method significantly outperforms current state-of-the-art prompt tuning methods on both novel class prediction and out-of-distribution generalization tasks.
count=1
* Learning Vision from Models Rivals Learning Vision from Data
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Tian_Learning_Vision_from_Models_Rivals_Learning_Vision_from_Data_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Tian_Learning_Vision_from_Models_Rivals_Learning_Vision_from_Data_CVPR_2024_paper.pdf)]
    * Title: Learning Vision from Models Rivals Learning Vision from Data
    * Year: `2024`
    * Authors: Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, Phillip Isola
    * Abstract: We introduce SynCLR a novel approach for learning visual representations exclusively from synthetic images without any real data. We synthesize a large dataset of image captions using LLMs then use an off-the-shelf text-to-image model to generate multiple images corresponding to each synthetic caption. We perform visual representation learning on these synthetic images via contrastive learning treating images sharing the same caption as positive pairs. The resulting representations demonstrate remarkable transferability competing favorably with other general-purpose visual representation learners such as CLIP and DINO v2 in image classification tasks. Furthermore in dense prediction tasks such as semantic segmentation SynCLR outperforms previous self-supervised methods by a significant margin e.g. improving over MAE and iBOT by 5.0 and 3.1 mIoU on ADE20k for ViT-B/16.
count=1
* GLACE: Global Local Accelerated Coordinate Encoding
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_GLACE_Global_Local_Accelerated_Coordinate_Encoding_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_GLACE_Global_Local_Accelerated_Coordinate_Encoding_CVPR_2024_paper.pdf)]
    * Title: GLACE: Global Local Accelerated Coordinate Encoding
    * Year: `2024`
    * Authors: Fangjinhua Wang, Xudong Jiang, Silvano Galliani, Christoph Vogel, Marc Pollefeys
    * Abstract: Scene coordinate regression (SCR) methods are a family of visual localization methods that directly regress 2D-3D matches for camera pose estimation. They are effective in small-scale scenes but face significant challenges in large-scale scenes that are further amplified in the absence of ground truth 3D point clouds for supervision. Here the model can only rely on reprojection constraints and needs to implicitly triangulate the points. The challenges stem from a fundamental dilemma: The network has to be invariant to observations of the same landmark at different viewpoints and lighting conditions etc. but at the same time discriminate unrelated but similar observations. The latter becomes more relevant and severe in larger scenes. In this work we tackle this problem by introducing the concept of co-visibility to the network. We propose GLACE which integrates pre-trained global and local encodings and enables SCR to scale to large scenes with only a single small-sized network. Specifically we propose a novel feature diffusion technique that implicitly groups the reprojection constraints with co-visibility and avoids overfitting to trivial solutions. Additionally our position decoder parameterizes the output positions for large-scale scenes more effectively. Without using 3D models or depth maps for supervision our method achieves state-of-the-art results on large-scale scenes with a low-map-size model. On Cambridge landmarks with a single model we achieve a 17% lower median position error than Poker the ensemble variant of the state-of-the-art SCR method ACE. Code is available at: https://github.com/cvg/glace.
count=1
* Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Pre-trained_Model_Guided_Fine-Tuning_for_Zero-Shot_Adversarial_Robustness_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Pre-trained_Model_Guided_Fine-Tuning_for_Zero-Shot_Adversarial_Robustness_CVPR_2024_paper.pdf)]
    * Title: Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness
    * Year: `2024`
    * Authors: Sibo Wang, Jie Zhang, Zheng Yuan, Shiguang Shan
    * Abstract: Large-scale pre-trained vision-language models like CLIP have demonstrated impressive performance across various tasks and exhibit remarkable zero-shot generalization capability while they are also vulnerable to imperceptible adversarial examples. Existing works typically employ adversarial training (fine-tuning) as a defense method against adversarial examples. However direct application to the CLIP model may result in overfitting compromising the model's capacity for generalization. In this paper we propose Pre-trained Model Guided Adversarial Fine-Tuning (PMG-AFT) method which leverages supervision from the original pre-trained model by carefully designing an auxiliary branch to enhance the model's zero-shot adversarial robustness. Specifically PMG-AFT minimizes the distance between the features of adversarial examples in the target model and those in the pre-trained model aiming to preserve the generalization features already captured by the pre-trained model. Extensive Experiments on 15 zero-shot datasets demonstrate that PMG-AFT significantly outperforms the state-of-the-art method improving the top-1 robust accuracy by an average of 4.99%. Furthermore our approach consistently improves clean accuracy by an average of 8.72%.
count=1
* Structured Model Probing: Empowering Efficient Transfer Learning by Structured Regularization
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_Structured_Model_Probing_Empowering_Efficient_Transfer_Learning_by_Structured_Regularization_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Structured_Model_Probing_Empowering_Efficient_Transfer_Learning_by_Structured_Regularization_CVPR_2024_paper.pdf)]
    * Title: Structured Model Probing: Empowering Efficient Transfer Learning by Structured Regularization
    * Year: `2024`
    * Authors: Zhi-Fan Wu, Chaojie Mao, Xue Wang, Jianwen Jiang, Yiliang Lv, Rong Jin
    * Abstract: Despite encouraging results from recent developments in transfer learning for adapting pre-trained model to downstream tasks the performance of model probing is still lagging behind the state-of-the-art parameter efficient tuning methods. Our investigation reveals that existing model probing methods perform well for the easy case when the source domain (where models are pre-trained) and the adapted domain are similar but fail for the difficult case when the two domains are significantly different. Simply incorporating features extracted from multiple layers and increasing complexity of the probing model can mitigate the gap in the difficult case but degrades the performance in the easy case. To address this challenge we propose structured model probing (SMP) that is able to deliver good performance for both cases through structured regularization. The regularization performs feature selection leveraging model structure as a prior and controls the complexity of the probing model through the weights of selected structures. This enables us to construct a simple adaptation model with a small number of selected features and a linear prediction model for the easy case; and to automatically increase the complexity of adaptation model with a large number of selected features and a non-linear model for the difficult case. Our extensive empirical studies show that SMP significantly outperforms the state-of-the-art methods for parameter efficient tuning and at the same time still maintains the advantage of computational efficiency for probing-based methods.
count=1
* NB-GTR: Narrow-Band Guided Turbulence Removal
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Xia_NB-GTR_Narrow-Band_Guided_Turbulence_Removal_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Xia_NB-GTR_Narrow-Band_Guided_Turbulence_Removal_CVPR_2024_paper.pdf)]
    * Title: NB-GTR: Narrow-Band Guided Turbulence Removal
    * Year: `2024`
    * Authors: Yifei Xia, Chu Zhou, Chengxuan Zhu, Minggui Teng, Chao Xu, Boxin Shi
    * Abstract: The removal of atmospheric turbulence is crucial for long-distance imaging. Leveraging the stochastic nature of atmospheric turbulence numerous algorithms have been developed that employ multi-frame input to mitigate the turbulence. However when limited to a single frame existing algorithms face substantial performance drops particularly in diverse real-world scenes. In this paper we propose a robust solution to turbulence removal from an RGB image under the guidance of an additional narrow-band image broadening the applicability of turbulence mitigation techniques in real-world imaging scenarios. Our approach exhibits a substantial suppression in the magnitude of turbulence artifacts by using only a pair of images thereby enhancing the clarity and fidelity of the captured scene.
count=1
* Any-Shift Prompting for Generalization over Distributions
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Xiao_Any-Shift_Prompting_for_Generalization_over_Distributions_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Any-Shift_Prompting_for_Generalization_over_Distributions_CVPR_2024_paper.pdf)]
    * Title: Any-Shift Prompting for Generalization over Distributions
    * Year: `2024`
    * Authors: Zehao Xiao, Jiayi Shen, Mohammad Mahdi Derakhshani, Shengcai Liao, Cees G. M. Snoek
    * Abstract: Image-language models with prompt learning have shown remarkable advances in numerous downstream vision tasks. Nevertheless conventional prompt learning methods overfit the training distribution and lose the generalization ability on the test distributions. To improve the generalization across various distribution shifts we propose any-shift prompting: a general probabilistic inference framework that considers the relationship between training and test distributions during prompt learning. We explicitly connect training and test distributions in the latent space by constructing training and test prompts in a hierarchical architecture. Within this framework the test prompt exploits the distribution relationships to guide the generalization of the CLIP image-language model from training to any test distribution. To effectively encode the distribution information and their relationships we further introduce a transformer inference network with a pseudo-shift training mechanism. The network generates the tailored test prompt with both training and test information in a feedforward pass avoiding extra training costs at test time. Extensive experiments on twenty-three datasets demonstrate the effectiveness of any-shift prompting on the generalization over various distribution shifts.
count=1
* MMA: Multi-Modal Adapter for Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_MMA_Multi-Modal_Adapter_for_Vision-Language_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_MMA_Multi-Modal_Adapter_for_Vision-Language_Models_CVPR_2024_paper.pdf)]
    * Title: MMA: Multi-Modal Adapter for Vision-Language Models
    * Year: `2024`
    * Authors: Lingxiao Yang, Ru-Yuan Zhang, Yanchen Wang, Xiaohua Xie
    * Abstract: Pre-trained Vision-Language Models (VLMs) have served as excellent foundation models for transfer learning in diverse downstream tasks. However tuning VLMs for few-shot generalization tasks faces a discrimination -- generalization dilemma i.e. general knowledge should be preserved and task-specific knowledge should be fine-tuned. How to precisely identify these two types of representations remains a challenge. In this paper we propose a Multi-Modal Adapter (MMA) for VLMs to improve the alignment between representations from text and vision branches. MMA aggregates features from different branches into a shared feature space so that gradients can be communicated across branches. To determine how to incorporate MMA we systematically analyze the discriminability and generalizability of features across diverse datasets in both the vision and language branches and find that (1) higher layers contain discriminable dataset-specific knowledge while lower layers contain more generalizable knowledge and (2) language features are more discriminable than visual features and there are large semantic gaps between the features of the two modalities especially in the lower layers. Therefore we only incorporate MMA to a few higher layers of transformers to achieve an optimal balance between discrimination and generalization. We evaluate the effectiveness of our approach on three tasks: generalization to novel classes novel target datasets and domain generalization. Compared to many state-of-the-art methods our MMA achieves leading performance in all evaluations. Code is at https://github.com/ZjjConan/Multi-Modal-Adapter
count=1
* Structure-Guided Adversarial Training of Diffusion Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Structure-Guided_Adversarial_Training_of_Diffusion_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Structure-Guided_Adversarial_Training_of_Diffusion_Models_CVPR_2024_paper.pdf)]
    * Title: Structure-Guided Adversarial Training of Diffusion Models
    * Year: `2024`
    * Authors: Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui
    * Abstract: Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling their training primarily emphasizes instance-level optimization overlooking valuable structural information within each mini-batch indicative of pair-wise relationships among samples. To address this limitation we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game distinguishing real manifold structures from the generated ones. SADM substantially outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512 respectively.
count=1
* TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Yao_TCPTextual-based_Class-aware_Prompt_tuning_for_Visual-Language_Model_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Yao_TCPTextual-based_Class-aware_Prompt_tuning_for_Visual-Language_Model_CVPR_2024_paper.pdf)]
    * Title: TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model
    * Year: `2024`
    * Authors: Hantao Yao, Rui Zhang, Changsheng Xu
    * Abstract: Prompt tuning represents a valuable technique for adapting pre-trained visual-language models (VLM) to various downstream tasks. Recent advancements in CoOp-based methods propose a set of learnable domain-shared or image-conditional textual tokens to facilitate the generation of task-specific textual classifiers. However those textual tokens have a limited generalization ability regarding unseen domains as they cannot dynamically adjust to the distribution of testing classes. To tackle this issue we present a novel Textual-based Class-aware Prompt tuning(TCP) that explicitly incorporates prior knowledge about classes to enhance their discriminability. The critical concept of TCP involves leveraging Textual Knowledge Embedding (TKE) to map the high generalizability of class-level textual knowledge into class aware textual tokens. By seamlessly integrating these class-aware prompts into the Text Encoder a dynamic class-aware classifier is generated to enhance discriminability for unseen domains. During inference TKE dynamically generates class-aware prompts related to the unseen classes. Comprehensive evaluations demonstrate that TKE serves as a plug-and-play module effortlessly combinable with existing methods. Furthermore TCP consistently achieves superior performance while demanding less training time.
count=1
* CNC-Net: Self-Supervised Learning for CNC Machining Operations
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Yavartanoo_CNC-Net_Self-Supervised_Learning_for_CNC_Machining_Operations_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Yavartanoo_CNC-Net_Self-Supervised_Learning_for_CNC_Machining_Operations_CVPR_2024_paper.pdf)]
    * Title: CNC-Net: Self-Supervised Learning for CNC Machining Operations
    * Year: `2024`
    * Authors: Mohsen Yavartanoo, Sangmin Hong, Reyhaneh Neshatavar, Kyoung Mu Lee
    * Abstract: CNC manufacturing is a process that employs computer numerical control (CNC) machines to govern the movements of various industrial tools and machinery encompassing equipment ranging from grinders and lathes to mills and CNC routers. However the reliance on manual CNC programming has become a bottleneck and the requirement for expert knowledge can result in significant costs. Therefore we introduce a pioneering approach named CNC-Net representing the use of deep neural networks (DNNs) to simulate CNC machines and grasp intricate operations when supplied with raw materials. CNC-Net constitutes a self-supervised framework that exclusively takes an input 3D model and subsequently generates the essential operation parameters required by the CNC machine to construct the object. Our method has the potential to transformative automation in manufacturing by offering a cost-effective alternative to the high costs of manual CNC programming while maintaining exceptional precision in 3D object production. Our experiments underscore the effectiveness of our CNC-Net in constructing the desired 3D objects through the utilization of CNC operations. Notably it excels in preserving finer local details exhibiting a marked enhancement in precision compared to the state-of-the-art 3D CAD reconstruction approaches. The codes are available at https://github.com/myavartanoo/CNC-Net_PyTorch.
count=1
* SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for Self-Supervised Landmark Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Yin_SCE-MAE_Selective_Correspondence_Enhancement_with_Masked_Autoencoder_for_Self-Supervised_Landmark_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Yin_SCE-MAE_Selective_Correspondence_Enhancement_with_Masked_Autoencoder_for_Self-Supervised_Landmark_CVPR_2024_paper.pdf)]
    * Title: SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for Self-Supervised Landmark Estimation
    * Year: `2024`
    * Authors: Kejia Yin, Varshanth Rao, Ruowei Jiang, Xudong Liu, Parham Aarabi, David B. Lindell
    * Abstract: Self-supervised landmark estimation is a challenging task that demands the formation of locally distinct feature representations to identify sparse facial landmarks in the absence of annotated data. To tackle this task existing state-of-the-art (SOTA) methods (1) extract coarse features from backbones that are trained with instance-level self-supervised learning (SSL) paradigms which neglect the dense prediction nature of the task (2) aggregate them into memory-intensive hypercolumn formations and (3) supervise lightweight projector networks to naively establish full local correspondences among all pairs of spatial features. In this paper we introduce SCE-MAE a framework that (1) leverages the MAE [??] a region-level SSL method that naturally better suits the landmark prediction task (2) operates on the vanilla feature map instead of on expensive hypercolumns and (3) employs a Correspondence Approximation and Refinement Block (CARB) that utilizes a simple density peak clustering algorithm and our proposed Locality-Constrained Repellence Loss to directly hone only select local correspondences. We demonstrate through extensive experiments that SCE-MAE is highly effective and robust outperforming existing SOTA methods by large margins of 20%-44% on the landmark matching and 9%-15% on the landmark detection tasks.
count=1
* Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Young_Fully_Convolutional_Slice-to-Volume_Reconstruction_for_Single-Stack_MRI_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Young_Fully_Convolutional_Slice-to-Volume_Reconstruction_for_Single-Stack_MRI_CVPR_2024_paper.pdf)]
    * Title: Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI
    * Year: `2024`
    * Authors: Sean I. Young, Yael Balbastre, Bruce Fischl, Polina Golland, Juan Eugenio Iglesias
    * Abstract: In magnetic resonance imaging (MRI) slice-to-volume reconstruction (SVR) refers to computational reconstruction of an unknown 3D magnetic resonance volume from stacks of 2D slices corrupted by motion. While promising current SVR methods require multiple slice stacks for accurate 3D reconstruction leading to long scans and limiting their use in time-sensitive applications such as fetal fMRI. Here we propose a SVR method that overcomes the shortcomings of previous work and produces state-of-the-art reconstructions in the presence of extreme inter-slice motion. Inspired by the recent success of single-view depth estimation methods we formulate SVR as a single-stack motion estimation task and train a fully convolutional network to predict a motion stack for a given slice stack producing a 3D reconstruction as a byproduct of the predicted motion. Extensive experiments on the SVR of adult and fetal brains demonstrate that our fully convolutional method is twice as accurate as previous SVR methods. Our code is available at github.com/seannz/svr.
count=1
* Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Yu_Boosting_Continual_Learning_of_Vision-Language_Models_via_Mixture-of-Experts_Adapters_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_Boosting_Continual_Learning_of_Vision-Language_Models_via_Mixture-of-Experts_Adapters_CVPR_2024_paper.pdf)]
    * Title: Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters
    * Year: `2024`
    * Authors: Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Huchuan Lu, You He
    * Abstract: Continual learning can empower vision-language models to continuously acquire new knowledge without the need for access to the entire historical dataset. However mitigating the performance degradation in large-scale models is non-trivial due to (i) parameter shifts throughout lifelong learning and (ii) significant computational burdens associated with full-model tuning. In this work we present a parameter-efficient continual learning framework to alleviate long-term forgetting in incremental learning with vision-language models. Our approach involves the dynamic expansion of a pre-trained CLIP model through the integration of Mixture-of-Experts (MoE) adapters in response to new tasks. To preserve the zero-shot recognition capability of vision-language models we further introduce a Distribution Discriminative Auto-Selector (DDAS) that automatically routes in-distribution and out-of-distribution inputs to the MoE Adapter and the original CLIP respectively. Through extensive experiments across various settings our proposed method consistently outperforms previous state-of-the-art approaches while concurrently reducing parameter training burdens by 60%.
count=1
* On the Test-Time Zero-Shot Generalization of Vision-Language Models: Do We Really Need Prompt Learning?
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Zanella_On_the_Test-Time_Zero-Shot_Generalization_of_Vision-Language_Models_Do_We_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Zanella_On_the_Test-Time_Zero-Shot_Generalization_of_Vision-Language_Models_Do_We_CVPR_2024_paper.pdf)]
    * Title: On the Test-Time Zero-Shot Generalization of Vision-Language Models: Do We Really Need Prompt Learning?
    * Year: `2024`
    * Authors: Maxime Zanella, Ismail Ben Ayed
    * Abstract: The development of large vision-language models notably CLIP has catalyzed research into effective adaptation techniques with a particular focus on soft prompt tuning. Conjointly test-time augmentation which utilizes multiple augmented views of a single image to enhance zero-shot generalization is emerging as a significant area of interest. This has predominantly directed research efforts towards test-time prompt tuning. In contrast we introduce a robust MeanShift for Test-time Augmentation (MTA) which surpasses prompt-based methods without requiring this intensive training procedure. This positions MTA as an ideal solution for both standalone and API-based applications. Additionally our method does not rely on ad hoc rules (e.g. confidence threshold) used in some previous test-time augmentation techniques to filter the augmented views. Instead MTA incorporates a quality assessment variable for each view directly into its optimization process termed as the inlierness score. This score is jointly optimized with a density mode seeking process leading to an efficient training- and hyperparameter-free approach. We extensively benchmark our method on 15 datasets and demonstrate MTA's superiority and computational efficiency. Deployed easily as plug-and-play module on top of zero-shot models and state-of-the-art few-shot methods MTA shows systematic and consistent improvements.
count=1
* DePT: Decoupled Prompt Tuning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_DePT_Decoupled_Prompt_Tuning_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_DePT_Decoupled_Prompt_Tuning_CVPR_2024_paper.pdf)]
    * Title: DePT: Decoupled Prompt Tuning
    * Year: `2024`
    * Authors: Ji Zhang, Shihan Wu, Lianli Gao, Heng Tao Shen, Jingkuan Song
    * Abstract: This work breaks through the Base-New Tradeoff (BNT) dilemma in prompt tuning i.e. the better the tuned model generalizes to the base (or target) task the worse it generalizes to new tasks and vice versa. Specifically through an in-depth analysis of the learned features of the base and new tasks we observe that the BNT stems from a channel bias issue--the vast majority of feature channels are occupied by base-specific knowledge leading to the collapse of task-shared knowledge important to new tasks. To address this we propose the Decoupled Prompt Tuning (DePT) framework which decouples base-specific knowledge from feature channels into an isolated feature space during prompt tuning so as to maximally preserve task-shared knowledge in the original feature space for achieving better zero-shot generalization on new tasks. Importantly our DePT is orthogonal to existing prompt tuning approaches and can enhance them with negligible additional computational cost. Extensive experiments on several datasets show the flexibility and effectiveness of DePT.
count=1
* Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Dual_Memory_Networks_A_Versatile_Adaptation_Approach_for_Vision-Language_Models_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Dual_Memory_Networks_A_Versatile_Adaptation_Approach_for_Vision-Language_Models_CVPR_2024_paper.pdf)]
    * Title: Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models
    * Year: `2024`
    * Authors: Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang
    * Abstract: With the emergence of pre-trained vision-language models like CLIP how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation few-shot adaptation and the recently-proposed training-free few-shot adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge enabling training-free few-shot adaptation while the dynamic memory preserves historical test features online during the testing process allowing for the exploration of additional data insights beyond the training set. This novel capability enhances model performance in the few-shot setting and enables model usability in the absence of training data. The two memory networks employ the same flexible memory interactive strategy which can operate in a training-free mode and can be further enhanced by incorporating learnable projection layers. Our approach is tested across 11 datasets under the three task settings. Remarkably in the zero-shot scenario it outperforms existing methods by over 3% and even shows superior results against methods utilizing external training data. Additionally our method exhibits robust performance against natural distribution shifts.
count=1
* Exploring the Transferability of Visual Prompting for Multimodal Large Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Exploring_the_Transferability_of_Visual_Prompting_for_Multimodal_Large_Language_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Exploring_the_Transferability_of_Visual_Prompting_for_Multimodal_Large_Language_CVPR_2024_paper.pdf)]
    * Title: Exploring the Transferability of Visual Prompting for Multimodal Large Language Models
    * Year: `2024`
    * Authors: Yichi Zhang, Yinpeng Dong, Siyuan Zhang, Tianzan Min, Hang Su, Jun Zhu
    * Abstract: Although Multimodal Large Language Models (MLLMs) have demonstrated promising versatile capabilities their performance is still inferior to specialized models on downstream tasks which makes adaptation necessary to enhance their utility. However fine-tuning methods require independent training for every model leading to huge computation and memory overheads. In this paper we propose a novel setting where we aim to improve the performance of diverse MLLMs with a group of shared parameters optimized for a downstream task. To achieve this we propose Transferable Visual Prompting (TVP) a simple and effective approach to generate visual prompts that can transfer to different models and improve their performance on downstream tasks after trained on only one model. We introduce two strategies to address the issue of cross-model feature corruption of existing visual prompting methods and enhance the transferability of the learned prompts including 1) Feature Consistency Alignment: which imposes constraints to the prompted feature changes to maintain task-agnostic knowledge; 2) Task Semantics Enrichment: which encourages the prompted images to contain richer task-specific semantics with language guidance. We validate the effectiveness of TVP through extensive experiments with 6 modern MLLMs on a wide variety of tasks ranging from object recognition and counting to multimodal reasoning and hallucination correction.
count=1
* Large Language Models are Good Prompt Learners for Low-Shot Image Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Large_Language_Models_are_Good_Prompt_Learners_for_Low-Shot_Image_CVPR_2024_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Large_Language_Models_are_Good_Prompt_Learners_for_Low-Shot_Image_CVPR_2024_paper.pdf)]
    * Title: Large Language Models are Good Prompt Learners for Low-Shot Image Classification
    * Year: `2024`
    * Authors: Zhaoheng Zheng, Jingmin Wei, Xuefeng Hu, Haidong Zhu, Ram Nevatia
    * Abstract: Low-shot image classification where training images are limited or inaccessible has benefited from recent progress on pre-trained vision-language (VL) models with strong generalizability e.g. CLIP. Prompt learning methods built with VL models generate text features from the class names that only have confined class-specific information. Large Language Models (LLMs) with their vast encyclopedic knowledge emerge as the complement. Thus in this paper we discuss the integration of LLMs to enhance pre-trained VL models specifically on low-shot classification. However the domain gap between language and vision blocks the direct application of LLMs. Thus we propose LLaMP Large Language Models as Prompt learners that produces adaptive prompts for the CLIP text encoder establishing it as the connecting bridge. Experiments show that compared with other state-of-the-art prompt learning methods LLaMP yields better performance on both zero-shot generalization and few-shot image classification over a spectrum of 11 datasets. Code will be made available at: https://github.com/zhaohengz/LLaMP.
count=1
* Learning Visual Representations via Language-Guided Sampling
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Banani_Learning_Visual_Representations_via_Language-Guided_Sampling_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Banani_Learning_Visual_Representations_via_Language-Guided_Sampling_CVPR_2023_paper.pdf)]
    * Title: Learning Visual Representations via Language-Guided Sampling
    * Year: `2023`
    * Authors: Mohamed El Banani, Karan Desai, Justin Johnson
    * Abstract: Although an object may appear in numerous contexts, we often describe it in a limited number of ways. Language allows us to abstract away visual variation to represent and communicate concepts. Building on this intuition, we propose an alternative approach to visual representation learning: using language similarity to sample semantically similar image pairs for contrastive learning. Our approach diverges from image-based contrastive learning by sampling view pairs using language similarity instead of hand-crafted augmentations or learned clusters. Our approach also differs from image-text contrastive learning by relying on pre-trained language models to guide the learning rather than directly minimizing a cross-modal loss. Through a series of experiments, we show that language-guided learning yields better features than image-based and image-text representation learning approaches.
count=1
* FlexiViT: One Model for All Patch Sizes
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Beyer_FlexiViT_One_Model_for_All_Patch_Sizes_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Beyer_FlexiViT_One_Model_for_All_Patch_Sizes_CVPR_2023_paper.pdf)]
    * Title: FlexiViT: One Model for All Patch Sizes
    * Year: `2023`
    * Authors: Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, Filip Pavetic
    * Abstract: Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, openworld detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to add compute-adaptive capabilities to most models relying on a ViT backbone architecture. Code and pretrained models are available at github.com/googleresearch/big_vision.
count=1
* LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision__CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision__CVPR_2023_paper.pdf)]
    * Title: LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models
    * Year: `2023`
    * Authors: Adrian Bulat, Georgios Tzimiropoulos
    * Abstract: Soft prompt learning has recently emerged as one of the methods of choice for adapting V&L models to a downstream task using a few training examples. However, current methods significantly overfit the training data, suffering from large accuracy degradation when tested on unseen classes from the same domain. To this end, in this paper, we make the following 4 contributions: (1) To alleviate base class overfitting, we propose a novel Language-Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts. (2) To increase the representation capacity of the prompts, we propose grouped LASP where each group of prompts is optimized with respect to a separate subset of textual prompts. (3) We identify a visual-language misalignment introduced by prompt learning and LASP, and more importantly, propose a re-calibration mechanism to address it. (4) We show that LASP is inherently amenable to including, during training, virtual classes, i.e. class names for which no visual samples are available, further increasing the robustness of the learned prompts. Through evaluations on 11 datasets, we show that our approach (a) significantly outperforms all prior works on soft prompting, and (b) matches and surpasses, for the first time, the accuracy on novel classes obtained by hand-crafted prompts and CLIP for 8 out of 11 test datasets. Code will be made available.
count=1
* Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Boundary_Unlearning_Rapid_Forgetting_of_Deep_Networks_via_Shifting_the_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Boundary_Unlearning_Rapid_Forgetting_of_Deep_Networks_via_Shifting_the_CVPR_2023_paper.pdf)]
    * Title: Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary
    * Year: `2023`
    * Authors: Min Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, Chen Wang
    * Abstract: The practical needs of the "right to be forgotten" and poisoned data removal call for efficient machine unlearning techniques, which enable machine learning models to unlearn, or to forget a fraction of training data and its lineage. Recent studies on machine unlearning for deep neural networks (DNNs) attempt to destroy the influence of the forgetting data by scrubbing the model parameters. However, it is prohibitively expensive due to the large dimension of the parameter space. In this paper, we refocus our attention from the parameter space to the decision space of the DNN model, and propose Boundary Unlearning, a rapid yet effective way to unlearn an entire class from a trained DNN model. The key idea is to shift the decision boundary of the original DNN model to imitate the decision behavior of the model retrained from scratch. We develop two novel boundary shift methods, namely Boundary Shrink and Boundary Expanding, both of which can rapidly achieve the utility and privacy guarantees. We extensively evaluate Boundary Unlearning on CIFAR-10 and Vggface2 datasets, and the results show that Boundary Unlearning can effectively forget the forgetting class on image classification and face recognition tasks, with an expected speed-up of 17x and 19x, respectively, compared with retraining from the scratch.
count=1
* Understanding and Improving Visual Prompting: A Label-Mapping Perspective
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Understanding_and_Improving_Visual_Prompting_A_Label-Mapping_Perspective_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Understanding_and_Improving_Visual_Prompting_A_Label-Mapping_Perspective_CVPR_2023_paper.pdf)]
    * Title: Understanding and Improving Visual Prompting: A Label-Mapping Perspective
    * Year: `2023`
    * Authors: Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, Sijia Liu
    * Abstract: We revisit and advance visual prompting (VP), an input prompting technique for vision tasks. VP can reprogram a fixed, pre-trained source model to accomplish downstream tasks in the target domain by simply incorporating universal prompts (in terms of input perturbation patterns) into downstream data points. Yet, it remains elusive why VP stays effective even given a ruleless label mapping (LM) between the source classes and the target classes. Inspired by the above, we ask: How is LM interrelated with VP? And how to exploit such a relationship to improve its accuracy on target tasks? We peer into the influence of LM on VP and provide an affirmative answer that a better 'quality' of LM (assessed by mapping precision and explanation) can consistently improve the effectiveness of VP. This is in contrast to the prior art where the factor of LM was missing. To optimize LM, we propose a new VP framework, termed ILM-VP (iterative label mapping-based visual prompting), which automatically re-maps the source labels to the target labels and progressively improves the target task accuracy of VP. Further, when using a contrastive language-image pretrained (CLIP) model, we propose to integrate an LM process to assist the text prompt selection of CLIP and to improve the target task accuracy. Extensive experiments demonstrate that our proposal significantly outperforms state-of-the-art VP methods. As highlighted below, we show that when reprogramming an ImageNet-pretrained ResNet-18 to 13 target tasks, our method outperforms baselines by a substantial margin, e.g., 7.9% and 6.7% accuracy improvements in transfer learning to the target Flowers102 and CIFAR100 datasets. Besides, our proposal on CLIP-based VP provides 13.7% and 7.1% accuracy improvements on Flowers102 and DTD respectively.
count=1
* Object-Goal Visual Navigation via Effective Exploration of Relations Among Historical Navigation States
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Du_Object-Goal_Visual_Navigation_via_Effective_Exploration_of_Relations_Among_Historical_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Object-Goal_Visual_Navigation_via_Effective_Exploration_of_Relations_Among_Historical_CVPR_2023_paper.pdf)]
    * Title: Object-Goal Visual Navigation via Effective Exploration of Relations Among Historical Navigation States
    * Year: `2023`
    * Authors: Heming Du, Lincheng Li, Zi Huang, Xin Yu
    * Abstract: Object-goal visual navigation aims at steering an agent toward an object via a series of moving steps. Previous works mainly focus on learning informative visual representations for navigation, but overlook the impacts of navigation states on the effectiveness and efficiency of navigation. We observe that high relevance among navigation states will cause navigation inefficiency or failure for existing methods. In this paper, we present a History-inspired Navigation Policy Learning (HiNL) framework to estimate navigation states effectively by exploring relationships among historical navigation states. In HiNL, we propose a History-aware State Estimation (HaSE) module to alleviate the impacts of dominant historical states on the current state estimation. Meanwhile, HaSE also encourages an agent to be alert to the current observation changes, thus enabling the agent to make valid actions. Furthermore, we design a History-based State Regularization (HbSR) to explicitly suppress the correlation among navigation states in training. As a result, our agent can update states more effectively while reducing the correlations among navigation states. Experiments on the artificial platform AI2-THOR (i.e.,, iTHOR and RoboTHOR) demonstrate that HiNL significantly outperforms state-of-the-art methods on both Success Rate and SPL in unseen testing environments.
count=1
* G-MSM: Unsupervised Multi-Shape Matching With Graph-Based Affinity Priors
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Eisenberger_G-MSM_Unsupervised_Multi-Shape_Matching_With_Graph-Based_Affinity_Priors_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Eisenberger_G-MSM_Unsupervised_Multi-Shape_Matching_With_Graph-Based_Affinity_Priors_CVPR_2023_paper.pdf)]
    * Title: G-MSM: Unsupervised Multi-Shape Matching With Graph-Based Affinity Priors
    * Year: `2023`
    * Authors: Marvin Eisenberger, Aysim Toker, Laura Leal-Taixé, Daniel Cremers
    * Abstract: We present G-MSM (Graph-based Multi-Shape Matching), a novel unsupervised learning approach for non-rigid shape correspondence. Rather than treating a collection of input poses as an unordered set of samples, we explicitly model the underlying shape data manifold. To this end, we propose an adaptive multi-shape matching architecture that constructs an affinity graph on a given set of training shapes in a self-supervised manner. The key idea is to combine putative, pairwise correspondences by propagating maps along shortest paths in the underlying shape graph. During training, we enforce cycle-consistency between such optimal paths and the pairwise matches which enables our model to learn topology-aware shape priors. We explore different classes of shape graphs and recover specific settings, like template-based matching (star graph) or learnable ranking/sorting (TSP graph), as special cases in our framework. Finally, we demonstrate state-of-the-art performance on several recent shape correspondence benchmarks, including real-world 3D scan meshes with topological noise and challenging inter-class pairs.
count=1
* Batch Model Consolidation (BMC)
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Fostiropoulos_Batch_Model_Consolidation_A_Multi-Task_Model_Consolidation_Framework_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Fostiropoulos_Batch_Model_Consolidation_A_Multi-Task_Model_Consolidation_Framework_CVPR_2023_paper.pdf)]
    * Title: Batch Model Consolidation: A Multi-Task Model Consolidation Framework
    * Year: `2023`
    * Authors: Iordanis Fostiropoulos, Jiaye Zhu, Laurent Itti
    * Abstract: In Continual Learning (CL), a model is required to learn a stream of tasks sequentially without significant performance degradation on previously learned tasks. Current approaches fail for a long sequence of tasks from diverse domains and difficulties. Many of the existing CL approaches are difficult to apply in practice due to excessive memory cost or training time, or are tightly coupled to a single device. With the intuition derived from the widely applied mini-batch training, we propose Batch Model Consolidation (BMC) to support more realistic CL under conditions where multiple agents are exposed to a range of tasks. During a regularization phase, BMC trains multiple expert models in parallel on a set of disjoint tasks. Each expert maintains weight similarity to a base model through a stability loss, and constructs a buffer from a fraction of the task's data. During the consolidation phase, we combine the learned knowledge on 'batches' of expert models using a batched consolidation loss in memory data that aggregates all buffers. We thoroughly evaluate each component of our method in an ablation study and demonstrate the effectiveness on standardized benchmark datasets Split-CIFAR-100, Tiny-ImageNet, and the Stream dataset composed of 71 image classification tasks from diverse domains and difficulties. Our method outperforms the next best CL approach by 70% and is the only approach that can maintain performance at the end of 71 tasks.
count=1
* Improving Zero-Shot Generalization and Robustness of Multi-Modal Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.pdf)]
    * Title: Improving Zero-Shot Generalization and Robustness of Multi-Modal Models
    * Year: `2023`
    * Authors: Yunhao Ge, Jie Ren, Andrew Gallagher, Yuxiao Wang, Ming-Hsuan Yang, Hartwig Adam, Laurent Itti, Balaji Lakshminarayanan, Jiaping Zhao
    * Abstract: Multi-modal image-text models such as CLIP and LiT have demonstrated impressive performance on image classification benchmarks and their zero-shot generalization ability is particularly exciting. While the top-5 zero-shot accuracies of these models are very high, the top-1 accuracies are much lower (over 25% gap in some cases). We investigate the reasons for this performance gap and find that many of the failure cases are caused by ambiguity in the text prompts. First, we develop a simple and efficient zero-shot post-hoc method to identify images whose top-1 prediction is likely to be incorrect, by measuring consistency of the predictions w.r.t. multiple prompts and image transformations. We show that our procedure better predicts mistakes, outperforming the popular max logit baseline on selective prediction tasks. Next, we propose a simple and efficient way to improve accuracy on such uncertain images by making use of the WordNet hierarchy; specifically we augment the original class by incorporating its parent and children from the semantic label hierarchy, and plug the augmentation into text prompts. We conduct experiments on both CLIP and LiT models with five different ImageNet- based datasets. For CLIP, our method improves the top-1 accuracy by 17.13% on the uncertain subset and 3.6% on the entire ImageNet validation set. We also show that our method improves across ImageNet shifted datasets, four other datasets, and other model architectures such as LiT. Our proposed method is hyperparameter-free, requires no additional model training and can be easily scaled to other large multi-modal architectures. Code is available at https://github.com/gyhandy/Hierarchy-CLIP.
count=1
* Diversity-Aware Meta Visual Prompting
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Diversity-Aware_Meta_Visual_Prompting_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Diversity-Aware_Meta_Visual_Prompting_CVPR_2023_paper.pdf)]
    * Title: Diversity-Aware Meta Visual Prompting
    * Year: `2023`
    * Authors: Qidong Huang, Xiaoyi Dong, Dongdong Chen, Weiming Zhang, Feifei Wang, Gang Hua, Nenghai Yu
    * Abstract: We present Diversity-Aware Meta Visual Prompting (DAM-VP), an efficient and effective prompting method for transferring pre-trained models to downstream tasks with frozen backbone. A challenging issue in visual prompting is that image datasets sometimes have a large data diversity whereas a per-dataset generic prompt can hardly handle the complex distribution shift toward the original pretraining data distribution properly. To address this issue, we propose a dataset Diversity-Aware prompting strategy whose initialization is realized by a Meta-prompt. Specifically, we cluster the downstream dataset into small homogeneity subsets in a diversity-adaptive way, with each subset has its own prompt optimized separately. Such a divide-and-conquer design reduces the optimization difficulty greatly and significantly boosts the prompting performance. Furthermore, all the prompts are initialized with a meta-prompt, which is learned across several datasets. It is a bootstrapped paradigm, with the key observation that the prompting knowledge learned from previous datasets could help the prompt to converge faster and perform better on a new dataset. During inference, we dynamically select a proper prompt for each input, based on the feature distance between the input and each subset. Through extensive experiments, our DAM-VP demonstrates superior efficiency and effectiveness, clearly surpassing previous prompting methods in a series of downstream datasets for different pretraining models. Our code is available at: https://github.com/shikiw/DAM-VP.
count=1
* Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_Understanding_and_Constructing_Latent_Modality_Structures_in_Multi-Modal_Representation_Learning_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Understanding_and_Constructing_Latent_Modality_Structures_in_Multi-Modal_Representation_Learning_CVPR_2023_paper.pdf)]
    * Title: Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning
    * Year: `2023`
    * Authors: Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing Ping, Son Dinh Tran, Yi Xu, Belinda Zeng, Trishul Chilimbi
    * Abstract: Contrastive loss has been increasingly used in learning representations from multiple modalities. In the limit, the nature of the contrastive loss encourages modalities to exactly match each other in the latent space. Yet it remains an open question how the modality alignment affects the downstream task performance. In this paper, based on an information-theoretic argument, we first prove that exact modality alignment is sub-optimal in general for downstream prediction tasks. Hence we advocate that the key of better performance lies in meaningful latent modality structures instead of perfect modality alignment. To this end, we propose three general approaches to construct latent modality structures. Specifically, we design 1) a deep feature separation loss for intra-modality regularization; 2) a Brownian-bridge loss for inter-modality regularization; and 3) a geometric consistency loss for both intra- and inter-modality regularization. Extensive experiments are conducted on two popular multi-modal representation learning frameworks: the CLIP-based two-tower model and the ALBEF-based fusion model. We test our model on a variety of tasks including zero/few-shot image classification, image-text retrieval, visual question answering, visual reasoning, and visual entailment. Our method achieves consistent improvements over existing methods, demonstrating the effectiveness and generalizability of our proposed approach on latent modality structure regularization.
count=1
* Video-Text As Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Jin_Video-Text_As_Game_Players_Hierarchical_Banzhaf_Interaction_for_Cross-Modal_Representation_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Video-Text_As_Game_Players_Hierarchical_Banzhaf_Interaction_for_Cross-Modal_Representation_CVPR_2023_paper.pdf)]
    * Title: Video-Text As Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning
    * Year: `2023`
    * Authors: Peng Jin, Jinfa Huang, Pengfei Xiong, Shangxuan Tian, Chang Liu, Xiangyang Ji, Li Yuan, Jie Chen
    * Abstract: Contrastive learning-based video-language representation learning approaches, e.g., CLIP, have achieved outstanding performance, which pursue semantic interaction upon pre-defined video-text pairs. To clarify this coarse-grained global interaction and move a step further, we have to encounter challenging shell-breaking interactions for fine-grained cross-modal learning. In this paper, we creatively model video-text as game players with multivariate cooperative game theory to wisely handle the uncertainty during fine-grained semantic interaction with diverse granularity, flexible combination, and vague intensity. Concretely, we propose Hierarchical Banzhaf Interaction (HBI) to value possible correspondence between video frames and text words for sensitive and explainable cross-modal contrast. To efficiently realize the cooperative game of multiple video frames and multiple text words, the proposed method clusters the original video frames (text words) and computes the Banzhaf Interaction between the merged tokens. By stacking token merge modules, we achieve cooperative games at different semantic levels. Extensive experiments on commonly used text-video retrieval and video-question answering benchmarks with superior performances justify the efficacy of our HBI. More encouragingly, it can also serve as a visualization tool to promote the understanding of cross-modal interaction, which may have a far-reaching impact on the community. Project page is available at https://jpthu17.github.io/HBI/.
count=1
* MaPLe: Multi-Modal Prompt Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.pdf)]
    * Title: MaPLe: Multi-Modal Prompt Learning
    * Year: `2023`
    * Authors: Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan
    * Abstract: Pre-trained vision-language (V-L) models such as CLIP have shown excellent generalization ability to downstream tasks. However, they are sensitive to the choice of input text prompts and require careful selection of prompt templates to perform well. Inspired by the Natural Language Processing (NLP) literature, recent CLIP adaptation approaches learn prompts as the textual inputs to fine-tune CLIP for downstream tasks. We note that using prompting to adapt representations in a single branch of CLIP (language or vision) is sub-optimal since it does not allow the flexibility to dynamically adjust both representation spaces on a downstream task. In this work, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations. Our design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions. Further, we learn separate prompts across different early stages to progressively model the stage-wise feature relationships to allow rich context learning. We evaluate the effectiveness of our approach on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorable performance and achieves an absolute gain of 3.45% on novel classes and 2.72% on overall harmonic-mean, averaged over 11 diverse image recognition datasets. Our code and pre-trained models are available at https://github.com/muzairkhattak/multimodal-prompt-learning.
count=1
* Two-Way Multi-Label Loss
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Kobayashi_Two-Way_Multi-Label_Loss_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Kobayashi_Two-Way_Multi-Label_Loss_CVPR_2023_paper.pdf)]
    * Title: Two-Way Multi-Label Loss
    * Year: `2023`
    * Authors: Takumi Kobayashi
    * Abstract: A natural image frequently contains multiple classification targets, accordingly providing multiple class labels rather than a single label per image. While the single-label classification is effectively addressed by applying a softmax cross-entropy loss, the multi-label task is tackled mainly in a binary cross-entropy (BCE) framework. In contrast to the softmax loss, the BCE loss involves issues regarding imbalance as multiple classes are decomposed into a bunch of binary classifications; recent works improve the BCE loss to cope with the issue by means of weighting. In this paper, we propose a multi-label loss by bridging a gap between the softmax loss and the multi-label scenario. The proposed loss function is formulated on the basis of relative comparison among classes which also enables us to further improve discriminative power of features by enhancing classification margin. The loss function is so flexible as to be applicable to a multi-label setting in two ways for discriminating classes as well as samples. In the experiments on multi-label classification, the proposed method exhibits competitive performance to the other multi-label losses, and it also provides transferrable features on single-label ImageNet training. Codes are available at https://github.com/tk1980/TwowayMultiLabelLoss.
count=1
* Multimodal Prompting With Missing Modalities for Visual Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Lee_Multimodal_Prompting_With_Missing_Modalities_for_Visual_Recognition_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Multimodal_Prompting_With_Missing_Modalities_for_Visual_Recognition_CVPR_2023_paper.pdf)]
    * Title: Multimodal Prompting With Missing Modalities for Visual Recognition
    * Year: `2023`
    * Authors: Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu, Chen-Yu Lee
    * Abstract: In this paper, we tackle two challenges in multimodal learning for visual recognition: 1) when missing-modality occurs either during training or testing in real-world situations; and 2) when the computation resources are not available to finetune on heavy transformer models. To this end, we propose to utilize prompt learning and mitigate the above two challenges together. Specifically, our modality-missing-aware prompts can be plugged into multimodal transformers to handle general missing-modality cases, while only requiring less than 1% learnable parameters compared to training the entire model. We further explore the effect of different prompt configurations and analyze the robustness to missing modality. Extensive experiments are conducted to show the effectiveness of our prompt learning framework that improves the performance under various missing-modality cases, while alleviating the requirement of heavy model re-training. Code is available.
count=1
* DISC: Learning From Noisy Labels via Dynamic Instance-Specific Selection and Correction
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Li_DISC_Learning_From_Noisy_Labels_via_Dynamic_Instance-Specific_Selection_and_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DISC_Learning_From_Noisy_Labels_via_Dynamic_Instance-Specific_Selection_and_CVPR_2023_paper.pdf)]
    * Title: DISC: Learning From Noisy Labels via Dynamic Instance-Specific Selection and Correction
    * Year: `2023`
    * Authors: Yifan Li, Hu Han, Shiguang Shan, Xilin Chen
    * Abstract: Existing studies indicate that deep neural networks (DNNs) can eventually memorize the label noise. We observe that the memorization strength of DNNs towards each instance is different and can be represented by the confidence value, which becomes larger and larger during the training process. Based on this, we propose a Dynamic Instance-specific Selection and Correction method (DISC) for learning from noisy labels (LNL). We first use a two-view-based backbone for image classification, obtaining confidence for each image from two views. Then we propose a dynamic threshold strategy for each instance, based on the momentum of each instance's memorization strength in previous epochs to select and correct noisy labeled data. Benefiting from the dynamic threshold strategy and two-view learning, we can effectively group each instance into one of the three subsets (i.e., clean, hard, and purified) based on the prediction consistency and discrepancy by two views at each epoch. Finally, we employ different regularization strategies to conquer subsets with different degrees of label noise, improving the whole network's robustness. Comprehensive evaluations on three controllable and four real-world LNL benchmarks show that our method outperforms the state-of-the-art (SOTA) methods to leverage useful information in noisy data while alleviating the pollution of label noise.
count=1
* Guided Recommendation for Model Fine-Tuning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Li_Guided_Recommendation_for_Model_Fine-Tuning_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Guided_Recommendation_for_Model_Fine-Tuning_CVPR_2023_paper.pdf)]
    * Title: Guided Recommendation for Model Fine-Tuning
    * Year: `2023`
    * Authors: Hao Li, Charless Fowlkes, Hao Yang, Onkar Dabeer, Zhuowen Tu, Stefano Soatto
    * Abstract: Model selection is essential for reducing the search cost of the best pre-trained model over a large-scale model zoo for a downstream task. After analyzing recent hand-designed model selection criteria with 400+ ImageNet pre-trained models and 40 downstream tasks, we find that they can fail due to invalid assumptions and intrinsic limitations. The prior knowledge on model capacity and dataset also can not be easily integrated into the existing criteria. To address these issues, we propose to convert model selection as a recommendation problem and to learn from the past training history. Specifically, we characterize the meta information of datasets and models as features, and use their transfer learning performance as the guided score. With thousands of historical training jobs, a recommendation system can be learned to predict the model selection score given the features of the dataset and the model as input. Our approach enables integrating existing model selection scores as additional features and scales with more historical data. We evaluate the prediction accuracy with 22 pre-trained models over 40 downstream tasks. With extensive evaluations, we show that the learned approach can outperform prior hand-designed model selection methods significantly when relevant training history is available.
count=1
* Rethinking Out-of-Distribution (OOD) Detection: Masked Image Modeling Is All You Need
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Li_Rethinking_Out-of-Distribution_OOD_Detection_Masked_Image_Modeling_Is_All_You_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Rethinking_Out-of-Distribution_OOD_Detection_Masked_Image_Modeling_Is_All_You_CVPR_2023_paper.pdf)]
    * Title: Rethinking Out-of-Distribution (OOD) Detection: Masked Image Modeling Is All You Need
    * Year: `2023`
    * Authors: Jingyao Li, Pengguang Chen, Zexin He, Shaozuo Yu, Shu Liu, Jiaya Jia
    * Abstract: The core of out-of-distribution (OOD) detection is to learn the in-distribution (ID) representation, which is distinguishable from OOD samples. Previous work applied recognition-based methods to learn the ID features, which tend to learn shortcuts instead of comprehensive representations. In this work, we find surprisingly that simply using reconstruction-based methods could boost the performance of OOD detection significantly. We deeply explore the main contributors of OOD detection and find that reconstruction-based pretext tasks have the potential to provide a generally applicable and efficacious prior, which benefits the model in learning intrinsic data distributions of the ID dataset. Specifically, we take Masked Image Modeling as a pretext task for our OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by 3.0%, and near-distribution OOD detection by 2.1%. It even defeats the 10-shot-per-class outlier exposure OOD detection, although we do not include any OOD samples for our detection.
count=1
* Robust Model-Based Face Reconstruction Through Weakly-Supervised Outlier Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Li_Robust_Model-Based_Face_Reconstruction_Through_Weakly-Supervised_Outlier_Segmentation_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Robust_Model-Based_Face_Reconstruction_Through_Weakly-Supervised_Outlier_Segmentation_CVPR_2023_paper.pdf)]
    * Title: Robust Model-Based Face Reconstruction Through Weakly-Supervised Outlier Segmentation
    * Year: `2023`
    * Authors: Chunlu Li, Andreas Morel-Forster, Thomas Vetter, Bernhard Egger, Adam Kortylewski
    * Abstract: In this work, we aim to enhance model-based face reconstruction by avoiding fitting the model to outliers, i.e. regions that cannot be well-expressed by the model such as occluders or make-up. The core challenge for localizing outliers is that they are highly variable and difficult to annotate. To overcome this challenging problem, we introduce a joint Face-autoencoder and outlier segmentation approach (FOCUS).In particular, we exploit the fact that the outliers cannot be fitted well by the face model and hence can be localized well given a high-quality model fitting. The main challenge is that the model fitting and the outlier segmentation are mutually dependent on each other, and need to be inferred jointly. We resolve this chicken-and-egg problem with an EM-type training strategy, where a face autoencoder is trained jointly with an outlier segmentation network. This leads to a synergistic effect, in which the segmentation network prevents the face encoder from fitting to the outliers, enhancing the reconstruction quality. The improved 3D face reconstruction, in turn, enables the segmentation network to better predict the outliers. To resolve the ambiguity between outliers and regions that are difficult to fit, such as eyebrows, we build a statistical prior from synthetic data that measures the systematic bias in model fitting. Experiments on the NoW testset demonstrate that FOCUS achieves SOTA 3D face reconstruction performance among all baselines that are trained without 3D annotation. Moreover, our results on CelebA-HQ and the AR database show that the segmentation network can localize occluders accurately despite being trained without any segmentation annotation.
count=1
* Hierarchical Prompt Learning for Multi-Task Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Hierarchical_Prompt_Learning_for_Multi-Task_Learning_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Hierarchical_Prompt_Learning_for_Multi-Task_Learning_CVPR_2023_paper.pdf)]
    * Title: Hierarchical Prompt Learning for Multi-Task Learning
    * Year: `2023`
    * Authors: Yajing Liu, Yuning Lu, Hao Liu, Yaozu An, Zhuoran Xu, Zhuokun Yao, Baofeng Zhang, Zhiwei Xiong, Chenguang Gui
    * Abstract: Vision-language models (VLMs) can effectively transfer to various vision tasks via prompt learning. Real-world scenarios often require adapting a model to multiple similar yet distinct tasks. Existing methods focus on learning a specific prompt for each task, limiting the ability to exploit potentially shared information from other tasks. Naively training a task-shared prompt using a combination of all tasks ignores fine-grained task correlations. Significant discrepancies across tasks could cause negative transferring. Considering this, we present Hierarchical Prompt (HiPro) learning, a simple and effective method for jointly adapting a pre-trained VLM to multiple downstream tasks. Our method quantifies inter-task affinity and subsequently constructs a hierarchical task tree. Task-shared prompts learned by internal nodes explore the information within the corresponding task group, while task-individual prompts learned by leaf nodes obtain fine-grained information targeted at each task. The combination of hierarchical prompts provides high-quality content of different granularity. We evaluate HiPro on four multi-task learning datasets. The results demonstrate the effectiveness of our method.
count=1
* Markerless Camera-to-Robot Pose Estimation via Self-Supervised Sim-to-Real Transfer
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Markerless_Camera-to-Robot_Pose_Estimation_via_Self-Supervised_Sim-to-Real_Transfer_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Markerless_Camera-to-Robot_Pose_Estimation_via_Self-Supervised_Sim-to-Real_Transfer_CVPR_2023_paper.pdf)]
    * Title: Markerless Camera-to-Robot Pose Estimation via Self-Supervised Sim-to-Real Transfer
    * Year: `2023`
    * Authors: Jingpei Lu, Florian Richter, Michael C. Yip
    * Abstract: Solving the camera-to-robot pose is a fundamental requirement for vision-based robot control, and is a process that takes considerable effort and cares to make accurate. Traditional approaches require modification of the robot via markers, and subsequent deep learning approaches enabled markerless feature extraction. Mainstream deep learning methods only use synthetic data and rely on Domain Randomization to fill the sim-to-real gap, because acquiring the 3D annotation is labor-intensive. In this work, we go beyond the limitation of 3D annotations for real-world data. We propose an end-to-end pose estimation framework that is capable of online camera-to-robot calibration and a self-supervised training method to scale the training to unlabeled real-world data. Our framework combines deep learning and geometric vision for solving the robot pose, and the pipeline is fully differentiable. To train the Camera-to-Robot Pose Estimation Network (CtRNet), we leverage foreground segmentation and differentiable rendering for image-level self-supervision. The pose prediction is visualized through a renderer and the image loss with the input image is back-propagated to train the neural network. Our experimental results on two public real datasets confirm the effectiveness of our approach over existing works. We also integrate our framework into a visual servoing system to demonstrate the promise of real-time precise robot pose estimation for automation tasks.
count=1
* Class-Incremental Exemplar Compression for Class-Incremental Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Luo_Class-Incremental_Exemplar_Compression_for_Class-Incremental_Learning_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Class-Incremental_Exemplar_Compression_for_Class-Incremental_Learning_CVPR_2023_paper.pdf)]
    * Title: Class-Incremental Exemplar Compression for Class-Incremental Learning
    * Year: `2023`
    * Authors: Zilin Luo, Yaoyao Liu, Bernt Schiele, Qianru Sun
    * Abstract: Exemplar-based class-incremental learning (CIL) finetunes the model with all samples of new classes but few-shot exemplars of old classes in each incremental phase, where the "few-shot" abides by the limited memory budget. In this paper, we break this "few-shot" limit based on a simple yet surprisingly effective idea: compressing exemplars by downsampling non-discriminative pixels and saving "many-shot" compressed exemplars in the memory. Without needing any manual annotation, we achieve this compression by generating 0-1 masks on discriminative pixels from class activation maps (CAM). We propose an adaptive mask generation model called class-incremental masking (CIM) to explicitly resolve two difficulties of using CAM: 1) transforming the heatmaps of CAM to 0-1 masks with an arbitrary threshold leads to a trade-off between the coverage on discriminative pixels and the quantity of exemplars, as the total memory is fixed; and 2) optimal thresholds vary for different object classes, which is particularly obvious in the dynamic environment of CIL. We optimize the CIM model alternatively with the conventional CIL model through a bilevel optimization problem. We conduct extensive experiments on high-resolution CIL benchmarks including Food-101, ImageNet-100, and ImageNet-1000, and show that using the compressed exemplars by CIM can achieve a new state-of-the-art CIL accuracy, e.g., 4.8 percentage points higher than FOSTER on 10-Phase ImageNet-1000. Our code is available at https://github.com/xfflzl/CIM-CIL.
count=1
* Open Vocabulary Semantic Segmentation With Patch Aligned Contrastive Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.pdf)]
    * Title: Open Vocabulary Semantic Segmentation With Patch Aligned Contrastive Learning
    * Year: `2023`
    * Authors: Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang, Ashish Shah, Philip H.S. Torr, Ser-Nam Lim
    * Abstract: We introduce Patch Aligned Contrastive Learning (PACL), a modified compatibility function for CLIP's contrastive loss, intending to train an alignment between the patch tokens of the vision encoder and the CLS token of the text encoder. With such an alignment, a model can identify regions of an image corresponding to a given text input, and therefore transfer seamlessly to the task of open vocabulary semantic segmentation without requiring any segmentation annotations during training. Using pre-trained CLIP encoders with PACL, we are able to set the state-of-the-art on the task of open vocabulary zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC, Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also applicable to image-level predictions and when used with a CLIP backbone, provides a general improvement in zero-shot classification accuracy compared to CLIP, across a suite of 12 image classification datasets.
count=1
* DARE-GRAM: Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Nejjar_DARE-GRAM_Unsupervised_Domain_Adaptation_Regression_by_Aligning_Inverse_Gram_Matrices_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Nejjar_DARE-GRAM_Unsupervised_Domain_Adaptation_Regression_by_Aligning_Inverse_Gram_Matrices_CVPR_2023_paper.pdf)]
    * Title: DARE-GRAM: Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices
    * Year: `2023`
    * Authors: Ismail Nejjar, Qin Wang, Olga Fink
    * Abstract: Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap between a labeled source dataset and an unlabelled target dataset for regression problems. Recent works mostly focus on learning a deep feature encoder by minimizing the discrepancy between source and target features. In this work, we present a different perspective for the DAR problem by analyzing the closed-form ordinary least square (OLS) solution to the linear regressor in the deep domain adaptation context. Rather than aligning the original feature embedding space, we propose to align the inverse Gram matrix of the features, which is motivated by its presence in the OLS solution and the Gram matrix's ability to capture the feature correlations. Specifically, we propose a simple yet effective DAR method which leverages the pseudo-inverse low-rank property to align the scale and angle in a selected subspace generated by the pseudo-inverse Gram matrix of the two domains. We evaluate our method on three domain adaptation regression benchmarks. Experimental results demonstrate that our method achieves state-of-the-art performance. Our code is available at https://github.com/ismailnejjar/DARE-GRAM.
count=1
* Learning To Name Classes for Vision and Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Parisot_Learning_To_Name_Classes_for_Vision_and_Language_Models_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Parisot_Learning_To_Name_Classes_for_Vision_and_Language_Models_CVPR_2023_paper.pdf)]
    * Title: Learning To Name Classes for Vision and Language Models
    * Year: `2023`
    * Authors: Sarah Parisot, Yongxin Yang, Steven McDonagh
    * Abstract: Large scale vision and language models can achieve impressive zero-shot recognition performance by mapping class specific text queries to image content. Two distinct challenges that remain however, are high sensitivity to the choice of handcrafted class names that define queries, and the difficulty of adaptation to new, smaller datasets. Towards addressing these problems, we propose to leverage available data to learn, for each class, an optimal word embedding as a function of the visual content. By learning new word embeddings on an otherwise frozen model, we are able to retain zero-shot capabilities for new classes, easily adapt models to new datasets, and adjust potentially erroneous, non-descriptive or ambiguous class names. We show that our solution can easily be integrated in image classification and object detection pipelines, yields significant performance gains in multiple scenarios and provides insights into model biases and labelling errors.
count=1
* LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Park_LANIT_Language-Driven_Image-to-Image_Translation_for_Unlabeled_Data_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_LANIT_Language-Driven_Image-to-Image_Translation_for_Unlabeled_Data_CVPR_2023_paper.pdf)]
    * Title: LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data
    * Year: `2023`
    * Authors: Jihye Park, Sunwoo Kim, Soohyun Kim, Seokju Cho, Jaejun Yoo, Youngjung Uh, Seungryong Kim
    * Abstract: Existing techniques for image-to-image translation commonly have suffered from two critical problems: heavy reliance on per-sample domain annotation and/or inability to handle multiple attributes per image. Recent truly-unsupervised methods adopt clustering approaches to easily provide per-sample one-hot domain labels. However, they cannot account for the real-world setting: one sample may have multiple attributes. In addition, the semantics of the clusters are not easily coupled to human understanding. To overcome these, we present LANguage-driven Image-to-image Translation model, dubbed LANIT. We leverage easy-to-obtain candidate attributes given in texts for a dataset: the similarity between images and attributes indicates per-sample domain labels. This formulation naturally enables multi-hot labels so that users can specify the target domain with a set of attributes in language. To account for the case that the initial prompts are inaccurate, we also present prompt learning. We further present domain regularization loss that enforces translated images to be mapped to the corresponding domain. Experiments on several standard benchmarks demonstrate that LANIT achieves comparable or superior performance to existing models. The code is available at github.com/KU-CVLAB/LANIT.
count=1
* RGB No More: Minimally-Decoded JPEG Vision Transformers
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Park_RGB_No_More_Minimally-Decoded_JPEG_Vision_Transformers_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Park_RGB_No_More_Minimally-Decoded_JPEG_Vision_Transformers_CVPR_2023_paper.pdf)]
    * Title: RGB No More: Minimally-Decoded JPEG Vision Transformers
    * Year: `2023`
    * Authors: Jeongsoo Park, Justin Johnson
    * Abstract: Most neural networks for computer vision are designed to infer using RGB images. However, these RGB images are commonly encoded in JPEG before saving to disk; decoding them imposes an unavoidable overhead for RGB networks. Instead, our work focuses on training Vision Transformers (ViT) directly from the encoded features of JPEG. This way, we can avoid most of the decoding overhead, accelerating data load. Existing works have studied this aspect but they focus on CNNs. Due to how these encoded features are structured, CNNs require heavy modification to their architecture to accept such data. Here, we show that this is not the case for ViTs. In addition, we tackle data augmentation directly on these encoded features, which to our knowledge, has not been explored in-depth for training in this setting. With these two improvements -- ViT and data augmentation -- we show that our ViT-Ti model achieves up to 39.2% faster training and 17.9% faster inference with no accuracy loss compared to the RGB counterpart.
count=1
* Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Radenovic_Filtering_Distillation_and_Hard_Negatives_for_Vision-Language_Pre-Training_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Radenovic_Filtering_Distillation_and_Hard_Negatives_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf)]
    * Title: Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training
    * Year: `2023`
    * Authors: Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende, Yash Patel, Yi Wen, Vignesh Ramanathan, Dhruv Mahajan
    * Abstract: Vision-language models trained with contrastive learning on large-scale noisy data are becoming increasingly popular for zero-shot recognition problems. In this paper we improve the following three aspects of the contrastive pre-training pipeline: dataset noise, model initialization and the training objective. First, we propose a straightforward filtering strategy titled Complexity, Action, and Text-spotting (CAT) that significantly reduces dataset size, while achieving improved performance across zero-shot vision-language tasks. Next, we propose an approach titled Concept Distillation to leverage strong unimodal representations for contrastive training that does not increase training complexity while outperforming prior work. Finally, we modify the traditional contrastive alignment objective, and propose an importance-sampling approach to up-sample the importance of hard-negatives without adding additional complexity. On an extensive zero-shot benchmark of 29 tasks, our Distilled and Hard-negative Training (DiHT) approach improves on 20 tasks compared to the baseline. Furthermore, for few-shot linear probing, we propose a novel approach that bridges the gap between zero-shot and few-shot performance, substantially improving over prior work. Models are available at github.com/facebookresearch/diht.
count=1
* Fake It Till You Make It: Learning Transferable Representations From Synthetic ImageNet Clones
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Sariyildiz_Fake_It_Till_You_Make_It_Learning_Transferable_Representations_From_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Sariyildiz_Fake_It_Till_You_Make_It_Learning_Transferable_Representations_From_CVPR_2023_paper.pdf)]
    * Title: Fake It Till You Make It: Learning Transferable Representations From Synthetic ImageNet Clones
    * Year: `2023`
    * Authors: Mert Bülent Sarıyıldız, Karteek Alahari, Diane Larlus, Yannis Kalantidis
    * Abstract: Recent image generation models such as Stable Diffusion have exhibited an impressive ability to generate fairly realistic images starting from a simple text prompt. Could such models render real images obsolete for training image prediction models? In this paper, we answer part of this provocative question by investigating the need for real images when training models for ImageNet classification. Provided only with the class names that have been used to build the dataset, we explore the ability of Stable Diffusion to generate synthetic clones of ImageNet and measure how useful these are for training classification models from scratch. We show that with minimal and class-agnostic prompt engineering, ImageNet clones are able to close a large part of the gap between models produced by synthetic images and models trained with real images, for the several standard classification benchmarks that we consider in this study. More importantly, we show that models trained on synthetic images exhibit strong generalization properties and perform on par with models trained on real data for transfer. Project page: https://europe.naverlabs.com/imagenet-sd
count=1
* NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Tang_NeuMap_Neural_Coordinate_Mapping_by_Auto-Transdecoder_for_Camera_Localization_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_NeuMap_Neural_Coordinate_Mapping_by_Auto-Transdecoder_for_Camera_Localization_CVPR_2023_paper.pdf)]
    * Title: NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization
    * Year: `2023`
    * Authors: Shitao Tang, Sicong Tang, Andrea Tagliasacchi, Ping Tan, Yasutaka Furukawa
    * Abstract: This paper presents an end-to-end neural mapping method for camera localization, dubbed NeuMap, encoding a whole scene into a grid of latent codes, with which a Transformer-based auto-decoder regresses 3D coordinates of query pixels. State-of-the-art feature matching methods require each scene to be stored as a 3D point cloud with per-point features, consuming several gigabytes of storage per scene. While compression is possible, performance drops significantly at high compression rates. Conversely, coordinate regression methods achieve high compression by storing scene information in a neural network but suffer from reduced robustness. NeuMap combines the advantages of both approaches by utilizing 1) learnable latent codes for efficient scene representation and 2) a scene-agnostic Transformer-based auto-decoder to infer coordinates for query pixels. This scene-agnostic network design learns robust matching priors from large-scale data and enables rapid optimization of codes for new scenes while keeping the network weights fixed. Extensive evaluations on five benchmarks show that NeuMap significantly outperforms other coordinate regression methods and achieves comparable performance to feature matching methods while requiring a much smaller scene representation size. For example, NeuMap achieves 39.1% accuracy in the Aachen night benchmark with only 6MB of data, whereas alternative methods require 100MB or several gigabytes and fail completely under high compression settings. The codes are available at https://github.com/Tangshitao/NeuMap.
count=1
* Defending Against Patch-Based Backdoor Attacks on Self-Supervised Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Tejankar_Defending_Against_Patch-Based_Backdoor_Attacks_on_Self-Supervised_Learning_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Tejankar_Defending_Against_Patch-Based_Backdoor_Attacks_on_Self-Supervised_Learning_CVPR_2023_paper.pdf)]
    * Title: Defending Against Patch-Based Backdoor Attacks on Self-Supervised Learning
    * Year: `2023`
    * Authors: Ajinkya Tejankar, Maziar Sanjabi, Qifan Wang, Sinong Wang, Hamed Firooz, Hamed Pirsiavash, Liang Tan
    * Abstract: Recently, self-supervised learning (SSL) was shown to be vulnerable to patch-based data poisoning backdoor attacks. It was shown that an adversary can poison a small part of the unlabeled data so that when a victim trains an SSL model on it, the final model will have a backdoor that the adversary can exploit. This work aims to defend self-supervised learning against such attacks. We use a three-step defense pipeline, where we first train a model on the poisoned data. In the second step, our proposed defense algorithm (PatchSearch) uses the trained model to search the training data for poisoned samples and removes them from the training set. In the third step, a final model is trained on the cleaned-up training set. Our results show that PatchSearch is an effective defense. As an example, it improves a model's accuracy on images containing the trigger from 38.2% to 63.7% which is very close to the clean model's accuracy, 64.6%. Moreover, we show that PatchSearch outperforms baselines and state-of-the-art defense approaches including those using additional clean, trusted data. Our code is available at https://github.com/UCDvision/PatchSearch
count=1
* Deep Factorized Metric Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Deep_Factorized_Metric_Learning_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Factorized_Metric_Learning_CVPR_2023_paper.pdf)]
    * Title: Deep Factorized Metric Learning
    * Year: `2023`
    * Authors: Chengkun Wang, Wenzhao Zheng, Junlong Li, Jie Zhou, Jiwen Lu
    * Abstract: Learning a generalizable and comprehensive similarity metric to depict the semantic discrepancies between images is the foundation of many computer vision tasks. While existing methods approach this goal by learning an ensemble of embeddings with diverse objectives, the backbone network still receives a mix of all the training signals. Differently, we propose a deep factorized metric learning method (DFML) to factorize the training signal and employ different samples to train various components of the backbone network. We factorize the network to different sub-blocks and devise a learnable router to adaptively allocate the training samples to each sub-block with the objective to capture the most information. The metric model trained by DFML captures different characteristics with different sub-blocks and constitutes a generalizable metric when using all the sub-blocks. The proposed DFML achieves state-of-the-art performance on all three benchmarks for deep metric learning including CUB-200-2011, Cars196, and Stanford Online Products. We also generalize DFML to the image classification task on ImageNet-1K and observe consistent improvement in accuracy/computation trade-off. Specifically, we improve the performance of ViT-B on ImageNet (+0.2% accuracy) with less computation load (-24% FLOPs).
count=1
* Few-Shot Learning With Visual Distribution Calibration and Cross-Modal Distribution Alignment
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Few-Shot_Learning_With_Visual_Distribution_Calibration_and_Cross-Modal_Distribution_Alignment_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Few-Shot_Learning_With_Visual_Distribution_Calibration_and_Cross-Modal_Distribution_Alignment_CVPR_2023_paper.pdf)]
    * Title: Few-Shot Learning With Visual Distribution Calibration and Cross-Modal Distribution Alignment
    * Year: `2023`
    * Authors: Runqi Wang, Hao Zheng, Xiaoyue Duan, Jianzhuang Liu, Yuning Lu, Tian Wang, Songcen Xu, Baochang Zhang
    * Abstract: Pre-trained vision-language models have inspired much research on few-shot learning. However, with only a few training images, there exist two crucial problems: (1) the visual feature distributions are easily distracted by class-irrelevant information in images, and (2) the alignment between the visual and language feature distributions is difficult. To deal with the distraction problem, we propose a Selective Attack module, which consists of trainable adapters that generate spatial attention maps of images to guide the attacks on class-irrelevant image areas. By messing up these areas, the critical features are captured and the visual distributions of image features are calibrated. To better align the visual and language feature distributions that describe the same object class, we propose a cross-modal distribution alignment module, in which we introduce a vision-language prototype for each class to align the distributions, and adopt the Earth Mover's Distance (EMD) to optimize the prototypes. For efficient computation, the upper bound of EMD is derived. In addition, we propose an augmentation strategy to increase the diversity of the images and the text prompts, which can reduce overfitting to the few-shot training images. Extensive experiments on 11 datasets demonstrate that our method consistently outperforms prior arts in few-shot learning.
count=1
* Multi-Agent Automated Machine Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Multi-Agent_Automated_Machine_Learning_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multi-Agent_Automated_Machine_Learning_CVPR_2023_paper.pdf)]
    * Title: Multi-Agent Automated Machine Learning
    * Year: `2023`
    * Authors: Zhaozhi Wang, Kefan Su, Jian Zhang, Huizhu Jia, Qixiang Ye, Xiaodong Xie, Zongqing Lu
    * Abstract: In this paper, we propose multi-agent automated machine learning (MA2ML) with the aim to effectively handle joint optimization of modules in automated machine learning (AutoML). MA2ML takes each machine learning module, such as data augmentation (AUG), neural architecture search (NAS), or hyper-parameters (HPO), as an agent and the final performance as the reward, to formulate a multi-agent reinforcement learning problem. MA2ML explicitly assigns credit to each agent according to its marginal contribution to enhance cooperation among modules, and incorporates off-policy learning to improve search efficiency. Theoretically, MA2ML guarantees monotonic improvement of joint optimization. Extensive experiments show that MA2ML yields the state-of-the-art top-1 accuracy on ImageNet under constraints of computational cost, e.g., 79.7%/80.5% with FLOPs fewer than 600M/800M. Extensive ablation studies verify the benefits of credit assignment and off-policy learning of MA2ML.
count=1
* Fine-Grained Classification With Noisy Labels
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Wei_Fine-Grained_Classification_With_Noisy_Labels_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Fine-Grained_Classification_With_Noisy_Labels_CVPR_2023_paper.pdf)]
    * Title: Fine-Grained Classification With Noisy Labels
    * Year: `2023`
    * Authors: Qi Wei, Lei Feng, Haoliang Sun, Ren Wang, Chenhui Guo, Yilong Yin
    * Abstract: Learning with noisy labels (LNL) aims to ensure model generalization given a label-corrupted training set. In this work, we investigate a rarely studied scenario of LNL on fine-grained datasets (LNL-FG), which is more practical and challenging as large inter-class ambiguities among fine-grained classes cause more noisy labels. We empirically show that existing methods that work well for LNL fail to achieve satisfying performance for LNL-FG, arising the practical need of effective solutions for LNL-FG. To this end, we propose a novel framework called stochastic noise-tolerated supervised contrastive learning (SNSCL) that confronts label noise by encouraging distinguishable representation. Specifically, we design a noise-tolerated supervised contrastive learning loss that incorporates a weight-aware mechanism for noisy label correction and selectively updating momentum queue lists. By this mechanism, we mitigate the effects of noisy anchors and avoid inserting noisy labels into the momentum-updated queue. Besides, to avoid manually-defined augmentation strategies in contrastive learning, we propose an efficient stochastic module that samples feature embeddings from a generated distribution, which can also enhance the representation ability of deep models. SNSCL is general and compatible with prevailing robust LNL strategies to improve their performance for LNL-FG. Extensive experiments demonstrate the effectiveness of SNSCL.
count=1
* RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.pdf)]
    * Title: RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training
    * Year: `2023`
    * Authors: Chen-Wei Xie, Siyang Sun, Xiong Xiong, Yun Zheng, Deli Zhao, Jingren Zhou
    * Abstract: Contrastive Language-Image Pre-training (CLIP) is attracting increasing attention for its impressive zero-shot recognition performance on different down-stream tasks. However, training CLIP is data-hungry and requires lots of image-text pairs to memorize various semantic concepts. In this paper, we propose a novel and efficient framework: Retrieval Augmented Contrastive Language-Image Pre-training (RA-CLIP) to augment embeddings by online retrieval. Specifically, we sample part of image-text data as a hold-out reference set. Given an input image, relevant image-text pairs are retrieved from the reference set to enrich the representation of input image. This process can be considered as an open-book exam: with the reference set as a cheat sheet, the proposed method doesn't need to memorize all visual concepts in the training data. It explores how to recognize visual concepts by exploiting correspondence between images and texts in the cheat sheet. The proposed RA-CLIP implements this idea and comprehensive experiments are conducted to show how RA-CLIP works. Performances on 10 image classification datasets and 2 object detection datasets show that RA-CLIP outperforms vanilla CLIP baseline by a large margin on zero-shot image classification task (+12.7%), linear probe image classification task (+6.9%) and zero-shot ROI classification task (+2.8%).
count=1
* Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Language_in_a_Bottle_Language_Model_Guided_Concept_Bottlenecks_for_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Language_in_a_Bottle_Language_Model_Guided_Concept_Bottlenecks_for_CVPR_2023_paper.pdf)]
    * Title: Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification
    * Year: `2023`
    * Authors: Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, Mark Yatskar
    * Abstract: Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into human-readable concepts. They allow people to easily understand why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and often under-perform their black box counterparts, preventing their broad adoption. We address these shortcomings and are first to show how to construct high-performance CBMs without manual specification of similar accuracy to black box models. Our approach, Language Guided Bottlenecks (LaBo), leverages a language model, GPT-3, to define a large space of possible bottlenecks. Given a problem domain, LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse information. Ultimately, GPT-3's sentential concepts can be aligned to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for concepts important to visual recognition. In the evaluation with 11 diverse datasets, LaBo bottlenecks excel at few-shot classification: they are 11.7% more accurate than black box linear probes at 1 shot and comparable with more data. Overall, LaBo demonstrates that inherently interpretable models can be widely applied at similar, or better, performance than black box approaches.
count=1
* Visual-Language Prompt Tuning With Knowledge-Guided Context Optimization
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Yao_Visual-Language_Prompt_Tuning_With_Knowledge-Guided_Context_Optimization_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Visual-Language_Prompt_Tuning_With_Knowledge-Guided_Context_Optimization_CVPR_2023_paper.pdf)]
    * Title: Visual-Language Prompt Tuning With Knowledge-Guided Context Optimization
    * Year: `2023`
    * Authors: Hantao Yao, Rui Zhang, Changsheng Xu
    * Abstract: Prompt tuning is an effective way to adapt the pretrained visual-language model (VLM) to the downstream task using task-related textual tokens. Representative CoOp-based works combine the learnable textual tokens with the class tokens to obtain specific textual knowledge. However, the specific textual knowledge has worse generalizable to the unseen classes because it forgets the essential general textual knowledge having a strong generalization ability. To tackle this issue, we introduce a novel Knowledge-guided Context Optimization (KgCoOp) to enhance the generalization ability of the learnable prompt for unseen classes. To remember the essential general knowledge, KgCoOp constructs a regularization term to ensure that the essential general textual knowledge can be embedded into the special textual knowledge generated by the learnable prompt. Especially, KgCoOp minimizes the discrepancy between the textual embeddings generated by learned prompts and the hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can make a discriminative prompt for both seen and unseen tasks. Extensive evaluation of several benchmarks demonstrates that the proposed Knowledge-guided Context Optimization is an efficient method for prompt tuning, i.e., achieves better performance with less training time.
count=1
* NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction From Multi-View Images
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Ye_NEF_Neural_Edge_Fields_for_3D_Parametric_Curve_Reconstruction_From_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_NEF_Neural_Edge_Fields_for_3D_Parametric_Curve_Reconstruction_From_CVPR_2023_paper.pdf)]
    * Title: NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction From Multi-View Images
    * Year: `2023`
    * Authors: Yunfan Ye, Renjiao Yi, Zhirui Gao, Chenyang Zhu, Zhiping Cai, Kai Xu
    * Abstract: We study the problem of reconstructing 3D feature curves of an object from a set of calibrated multi-view images. To do so, we learn a neural implicit field representing the density distribution of 3D edges which we refer to as Neural Edge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based rendering loss where a 2D edge map is rendered at a given view and is compared to the ground-truth edge map extracted from the image of that view. The rendering-based differentiable optimization of NEF fully exploits 2D edge detection, without needing a supervision of 3D edges, a 3D geometric operator or cross-view edge correspondence. Several technical designs are devised to ensure learning a range-limited and view-independent NEF for robust edge extraction. The final parametric 3D curves are extracted from NEF with an iterative optimization method. On our benchmark with synthetic data, we demonstrate that NEF outperforms existing state-of-the-art methods on all metrics. Project page: https://yunfan1202.github.io/NEF/.
count=1
* Hint-Aug: Drawing Hints From Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.pdf)]
    * Title: Hint-Aug: Drawing Hints From Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning
    * Year: `2023`
    * Authors: Zhongzhi Yu, Shang Wu, Yonggan Fu, Shunyao Zhang, Yingyan (Celine) Lin
    * Abstract: Despite the growing demand for tuning foundation vision transformers (FViTs) on downstream tasks, fully unleashing FViTs' potential under data-limited scenarios (e.g., few-shot tuning) remains a challenge due to FViTs' data-hungry nature. Common data augmentation techniques fall short in this context due to the limited features contained in the few-shot tuning data. To tackle this challenge, we first identify an opportunity for FViTs in few-shot tuning: pretrained FViTs themselves have already learned highly representative features from large-scale pretraining data, which are fully preserved during widely used parameter-efficient tuning. We thus hypothesize that leveraging those learned features to augment the tuning data can boost the effectiveness of few-shot FViT tuning. To this end, we propose a framework called Hint-based Data Augmentation (Hint-Aug), which aims to boost FViT in few-shot tuning by augmenting the over-fitted parts of tuning samples with the learned features of pretrained FViTs. Specifically, Hint-Aug integrates two key enablers: (1) an Attentive Over-fitting Detector (AOD) to detect over-confident patches of foundation ViTs for potentially alleviating their over-fitting on the few-shot tuning data and (2) a Confusion-based Feature Infusion (CFI) module to infuse easy-to-confuse features from the pretrained FViTs with the over-confident patches detected by the above AOD in order to enhance the feature diversity during tuning. Extensive experiments and ablation studies on five datasets and three parameter-efficient tuning techniques consistently validate Hint-Aug's effectiveness: 0.04% 32.91% higher accuracy over the state-of-the-art (SOTA) data augmentation method under various low-shot settings. For example, on the Pet dataset, Hint-Aug achieves a 2.22% higher accuracy with 50% less training data over SOTA data augmentation methods.
count=1
* Task Residual for Tuning Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.pdf)]
    * Title: Task Residual for Tuning Vision-Language Models
    * Year: `2023`
    * Authors: Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, Xinchao Wang
    * Abstract: Large-scale vision-language models (VLMs) pre-trained on billion-level data have learned general visual representations and broad visual concepts. In principle, the well-learned knowledge structure of the VLMs should be inherited appropriately when being transferred to downstream tasks with limited data. However, most existing efficient transfer learning (ETL) approaches for VLMs either damage or are excessively biased towards the prior knowledge, e.g., prompt tuning (PT) discards the pre-trained text-based classifier and builds a new one while adapter-style tuning (AT) fully relies on the pre-trained features. To address this, we propose a new efficient tuning approach for VLMs named Task Residual Tuning (TaskRes), which performs directly on the text-based classifier and explicitly decouples the prior knowledge of the pre-trained models and new knowledge regarding a target task. Specifically, TaskRes keeps the original classifier weights from the VLMs frozen and obtains a new classifier for the target task by tuning a set of prior-independent parameters as a residual to the original one, which enables reliable prior knowledge preservation and flexible task-specific knowledge exploration. The proposed TaskRes is simple yet effective, which significantly outperforms previous ETL methods (e.g., PT and AT) on 11 benchmark datasets while requiring minimal effort for the implementation. Our code is available at https://github.com/geekyutao/TaskRes.
count=1
* Train/Test-Time Adaptation With Retrieval
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Zancato_TrainTest-Time_Adaptation_With_Retrieval_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zancato_TrainTest-Time_Adaptation_With_Retrieval_CVPR_2023_paper.pdf)]
    * Title: Train/Test-Time Adaptation With Retrieval
    * Year: `2023`
    * Authors: Luca Zancato, Alessandro Achille, Tian Yu Liu, Matthew Trager, Pramuditha Perera, Stefano Soatto
    * Abstract: We introduce Train/Test-Time Adaptation with Retrieval (T3AR), a method to adapt models both at train and test time by means of a retrieval module and a searchable pool of external samples. Before inference, T3AR adapts a given model to the downstream task using refined pseudo-labels and a self-supervised contrastive objective function whose noise distribution leverages retrieved real samples to improve feature adaptation on the target data manifold. The retrieval of real images is key to T3AR since it does not rely solely on synthetic data augmentations to compensate for the lack of adaptation data, as typically done by other adaptation algorithms. Furthermore, thanks to the retrieval module, our method gives the user or service provider the possibility to improve model adaptation on the downstream task by incorporating further relevant data or to fully remove samples that may no longer be available due to changes in user preference after deployment. First, we show that T3AR can be used at training time to improve downstream fine-grained classification over standard fine-tuning baselines, and the fewer the adaptation data the higher the relative improvement (up to 13%). Second, we apply T3AR for test-time adaptation and show that exploiting a pool of external images at test-time leads to more robust representations over existing methods on DomainNet-126 and VISDA-C, especially when few adaptation data are available (up to 8%).
count=1
* Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Gradient_Norm_Aware_Minimization_Seeks_First-Order_Flatness_and_Improves_Generalization_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Gradient_Norm_Aware_Minimization_Seeks_First-Order_Flatness_and_Improves_Generalization_CVPR_2023_paper.pdf)]
    * Title: Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization
    * Year: `2023`
    * Authors: Xingxuan Zhang, Renzhe Xu, Han Yu, Hao Zou, Peng Cui
    * Abstract: Recently, flat minima are proven to be effective for improving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the current definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate minima with low generalization error from those with high generalization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flatness focusing on the maximal gradient norm within a perturbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Experimental results show that GAM improves the generalization of models trained with current optimizers such as SGD and AdamW on various datasets and networks. Furthermore, we show that GAM can help SAM find flatter minima and achieve better generalization.
count=1
* Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.pdf)]
    * Title: Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners
    * Year: `2023`
    * Authors: Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, Hongsheng Li
    * Abstract: Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance benefited from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning. In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pre training paradigms for better few-shot learning. Our CaFo incorporates CLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge, DALL-E's vision generative knowledge, and GPT-3's language-generative knowledge. Specifically, CaFo works by 'Prompt, Generate, then Cache'. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower. At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such col laboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-of the-art for few-shot classification. Code is available at https://github.com/ZrrSkywalker/CaFo.
count=1
* TokenHPE: Learning Orientation Tokens for Efficient Head Pose Estimation via Transformers
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_TokenHPE_Learning_Orientation_Tokens_for_Efficient_Head_Pose_Estimation_via_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_TokenHPE_Learning_Orientation_Tokens_for_Efficient_Head_Pose_Estimation_via_CVPR_2023_paper.pdf)]
    * Title: TokenHPE: Learning Orientation Tokens for Efficient Head Pose Estimation via Transformers
    * Year: `2023`
    * Authors: Cheng Zhang, Hai Liu, Yongjian Deng, Bochen Xie, Youfu Li
    * Abstract: Head pose estimation (HPE) has been widely used in the fields of human machine interaction, self-driving, and attention estimation. However, existing methods cannot deal with extreme head pose randomness and serious occlusions. To address these challenges, we identify three cues from head images, namely, neighborhood similarities, significant facial changes, and critical minority relationships. To leverage the observed findings, we propose a novel critical minority relationship-aware method based on the Transformer architecture in which the facial part relationships can be learned. Specifically, we design several orientation tokens to explicitly encode the basic orientation regions. Meanwhile, a novel token guide multi-loss function is designed to guide the orientation tokens as they learn the desired regional similarities and relationships. We evaluate the proposed method on three challenging benchmark HPE datasets. Experiments show that our method achieves better performance compared with state-of-the-art methods. Our code is publicly available at https://github.com/zc2023/TokenHPE.
count=1
* Unlearnable Clusters: Towards Label-Agnostic Unlearnable Examples
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Unlearnable_Clusters_Towards_Label-Agnostic_Unlearnable_Examples_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Unlearnable_Clusters_Towards_Label-Agnostic_Unlearnable_Examples_CVPR_2023_paper.pdf)]
    * Title: Unlearnable Clusters: Towards Label-Agnostic Unlearnable Examples
    * Year: `2023`
    * Authors: Jiaming Zhang, Xingjun Ma, Qi Yi, Jitao Sang, Yu-Gang Jiang, Yaowei Wang, Changsheng Xu
    * Abstract: There is a growing interest in developing unlearnable examples (UEs) against visual privacy leaks on the Internet. UEs are training samples added with invisible but unlearnable noise, which have been found can prevent unauthorized training of machine learning models. UEs typically are generated via a bilevel optimization framework with a surrogate model to remove (minimize) errors from the original samples, and then applied to protect the data against unknown target models. However, existing UE generation methods all rely on an ideal assumption called labelconsistency, where the hackers and protectors are assumed to hold the same label for a given sample. In this work, we propose and promote a more practical label-agnostic setting, where the hackers may exploit the protected data quite differently from the protectors. E.g., a m-class unlearnable dataset held by the protector may be exploited by the hacker as a n-class dataset. Existing UE generation methods are rendered ineffective in this challenging setting. To tackle this challenge, we present a novel technique called Unlearnable Clusters (UCs) to generate label-agnostic unlearnable examples with cluster-wise perturbations. Furthermore, we propose to leverage Vision-and-Language Pretrained Models (VLPMs) like CLIP as the surrogate model to improve the transferability of the crafted UCs to diverse domains. We empirically verify the effectiveness of our proposed approach under a variety of settings with different datasets, target models, and even commercial platforms Microsoft Azure and Baidu PaddlePaddle. Code is available at https://github.com/jiamingzhang94/ Unlearnable-Clusters.
count=1
* Multi-Granularity Archaeological Dating of Chinese Bronze Dings Based on a Knowledge-Guided Relation Graph
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_Multi-Granularity_Archaeological_Dating_of_Chinese_Bronze_Dings_Based_on_a_CVPR_2023_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Multi-Granularity_Archaeological_Dating_of_Chinese_Bronze_Dings_Based_on_a_CVPR_2023_paper.pdf)]
    * Title: Multi-Granularity Archaeological Dating of Chinese Bronze Dings Based on a Knowledge-Guided Relation Graph
    * Year: `2023`
    * Authors: Rixin Zhou, Jiafu Wei, Qian Zhang, Ruihua Qi, Xi Yang, Chuntao Li
    * Abstract: The archaeological dating of bronze dings has played a critical role in the study of ancient Chinese history. Current archaeology depends on trained experts to carry out bronze dating, which is time-consuming and labor-intensive. For such dating, in this study, we propose a learning-based approach to integrate advanced deep learning techniques and archaeological knowledge. To achieve this, we first collect a large-scale image dataset of bronze dings, which contains richer attribute information than other existing fine-grained datasets. Second, we introduce a multihead classifier and a knowledge-guided relation graph to mine the relationship between attributes and the ding era. Third, we conduct comparison experiments with various existing methods, the results of which show that our dating method achieves a state-of-the-art performance. We hope that our data and applied networks will enrich fine-grained classification research relevant to other interdisciplinary areas of expertise. The dataset and source code used are included in our supplementary materials, and will be open after submission owing to the anonymity policy. Source codes and data are available at: https://github.com/zhourixin/bronze-Ding.
count=1
* Robust Cross-Modal Representation Learning With Progressive Self-Distillation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Andonian_Robust_Cross-Modal_Representation_Learning_With_Progressive_Self-Distillation_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Andonian_Robust_Cross-Modal_Representation_Learning_With_Progressive_Self-Distillation_CVPR_2022_paper.pdf)]
    * Title: Robust Cross-Modal Representation Learning With Progressive Self-Distillation
    * Year: `2022`
    * Authors: Alex Andonian, Shixing Chen, Raffay Hamid
    * Abstract: The learning objective of vision-language approach of CLIP does not effectively account for the noisy many-to-many correspondences found in web-harvested image captioning datasets, which contributes to its compute and data inefficiency. To address this challenge, we introduce a novel training framework based on cross-modal contrastive learning that uses progressive self-distillation and soft image-text alignments to more efficiently learn robust representations from noisy data. Our model distills its own knowledge to dynamically generate soft-alignment targets for a subset of images and captions in every minibatch, which are then used to update its parameters. Extensive evaluation across 14 benchmark datasets shows that our method consistently outperforms its CLIP counterpart in multiple settings, including: (a) zero-shot classification, (b) linear probe transfer, and (c) image-text retrieval, without incurring added computational cost. Analysis using an ImageNet-based robustness test-bedreveals that our method offers better effective robustness to natural distribution shifts compared to both ImageNet-trained models and CLIP itself. Lastly, pretraining with datasets spanning two orders of magnitude in size shows that our improvements over CLIP tend to scale with number of training examples.
count=1
* GASP, a Generalized Framework for Agglomerative Clustering of Signed Graphs and Its Application to Instance Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Bailoni_GASP_a_Generalized_Framework_for_Agglomerative_Clustering_of_Signed_Graphs_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Bailoni_GASP_a_Generalized_Framework_for_Agglomerative_Clustering_of_Signed_Graphs_CVPR_2022_paper.pdf)]
    * Title: GASP, a Generalized Framework for Agglomerative Clustering of Signed Graphs and Its Application to Instance Segmentation
    * Year: `2022`
    * Authors: Alberto Bailoni, Constantin Pape, Nathan Hütsch, Steffen Wolf, Thorsten Beier, Anna Kreshuk, Fred A. Hamprecht
    * Abstract: We propose a theoretical framework that generalizes simple and fast algorithms for hierarchical agglomerative clustering to weighted graphs with both attractive and repulsive interactions between the nodes. This framework defines GASP, a Generalized Algorithm for Signed graph Partitioning, and allows us to explore many combinations of different linkage criteria and cannot-link constraints. We prove the equivalence of existing clustering methods to some of those combinations and introduce new algorithms for combinations that have not been studied before. We study both theoretical and empirical properties of these combinations and prove that some of these define an ultrametric on the graph. We conduct a systematic comparison of various instantiations of GASP on a large variety of both synthetic and existing signed clustering problems, in terms of accuracy but also efficiency and robustness to noise. Lastly, we show that some of the algorithms included in our framework, when combined with the predictions from a CNN model, result in a simple bottom-up instance segmentation pipeline. Going all the way from pixels to final segments with a simple procedure, we achieve state-of-the-art accuracy on the CREMI 2016 EM segmentation benchmark without requiring domain-specific superpixels.
count=1
* The Auto Arborist Dataset: A Large-Scale Benchmark for Multiview Urban Forest Monitoring Under Domain Shift
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Beery_The_Auto_Arborist_Dataset_A_Large-Scale_Benchmark_for_Multiview_Urban_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Beery_The_Auto_Arborist_Dataset_A_Large-Scale_Benchmark_for_Multiview_Urban_CVPR_2022_paper.pdf)]
    * Title: The Auto Arborist Dataset: A Large-Scale Benchmark for Multiview Urban Forest Monitoring Under Domain Shift
    * Year: `2022`
    * Authors: Sara Beery, Guanhang Wu, Trevor Edwards, Filip Pavetic, Bo Majewski, Shreyasee Mukherjee, Stanley Chan, John Morgan, Vivek Rathod, Jonathan Huang
    * Abstract: Generalization to novel domains is a fundamental challenge for computer vision. Near-perfect accuracy on benchmarks is common, but these models do not work as expected when deployed outside of the training distribution. To build computer vision systems that truly solve real-world problems at global scale, we need benchmarks that fully capture real-world complexity, including geographic domain shift, long-tailed distributions, and data noise. We propose urban forest monitoring as an ideal testbed for studying and improving upon these computer vision challenges, while simultaneously working towards filling a crucial environmental and societal need. Urban forests provide significant benefits to urban societies (e.g., cleaner air and water, carbon sequestration, and energy savings among others). However, planning and maintaining these forests is expensive. One particularly costly aspect of urban forest management is monitoring the existing trees in a city: e.g., tracking tree locations, species, and health. Monitoring efforts are currently based on tree censuses built by human experts, costing cities millions of dollars per census and thus collected infrequently. Previous investigations into automating urban forest monitoring focused on small datasets from single cities, covering only common categories. To address these shortcomings, we introduce a new large-scale dataset that joins public tree censuses from 23 cities with a large collection of street level and aerial imagery. Our Auto Arborist dataset contains over 2.5M trees and 344 genera and is >2 orders of magnitude larger than the closest dataset in the literature. We introduce baseline results on our dataset across modalities as well as metrics for the detailed analysis of generalization with respect to geographic distribution shifts, vital for such a system to be deployed at-scale.
count=1
* CoSSL: Co-Learning of Representation and Classifier for Imbalanced Semi-Supervised Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Fan_CoSSL_Co-Learning_of_Representation_and_Classifier_for_Imbalanced_Semi-Supervised_Learning_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_CoSSL_Co-Learning_of_Representation_and_Classifier_for_Imbalanced_Semi-Supervised_Learning_CVPR_2022_paper.pdf)]
    * Title: CoSSL: Co-Learning of Representation and Classifier for Imbalanced Semi-Supervised Learning
    * Year: `2022`
    * Authors: Yue Fan, Dengxin Dai, Anna Kukleva, Bernt Schiele
    * Abstract: Standard semi-supervised learning (SSL) using class-balanced datasets has shown great progress to leverage unlabeled data effectively. However, the more realistic setting of class-imbalanced data - called imbalanced SSL - is largely underexplored and standard SSL tends to underperform. In this paper, we propose a novel co-learning framework (CoSSL), which decouples representation and classifier learning while coupling them closely. To handle the data imbalance, we devise Tail-class Feature Enhancement (TFE) for classifier learning. Furthermore, the current evaluation protocol for imbalanced SSL focuses only on balanced test sets, which has limited practicality in real-world scenarios. Therefore, we further conduct a comprehensive evaluation under various shifted test distributions. In experiments, we show that our approach outperforms other methods over a large range of shifted distributions, achieving state-of-the-art performance on benchmark datasets ranging from CIFAR-10, CIFAR-100, ImageNet, to Food-101. Our code will be made publicly available.
count=1
* Multimodal Dynamics: Dynamical Fusion for Trustworthy Multimodal Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Han_Multimodal_Dynamics_Dynamical_Fusion_for_Trustworthy_Multimodal_Classification_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Multimodal_Dynamics_Dynamical_Fusion_for_Trustworthy_Multimodal_Classification_CVPR_2022_paper.pdf)]
    * Title: Multimodal Dynamics: Dynamical Fusion for Trustworthy Multimodal Classification
    * Year: `2022`
    * Authors: Zongbo Han, Fan Yang, Junzhou Huang, Changqing Zhang, Jianhua Yao
    * Abstract: Integration of heterogeneous and high-dimensional data (e.g., multiomics) is becoming increasingly important. Existing multimodal classification algorithms mainly focus on improving performance by exploiting the complementarity from different modalities. However, conventional approaches are basically weak in providing trustworthy multimodal fusion, especially for safety-critical applications (e.g., medical diagnosis). For this issue, we propose a novel trustworthy multimodal classification algorithm termed Multimodal Dynamics, which dynamically evaluates both the feature-level and modality-level informativeness for different samples and thus trustworthily integrates multiple modalities. Specifically, a sparse gating is introduced to capture the information variation of each within-modality feature and the true class probability is employed to assess the classification confidence of each modality. Then a transparent fusion algorithm based on the dynamical informativeness estimation strategy is induced. To the best of our knowledge, this is the first work to jointly model both feature and modality variation for different samples to provide trustworthy fusion in multi-modal classification. Extensive experiments are conducted on multimodal medical classification datasets. In these experiments, superior performance and trustworthiness of our algorithm are clearly validated compared to the state-of-the-art methods.
count=1
* How Well Do Sparse ImageNet Models Transfer?
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Iofinova_How_Well_Do_Sparse_ImageNet_Models_Transfer_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Iofinova_How_Well_Do_Sparse_ImageNet_Models_Transfer_CVPR_2022_paper.pdf)]
    * Title: How Well Do Sparse ImageNet Models Transfer?
    * Year: `2022`
    * Authors: Eugenia Iofinova, Alexandra Peste, Mark Kurtz, Dan Alistarh
    * Abstract: Transfer learning is a classic paradigm by which models pretrained on large "upstream" datasets are adapted to yield good results on "downstream" specialized datasets. Generally, more accurate models on the "upstream" dataset tend to provide better transfer accuracy "downstream". In this work, we perform an in-depth investigation of this phenomenon in the context of convolutional neural networks (CNNs) trained on the ImageNet dataset, which have been pruned--that is, compressed by sparsifiying their connections. We consider transfer using unstructured pruned models obtained by applying several state-of-the-art pruning methods, including magnitude-based, second-order, re-growth, lottery-ticket, and regularization approaches, in the context of twelve standard transfer tasks. In a nutshell, our study shows that sparse models can match or even outperform the transfer performance of dense models, even at high sparsities, and, while doing so, can lead to significant inference and even training speedups. At the same time, we observe and analyze significant differences in the behaviour of different pruning methods. The code is available at: https://github.com/IST-DASLab/sparse-imagenet-transfer.
count=1
* UniCon: Combating Label Noise Through Uniform Selection and Contrastive Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Karim_UniCon_Combating_Label_Noise_Through_Uniform_Selection_and_Contrastive_Learning_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Karim_UniCon_Combating_Label_Noise_Through_Uniform_Selection_and_Contrastive_Learning_CVPR_2022_paper.pdf)]
    * Title: UniCon: Combating Label Noise Through Uniform Selection and Contrastive Learning
    * Year: `2022`
    * Authors: Nazmul Karim, Mamshad Nayeem Rizve, Nazanin Rahnavard, Ajmal Mian, Mubarak Shah
    * Abstract: Supervised deep learning methods require a large repository of annotated data; hence, label noise is inevitable. Training with such noisy data negatively impacts the generalization performance of deep neural networks. To combat label noise, recent state-of-the-art methods employ some sort of sample selection mechanism to select a possibly clean subset of data. Next, an off-the-shelf semi-supervised learning method is used for training where rejected samples are treated as unlabeled data. Our comprehensive analysis shows that current selection methods disproportionately select samples from easy (fast learnable) classes while rejecting those from relatively harder ones. This creates class imbalance in the selected clean set and in turn, deteriorates performance under high label noise. In this work, we propose UNICON, a simple yet effective sample selection method which is robust to high label noise. To address the disproportionate selection of easy and hard samples, we introduce a Jensen-Shannon divergence based uniform selection mechanism which does not require any probabilistic modeling and hyperparameter tuning. We complement our selection method with contrastive learning to further combat the memorization of noisy labels. Extensive experimentation on multiple benchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4% improvement over the current state-of-the-art on CIFAR100 dataset with a 90% noise rate. Our code is publicly available.
count=1
* Temporal Context Matters: Enhancing Single Image Prediction With Disease Progression Representations
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Konwer_Temporal_Context_Matters_Enhancing_Single_Image_Prediction_With_Disease_Progression_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Konwer_Temporal_Context_Matters_Enhancing_Single_Image_Prediction_With_Disease_Progression_CVPR_2022_paper.pdf)]
    * Title: Temporal Context Matters: Enhancing Single Image Prediction With Disease Progression Representations
    * Year: `2022`
    * Authors: Aishik Konwer, Xuan Xu, Joseph Bae, Chao Chen, Prateek Prasanna
    * Abstract: Clinical outcome or severity prediction from medical images has largely focused on learning representations from single-timepoint or snapshot scans. It has been shown that disease progression can be better characterized by temporal imaging. We therefore hypothesized that outcome predictions can be improved by utilizing the disease progression information from sequential images. We present a deep learning approach that leverages temporal progression information to improve clinical outcome predictions from single-timepoint images. In our method, a self-attention based Temporal Convolutional Network (TCN) is used to learn a representation that is most reflective of the disease trajectory. Meanwhile, a Vision Transformer is pretrained in a self-supervised fashion to extract features from single-timepoint images. The key contribution is to design a recalibration module that employs maximum mean discrepancy loss (MMD) to align distributions of the above two contextual representations. We train our system to predict clinical outcomes and severity grades from single-timepoint images. Experiments on chest and osteoarthritis radiography datasets demonstrate that our approach outperforms other state-of-the-art techniques.
count=1
* Unimodal-Concentrated Loss: Fully Adaptive Label Distribution Learning for Ordinal Regression
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Unimodal-Concentrated_Loss_Fully_Adaptive_Label_Distribution_Learning_for_Ordinal_Regression_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Unimodal-Concentrated_Loss_Fully_Adaptive_Label_Distribution_Learning_for_Ordinal_Regression_CVPR_2022_paper.pdf)]
    * Title: Unimodal-Concentrated Loss: Fully Adaptive Label Distribution Learning for Ordinal Regression
    * Year: `2022`
    * Authors: Qiang Li, Jingjing Wang, Zhaoliang Yao, Yachun Li, Pengju Yang, Jingwei Yan, Chunmao Wang, Shiliang Pu
    * Abstract: Learning from a label distribution has achieved promising results on ordinal regression tasks such as facial age and head pose estimation wherein, the concept of adaptive label distribution learning (ALDL) has drawn lots of attention recently for its superiority in theory. However, compared with the methods assuming fixed form label distribution, ALDL methods have not achieved better performance. We argue that existing ALDL algorithms do not fully exploit the intrinsic properties of ordinal regression. In this paper, we emphatically summarize that learning an adaptive label distribution on ordinal regression tasks should follow three principles. First, the probability corresponding to the ground-truth should be the highest in label distribution. Second, the probabilities of neighboring labels should decrease with the increase of distance away from the ground-truth, i.e., the distribution is unimodal. Third, the label distribution should vary with samples changing, and even be distinct for different instances with the same label, due to the different levels of difficulty and ambiguity. Under the premise of these principles, we propose a novel loss function for fully adaptive label distribution learning, namely unimodal-concentrated loss. Specifically, the unimodal loss derived from the learning to rank strategy constrains the distribution to be unimodal. Furthermore, the estimation error and the variance of the predicted distribution for a specific sample are integrated into the proposed concentrated loss to make the predicted distribution maximize at the ground-truth and vary according to the predicting uncertainty. Extensive experimental results on typical ordinal regression tasks including age and head pose estimation, show the superiority of our proposed unimodal-concentrated loss compared with existing loss functions.
count=1
* Coupled Iterative Refinement for 6D Multi-Object Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Lipson_Coupled_Iterative_Refinement_for_6D_Multi-Object_Pose_Estimation_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Lipson_Coupled_Iterative_Refinement_for_6D_Multi-Object_Pose_Estimation_CVPR_2022_paper.pdf)]
    * Title: Coupled Iterative Refinement for 6D Multi-Object Pose Estimation
    * Year: `2022`
    * Authors: Lahav Lipson, Zachary Teed, Ankit Goyal, Jia Deng
    * Abstract: We address the task of 6D multi-object pose: given a set of known 3D objects and an RGB or RGB-D input image, we detect and estimate the 6D pose of each object. We propose a new approach to 6D object pose estimation which consists of an end-to-end differentiable architecture that makes use of geometric knowledge. Our approach iteratively refines both pose and correspondence in a tightly coupled manner, allowing us to dynamically remove outliers to improve accuracy. We use a novel differentiable layer to perform pose refinement by solving an optimization problem we refer to as Bidirectional Depth-Augmented Perspective-N-Point (BD-PnP). Our method achieves state-of-the-art accuracy on standard 6D Object Pose benchmarks.
count=1
* Geometric and Textural Augmentation for Domain Gap Reduction
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Geometric_and_Textural_Augmentation_for_Domain_Gap_Reduction_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Geometric_and_Textural_Augmentation_for_Domain_Gap_Reduction_CVPR_2022_paper.pdf)]
    * Title: Geometric and Textural Augmentation for Domain Gap Reduction
    * Year: `2022`
    * Authors: Xiao-Chang Liu, Yong-Liang Yang, Peter Hall
    * Abstract: Research has shown that convolutional neural networks for object recognition are vulnerable to changes in depiction because learning is biased towards the low-level statistics of texture patches. Recent works concentrate on improving robustness by applying style transfer to training examples to mitigate against over-fitting to one depiction style. These new approaches improve performance, but they ignore the geometric variations in object shape that real art exhibits: artists deform and warp objects for artistic effect. Motivated by this observation, we propose a method to reduce bias by jointly increasing the texture and geometry diversities of the training data. In effect, we extend the visual object class to include examples with shape changes that artists use. Specifically, we learn the distribution of warps that cover each given object class. Together with augmenting textures based on a broad distribution of styles, we show by experiments that our method improves performance on several cross-domain benchmarks.
count=1
* Leveraging Self-Supervision for Cross-Domain Crowd Counting
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Leveraging_Self-Supervision_for_Cross-Domain_Crowd_Counting_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Leveraging_Self-Supervision_for_Cross-Domain_Crowd_Counting_CVPR_2022_paper.pdf)]
    * Title: Leveraging Self-Supervision for Cross-Domain Crowd Counting
    * Year: `2022`
    * Authors: Weizhe Liu, Nikita Durasov, Pascal Fua
    * Abstract: State-of-the-art methods for counting people in crowded scenes rely on deep networks to estimate crowd density. While effective, these data-driven approaches rely on large amount of data annotation to achieve good performance, which stops these models from being deployed in emergencies during which data annotation is either too costly or cannot be obtained fast enough. One popular solution is to use synthetic data for training. Unfortunately, due to domain shift, the resulting models generalize poorly on real imagery. We remedy this shortcoming by training with both synthetic images, along with their associated labels, and unlabeled real images. To this end, we force our network to learn perspective-aware features by training it to recognize upside-down real images from regular ones and incorporate into it the ability to predict its own uncertainty so that it can generate useful pseudo labels for fine-tuning purposes. This yields an algorithm that consistently outperforms state-of-the-art cross-domain crowd counting ones without any extra computation at inference time.
count=1
* Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Temporal_Feature_Alignment_and_Mutual_Information_Maximization_for_Video-Based_Human_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Temporal_Feature_Alignment_and_Mutual_Information_Maximization_for_Video-Based_Human_CVPR_2022_paper.pdf)]
    * Title: Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation
    * Year: `2022`
    * Authors: Zhenguang Liu, Runyang Feng, Haoming Chen, Shuang Wu, Yixing Gao, Yunjun Gao, Xiang Wang
    * Abstract: Multi-frame human pose estimation has long been a compelling and fundamental problem in computer vision. This task is challenging due to fast motion and pose occlusion that frequently occur in videos. State-of-the-art methods strive to incorporate additional visual evidences from neighboring frames (supporting frames) to facilitate the pose estimation of the current frame (key frame). One aspect that has been obviated so far, is the fact that current methods directly aggregate unaligned contexts across frames. The spatial-misalignment between pose features of the current frame and neighboring frames might lead to unsatisfactory results. More importantly, existing approaches build upon the straightforward pose estimation loss, which unfortunately cannot constrain the network to fully leverage useful information from neighboring frames. To tackle these problems, we present a novel hierarchical alignment framework, which leverages coarse-to-fine deformations to progressively update a neighboring frame to align with the current frame at the feature level. We further propose to explicitly supervise the knowledge extraction from neighboring frames, guaranteeing that useful complementary cues are extracted. To achieve this goal, we theoretically analyzed the mutual information between the frames and arrived at a loss that maximizes the taskrelevant mutual information. These allow us to rank No.1 in the Multi-frame Person Pose Estimation Challenge on benchmark dataset PoseTrack2017, and obtain state-of-the-art performance on benchmarks Sub-JHMDB and PoseTrack2018. Our code is released at https://github.com/Pose-Group/FAMI-Pose, hoping that it will be useful to the community.
count=1
* Prompt Distribution Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.pdf)]
    * Title: Prompt Distribution Learning
    * Year: `2022`
    * Authors: Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, Xinmei Tian
    * Abstract: We present prompt distribution learning for effectively adapting a pre-trained vision-language model to address downstream recognition tasks. Our method not only learns low-bias prompts from a few samples but also captures the distribution of diverse prompts to handle the varying visual representations. In this way, we provide high-quality task-related content for facilitating recognition. This prompt distribution learning is realized by an efficient approach that learns the output embeddings of prompts instead of the input embeddings. Thus, we can employ a Gaussian distribution to model them effectively and derive a surrogate loss for efficient training. Extensive experiments on 12 datasets demonstrate that our method consistently and significantly outperforms existing methods. For example, with 1 sample per category, it relatively improves the average result by 9.1% compared to human-crafted prompts.
count=1
* Are Multimodal Transformers Robust to Missing Modality?
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf)]
    * Title: Are Multimodal Transformers Robust to Missing Modality?
    * Year: `2022`
    * Authors: Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, Xi Peng
    * Abstract: Multimodal data collected from the real world are often imperfect due to missing modalities. Therefore multimodal models that are robust against modal-incomplete data are highly preferred. Recently, Transformer models have shown great success in processing multimodal data. However, existing work has been limited to either architecture designs or pre-training strategies; whether Transformer models are naturally robust against missing-modal data has rarely been investigated. In this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of Transformers in the presence of modal-incomplete data. Unsurprising, we find Transformer models are sensitive to missing modalities while different modal fusion strategies will significantly affect the robustness. What surprised us is that the optimal fusion strategy is dataset dependent even for the same Transformer model; there does not exist a universal strategy that works in general cases. Based on these findings, we propose a principle method to improve the robustness of Transformer models by automatically searching for an optimal fusion strategy regarding input data. Experimental validations on three benchmarks support the superior performance of the proposed method.
count=1
* Learning Program Representations for Food Images and Cooking Recipes
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Papadopoulos_Learning_Program_Representations_for_Food_Images_and_Cooking_Recipes_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Papadopoulos_Learning_Program_Representations_for_Food_Images_and_Cooking_Recipes_CVPR_2022_paper.pdf)]
    * Title: Learning Program Representations for Food Images and Cooking Recipes
    * Year: `2022`
    * Authors: Dim P. Papadopoulos, Enrique Mora, Nadiia Chepurko, Kuan Wei Huang, Ferda Ofli, Antonio Torralba
    * Abstract: In this paper, we are interested in modeling a how-to instructional procedure, such as a cooking recipe, with a meaningful and rich high-level representation. Specifically, we propose to represent cooking recipes and food images as cooking programs. Programs provide a structured representation of the task, capturing cooking semantics and sequential relationships of actions in the form of a graph. This allows them to be easily manipulated by users and executed by agents. To this end, we build a model that is trained to learn a joint embedding between recipes and food images via self-supervision and jointly generate a program from this embedding as a sequence. To validate our idea, we crowdsource programs for cooking recipes and show that: (a) projecting the image-recipe embeddings into programs leads to better cross-modal retrieval results; (b) generating programs from images leads to better recognition results compared to predicting raw cooking instructions; and (c) we can generate food images by manipulating programs via optimizing the latent code of a GAN. Code, data, and models are available online.
count=1
* On Guiding Visual Attention With Language Specification
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Petryk_On_Guiding_Visual_Attention_With_Language_Specification_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Petryk_On_Guiding_Visual_Attention_With_Language_Specification_CVPR_2022_paper.pdf)]
    * Title: On Guiding Visual Attention With Language Specification
    * Year: `2022`
    * Authors: Suzanne Petryk, Lisa Dunlap, Keyan Nasseri, Joseph Gonzalez, Trevor Darrell, Anna Rohrbach
    * Abstract: While real world challenges typically define visual categories with language words or phrases, most visual classification methods define categories with numerical indicies. However, the language specification of the classes provides an especially useful prior for biased and noisy datasets, where it can help disambiguate what features are task-relevant. Recently, large-scale multimodal models have been shown to recognize a wide variety of high-level concepts from a language specification even without additional image training data, but they are often unable to distinguish classes for more fine-grained tasks. CNNs, in contrast, can extract subtle image features that are required for fine-grained discrimination, but will overfit to any bias or noise in datasets. Our insight is to use high-level language specification as advice for constraining the prediction evidence to task-relevant features, instead of distractors. To do this, we ground task-relevant words or phrases with attention maps from a pretrained large-scale model. We then use this grounding to supervise a classifier's spatial attention away from distracting context. We show that supervising spatial attention in this way improves performance on classification tasks with biased and noisy data, including 3-15% worst-group accuracy improvements and 41-45% relative improvements on fairness metrics.
count=1
* Which Images To Label for Few-Shot Medical Landmark Detection?
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Quan_Which_Images_To_Label_for_Few-Shot_Medical_Landmark_Detection_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Quan_Which_Images_To_Label_for_Few-Shot_Medical_Landmark_Detection_CVPR_2022_paper.pdf)]
    * Title: Which Images To Label for Few-Shot Medical Landmark Detection?
    * Year: `2022`
    * Authors: Quan Quan, Qingsong Yao, Jun Li, S. Kevin Zhou
    * Abstract: The success of deep learning methods relies on the availability of well-labeled large-scale datasets. However, for medical images, annotating such abundant training data often requires experienced radiologists and consumes their limited time. Few-shot learning is developed to alleviate this burden, which achieves competitive performance with only several labeled data. However, a crucial yet previously overlooked problem in few-shot learning is about the selection of the template images for annotation before learning, which affects the final performance. We herein propose a novel Sample Choosing Policy (SCP) to select "the most worthy" images as the templates, in the context of medical landmark detection. SCP consists of three parts: 1) Self-supervised training for building a pre-trained deep model to extract features from radiological images, 2) Key Point Proposal for localizing informative patches, and 3) Representative Score Estimation for searching most representative samples or templates. The performance of SCP is demonstrated by various experiments on several widely-used public datasets. For one-shot medical landmark detection, the mean radial errors on Cephalometric and HandXray datasets are reduced from 3.595mm to 3.083mm and 4.114mm to 2.653mm, respectively.
count=1
* A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Roetzer_A_Scalable_Combinatorial_Solver_for_Elastic_Geometrically_Consistent_3D_Shape_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Roetzer_A_Scalable_Combinatorial_Solver_for_Elastic_Geometrically_Consistent_3D_Shape_CVPR_2022_paper.pdf)]
    * Title: A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching
    * Year: `2022`
    * Authors: Paul Roetzer, Paul Swoboda, Daniel Cremers, Florian Bernard
    * Abstract: We present a scalable combinatorial algorithm for globally optimizing over the space of geometrically consistent mappings between 3D shapes. We use the mathematically elegant formalism proposed by Windheuser et al. (ICCV, 2011) where 3D shape matching was formulated as an integer linear program over the space of orientation-preserving diffeomorphisms. Until now, the resulting formulation had limited practical applicability due to its complicated constraint structure and its large size. We propose a novel primal heuristic coupled with a Lagrange dual problem that is several orders of magnitudes faster compared to previous solvers. This allows us to handle shapes with substantially more triangles than previously solvable. We demonstrate compelling results on diverse datasets, and, even showcase that we can address the challenging setting of matching two partial shapes without availability of complete shapes. Our code is publicly available at http://github.com/paul0noah/sm-comb.
count=1
* FLAVA: A Foundational Language and Vision Alignment Model
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.pdf)]
    * Title: FLAVA: A Foundational Language and Vision Alignment Model
    * Year: `2022`
    * Authors: Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela
    * Abstract: State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising direction would be to use a single holistic universal model, as a "foundation", that targets all modalities at once---a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities.
count=1
* Debiased Learning From Naturally Imbalanced Pseudo-Labels
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Debiased_Learning_From_Naturally_Imbalanced_Pseudo-Labels_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Debiased_Learning_From_Naturally_Imbalanced_Pseudo-Labels_CVPR_2022_paper.pdf)]
    * Title: Debiased Learning From Naturally Imbalanced Pseudo-Labels
    * Year: `2022`
    * Authors: Xudong Wang, Zhirong Wu, Long Lian, Stella X. Yu
    * Abstract: This work studies the bias issue of pseudo-labeling, a natural phenomenon that widely occurs but often overlooked by prior research. Pseudo-labels are generated when a classifier trained on source data is transferred to unlabeled target data. We observe heavy long-tailed pseudo-labels when a semi-supervised learning model FixMatch predicts labels on the unlabeled set even though the unlabeled data is curated to be balanced. Without intervention, the training model inherits the bias from the pseudo-labels and end up being sub-optimal. To eliminate the model bias, we propose a simple yet effective method DebiasMatch, comprising of an adaptive debiasing module and an adaptive marginal loss. The strength of debiasing and the size of margins can be automatically adjusted by making use of an online updated queue. Benchmarked on ImageNet-1K, DebiasMatch significantly outperforms previous state-of-the-arts by more than 26% and 10.5% on semi-supervised learning (0.2% annotated data) and zero-shot learning tasks respectively.
count=1
* Robust Fine-Tuning of Zero-Shot Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.pdf)]
    * Title: Robust Fine-Tuning of Zero-Shot Models
    * Year: `2022`
    * Authors: Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, Ludwig Schmidt
    * Abstract: Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution. On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large robustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard fine-tuning on commonly used transfer learning datasets. These improvements come at no additional computational cost during fine-tuning or inference.
count=1
* Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Xia_Sparse_Local_Patch_Transformer_for_Robust_Face_Alignment_and_Landmarks_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Xia_Sparse_Local_Patch_Transformer_for_Robust_Face_Alignment_and_Landmarks_CVPR_2022_paper.pdf)]
    * Title: Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning
    * Year: `2022`
    * Authors: Jiahao Xia, Weiwei Qu, Wenjian Huang, Jianguo Zhang, Xi Wang, Min Xu
    * Abstract: Heatmap regression methods have dominated face alignment area in recent years while they ignore the inherent relation between different landmarks. In this paper, we propose a Sparse Local Patch Transformer (SLPT) for learning the inherent relation. The SLPT generates the representation of each single landmark from a local patch and aggregates them by an adaptive inherent relation based on the attention mechanism. The subpixel coordinate of each landmark is predicted independently based on the aggregated feature. Moreover, a coarse-to-fine framework is further introduced to incorporate with the SLPT, which enables the initial landmarks to gradually converge to the target facial landmarks using fine-grained features from dynamically resized local patches. Extensive experiments carried out on three popular benchmarks, including WFLW, 300W and COFW, demonstrate that the proposed method works at the state-of-the-art level with much less computational complexity by learning the inherent relation between facial landmarks. The code is available at the project website.
count=1
* Point Cloud Color Constancy
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Xing_Point_Cloud_Color_Constancy_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Xing_Point_Cloud_Color_Constancy_CVPR_2022_paper.pdf)]
    * Title: Point Cloud Color Constancy
    * Year: `2022`
    * Authors: Xiaoyan Xing, Yanlin Qian, Sibo Feng, Yuhan Dong, Jiří Matas
    * Abstract: In this paper, we present Point Cloud Color Constancy, in short PCCC, an illumination chromaticity estimation algorithm exploiting a point cloud. We leverage the depth information captured by the time-of-flight (ToF) sensor mounted rigidly with the RGB sensor, and form a 6D cloud where each point contains the coordinates and RGB intensities, noted as (x,y,z,r,g,b). PCCC applies the PointNet architecture to the color constancy problem, deriving the illumination vector point-wise and then making a global decision about the global illumination chromaticity. On two popular RGB-D datasets, which we extend with illumination information, as well as on a novel benchmark, PCCC obtains lower error than the state-of-the-art algorithms. Our method is simple and fast, requiring merely 16*16-size input and reaching speed over 500 fps, including the cost of building the point cloud and net inference.
count=1
* Rep-Net: Efficient On-Device Learning via Feature Reprogramming
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Rep-Net_Efficient_On-Device_Learning_via_Feature_Reprogramming_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Rep-Net_Efficient_On-Device_Learning_via_Feature_Reprogramming_CVPR_2022_paper.pdf)]
    * Title: Rep-Net: Efficient On-Device Learning via Feature Reprogramming
    * Year: `2022`
    * Authors: Li Yang, Adnan Siraj Rakin, Deliang Fan
    * Abstract: Transfer learning, where the goal is to transfer the well-trained deep learning models from a primary source task to a new task, is a crucial learning scheme for on-device machine learning, due to the fact that IoT/edge devices collect and then process massive data in our daily life. However, due to the tiny memory constraint in IoT/edge devices, such on-device learning requires ultra-small training memory footprint, bringing new challenges for memory-efficient learning. Many existing works solve this problem by reducing the number of trainable parameters. However, this doesn't directly translate to memory-saving since the major bottleneck is the activations, not parameters. To develop memory-efficient on-device transfer learning, in this work, we are the first to approach the concept of transfer learning from a new perspective of intermediate feature reprogramming of a pre-trained model (i.e., backbone). To perform this lightweight and memory-efficient reprogramming, we propose to train a tiny Reprogramming Network (Rep-Net) directly from the new task input data, while freezing the backbone model. The proposed Rep-Net model interchanges the features with the backbone model using an activation connector at regular intervals to mutually benefit both the backbone model and Rep-Net model features. Through extensive experiments, we validate each design specs of the proposed Rep-Net model in achieving highly memory-efficient on-device reprogramming. Our experiments establish the superior performance (i.e., low training memory and high accuracy) of Rep-Net compared to SOTA on-device transfer learning schemes across multiple benchmarks.
count=1
* SimMatch: Semi-Supervised Learning With Similarity Matching
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Zheng_SimMatch_Semi-Supervised_Learning_With_Similarity_Matching_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_SimMatch_Semi-Supervised_Learning_With_Similarity_Matching_CVPR_2022_paper.pdf)]
    * Title: SimMatch: Semi-Supervised Learning With Similarity Matching
    * Year: `2022`
    * Authors: Mingkai Zheng, Shan You, Lang Huang, Fei Wang, Chen Qian, Chang Xu
    * Abstract: Learning with few labeled data has been a longstanding problem in the computer vision and machine learning research community. In this paper, we introduced a new semi-supervised learning framework, SimMatch, which simultaneously considers semantic similarity and instance similarity. In SimMatch, the consistency regularization will be applied on both semantic-level and instance-level. The different augmented views of the same instance are encouraged to have the same class prediction and similar similarity relationship respected to other instances. Next, we instantiated a labeled memory buffer to fully leverage the ground truth labels on instance-level and bridge the gaps between the semantic and instance similarities. Finally, we proposed the unfolding and aggregation operation which allows these two similarities be isomorphically transformed with each other. In this way, the semantic and instance pseudo-labels can be mutually propagated to generate more high-quality and reliable matching targets. Extensive experimental results demonstrate that SimMatch improves the performance of semi-supervised learning tasks across different benchmark datasets and different settings. Notably, with 400 epochs of training, SimMatch achieves 67.2%, and 74.4% Top-1 Accuracy with 1% and 10% labeled examples on ImageNet, which significantly outperforms the baseline methods and is better than previous semi-supervised learning frameworks.
count=1
* Conditional Prompt Learning for Vision-Language Models
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf)]
    * Title: Conditional Prompt Learning for Vision-Language Models
    * Year: `2022`
    * Authors: Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu
    * Abstract: With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning---a recent trend in NLP---to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/KaiyangZhou/CoOp.
count=1
* Learning Decision Trees Recurrently Through Communication
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Alaniz_Learning_Decision_Trees_Recurrently_Through_Communication_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Alaniz_Learning_Decision_Trees_Recurrently_Through_Communication_CVPR_2021_paper.pdf)]
    * Title: Learning Decision Trees Recurrently Through Communication
    * Year: `2021`
    * Authors: Stephan Alaniz, Diego Marcos, Bernt Schiele, Zeynep Akata
    * Abstract: Integrated interpretability without sacrificing the prediction accuracy of decision making algorithms has the potential of greatly improving their value to the user. Instead of assigning a label to an image directly, we propose to learn iterative binary sub-decisions, inducing sparsity and transparency in the decision making process. The key aspect of our model is its ability to build a decision tree whose structure is encoded into the memory representation of a Recurrent Neural Network jointly learned by two models communicating through message passing. In addition, our model assigns a semantic meaning to each decision in the form of binary attributes, providing concise, semantic and relevant rationalizations to the user. On three benchmark image classification datasets, including the large-scale ImageNet, our model generates human interpretable binary decision sequences explaining the predictions of the network while maintaining state-of-the-art accuracy.
count=1
* img2pose: Face Alignment and Detection via 6DoF, Face Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Albiero_img2pose_Face_Alignment_and_Detection_via_6DoF_Face_Pose_Estimation_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Albiero_img2pose_Face_Alignment_and_Detection_via_6DoF_Face_Pose_Estimation_CVPR_2021_paper.pdf)]
    * Title: img2pose: Face Alignment and Detection via 6DoF, Face Pose Estimation
    * Year: `2021`
    * Authors: Vitor Albiero, Xingyu Chen, Xi Yin, Guan Pang, Tal Hassner
    * Abstract: We propose real-time, six degrees of freedom (6DoF), 3D face pose estimation without face detection or landmark localization. We observe that estimating the 6DoF rigid transformation of a face is a simpler problem than facial landmark detection, often used for 3D face alignment. In addition, 6DoF offers more information than face bounding box labels. We leverage these observations to make multiple contributions: (a) We describe an easily trained, efficient, Faster R-CNN-based model which regresses 6DoF pose for all faces in the photo, without preliminary face detection. (b) We explain how pose is converted and kept consistent between the input photo and arbitrary crops created while training and evaluating our model. (c) Finally, we show how face poses can replace detection bounding box training labels. Tests on AFLW2000-3D and BIWI show that our method runs at real-time and outperforms state of the art (SotA) face pose estimators. Remarkably, our method also surpasses SotA models of comparable complexity on the WIDER FACE detection benchmark, despite not been optimized on bounding box labels.
count=1
* How Privacy-Preserving Are Line Clouds? Recovering Scene Details From 3D Lines
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Chelani_How_Privacy-Preserving_Are_Line_Clouds_Recovering_Scene_Details_From_3D_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Chelani_How_Privacy-Preserving_Are_Line_Clouds_Recovering_Scene_Details_From_3D_CVPR_2021_paper.pdf)]
    * Title: How Privacy-Preserving Are Line Clouds? Recovering Scene Details From 3D Lines
    * Year: `2021`
    * Authors: Kunal Chelani, Fredrik Kahl, Torsten Sattler
    * Abstract: Visual localization is the problem of estimating the camera pose of a given image with respect to a known scene. Visual localization algorithms are a fundamental building block in advanced computer vision applications, including Mixed and Virtual Reality systems. Many algorithms used in practice represent the scene through a Structure-from-Motion (SfM) point cloud, where each 3D point is associated with one or more local image features. Establishing 2D-3D matches between features in a query image and the 3D points through descriptor matching Visual localization is the problem of estimating the camera pose of a given image with respect to a known scene. Visual localization algorithms are a fundamental building block in advanced computer vision applications, including Mixed and Virtual Reality systems. Many algorithms used in practice represent the scene through a Structure-from-Motion (SfM) point cloud and use 2D-3D matches between a query image and the 3D points for camera pose estimation. As recently shown, image details can be accurately recovered from SfM point clouds by translating renderings of the sparse point clouds to images. To address the resulting potential privacy risks for user-generated content, it was recently proposed to lift point clouds to line clouds by replacing 3D points by randomly oriented 3D lines passing through these points. The resulting representation is unintelligible to humans and effectively prevents point cloud-to-image translation. This paper shows that a significant amount of information about the 3D scene geometry is preserved in these line clouds, allowing us to (approximately) recover the 3D point positions and thus to (approximately) recover image content. Our approach is based on the observation that the closest points between lines can yield a good approximation to the original 3D points. Code is available at \href https://github.com/kunalchelani/Line2Point https://github.com/kunalchelani/Line2Point .
count=1
* Generalizable Person Re-Identification With Relevance-Aware Mixture of Experts
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Dai_Generalizable_Person_Re-Identification_With_Relevance-Aware_Mixture_of_Experts_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Dai_Generalizable_Person_Re-Identification_With_Relevance-Aware_Mixture_of_Experts_CVPR_2021_paper.pdf)]
    * Title: Generalizable Person Re-Identification With Relevance-Aware Mixture of Experts
    * Year: `2021`
    * Authors: Yongxing Dai, Xiaotong Li, Jun Liu, Zekun Tong, Ling-Yu Duan
    * Abstract: Domain generalizable (DG) person re-identification (ReID) is a challenging problem because we cannot access any unseen target domain data during training. Almost all the existing DG ReID methods follow the same pipeline where they use a hybrid dataset from multiple source domains for training, and then directly apply the trained model to the unseen target domains for testing. These methods often neglect individual source domains' discriminative characteristics and their relevances w.r.t. the unseen target domains, though both of which can be leveraged to help the model's generalization. To handle the above two issues, we propose a novel method called the relevance-aware mixture of experts (RaMoE), using an effective voting-based mixture mechanism to dynamically leverage source domains' diverse characteristics to improve the model's generalization. Specifically, we propose a decorrelation loss to make the source domain networks (experts) keep the diversity and discriminability of individual domains' characteristics. Besides, we design a voting network to adaptively integrate all the experts' features into the more generalizable aggregated features with domain relevance. Considering the target domains' invisibility during training, we propose a novel learning-to-learn algorithm combined with our relation alignment loss to update the voting network. Extensive experiments demonstrate that our proposed RaMoE outperforms the state-of-the-art methods.
count=1
* Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Dong_Robust_Neural_Routing_Through_Space_Partitions_for_Camera_Relocalization_in_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Dong_Robust_Neural_Routing_Through_Space_Partitions_for_Camera_Relocalization_in_CVPR_2021_paper.pdf)]
    * Title: Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments
    * Year: `2021`
    * Authors: Siyan Dong, Qingnan Fan, He Wang, Ji Shi, Li Yi, Thomas Funkhouser, Baoquan Chen, Leonidas J. Guibas
    * Abstract: Localizing the camera in a known indoor environment is a key building block for scene mapping, robot navigation, AR, etc. Recent advances estimate the camera pose via optimization over the 2D/3D-3D correspondences established between the coordinates in 2D/3D camera space and 3D world space. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world. To address the aforementioned issues, in this paper, we propose a novel outlier-aware neural tree which bridges the two worlds, deep learning and decision tree approaches. It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark developed for camera relocalization in dynamic indoor environments. It achieves robust neural routing through space partitions and outperforms the state-of-the-art approaches by around 30% on camera pose accuracy, while running comparably fast for evaluation.
count=1
* How Well Do Self-Supervised Models Transfer?
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Ericsson_How_Well_Do_Self-Supervised_Models_Transfer_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Ericsson_How_Well_Do_Self-Supervised_Models_Transfer_CVPR_2021_paper.pdf)]
    * Title: How Well Do Self-Supervised Models Transfer?
    * Year: `2021`
    * Authors: Linus Ericsson, Henry Gouk, Timothy M. Hospedales
    * Abstract: Self-supervised visual representation learning has seen huge progress recently, but no large scale evaluation has compared the many models now available. We evaluate the transfer performance of 13 top self-supervised models on 40 downstream tasks, including many-shot and few-shot recognition, object detection, and dense prediction. We compare their performance to a supervised baseline and show that on most tasks the best self-supervised models outperform supervision, confirming the recently observed trend in the literature. We find ImageNet Top-1 accuracy to be highly correlated with transfer to many-shot recognition, but increasingly less so for few-shot, object detection and dense prediction. No single self-supervised method dominates overall, suggesting that universal pre-training is still unsolved. Our analysis of features suggests that top self-supervised learners fail to preserve colour information as well as supervised alternatives, but tend to induce better classifier calibration, and less attentive overfitting than supervised learners.
count=1
* Panoptic Segmentation Forecasting
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Graber_Panoptic_Segmentation_Forecasting_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Graber_Panoptic_Segmentation_Forecasting_CVPR_2021_paper.pdf)]
    * Title: Panoptic Segmentation Forecasting
    * Year: `2021`
    * Authors: Colin Graber, Grace Tsai, Michael Firman, Gabriel Brostow, Alexander G. Schwing
    * Abstract: Our goal is to forecast the near future given a set of recent observations. We think this ability to forecast, i.e., to anticipate, is integral for the success of autonomous agents which need not only passively analyze an observation but also must react to it in real-time. Importantly, accurate forecasting hinges upon the chosen scene decomposition. We think that superior forecasting can be achieved by decomposing a dynamic scene into individual 'things' and background 'stuff'. Background 'stuff' largely moves because of camera motion, while foreground 'things' move because of both camera and individual object motion. Following this decomposition, we introduce panoptic segmentation forecasting. Panoptic segmentation forecasting opens up a middle-ground between existing extremes, which either forecast instance trajectories or predict the appearance of future image frames. To address this task we develop a two-component model: one component learns the dynamics of the background stuff by anticipating odometry, the other one anticipates the dynamics of detected things. We establish a leaderboard for this novel task, and validate a state-of-the-art model that outperforms available baselines.
count=1
* Rethinking Channel Dimensions for Efficient Model Design
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Han_Rethinking_Channel_Dimensions_for_Efficient_Model_Design_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Han_Rethinking_Channel_Dimensions_for_Efficient_Model_Design_CVPR_2021_paper.pdf)]
    * Title: Rethinking Channel Dimensions for Efficient Model Design
    * Year: `2021`
    * Authors: Dongyoon Han, Sangdoo Yun, Byeongho Heo, YoungJoon Yoo
    * Abstract: Designing an efficient model within the limited computational cost is challenging. We argue the accuracy of a lightweight model has been further limited by the design convention: a stage-wise configuration of the channel dimensions, which looks like a piecewise linear function of the network stage. In this paper, we study an effective channel dimension configuration towards better performance than the convention. To this end, we empirically study how to design a single layer properly by analyzing the rank of the output feature. We then investigate the channel configuration of a model by searching network architectures concerning the channel configuration under the computational cost restriction. Based on the investigation, we propose a simple yet effective channel configuration that can be parameterized by the layer index. As a result, our proposed model following the channel parameterization achieves remarkable performance on ImageNet classification and transfer learning tasks including COCO object detection, COCO instance segmentation, and fine-grained classifications. Code and ImageNet pretrained models are available at https://github.com/clovaai/rexnet.
count=1
* Efficient Deformable Shape Correspondence via Multiscale Spectral Manifold Wavelets Preservation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Efficient_Deformable_Shape_Correspondence_via_Multiscale_Spectral_Manifold_Wavelets_Preservation_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Efficient_Deformable_Shape_Correspondence_via_Multiscale_Spectral_Manifold_Wavelets_Preservation_CVPR_2021_paper.pdf)]
    * Title: Efficient Deformable Shape Correspondence via Multiscale Spectral Manifold Wavelets Preservation
    * Year: `2021`
    * Authors: Ling Hu, Qinsong Li, Shengjun Liu, Xinru Liu
    * Abstract: The functional map framework has proven to be extremely effective for representing dense correspondences between deformable shapes. A key step in this framework is to formulate suitable preservation constraints to encode the geometric information that must be preserved by the unknown map. For this issue, we construct novel and powerful constraints to determine the functional map, where multiscale spectral manifold wavelets are required to be preserved at each scale correspondingly. Such constraints allow us to extract significantly more information than previous methods, especially those based on descriptor preservation constraints, and strongly ensure the isometric property of the map. In addition, we also propose a remarkable efficient iterative method to alternatively update the functional maps and pointwise maps. Moreover, when we use the tight wavelet frames in iterations, the computation of the functional maps boils down to a simple filtering procedure with low-pass and various band-pass filters, which avoids time-consuming solving large systems of linear equations commonly presented in functional maps. We demonstrate on a wide variety of experiments with different datasets that our approach achieves significant improvements both in the shape correspondence quality and the computing efficiency.
count=1
* Geo-FARM: Geodesic Factor Regression Model for Misaligned Pre-Shape Responses in Statistical Shape Analysis
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Huang_Geo-FARM_Geodesic_Factor_Regression_Model_for_Misaligned_Pre-Shape_Responses_in_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Geo-FARM_Geodesic_Factor_Regression_Model_for_Misaligned_Pre-Shape_Responses_in_CVPR_2021_paper.pdf)]
    * Title: Geo-FARM: Geodesic Factor Regression Model for Misaligned Pre-Shape Responses in Statistical Shape Analysis
    * Year: `2021`
    * Authors: Chao Huang, Anuj Srivastava, Rongjie Liu
    * Abstract: The problem of using covariates to predict shapes of objects in a regression setting is important in many fields. A formal statistical approach, termed geodesic regression model, is commonly used for modeling and analyzing relationships between Euclidean predictors and shape responses. Despite its popularity, this model faces several key challenges, including (i) misalignment of shapes due to pre-processing steps, (ii) difficulties in shape alignment due to imaging heterogeneity, and (iii) lack of spatial correlation in shape structures. This paper proposes a comprehensive geodesic factor regression model that addresses all these challenges. Instead of using shapes as extracted from pre-registered data, it takes a more fundamental approach, incorporating alignment step within the proposed regression model and learns them using both pre-shape and covariate data. Additionally, it specifies spatial correlation structures using low-dimensional representations, including latent factors on the tangent space and isotropic error terms. The proposed framework results in substantial improvements in regression performance, as demonstrated through simulation studies and a real data analysis on Corpus Callosum contour data obtained from the ADNI study.
count=1
* Scalability vs. Utility: Do We Have To Sacrifice One for the Other in Data Importance Quantification?
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Jia_Scalability_vs._Utility_Do_We_Have_To_Sacrifice_One_for_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Jia_Scalability_vs._Utility_Do_We_Have_To_Sacrifice_One_for_CVPR_2021_paper.pdf)]
    * Title: Scalability vs. Utility: Do We Have To Sacrifice One for the Other in Data Importance Quantification?
    * Year: `2021`
    * Authors: Ruoxi Jia, Fan Wu, Xuehui Sun, Jiacen Xu, David Dao, Bhavya Kailkhura, Ce Zhang, Bo Li, Dawn Song
    * Abstract: Quantifying the importance of each training point to a learning task is a fundamental problem in machine learning and the estimated importance scores have been leveraged to guide a range of data workflows such as data summarization and domain adaption. One simple idea is to use the leave-one-out error of each training point to indicate its importance. Recent work has also proposed to use the Shapley value, as it defines a unique value distribution scheme that satisfies a set of appealing properties. However, calculating Shapley values is often expensive, which limits its applicability in real-world applications at scale. Multiple heuristics to improve the scalability of calculating Shapley values have been proposed recently, with the potential risk of compromising their utility in real-world applications. How well do existing data quantification methods perform on existing workflows? How do these methods compare with each other, empirically and theoretically? Must we sacrifice scalability for the utility in these workflows when using these methods? In this paper, we conduct a novel theoretical analysis comparing the utility of different importance quantification methods, and report extensive experimental studies on settings such as noisy label detection, watermark removal, data summarization, data acquisition, and domain adaptation on existing and proposed workflows. We show that Shapley value approximation based on a KNN surrogate over pre-trained feature embeddings obtains comparable utility with existing algorithms while achieving significant scalability improvement, often by orders of magnitude. Our theoretical analysis also justifies its advantage over the leave-one-out error. The code is available at https://github.com/AI-secure/Shapley-Study.
count=1
* Restore From Restored: Video Restoration With Pseudo Clean Video
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Lee_Restore_From_Restored_Video_Restoration_With_Pseudo_Clean_Video_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Restore_From_Restored_Video_Restoration_With_Pseudo_Clean_Video_CVPR_2021_paper.pdf)]
    * Title: Restore From Restored: Video Restoration With Pseudo Clean Video
    * Year: `2021`
    * Authors: Seunghwan Lee, Donghyeon Cho, Jiwon Kim, Tae Hyun Kim
    * Abstract: In this study, we propose a self-supervised video denoising method called ""restore-from-restored."" This method fine-tunes a pre-trained network by using a pseudo clean video during the test phase. The pseudo clean video is obtained by applying a noisy video to the baseline network. By adopting a fully convolutional neural network (FCN) as the baseline, we can improve video denoising performance without accurate optical flow estimation and registration steps, in contrast to many conventional video restoration methods, due to the translation equivariant property of the FCN. Specifically, the proposed method can take advantage of plentiful similar patches existing across multiple consecutive frames (i.e., patch-recurrence); these patches can boost the performance of the baseline network by a large margin. We analyze the restoration performance of the fine-tuned video denoising networks with the proposed self-supervision-based learning algorithm, and demonstrate that the FCN can utilize recurring patches without requiring accurate registration among adjacent frames. In our experiments, we apply the proposed method to state-of-the-art denoisers and show that our fine-tuned networks achieve a considerable improvement in denoising performance.
count=1
* Exploring Adversarial Fake Images on Face Manifold
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Li_Exploring_Adversarial_Fake_Images_on_Face_Manifold_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Exploring_Adversarial_Fake_Images_on_Face_Manifold_CVPR_2021_paper.pdf)]
    * Title: Exploring Adversarial Fake Images on Face Manifold
    * Year: `2021`
    * Authors: Dongze Li, Wei Wang, Hongxing Fan, Jing Dong
    * Abstract: Images synthesized by powerful generative adversarial network (GAN) based methods have drawn moral and privacy concerns. Although image forensic models have reached great performance in detecting fake images from real ones, these models can be easily fooled with a simple adversarial attack. But, the noise adding adversarial samples are also arousing suspicion. In this paper, instead of adding adversarial noise, we optimally search adversarial points on face manifold to generate anti-forensic fake face images. We iteratively do a gradient-descent with each small step in the latent space of a generative model, e.g. Style-GAN, to find an adversarial latent vector, which is similar to norm-based adversarial attack but in latent space. Then, the generated fake images driven by the adversarial latent vectors with the help of GANs can defeat main-stream forensic models. For examples, they make the accuracy of deepfake detection models based on Xception or EfficientNet drop from over 90% to nearly 0%, meanwhile maintaining high visual quality. In addition, we find manipulating noise vectors n at different levels have different impacts on attack success rate, and the generated adversarial images mainly have changes on facial texture or face attributes.
count=1
* Noise-Resistant Deep Metric Learning With Ranking-Based Instance Selection
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Noise-Resistant_Deep_Metric_Learning_With_Ranking-Based_Instance_Selection_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Noise-Resistant_Deep_Metric_Learning_With_Ranking-Based_Instance_Selection_CVPR_2021_paper.pdf)]
    * Title: Noise-Resistant Deep Metric Learning With Ranking-Based Instance Selection
    * Year: `2021`
    * Authors: Chang Liu, Han Yu, Boyang Li, Zhiqi Shen, Zhanning Gao, Peiran Ren, Xuansong Xie, Lizhen Cui, Chunyan Miao
    * Abstract: The existence of noisy labels in real-world data negatively impacts the performance of deep learning models. Although much research effort has been devoted to improving robustness to noisy labels in classification tasks, the problem of noisy labels in deep metric learning (DML) remains open. In this paper, we propose a noise-resistant training technique for DML, which we name Probabilistic Ranking-based Instance Selection with Memory (PRISM). PRISM identifies noisy data in a minibatch using average similarity against image features extracted by several previous versions of the neural network. These features are stored in and retrieved from a memory bank. To alleviate the high computational cost brought by the memory bank, we introduce an acceleration method that replaces individual data points with the class centers. In extensive comparisons with 12 existing approaches under both synthetic and real-world label noise, PRISM demonstrates superior performance of up to 6.06% in Precision@1.
count=1
* SDD-FIQA: Unsupervised Face Image Quality Assessment With Similarity Distribution Distance
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Ou_SDD-FIQA_Unsupervised_Face_Image_Quality_Assessment_With_Similarity_Distribution_Distance_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Ou_SDD-FIQA_Unsupervised_Face_Image_Quality_Assessment_With_Similarity_Distribution_Distance_CVPR_2021_paper.pdf)]
    * Title: SDD-FIQA: Unsupervised Face Image Quality Assessment With Similarity Distribution Distance
    * Year: `2021`
    * Authors: Fu-Zhao Ou, Xingyu Chen, Ruixin Zhang, Yuge Huang, Shaoxin Li, Jilin Li, Yong Li, Liujuan Cao, Yuan-Gen Wang
    * Abstract: In recent years, Face Image Quality Assessment (FIQA) has become an indispensable part of the face recognition system to guarantee the stability and reliability of recognition performance in an unconstrained scenario. For this purpose, the FIQA method should consider both the intrinsic property and the recognizability of the face image. Most previous works aim to estimate the sample-wise embedding uncertainty or pair-wise similarity as the quality score, which only considers the partial information from the intra-class. However, these methods ignore the valuable information from the inter-class, which is for estimating the recognizability of face image. In this work, we argue that a high-quality face image should be similar to its intra-class samples and dissimilar to its inter-class samples. Thus, we propose a novel unsupervised FIQA method that incorporates Similarity Distribution Distance for Face Image Quality Assessment (SDD-FIQA). Our method generates quality pseudo-labels by calculating the Wasserstein Distance (WD) between the intra-class and inter-class similarity distributions. With these quality pseudo-labels, we are capable of training a regression network for quality prediction. Extensive experiments on benchmark datasets demonstrate that the proposed SDD-FIQA surpasses the state-of-the-arts by an impressive margin. Meanwhile, our method shows good generalization across different recognition systems.
count=1
* On the Difficulty of Membership Inference Attacks
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Rezaei_On_the_Difficulty_of_Membership_Inference_Attacks_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Rezaei_On_the_Difficulty_of_Membership_Inference_Attacks_CVPR_2021_paper.pdf)]
    * Title: On the Difficulty of Membership Inference Attacks
    * Year: `2021`
    * Authors: Shahbaz Rezaei, Xin Liu
    * Abstract: Recent studies propose membership inference (MI) attacks on deep models, where the goal is to infer if a sample has been used in the training process. Despite their apparent success, these studies only report accuracy, precision, and recall of the positive class (member class). Hence, the performance of these attacks have not been clearly reported on negative class (non-member class). In this paper, we show that the way the MI attack performance has been reported is often misleading because they suffer from high false positive rate or false alarm rate (FAR) that has not been reported. FAR shows how often the attack model mislabel non-training samples (non-member) as training (member) ones. The high FAR makes MI attacks fundamentally impractical, which is particularly more significant for tasks such as membership inference where the majority of samples in reality belong to the negative (non-training) class. Moreover, we show that the current MI attack models can only identify the membership of misclassified samples with mediocre accuracy at best, which only constitute a very small portion of training samples. We analyze several new features that have not been comprehensively explored for membership inference before, including distance to the decision boundary and gradient norms, and conclude that deep models' responses are mostly similar among train and non-train samples. We conduct several experiments on image classification tasks, including MNIST, CIFAR-10, CIFAR-100, and ImageNet, using various model architecture, including LeNet, AlexNet, ResNet, etc. We show that the current state-of-the-art MI attacks cannot achieve high accuracy and low FAR at the same time, even when the attacker is given several advantages. The source code is available at https://github.com/shrezaei/MI-Attack.
count=1
* Learning To Relate Depth and Semantics for Unsupervised Domain Adaptation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Saha_Learning_To_Relate_Depth_and_Semantics_for_Unsupervised_Domain_Adaptation_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Saha_Learning_To_Relate_Depth_and_Semantics_for_Unsupervised_Domain_Adaptation_CVPR_2021_paper.pdf)]
    * Title: Learning To Relate Depth and Semantics for Unsupervised Domain Adaptation
    * Year: `2021`
    * Authors: Suman Saha, Anton Obukhov, Danda Pani Paudel, Menelaos Kanakis, Yuhua Chen, Stamatios Georgoulis, Luc Van Gool
    * Abstract: We present an approach for encoding visual task relationships to improve model performance in an Unsupervised Domain Adaptation (UDA) setting. Semantic segmentation and monocular depth estimation are shown to be complementary tasks; in a multi-task learning setting, a proper encoding of their relationships can further improve performance on both tasks. Motivated by this observation, we propose a novel Cross-Task Relation Layer (CTRL), which encodes task dependencies between the semantic and depth predictions. To capture the cross-task relationships, we propose a neural network architecture that contains task-specific and cross-task refinement heads. Furthermore, we propose an Iterative Self-Learning (ISL) training scheme, which exploits semantic pseudo-labels to provide extra supervision on the target domain. We experimentally observe improvements in both tasks' performance because the complementary information present in these tasks is better captured. Specifically, we show that: (1) our approach improves performance on all tasks when they are complementary and mutually dependent; (2) the CTRL helps to improve both semantic segmentation and depth estimation tasks performance in the challenging UDA setting; (3) the proposed ISL training scheme further improves the semantic segmentation performance. The implementation is available at https://github.com/susaha/ctrl-uda.
count=1
* Revamping Cross-Modal Recipe Retrieval With Hierarchical Transformers and Self-Supervised Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Salvador_Revamping_Cross-Modal_Recipe_Retrieval_With_Hierarchical_Transformers_and_Self-Supervised_Learning_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Salvador_Revamping_Cross-Modal_Recipe_Retrieval_With_Hierarchical_Transformers_and_Self-Supervised_Learning_CVPR_2021_paper.pdf)]
    * Title: Revamping Cross-Modal Recipe Retrieval With Hierarchical Transformers and Self-Supervised Learning
    * Year: `2021`
    * Authors: Amaia Salvador, Erhan Gundogdu, Loris Bazzani, Michael Donoser
    * Abstract: Cross-modal recipe retrieval has recently gained substantial attention due to the importance of food in people's lives, as well as the availability of vast amounts of digital cooking recipes and food images to train machine learning models. In this work, we revisit existing approaches for cross-modal recipe retrieval and propose a simplified end-to-end model based on well established and high performing encoders for text and images. We introduce a hierarchical recipe Transformer which attentively encodes individual recipe components (titles, ingredients and instructions). Further, we propose a self-supervised loss function computed on top of pairs of individual recipe components, which is able to leverage semantic relationships within recipes, and enables training using both image-recipe and recipe-only samples. We conduct a thorough analysis and ablation studies to validate our design choices. As a result, our proposed method achieves state-of-the-art performance in the cross-modal recipe retrieval task on the Recipe1M dataset. We make code and models publicly available.
count=1
* Affective Processes: Stochastic Modelling of Temporal Context for Emotion and Facial Expression Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Sanchez_Affective_Processes_Stochastic_Modelling_of_Temporal_Context_for_Emotion_and_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Sanchez_Affective_Processes_Stochastic_Modelling_of_Temporal_Context_for_Emotion_and_CVPR_2021_paper.pdf)]
    * Title: Affective Processes: Stochastic Modelling of Temporal Context for Emotion and Facial Expression Recognition
    * Year: `2021`
    * Authors: Enrique Sanchez, Mani Kumar Tellamekala, Michel Valstar, Georgios Tzimiropoulos
    * Abstract: Temporal context is key to the recognition of expressions of emotion. Existing methods, that rely on recurrent or self-attention models to enforce temporal consistency, work on the feature level, ignoring the task-specific temporal dependencies, and fail to model context uncertainty. To alleviate these issues, we build upon the framework of Neural Processes to propose a method for apparent emotion recognition with three key novel components: (a) probabilistic contextual representation with a global latent variable model; (b) temporal context modelling using task-specific predictions in addition to features; and (c) smart temporal context selection. We validate our approach on four databases, two for Valence and Arousal estimation (SEWA and AffWild2), and two for Action Unit intensity estimation (DISFA and BP4D). Results show a consistent improvement over a series of strong baselines as well as over state-of-the-art methods.
count=1
* DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Shen_DCT-Mask_Discrete_Cosine_Transform_Mask_Representation_for_Instance_Segmentation_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_DCT-Mask_Discrete_Cosine_Transform_Mask_Representation_for_Instance_Segmentation_CVPR_2021_paper.pdf)]
    * Title: DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation
    * Year: `2021`
    * Authors: Xing Shen, Jirui Yang, Chunbo Wei, Bing Deng, Jianqiang Huang, Xian-Sheng Hua, Xiaoliang Cheng, Kewei Liang
    * Abstract: Binary grid mask representation is broadly used in instance segmentation. A representative instantiation is Mask R-CNN which predicts masks on a 28*28 binary grid. Generally, a low-resolution grid is not sufficient to capture the details, while a high-resolution grid dramatically increases the training complexity. In this paper, we propose a new mask representation by applying the discrete cosine transform(DCT) to encode the high-resolution binary grid mask into a compact vector. Our method, termed DCT-Mask, could be easily integrated into most pixel-based instance segmentation methods. Without any bells and whistles, DCT-Mask yields significant gains on different frameworks, backbones, datasets, and training schedules. It does not require any pre-processing or pre-training, and almost no harm to the running speed. Especially, for higher-quality annotations and more complex backbones, our method has a greater improvement. Moreover, we analyze the performance of our method from the perspective of the quality of mask representation. The main reason why DCT-Mask works well is that it obtains a high-quality mask representation with low complexity.
count=1
* HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Tankovich_HITNet_Hierarchical_Iterative_Tile_Refinement_Network_for_Real-time_Stereo_Matching_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Tankovich_HITNet_Hierarchical_Iterative_Tile_Refinement_Network_for_Real-time_Stereo_Matching_CVPR_2021_paper.pdf)]
    * Title: HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching
    * Year: `2021`
    * Authors: Vladimir Tankovich, Christian Hane, Yinda Zhang, Adarsh Kowdle, Sean Fanello, Sofien Bouaziz
    * Abstract: This paper presents HITNet, a novel neural network architecture for real-time stereo matching. Contrary to many recent neural network approaches that operate on a full costvolume and rely on 3D convolutions, our approach does not explicitly build a volume and instead relies on a fast multi-resolution initialization step, differentiable 2D geometric propagation and warping mechanisms to infer disparity hypotheses. To achieve a high level of accuracy, our network not only geometrically reasons about disparities but also infers slanted plane hypotheses allowing to more accurately perform geometric warping and upsampling operations. Our architecture is inherently multi-resolution allowing the propagation of information across different levels. Multiple experiments prove the effectiveness of the proposed approach at a fraction of the computation required by the state-of-the-art methods. At the time of writing, HITNet ranks 1st-3rd on all the metrics published on the ETH3D website for two view stereo, ranks 1st on most of the metrics amongst all the end-to-end learning approaches on Middleburyv3, ranks 1st on the popular KITTI 2012 and 2015 benchmarks among the published methods faster than 100ms.
count=1
* Benchmarking Representation Learning for Natural World Image Collections
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Van_Horn_Benchmarking_Representation_Learning_for_Natural_World_Image_Collections_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Van_Horn_Benchmarking_Representation_Learning_for_Natural_World_Image_Collections_CVPR_2021_paper.pdf)]
    * Title: Benchmarking Representation Learning for Natural World Image Collections
    * Year: `2021`
    * Authors: Grant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, Oisin Mac Aodha
    * Abstract: Recent progress in self-supervised learning has resulted in models that are capable of extracting rich representations from image collections without requiring any explicit label supervision. However, to date the vast majority of these approaches have restricted themselves to training on standard benchmark datasets such as ImageNet. We argue that fine-grained visual categorization problems, such as plant and animal species classification, provide an informative testbed for self-supervised learning. In order to facilitate progress in this area we present two new natural world visual classification datasets, iNat2021 and NeWT. The former consists of 2.7M images from 10k different species uploaded by users of the citizen science application iNaturalist. We designed the latter, NeWT, in collaboration with domain experts with the aim of benchmarking the performance of representation learning algorithms on a suite of challenging natural world binary classification tasks that go beyond standard species classification. These two new datasets allow us to explore questions related to large-scale representation and transfer learning in the context of fine-grained categories. We provide a comprehensive analysis of feature extractors trained with and without supervision on ImageNet and iNat2021, shedding light on the strengths and weaknesses of different learned features across a diverse set of tasks. We find that features produced by standard supervised methods still outperform those produced by self-supervised approaches such as SimCLR. However, improved self-supervised learning methods are constantly being released and the iNat2021 and NeWT datasets are a valuable resource for tracking their progress.
count=1
* Rich Features for Perceptual Quality Assessment of UGC Videos
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Rich_Features_for_Perceptual_Quality_Assessment_of_UGC_Videos_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Rich_Features_for_Perceptual_Quality_Assessment_of_UGC_Videos_CVPR_2021_paper.pdf)]
    * Title: Rich Features for Perceptual Quality Assessment of UGC Videos
    * Year: `2021`
    * Authors: Yilin Wang, Junjie Ke, Hossein Talebi, Joong Gon Yim, Neil Birkbeck, Balu Adsumilli, Peyman Milanfar, Feng Yang
    * Abstract: Video quality assessment for User Generated Content (UGC) is an important topic in both industry and academia. Most existing methods only focus on one aspect of the perceptual quality assessment, such as technical quality or compression artifacts. In this paper, we create a large scale dataset to comprehensively investigate characteristics of generic UGC video quality. Besides the subjective ratings and content labels of the dataset, we also propose a DNN-based framework to thoroughly analyze importance of content, technical quality, and compression level in perceptual quality. Our model is able to provide quality scores as well as human-friendly quality indicators, to bridge the gap between low level video signals to human perceptual quality. Experimental results show that our model achieves state-of-the-art correlation with Mean Opinion Scores (MOS).
count=1
* NeuralFusion: Online Depth Fusion in Latent Space
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Weder_NeuralFusion_Online_Depth_Fusion_in_Latent_Space_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Weder_NeuralFusion_Online_Depth_Fusion_in_Latent_Space_CVPR_2021_paper.pdf)]
    * Title: NeuralFusion: Online Depth Fusion in Latent Space
    * Year: `2021`
    * Authors: Silvan Weder, Johannes L. Schonberger, Marc Pollefeys, Martin R. Oswald
    * Abstract: We present a novel online depth map fusion approach that learns depth map aggregation in a latent feature space. While previous fusion methods use an explicit scene representation like signed distance functions (SDFs), we propose a learned feature representation for the fusion. The key idea is a separation between the scene representation used for the fusion and the output scene representation, via an additional translator network. Our neural network architecture consists of two main parts: a depth and feature fusion sub-network, which is followed by a translator sub-network to produce the final surface representation (e.g. TSDF) for visualization or other tasks. Our approach is an online process, handles high noise levels, and is particularly able to deal with gross outliers common for photometric stereo-based depth maps. Experiments on real and synthetic data demonstrate improved results compared to the state of the art, especially in challenging scenarios with large amounts of noise and outliers. The source code will be made available at https://github.com/weders/NeuralFusion.
count=1
* Re-Labeling ImageNet: From Single to Multi-Labels, From Global to Localized Labels
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Yun_Re-Labeling_ImageNet_From_Single_to_Multi-Labels_From_Global_to_Localized_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Yun_Re-Labeling_ImageNet_From_Single_to_Multi-Labels_From_Global_to_Localized_CVPR_2021_paper.pdf)]
    * Title: Re-Labeling ImageNet: From Single to Multi-Labels, From Global to Localized Labels
    * Year: `2021`
    * Authors: Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Junsuk Choe, Sanghyuk Chun
    * Abstract: ImageNet has been the most popular image classification benchmark, but it is also the one with a significant level of label noise. Recent studies have shown that many samples contain multiple classes, despite being assumed to be a single-label benchmark. They have thus proposed to turn ImageNet evaluation into a multi-label task, with exhaustive multi-label annotations per image. However, they have not fixed the training set, presumably because of a formidable annotation cost. We argue that the mismatch between single-label annotations and effectively multi-label images is equally, if not more, problematic in the training setup, where random crops are applied. With the single-label annotations, a random crop of an image may contain an entirely different object from the ground truth, introducing noisy or even incorrect supervision during training. We thus re-label the ImageNet training set with multi-labels. We address the annotation cost barrier by letting a strong image classifier, trained on an extra source of data, generate the multi-labels. We utilize the pixel-wise multi-label predictions before the final pooling layer, in order to exploit the additional location-specific supervision signals. Training on the re-labeled samples results in improved model performances across the board. ResNet-50 attains the top-1 accuracy of 78.9% on ImageNet with our localized multi-labels, which can be further boosted to 80.2% with the CutMix regularization. We show that the models trained with localized multi-labels also outperform the baselines on transfer learning to object detection and instance segmentation tasks, and various robustness benchmarks. The re-labeled ImageNet training set, pre-trained weights, and the source code are available at https://github.com/naver-ai/relabel_imagenet.
count=1
* ABMDRNet: Adaptive-Weighted Bi-Directional Modality Difference Reduction Network for RGB-T Semantic Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_ABMDRNet_Adaptive-Weighted_Bi-Directional_Modality_Difference_Reduction_Network_for_RGB-T_Semantic_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_ABMDRNet_Adaptive-Weighted_Bi-Directional_Modality_Difference_Reduction_Network_for_RGB-T_Semantic_CVPR_2021_paper.pdf)]
    * Title: ABMDRNet: Adaptive-Weighted Bi-Directional Modality Difference Reduction Network for RGB-T Semantic Segmentation
    * Year: `2021`
    * Authors: Qiang Zhang, Shenlu Zhao, Yongjiang Luo, Dingwen Zhang, Nianchang Huang, Jungong Han
    * Abstract: Semantic segmentation models gain robustness against poor lighting conditions by virtue of complementary information from visible (RGB) and thermal images. Despite its importance, most existing RGB-T semantic segmentation models perform primitive fusion strategies, such as concatenation, element-wise summation and weighted summation, to fuse features from different modalities. These strategies, unfortunately, overlook the modality differences due to different imaging mechanisms, so that they suffer from the reduced discriminability of the fused features. To address such an issue, we propose, for the first time, the strategy of bridging-then-fusing, where the innovation lies in a novel Adaptive-weighted Bi-directional Modality Difference Reduction Network (ABMDRNet). Concretely, a Modality Difference Reduction and Fusion (MDRF) subnetwork is designed, which first employs a bi-directional image-to-image translation based method to reduce the modality differences between RGB features and thermal features, and then adaptively selects those discriminative multi-modality features for RGB-T semantic segmentation in a channel-wise weighted fusion way. Furthermore, considering the importance of contextual information in semantic segmentation, a Multi-Scale Spatial Context (MSC) module and a Multi-Scale Channel Context (MCC) module are proposed to exploit the interactions among multi-scale contextual information of cross-modality features together with their long-range dependencies along spatial and channel dimensions, respectively. Comprehensive experiments on MFNet dataset demonstrate that our method achieves new state-of-the-art results.
count=1
* Deep Compositional Metric Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Deep_Compositional_Metric_Learning_CVPR_2021_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Deep_Compositional_Metric_Learning_CVPR_2021_paper.pdf)]
    * Title: Deep Compositional Metric Learning
    * Year: `2021`
    * Authors: Wenzhao Zheng, Chengkun Wang, Jiwen Lu, Jie Zhou
    * Abstract: In this paper, we propose a deep compositional metric learning (DCML) framework for effective and generalizable similarity measurement between images. Conventional deep metric learning methods minimize a discriminative loss to enlarge interclass distances while suppressing intraclass variations, which might lead to inferior generalization performance since samples even from the same class may present diverse characteristics. This motivates the adoption of the ensemble technique to learn a number of sub-embeddings using different and diverse subtasks. However, most subtasks impose weaker or contradictory constraints, which essentially sacrifices the discrimination ability of each sub-embedding to improve the generalization ability of their combination. To achieve a better generalization ability without compromising, we propose to separate the sub-embeddings from direct supervisions from the subtasks and apply the losses on different composites of the sub-embeddings. We employ a set of learnable compositors to combine the sub-embeddings and use a self-reinforced loss to train the compositors, which serve as relays to distribute the diverse training signals to avoid destroying the discrimination ability. Experimental results on the CUB-200-2011, Cars196, and Stanford Online Products datasets demonstrate the superior performance of our framework.
count=1
* UniPose: Unified Human Pose Estimation in Single Images and Videos
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Artacho_UniPose_Unified_Human_Pose_Estimation_in_Single_Images_and_Videos_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Artacho_UniPose_Unified_Human_Pose_Estimation_in_Single_Images_and_Videos_CVPR_2020_paper.pdf)]
    * Title: UniPose: Unified Human Pose Estimation in Single Images and Videos
    * Year: `2020`
    * Authors: Bruno Artacho,  Andreas Savakis
    * Abstract: We propose UniPose, a unified framework for human pose estimation, based on our "Waterfall" Atrous Spatial Pooling architecture, that achieves state-of-art-results on several pose estimation metrics. UniPose incorporates contextual segmentation and joint localization to estimate the human pose in a single stage, with high accuracy, without relying on statistical postprocessing methods. The Waterfall module in UniPose leverages the efficiency of progressive filtering in the cascade architecture, while maintaining multi-scale fields-of-view comparable to spatial pyramid configurations. Additionally, our method is extended to UniPose-LSTM for multi-frame processing and achieves state-of-the-art results for temporal pose estimation in Video. Our results on multiple datasets demonstrate that UniPose, with a ResNet backbone and Waterfall module, is a robust and efficient architecture for pose estimation obtaining state-of-the-art results in single person pose detection for both single images and videos.
count=1
* DeepDeform: Learning Non-Rigid RGB-D Reconstruction With Semi-Supervised Data
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Bozic_DeepDeform_Learning_Non-Rigid_RGB-D_Reconstruction_With_Semi-Supervised_Data_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Bozic_DeepDeform_Learning_Non-Rigid_RGB-D_Reconstruction_With_Semi-Supervised_Data_CVPR_2020_paper.pdf)]
    * Title: DeepDeform: Learning Non-Rigid RGB-D Reconstruction With Semi-Supervised Data
    * Year: `2020`
    * Authors: Aljaz Bozic,  Michael Zollhofer,  Christian Theobalt,  Matthias Niessner
    * Abstract: Applying data-driven approaches to non-rigid 3D reconstruction has been difficult, which we believe can be attributed to the lack of a large-scale training corpus. Unfortunately, this method fails for important cases such as highly non-rigid deformations. We first address this problem of lack of data by introducing a novel semi-supervised strategy to obtain dense inter-frame correspondences from a sparse set of annotations. This way, we obtain a large dataset of 400 scenes, over 390,000 RGB-D frames, and 5,533 densely aligned frame pairs; in addition, we provide a test set along with several metrics for evaluation. Based on this corpus, we introduce a data-driven non-rigid feature matching approach, which we integrate into an optimization-based reconstruction pipeline. Here, we propose a new neural network that operates on RGB-D frames, while maintaining robustness under large non-rigid deformations and producing accurate predictions. Our approach significantly outperforms existing non-rigid reconstruction methods that do not use learned data terms, as well as learning-based approaches that only use self-supervision.
count=1
* Reconstruct Locally, Localize Globally: A Model Free Method for Object Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Cai_Reconstruct_Locally_Localize_Globally_A_Model_Free_Method_for_Object_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_Reconstruct_Locally_Localize_Globally_A_Model_Free_Method_for_Object_CVPR_2020_paper.pdf)]
    * Title: Reconstruct Locally, Localize Globally: A Model Free Method for Object Pose Estimation
    * Year: `2020`
    * Authors: Ming Cai,  Ian Reid
    * Abstract: Six degree-of-freedom pose estimation of a known object in a single image is a long-standing computer vision objective. It is classically posed as a correspondence problem between a known geometric model, such as a CAD model, and image locations. If a CAD model is not available, it is possible to use multi-view visual reconstruction methods to create a geometric model, and use this in the same manner. Instead, we propose a learning-based method whose input is a collection of images of a target object, and whose output is the pose of the object in a novel view. At inference time, our method maps from the RoI features of the input image to a dense collection of object-centric 3D coordinates, one per pixel. This dense 2D-3D mapping is then used to determine 6dof pose using standard PnP plus RANSAC. The model that maps 2D to object 3D coordinates is established at training time by automatically discovering and matching image landmarks that are consistent across multiple views. We show that this method eliminates the requirement for a 3D CAD model (needed by classical geometry-based methods and state-of-the-art learning-based methods alike) but still achieves performance on a par with the prior art.
count=1
* Unsupervised Learning of Intrinsic Structural Representation Points
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Unsupervised_Learning_of_Intrinsic_Structural_Representation_Points_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Unsupervised_Learning_of_Intrinsic_Structural_Representation_Points_CVPR_2020_paper.pdf)]
    * Title: Unsupervised Learning of Intrinsic Structural Representation Points
    * Year: `2020`
    * Authors: Nenglun Chen,  Lingjie Liu,  Zhiming Cui,  Runnan Chen,  Duygu Ceylan,  Changhe Tu,  Wenping Wang
    * Abstract: Learning structures of 3D shapes is a fundamental problem in the field of computer graphics and geometry processing. We present a simple yet interpretable unsupervised method for learning a new structural representation in the form of 3D structure points. The 3D structure points produced by our method encode the shape structure intrinsically and exhibit semantic consistency across all the shape instances with similar structures. This is a challenging goal that has not fully been achieved by other methods. Specifically, our method takes a 3D point cloud as input and encodes it as a set of local features. The local features are then passed through a novel point integration module to produce a set of 3D structure points. The chamfer distance is used as reconstruction loss to ensure the structure points lie close to the input point cloud. Extensive experiments have shown that our method outperforms the state-of-the-art on the semantic shape correspondence task and achieves comparable performance with the state-of-the-art on the segmentation label transfer task. Moreover, the PCA based shape embedding built upon consistent structure points demonstrates good performance in preserving the shape structures. Code is available at https://github.com/NolenChen/3DStructurePoints
count=1
* Scene-Adaptive Video Frame Interpolation via Meta-Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Choi_Scene-Adaptive_Video_Frame_Interpolation_via_Meta-Learning_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Choi_Scene-Adaptive_Video_Frame_Interpolation_via_Meta-Learning_CVPR_2020_paper.pdf)]
    * Title: Scene-Adaptive Video Frame Interpolation via Meta-Learning
    * Year: `2020`
    * Authors: Myungsub Choi,  Janghoon Choi,  Sungyong Baik,  Tae Hyun Kim,  Kyoung Mu Lee
    * Abstract: Video frame interpolation is a challenging problem because there are different scenarios for each video depending on the variety of foreground and background motion, frame rate, and occlusion. It is therefore difficult for a single network with fixed parameters to generalize across different videos. Ideally, one could have a different network for each scenario, but this is computationally infeasible for practical applications. In this work, we propose to adapt the model to each video by making use of additional information that is readily available at test time and yet has not been exploited in previous works. We first show the benefits of 'test-time adaptation' through simple fine-tuning of a network, then we greatly improve its efficiency by incorporating meta-learning. We obtain significant performance gains with only a single gradient update without any additional parameters. Finally, we show that our meta-learning framework can be easily employed to any video frame interpolation network and can consistently improve its performance on multiple benchmark datasets.
count=1
* Detecting Attended Visual Targets in Video
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Chong_Detecting_Attended_Visual_Targets_in_Video_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chong_Detecting_Attended_Visual_Targets_in_Video_CVPR_2020_paper.pdf)]
    * Title: Detecting Attended Visual Targets in Video
    * Year: `2020`
    * Authors: Eunji Chong,  Yongxin Wang,  Nataniel Ruiz,  James M. Rehg
    * Abstract: We address the problem of detecting attention targets in video. Our goal is to identify where each person in each frame of a video is looking, and correctly handle the case where the gaze target is out-of-frame. Our novel architecture models the dynamic interaction between the scene and head features and infers time-varying attention targets. We introduce a new annotated dataset, VideoAttentionTarget, containing complex and dynamic patterns of real-world gaze behavior. Our experiments show that our model can effectively infer dynamic attention in videos. In addition, we apply our predicted attention maps to two social gaze behavior recognition tasks, and show that the resulting classifiers significantly outperform existing methods. We achieve state-of-the-art performance on three datasets: GazeFollow (static images), VideoAttentionTarget (videos), and VideoCoAtt (videos), and obtain the first results for automatically classifying clinically-relevant gaze behavior without wearable cameras or eye trackers.
count=1
* Learning Geocentric Object Pose in Oblique Monocular Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Christie_Learning_Geocentric_Object_Pose_in_Oblique_Monocular_Images_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Christie_Learning_Geocentric_Object_Pose_in_Oblique_Monocular_Images_CVPR_2020_paper.pdf)]
    * Title: Learning Geocentric Object Pose in Oblique Monocular Images
    * Year: `2020`
    * Authors: Gordon Christie,  Rodrigo Rene Rai Munoz Abujder,  Kevin Foster,  Shea Hagstrom,  Gregory D. Hager,  Myron Z. Brown
    * Abstract: An object's geocentric pose, defined as the height above ground and orientation with respect to gravity, is a powerful representation of real-world structure for object detection, segmentation, and localization tasks using RGBD images. For close-range vision tasks, height and orientation have been derived directly from stereo-computed depth and more recently from monocular depth predicted by deep networks. For long-range vision tasks such as Earth observation, depth cannot be reliably estimated with monocular images. Inspired by recent work in monocular height above ground prediction and optical flow prediction from static images, we develop an encoding of geocentric pose to address this challenge and train a deep network to compute the representation densely, supervised by publicly available airborne lidar. We exploit these attributes to rectify oblique images and remove observed object parallax to dramatically improve the accuracy of localization and to enable accurate alignment of multiple images taken from very different oblique viewpoints. We demonstrate the value of our approach by extending two large-scale public datasets for semantic segmentation in oblique satellite images. All of our data and code are publicly available.
count=1
* TPNet: Trajectory Proposal Network for Motion Prediction
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Fang_TPNet_Trajectory_Proposal_Network_for_Motion_Prediction_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Fang_TPNet_Trajectory_Proposal_Network_for_Motion_Prediction_CVPR_2020_paper.pdf)]
    * Title: TPNet: Trajectory Proposal Network for Motion Prediction
    * Year: `2020`
    * Authors: Liangji Fang,  Qinhong Jiang,  Jianping Shi,  Bolei Zhou
    * Abstract: Making accurate motion prediction of the surrounding traffic agents such as pedestrians, vehicles, and cyclists is crucial for autonomous driving. Recent data-driven motion prediction methods have attempted to learn to directly regress the exact future position or its distribution from massive amount of trajectory data. However, it remains difficult for these methods to provide multimodal predictions as well as integrate physical constraints such as traffic rules and movable areas. In this work we propose a novel two-stage motion prediction framework, Trajectory Proposal Network (TPNet). TPNet first generates a candidate set of future trajectories as hypothesis proposals, then makes the final predictions by classifying and refining the proposals which meets the physical constraints. By steering the proposal generation process, safe and multimodal predictions are realized. Thus this framework effectively mitigates the complexity of motion prediction problem while ensuring the multimodal output. Experiments on four large-scale trajectory prediction datasets, i.e. the ETH, UCY, Apollo and Argoverse datasets, show that TPNet achieves the state-of-the-art results both quantitatively and qualitatively.
count=1
* MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images with Latent Variable Model
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Fu_MCEN_Bridging_Cross-Modal_Gap_between_Cooking_Recipes_and_Dish_Images_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Fu_MCEN_Bridging_Cross-Modal_Gap_between_Cooking_Recipes_and_Dish_Images_CVPR_2020_paper.pdf)]
    * Title: MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images with Latent Variable Model
    * Year: `2020`
    * Authors: Han Fu,  Rui Wu,  Chenghao Liu,  Jianling Sun
    * Abstract: Nowadays, driven by the increasing concern on diet and health, food computing has attracted enormous attention from both industry and research community. One of the most popular research topics in this domain is Food Retrieval, due to its profound influence on health-oriented applications. In this paper, we focus on the task of cross-modal retrieval between food images and cooking recipes. We present Modality-Consistent Embedding Network (MCEN) that learns modality-invariant representations by projecting images and texts to the same embedding space. To capture the latent alignments between modalities, we incorporate stochastic latent variables to explicitly exploit the interactions between textual and visual features. Importantly, our method learns the cross-modal alignments during training but computes embeddings of different modalities independently at inference time for the sake of efficiency. Extensive experimental results clearly demonstrate that the proposed MCEN outperforms all existing approaches on the benchmark Recipe1M dataset and requires less computational cost.
count=1
* SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Guo_SiamCAR_Siamese_Fully_Convolutional_Classification_and_Regression_for_Visual_Tracking_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_SiamCAR_Siamese_Fully_Convolutional_Classification_and_Regression_for_Visual_Tracking_CVPR_2020_paper.pdf)]
    * Title: SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking
    * Year: `2020`
    * Authors: Dongyan Guo,  Jun Wang,  Ying Cui,  Zhenhua Wang,  Shengyong Chen
    * Abstract: By decomposing the visual tracking task into two subproblems as classification for pixel category and regression for object bounding box at this pixel, we propose a novel fully convolutional Siamese network to solve visual tracking end-to-end in a per-pixel manner. The proposed framework SiamCAR consists of two simple subnetworks: one Siamese subnetwork for feature extraction and one classification-regression subnetwork for bounding box prediction. Different from state-of-the-art trackers like Siamese-RPN, SiamRPN++ and SPM, which are based on region proposal, the proposed framework is both proposal and anchor free. Consequently, we are able to avoid the tricky hyper-parameter tuning of anchors and reduce human intervention. The proposed framework is simple, neat and effective. Extensive experiments and comparisons with state-of-the-art trackers are conducted on challenging benchmarks including GOT-10K, LaSOT, UAV123 and OTB-50. Without bells and whistles, our SiamCAR achieves the leading performance with a considerable real-time speed. The code is available at https://github.com/ohhhyeahhh/SiamCAR.
count=1
* HOnnotate: A Method for 3D Annotation of Hand and Object Poses
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Hampali_HOnnotate_A_Method_for_3D_Annotation_of_Hand_and_Object_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Hampali_HOnnotate_A_Method_for_3D_Annotation_of_Hand_and_Object_CVPR_2020_paper.pdf)]
    * Title: HOnnotate: A Method for 3D Annotation of Hand and Object Poses
    * Year: `2020`
    * Authors: Shreyas Hampali,  Mahdi Rad,  Markus Oberweger,  Vincent Lepetit
    * Abstract: We propose a method for annotating images of a hand manipulating an object with the 3D poses of both the hand and the object, together with a dataset created using this method. Our motivation is the current lack of annotated real images for this problem, as estimating the 3D poses is challenging, mostly because of the mutual occlusions between the hand and the object. To tackle this challenge, we capture sequences with one or several RGB-D cameras and jointly optimize the 3D hand and object poses over all the frames simultaneously. This method allows us to automatically annotate each frame with accurate estimates of the poses, despite large mutual occlusions. With this method, we created HO-3D, the first markerless dataset of color images with 3D annotations for both the hand and object. This dataset is currently made of 77,558 frames, 68 sequences, 10 persons, and 10 objects. Using our dataset, we develop a single RGB image-based method to predict the hand pose when interacting with objects under severe occlusions and show it generalizes to objects not seen in the dataset.
count=1
* Incremental Learning in Online Scenario
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/He_Incremental_Learning_in_Online_Scenario_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Incremental_Learning_in_Online_Scenario_CVPR_2020_paper.pdf)]
    * Title: Incremental Learning in Online Scenario
    * Year: `2020`
    * Authors: Jiangpeng He,  Runyu Mao,  Zeman Shao,  Fengqing Zhu
    * Abstract: Modern deep learning approaches have achieved great success in many vision applications by training a model using all available task-specific data. However, there are two major obstacles making it challenging to implement for real life applications: (1) Learning new classes makes the trained model quickly forget old classes knowledge, which is referred to as catastrophic forgetting. (2) As new observations of old classes come sequentially over time, the distribution may change in unforeseen way, making the performance degrade dramatically on future data, which is referred to as concept drift. Current state-of-the-art incremental learning methods require a long time to train the model whenever new classes are added and none of them takes into consideration the new observations of old classes. In this paper, we propose an incremental learning framework that can work in the challenging online learning scenario and handle both new classes data and new observations of old classes. We address problem (1) in online mode by introducing a modified cross-distillation loss together with a two-step learning technique. Our method outperforms the results obtained from current state-of-the-art offline incremental learning methods on the CIFAR-100 and ImageNet-1000 (ILSVRC 2012) datasets under the same experiment protocol but in online scenario. We also provide a simple yet effective method to mitigate problem (2) by updating exemplar set using the feature of each new observation of old classes and demonstrate a real life application of online food image classification based on our complete framework using the Food-101 dataset.
count=1
* PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/He_PVN3D_A_Deep_Point-Wise_3D_Keypoints_Voting_Network_for_6DoF_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_PVN3D_A_Deep_Point-Wise_3D_Keypoints_Voting_Network_for_6DoF_CVPR_2020_paper.pdf)]
    * Title: PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation
    * Year: `2020`
    * Authors: Yisheng He,  Wei Sun,  Haibin Huang,  Jianran Liu,  Haoqiang Fan,  Jian Sun
    * Abstract: In this work, we present a novel data-driven method for robust 6DoF object pose estimation from a single RGBD image. Unlike previous methods that directly regressing pose parameters, we tackle this challenging task with a keypoint-based approach. Specifically, we propose a deep Hough voting network to detect 3D keypoints of objects and then estimate the 6D pose parameters within a least-squares fitting manner. Our method is a natural extension of 2D-keypoint approaches that successfully work on RGB based 6DoF estimation. It allows us to fully utilize the geometric constraint of rigid objects with the extra depth information and is easy for a network to learn and optimize. Extensive experiments were conducted to demonstrate the effectiveness of 3D-keypoint detection in the 6D pose estimation task. Experimental results also show our method outperforms the state-of-the-art methods by large margins on several benchmarks. Code and video are available at https://github.com/ethnhe/PVN3D.git.
count=1
* Attention Scaling for Crowd Counting
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Jiang_Attention_Scaling_for_Crowd_Counting_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_Attention_Scaling_for_Crowd_Counting_CVPR_2020_paper.pdf)]
    * Title: Attention Scaling for Crowd Counting
    * Year: `2020`
    * Authors: Xiaoheng Jiang,  Li Zhang,  Mingliang Xu,  Tianzhu Zhang,  Pei Lv,  Bing Zhou,  Xin Yang,  Yanwei Pang
    * Abstract: Convolutional Neural Network (CNN) based methods generally take crowd counting as a regression task by outputting crowd densities. They learn the mapping between image contents and crowd density distributions. Though having achieved promising results, these data-driven counting networks are prone to overestimate or underestimate people counts of regions with different density patterns, which degrades the whole count accuracy. To overcome this problem, we propose an approach to alleviate the counting performance differences in different regions. Specifically, our approach consists of two networks named Density Attention Network (DANet) and Attention Scaling Network (ASNet). DANet provides ASNet with attention masks related to regions of different density levels. ASNet first generates density maps and scaling factors and then multiplies them by attention masks to output separate attention-based density maps. These density maps are summed to give the final density map. The attention scaling factors help attenuate the estimation errors in different regions. Furthermore, we present a novel Adaptive Pyramid Loss (APLoss) to hierarchically calculate the estimation losses of sub-regions, which alleviates the training bias. Extensive experiments on four challenging datasets (ShanghaiTech Part A, UCF_CC_50, UCF-QNRF, and WorldExpo'10) demonstrate the superiority of the proposed approach.
count=1
* RGBD-Dog: Predicting Canine Pose from RGBD Sensors
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Kearney_RGBD-Dog_Predicting_Canine_Pose_from_RGBD_Sensors_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Kearney_RGBD-Dog_Predicting_Canine_Pose_from_RGBD_Sensors_CVPR_2020_paper.pdf)]
    * Title: RGBD-Dog: Predicting Canine Pose from RGBD Sensors
    * Year: `2020`
    * Authors: Sinead Kearney,  Wenbin Li,  Martin Parsons,  Kwang In Kim,  Darren Cosker
    * Abstract: The automatic extraction of animal 3D pose from images without markers is of interest in a range of scientific fields. Most work to date predicts animal pose from RGB images, based on 2D labelling of joint positions. However, due to the difficult nature of obtaining training data, no ground truth dataset of 3D animal motion is available to quantitatively evaluate these approaches. In addition, a lack of 3D animal pose data also makes it difficult to train 3D pose-prediction methods in a similar manner to the popular field of body-pose prediction. In our work, we focus on the problem of 3D canine pose estimation from RGBD images, recording a diverse range of dog breeds with several Microsoft Kinect v2s, simultaneously obtaining the 3D ground truth skeleton via a motion capture system. We generate a dataset of synthetic RGBD images from this data. A stacked hourglass network is trained to predict 3D joint locations, which is then constrained using prior models of shape and pose. We evaluate our model on both synthetic and real RGBD images and compare our results to previously published work fitting canine models to images. Finally, despite our training set consisting only of dog data, visual inspection implies that our network can produce good predictions for images of other quadrupeds - e.g. horses or cats - when their pose is similar to that contained in our training set.
count=1
* A Programmatic and Semantic Approach to Explaining and Debugging Neural Network Based Object Detectors
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Kim_A_Programmatic_and_Semantic_Approach_to_Explaining_and_Debugging_Neural_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_A_Programmatic_and_Semantic_Approach_to_Explaining_and_Debugging_Neural_CVPR_2020_paper.pdf)]
    * Title: A Programmatic and Semantic Approach to Explaining and Debugging Neural Network Based Object Detectors
    * Year: `2020`
    * Authors: Edward Kim,  Divya Gopinath,  Corina Pasareanu,  Sanjit A. Seshia
    * Abstract: Even as deep neural networks have become very effective for tasks in vision and perception, it remains difficult to explain and debug their behavior. In this paper, we present a programmatic and semantic approach to explaining, understanding, and debugging the correct and incorrect behaviors of a neural network based perception system. Our approach is semantic in that it employs a high-level representation of the distribution of environment scenarios that the detector is intended to work on. It is programmatic in that the representation is a program in a domain-specific probabilistic programming language using which synthetic data can be generated to train and test the neural network. We present a framework that assesses the performance of the neural network to identify correct and incorrect detections, extracts rules from those results that semantically characterizes the correct and incorrect scenarios, and then specializes the probabilistic program with those rules in order to more precisely characterize the scenarios in which the neural network operates correctly or not, without human intervention. We demonstrate our results using the Scenic probabilistic programming language and a neural network-based object detector. Our experiments show that it is possible to automatically generate compact rules that significantly increase the correct detection rate (or conversely the incorrect detection rate) of the network and can thus help with debugging and understanding its behavior.
count=1
* LUVLi Face Alignment: Estimating Landmarks' Location, Uncertainty, and Visibility Likelihood
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Kumar_LUVLi_Face_Alignment_Estimating_Landmarks_Location_Uncertainty_and_Visibility_Likelihood_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Kumar_LUVLi_Face_Alignment_Estimating_Landmarks_Location_Uncertainty_and_Visibility_Likelihood_CVPR_2020_paper.pdf)]
    * Title: LUVLi Face Alignment: Estimating Landmarks' Location, Uncertainty, and Visibility Likelihood
    * Year: `2020`
    * Authors: Abhinav Kumar,  Tim K. Marks,  Wenxuan Mou,  Ye Wang,  Michael Jones,  Anoop Cherian,  Toshiaki Koike-Akino,  Xiaoming Liu,  Chen Feng
    * Abstract: Modern face alignment methods have become quite accurate at predicting the locations of facial landmarks, but they do not typically estimate the uncertainty of their predicted locations nor predict whether landmarks are visible. In this paper, we present a novel framework for jointly predicting landmark locations, associated uncertainties of these predicted locations, and landmark visibilities. We model these as mixed random variables and estimate them using a deep network trained using our proposed Location, Uncertainty, and Visibility Likelihood (LUVLi) loss. In addition, we release an entirely new labeling of a large face alignment dataset with over 19,000 face images in a full range of head poses. Each face is manually labeled with the ground-truth locations of 68 landmarks, with the additional information of whether each landmarks is visible, self-occluded (due to extreme head poses), or externally occluded. Not only does our joint estimation yield accurate estimates of the uncertainty of predicted landmark locations, but it also yields state-of-the-art estimates for the landmark locations themselves on mulitple standard face alignment datasets. Our method's estimates of the uncertainty of predicted landmark locations could be used to automatically identify input images on which face alignment fails, which can be critical for downstream tasks.
count=1
* Category-Level Articulated Object Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Category-Level_Articulated_Object_Pose_Estimation_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Category-Level_Articulated_Object_Pose_Estimation_CVPR_2020_paper.pdf)]
    * Title: Category-Level Articulated Object Pose Estimation
    * Year: `2020`
    * Authors: Xiaolong Li,  He Wang,  Li Yi,  Leonidas J. Guibas,  A. Lynn Abbott,  Shuran Song
    * Abstract: This paper addresses the task of category-level pose estimation for articulated objects from a single depth image. We present a novel category-level approach that correctly accommodates object instances previously unseen during training. We introduce Articulation-aware Normalized Coordinate Space Hierarchy (ANCSH) - a canonical representation for different articulated objects in a given category. As the key to achieve intra-category generalization, the representation constructs a canonical object space as well as a set of canonical part spaces. The canonical object space normalizes the object orientation, scales and articulations (e.g. joint parameters and states) while each canonical part space further normalizes its part pose and scale. We develop a deep network based on PointNet++ that predicts ANCSH from a single depth point cloud, including part segmentation, normalized coordinates, and joint parameters in the canonical object space. By leveraging the canonicalized joints, we demonstrate: 1) improved performance in part pose and scale estimations using the induced kinematic constraints from joints; 2) high accuracy for joint parameter estimation in camera space.
count=1
* Dynamic Multiscale Graph Neural Networks for 3D Skeleton Based Human Motion Prediction
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Dynamic_Multiscale_Graph_Neural_Networks_for_3D_Skeleton_Based_Human_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Dynamic_Multiscale_Graph_Neural_Networks_for_3D_Skeleton_Based_Human_CVPR_2020_paper.pdf)]
    * Title: Dynamic Multiscale Graph Neural Networks for 3D Skeleton Based Human Motion Prediction
    * Year: `2020`
    * Authors: Maosen Li,  Siheng Chen,  Yangheng Zhao,  Ya Zhang,  Yanfeng Wang,  Qi Tian
    * Abstract: We propose novel dynamic multiscale graph neural networks (DMGNN) to predict 3D skeleton-based human motions. The core idea of DMGNN is to use a multiscale graph to comprehensively model the internal relations of a human body for motion feature learning. This multiscale graph is adaptive during training and dynamic across network layers. Based on this graph, we propose a multiscale graph computational unit (MGCU) to extract features at individual scales and fuse features across scales. The entire model is action-category-agnostic and follows an encoder-decoder framework. The encoder consists of a sequence of MGCUs to learn motion features. The decoder uses a proposed graph-based gate recurrent unit to generate future poses. Extensive experiments show that the proposed DMGNN outperforms state-of-the-art methods in both short and long-term predictions on the datasets of Human 3.6M and CMU Mocap. We further investigate the learned multiscale graphs for the interpretability. The codes could be downloaded from https://github.com/limaosen0/DMGNN.
count=1
* Training a Steerable CNN for Guidewire Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Training_a_Steerable_CNN_for_Guidewire_Detection_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Training_a_Steerable_CNN_for_Guidewire_Detection_CVPR_2020_paper.pdf)]
    * Title: Training a Steerable CNN for Guidewire Detection
    * Year: `2020`
    * Authors: Donghang Li,  Adrian Barbu
    * Abstract: Guidewires are thin wires used in coronary angioplasty to guide different tools to access and repair the obstructed artery. The whole procedure is monitored using fluoroscopic (real-time X-ray) images. Due to the guidewire being thin in the low quality fluoroscopic images, it is usually poorly visible. The poor quality of the X-ray images makes the guidewire detection a challenging problem in image-guided interventions. Localizing the guidewire could help in enhancing its visibility and for other automatic procedures. Guidewire localization methods usually contain a first step of computing a pixelwise guidewire response map on the entire image. In this paper, we present a steerable Convolutional Neural Network (CNN), which is a Fully Convolutional Neural Network (FCNN) that can detect objects rotated by an arbitrary 2D angle, without being rotation invariant. In fact, the steerable CNN has an angle parameter that can be changed to make it sensitive to objects rotated by that angle. We present an application of this idea to detecting the guidewire pixels, and compare it with an FCNN trained to be invariant to the guidewire orientation. Results reveal that the proposed method is a good choice, outperforming some popular filter-based and learning-based approaches such as Frangi Filter, Spherical Quadrature Filter, FCNN and a state of the art trained classifier based on hand-crafted feature.
count=1
* Cross-View Correspondence Reasoning Based on Bipartite Graph Convolutional Network for Mammogram Mass Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Liu_Cross-View_Correspondence_Reasoning_Based_on_Bipartite_Graph_Convolutional_Network_for_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Cross-View_Correspondence_Reasoning_Based_on_Bipartite_Graph_Convolutional_Network_for_CVPR_2020_paper.pdf)]
    * Title: Cross-View Correspondence Reasoning Based on Bipartite Graph Convolutional Network for Mammogram Mass Detection
    * Year: `2020`
    * Authors: Yuhang Liu,  Fandong Zhang,  Qianyi Zhang,  Siwen Wang,  Yizhou Wang,  Yizhou Yu
    * Abstract: Mammogram mass detection is of great clinical significance due to its high proportion in breast cancers. The information from cross views (i.e., mediolateral oblique and cranio-caudal) is highly related and complementary, and is helpful to make comprehensive decisions. However, unlike radiologists who are able to recognize masses with reasoning ability in cross-view images, most existing methods lack the ability to reason under the guidance of domain knowledge, thus it limits the performance. In this paper, we introduce bipartite graph convolutional network to endow existing methods with cross-view reasoning ability of radiologists in mammogram mass detection. The bipartite node sets are constructed by cross-view images respectively to represent relatively consistent regions in breasts, while the bipartite edge learns to model both inherent cross-view geometric constraints and appearance similarities between correspondences. Based on the bipartite graph, the information propagates methodically through correspondences and enables spatial visual features equipped with customized cross-view reasoning ability. Experimental results on DDSM dataset demonstrate that the proposed algorithm achieves state-of-the-art performance. Besides, visual analysis shows the model has a clear physical meaning, which is helpful for radiologists in clinical interpretation.
count=1
* Can We Learn Heuristics for Graphical Model Inference Using Reinforcement Learning?
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Messaoud_Can_We_Learn_Heuristics_for_Graphical_Model_Inference_Using_Reinforcement_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Messaoud_Can_We_Learn_Heuristics_for_Graphical_Model_Inference_Using_Reinforcement_CVPR_2020_paper.pdf)]
    * Title: Can We Learn Heuristics for Graphical Model Inference Using Reinforcement Learning?
    * Year: `2020`
    * Authors: Safa Messaoud,  Maghav Kumar,  Alexander G. Schwing
    * Abstract: Combinatorial optimization is frequently used in computer vision. For instance, in applications like semantic segmentation, human pose estimation and action recognition, programs are formulated for solving inference in Conditional Random Fields (CRFs) to produce a structured output that is consistent with visual features of the image. However, solving inference in CRFs is in general intractable, and approximation methods are computationally demanding and limited to unary, pairwise and hand-crafted forms of higher order potentials. In this paper, we show that we can learn program heuristics, i.e., policies, for solving inference in higher order CRFs for the task of semantic segmentation, using reinforcement learning. Our method solves inference tasks efficiently without imposing any constraints on the form of the potentials. We show compelling results on the Pascal VOC and MOTS datasets.
count=1
* Self-Supervised Viewpoint Learning From Image Collections
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Mustikovela_Self-Supervised_Viewpoint_Learning_From_Image_Collections_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Mustikovela_Self-Supervised_Viewpoint_Learning_From_Image_Collections_CVPR_2020_paper.pdf)]
    * Title: Self-Supervised Viewpoint Learning From Image Collections
    * Year: `2020`
    * Authors: Siva Karthik Mustikovela,  Varun Jampani,  Shalini De Mello,  Sifei Liu,  Umar Iqbal,  Carsten Rother,  Jan Kautz
    * Abstract: Training deep neural networks to estimate the viewpoint of objects requires large labeled training datasets. However, manually labeling viewpoints is notoriously hard, error-prone, and time-consuming. On the other hand, it is relatively easy to mine many unlabeled images of an object category from the internet, e.g., of cars or faces. We seek to answer the research question of whether such unlabeled collections of in-the-wild images can be successfully utilized to train viewpoint estimation networks for general object categories purely via self-supervision. Self-supervision here refers to the fact that the only true supervisory signal that the network has is the input image itself. We propose a novel learning framework which incorporates an analysis-by-synthesis paradigm to reconstruct images in a viewpoint aware manner with a generative network, along with symmetry and adversarial constraints to successfully supervise our viewpoint estimation network. We show that our approach performs competitively to fully-supervised approaches for several object categories like human faces, cars, buses, and trains. Our work opens up further research in self-supervised viewpoint learning and serves as a robust baseline for it. We open-source our code at https://github.com/NVlabs/SSV.
count=1
* SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Qiao_SEED_Semantics_Enhanced_Encoder-Decoder_Framework_for_Scene_Text_Recognition_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_SEED_Semantics_Enhanced_Encoder-Decoder_Framework_for_Scene_Text_Recognition_CVPR_2020_paper.pdf)]
    * Title: SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition
    * Year: `2020`
    * Authors: Zhi Qiao,  Yu Zhou,  Dongbao Yang,  Yucan Zhou,  Weiping Wang
    * Abstract: Scene text recognition is a hot research topic in computer vision. Recently, many recognition methods based on the encoder-decoder framework have been proposed, and they can handle scene texts of perspective distortion and curve shape. Nevertheless, they still face lots of challenges like image blur, uneven illumination, and incomplete characters. We argue that most encoder-decoder methods are based on local visual features without explicit global semantic information. In this work, we propose a semantics enhanced encoder-decoder framework to robustly recognize low-quality scene texts. The semantic information is used both in the encoder module for supervision and in the decoder module for initializing. In particular, the state-of-the-art ASTER method is integrated into the proposed framework as an exemplar. Extensive experiments demonstrate that the proposed framework is more robust for low-quality text images, and achieves state-of-the-art results on several benchmark datasets. The source code will be available.
count=1
* Where Does It End? - Reasoning About Hidden Surfaces by Object Intersection Constraints
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Strecke_Where_Does_It_End_-_Reasoning_About_Hidden_Surfaces_by_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Strecke_Where_Does_It_End_-_Reasoning_About_Hidden_Surfaces_by_CVPR_2020_paper.pdf)]
    * Title: Where Does It End? - Reasoning About Hidden Surfaces by Object Intersection Constraints
    * Year: `2020`
    * Authors: Michael Strecke,  Jorg Stuckler
    * Abstract: Dynamic scene understanding is an essential capability in robotics and VR/AR. In this paper we propose Co-Section, an optimization-based approach to 3D dynamic scene reconstruction, which infers hidden shape information from intersection constraints. An object-level dynamic SLAM frontend detects, segments, tracks and maps dynamic objects in the scene. Our optimization backend completes the shapes using hull and intersection constraints between the objects. In experiments, we demonstrate our approach on real and synthetic dynamic scene datasets. We also assess the shape completion performance of our method quantitatively. To the best of our knowledge, our approach is the first method to incorporate such physical plausibility constraints on object intersections for shape completion of dynamic objects in an energy minimization framework.
count=1
* Learning From Web Data With Self-Organizing Memory Module
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Tu_Learning_From_Web_Data_With_Self-Organizing_Memory_Module_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Tu_Learning_From_Web_Data_With_Self-Organizing_Memory_Module_CVPR_2020_paper.pdf)]
    * Title: Learning From Web Data With Self-Organizing Memory Module
    * Year: `2020`
    * Authors: Yi Tu,  Li Niu,  Junjie Chen,  Dawei Cheng,  Liqing Zhang
    * Abstract: Learning from web data has attracted lots of research interest in recent years. However, crawled web images usually have two types of noises, label noise and background noise, which induce extra difficulties in utilizing them effectively. Most existing methods either rely on human supervision or ignore the background noise. In this paper, we propose a novel method, which is capable of handling these two types of noises together, without the supervision of clean images in the training stage. Particularly, we formulate our method under the framework of multi-instance learning by grouping ROIs (i.e., images and their region proposals) from the same category into bags. ROIs in each bag are assigned with different weights based on the representative/discriminative scores of their nearest clusters, in which the clusters and their scores are obtained via our designed memory module. Our memory module could be naturally integrated with the classification module, leading to an end-to-end trainable system. Extensive experiments on four benchmark datasets demonstrate the effectiveness of our method.
count=1
* Dynamic Convolutions: Exploiting Spatial Sparsity for Faster Inference
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Verelst_Dynamic_Convolutions_Exploiting_Spatial_Sparsity_for_Faster_Inference_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Verelst_Dynamic_Convolutions_Exploiting_Spatial_Sparsity_for_Faster_Inference_CVPR_2020_paper.pdf)]
    * Title: Dynamic Convolutions: Exploiting Spatial Sparsity for Faster Inference
    * Year: `2020`
    * Authors: Thomas Verelst,  Tinne Tuytelaars
    * Abstract: Modern convolutional neural networks apply the same operations on every pixel in an image. However, not all image regions are equally important. To address this inefficiency, we propose a method to dynamically apply convolutions conditioned on the input image. We introduce a residual block where a small gating branch learns which spatial positions should be evaluated. These discrete gating decisions are trained end-to-end using the Gumbel-Softmax trick, in combination with a sparsity criterion. Our experiments on CIFAR, ImageNet, Food-101 and MPII show that our method has better focus on the region of interest and better accuracy than existing methods, at a lower computational complexity. Moreover, we provide an efficient CUDA implementation of our dynamic convolutions using a gather-scatter approach, achieving a significant improvement in inference speed on MobileNetV2 and ShuffleNetV2. On human pose estimation, a task that is inherently spatially sparse, the processing speed is increased by 60% with no loss in accuracy.
count=1
* Siam R-CNN: Visual Tracking by Re-Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Voigtlaender_Siam_R-CNN_Visual_Tracking_by_Re-Detection_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Voigtlaender_Siam_R-CNN_Visual_Tracking_by_Re-Detection_CVPR_2020_paper.pdf)]
    * Title: Siam R-CNN: Visual Tracking by Re-Detection
    * Year: `2020`
    * Authors: Paul Voigtlaender,  Jonathon Luiten,  Philip H.S. Torr,  Bastian Leibe
    * Abstract: We present Siam R-CNN, a Siamese re-detection architecture which unleashes the full power of two-stage object detection approaches for visual object tracking. We combine this with a novel tracklet-based dynamic programming algorithm, which takes advantage of re-detections of both the first-frame template and previous-frame predictions, to model the full history of both the object to be tracked and potential distractor objects. This enables our approach to make better tracking decisions, as well as to re-detect tracked objects after long occlusion. Finally, we propose a novel hard example mining strategy to improve Siam R-CNN's robustness to similar looking objects. Siam R-CNN achieves the current best performance on ten tracking benchmarks, with especially strong results for long-term tracking. We make our code and models available at www.vision.rwth-aachen.de/page/siamrcnn.
count=1
* NestedVAE: Isolating Common Factors via Weak Supervision
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Vowels_NestedVAE_Isolating_Common_Factors_via_Weak_Supervision_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Vowels_NestedVAE_Isolating_Common_Factors_via_Weak_Supervision_CVPR_2020_paper.pdf)]
    * Title: NestedVAE: Isolating Common Factors via Weak Supervision
    * Year: `2020`
    * Authors: Matthew J. Vowels,  Necati Cihan Camgoz,  Richard Bowden
    * Abstract: Fair and unbiased machine learning is an important and active field of research, as decision processes are increasingly driven by models that learn from data. Unfortunately, any biases present in the data may be learned by the model, thereby inappropriately transferring that bias into the decision making process. We identify the connection between the task of bias reduction and that of isolating factors common between domains whilst encouraging domain specific invariance. To isolate the common factors we combine the theory of deep latent variable models with information bottleneck theory for scenarios whereby data may be naturally paired across domains and no additional supervision is required. The result is the Nested Variational AutoEncoder (NestedVAE). Two outer VAEs with shared weights attempt to reconstruct the input and infer a latent space, whilst a nested VAE attempts to reconstruct the latent representation of one image, from the latent representation of its paired image. In so doing, the nested VAE isolates the common latent factors/causes and becomes invariant to unwanted factors that are not shared between paired images. We also propose a new metric to provide a balanced method of evaluating consistency and classifier performance across domains which we refer to as the Adjusted Parity metric. An evaluation of NestedVAE on both domain and attribute invariance, change detection, and learning common factors for the prediction of biological sex demonstrates that NestedVAE significantly outperforms alternative methods.
count=1
* Deep Spatial Gradient and Temporal Depth Learning for Face Anti-Spoofing
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Deep_Spatial_Gradient_and_Temporal_Depth_Learning_for_Face_Anti-Spoofing_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Deep_Spatial_Gradient_and_Temporal_Depth_Learning_for_Face_Anti-Spoofing_CVPR_2020_paper.pdf)]
    * Title: Deep Spatial Gradient and Temporal Depth Learning for Face Anti-Spoofing
    * Year: `2020`
    * Authors: Zezheng Wang,  Zitong Yu,  Chenxu Zhao,  Xiangyu Zhu,  Yunxiao Qin,  Qiusheng Zhou,  Feng Zhou,  Zhen Lei
    * Abstract: Face anti-spoofing is critical to the security of face recognition systems. Depth supervised learning has been proven as one of the most effective methods for face anti-spoofing. Despite the great success, most previous works still formulate the problem as a single-frame multi-task one by simply augmenting the loss with depth, while neglecting the detailed fine-grained information and the interplay between facial depths and moving patterns. In contrast, we design a new approach to detect presentation attacks from multiple frames based on two insights: 1) detailed discriminative clues (e.g., spatial gradient magnitude) between living and spoofing face may be discarded through stacked vanilla convolutions, and 2) the dynamics of 3D moving faces provide important clues in detecting the spoofing faces. The proposed method is able to capture discriminative details via Residual Spatial Gradient Block (RSGB) and encode spatio-temporal information from Spatio-Temporal Propagation Module (STPM) efficiently. Moreover, a novel Contrastive Depth Loss is presented for more accurate depth supervision. To assess the efficacy of our method, we also collect a Double-modal Anti-spoofing Dataset (DMAD) which provides actual depth for each sample. The experiments demonstrate that the proposed approach achieves state-of-the-art results on five benchmark datasets including OULU-NPU, SiW, CASIA-MFSD, Replay-Attack, and the new DMAD. Codes will be available at https://github.com/clks-wzz/FAS-SGTD.
count=1
* Progressive Adversarial Networks for Fine-Grained Domain Adaptation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Progressive_Adversarial_Networks_for_Fine-Grained_Domain_Adaptation_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Progressive_Adversarial_Networks_for_Fine-Grained_Domain_Adaptation_CVPR_2020_paper.pdf)]
    * Title: Progressive Adversarial Networks for Fine-Grained Domain Adaptation
    * Year: `2020`
    * Authors: Sinan Wang,  Xinyang Chen,  Yunbo Wang,  Mingsheng Long,  Jianmin Wang
    * Abstract: Fine-grained visual categorization has long been considered as an important problem, however, its real application is still restricted, since precisely annotating a large fine-grained image dataset is a laborious task and requires expert-level human knowledge. A solution to this problem is applying domain adaptation approaches to fine-grained scenarios, where the key idea is to discover the commonality between existing fine-grained image datasets and massive unlabeled data in the wild. The main technical bottleneck lies in that the large inter-domain variation will deteriorate the subtle boundaries of small inter-class variation during domain alignment. This paper presents the Progressive Adversarial Networks (PAN) to align fine-grained categories across domains with a curriculum-based adversarial learning framework. In particular, throughout the learning process, domain adaptation is carried out through all multi-grained features, progressively exploiting the label hierarchy from coarse to fine. The progressive learning is applied upon both category classification and domain alignment, boosting both the discriminability and the transferability of the fine-grained features. Our method is evaluated on three benchmarks, two of which are proposed by us, and it outperforms the state-of-the-art domain adaptation methods.
count=1
* Sequential 3D Human Pose and Shape Estimation From Point Clouds
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Sequential_3D_Human_Pose_and_Shape_Estimation_From_Point_Clouds_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Sequential_3D_Human_Pose_and_Shape_Estimation_From_Point_Clouds_CVPR_2020_paper.pdf)]
    * Title: Sequential 3D Human Pose and Shape Estimation From Point Clouds
    * Year: `2020`
    * Authors: Kangkan Wang,  Jin Xie,  Guofeng Zhang,  Lei Liu,  Jian Yang
    * Abstract: This work addresses the problem of 3D human pose and shape estimation from a sequence of point clouds. Existing sequential 3D human shape estimation methods mainly focus on the template model fitting from a sequence of depth images or the parametric model regression from a sequence of RGB images. In this paper, we propose a novel sequential 3D human pose and shape estimation framework from a sequence of point clouds. Specifically, the proposed framework can regress 3D coordinates of mesh vertices at different resolutions from the latent features of point clouds. Based on the estimated 3D coordinates and features at the low resolution, we develop a spatial-temporal mesh attention convolution (MAC) to predict the 3D coordinates of mesh vertices at the high resolution. By assigning specific attentional weights to different neighboring points in the spatial and temporal domains, our spatial-temporal MAC can capture structured spatial and temporal features of point clouds. We further generalize our framework to the real data of human bodies with a weakly supervised fine-tuning method. The experimental results on SURREAL, Human3.6M, DFAUST and the real detailed data demonstrate that the proposed approach can accurately recover the 3D body model sequence from a sequence of point clouds.
count=1
* RoutedFusion: Learning Real-Time Depth Map Fusion
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Weder_RoutedFusion_Learning_Real-Time_Depth_Map_Fusion_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Weder_RoutedFusion_Learning_Real-Time_Depth_Map_Fusion_CVPR_2020_paper.pdf)]
    * Title: RoutedFusion: Learning Real-Time Depth Map Fusion
    * Year: `2020`
    * Authors: Silvan Weder,  Johannes Schonberger,  Marc Pollefeys,  Martin R. Oswald
    * Abstract: The efficient fusion of depth maps is a key part of most state-of-the-art 3D reconstruction methods. Besides requiring high accuracy, these depth fusion methods need to be scalable and real-time capable. To this end, we present a novel real-time capable machine learning-based method for depth map fusion. Similar to the seminal depth map fusion approach by Curless and Levoy, we only update a local group of voxels to ensure real-time capability. Instead of a simple linear fusion of depth information, we propose a neural network that predicts non-linear updates to better account for typical fusion errors. Our network is composed of a 2D depth routing network and a 3D depth fusion network which efficiently handle sensor-specific noise and outliers. This is especially useful for surface edges and thin objects for which the original approach suffers from thickening artifacts. Our method outperforms the traditional fusion approach and related learned approaches on both synthetic and real data. We demonstrate the performance of our method in reconstructing fine geometric details from noise and outlier contaminated data on various scenes.
count=1
* Deep 3D Portrait From a Single Image
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Xu_Deep_3D_Portrait_From_a_Single_Image_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Deep_3D_Portrait_From_a_Single_Image_CVPR_2020_paper.pdf)]
    * Title: Deep 3D Portrait From a Single Image
    * Year: `2020`
    * Authors: Sicheng Xu,  Jiaolong Yang,  Dong Chen,  Fang Wen,  Yu Deng,  Yunde Jia,  Xin Tong
    * Abstract: In this paper, we present a learning-based approach for recovering the 3D geometry of human head from a single portrait image. Our method is learned in an unsupervised manner without any ground-truth 3D data. We represent the head geometry with a parametric 3D face model together with a depth map for other head regions including hair and ear. A two-step geometry learning scheme is proposed to learn 3D head reconstruction from in-the-wild face images, where we first learn face shape on single images using self-reconstruction and then learn hair and ear geometry using pairs of images in a stereo-matching fashion. The second step is based on the output of the first to not only improve the accuracy but also ensure the consistency of overall head geometry. We evaluate the accuracy of our method both in 3D and with pose manipulation tasks on 2D images. We alter pose based on the recovered geometry and apply a refinement network trained with adversarial learning to ameliorate the reprojected images and translate them to the real image domain. Extensive evaluations and comparison with previous methods show that our new method can produce high-fidelity 3D head geometry and head pose manipulation results.
count=1
* 3DSSD: Point-Based 3D Single Stage Object Detector
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_3DSSD_Point-Based_3D_Single_Stage_Object_Detector_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_3DSSD_Point-Based_3D_Single_Stage_Object_Detector_CVPR_2020_paper.pdf)]
    * Title: 3DSSD: Point-Based 3D Single Stage Object Detector
    * Year: `2020`
    * Authors: Zetong Yang,  Yanan Sun,  Shu Liu,  Jiaya Jia
    * Abstract: Prevalence of voxel-based 3D single-stage detectors contrast with underexplored point-based methods. In this paper, we present a lightweight point-based 3D single stage object detector 3DSSD to achieve decent balance of accuracy and efficiency. In this paradigm, all upsampling layers and the refinement stage, which are indispensable in all existing point-based methods, are abandoned. We instead propose a fusion sampling strategy in downsampling process to make detection on less representative points feasible. A delicate box prediction network, including a candidate generation layer and an anchor-free regression head with a 3D center-ness assignment strategy, is developed to meet the demand of high accuracy and speed. Our 3DSSD paradigm is an elegant single-stage anchor-free one. We evaluate it on widely used KITTI dataset and more challenging nuScenes dataset. Our method outperforms all state-of-the-art voxel-based single-stage methods by a large margin, and even yields comparable performance with two-stage point-based methods, with amazing inference speed of 25+ FPS, 2x faster than former state-of-the-art point-based methods.
count=1
* Unsupervised Representation Learning for Gaze Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Yu_Unsupervised_Representation_Learning_for_Gaze_Estimation_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Unsupervised_Representation_Learning_for_Gaze_Estimation_CVPR_2020_paper.pdf)]
    * Title: Unsupervised Representation Learning for Gaze Estimation
    * Year: `2020`
    * Authors: Yu Yu,  Jean-Marc Odobez
    * Abstract: Although automatic gaze estimation is very important to a large variety of application areas, it is difficult to train accurate and robust gaze models, in great part due to the difficulty in collecting large and diverse data (annotating 3D gaze is expensive and existing datasets use different setups). To address this issue, our main contribution in this paper is to propose an effective approach to learn a low dimensional gaze representation without gaze annotations, which to the best of our best knowledge, is the first work to do so. The main idea is to rely on a gaze redirection network and use the gaze representation difference of the input and target images (of the redirection network) as the redirection variable. A redirection loss in image domain allows the joint training of both the redirection network and the gaze representation network. In addition, we propose a warping field regularization which not only provides an explicit physical meaning to the gaze representations but also avoids redirection distortions. Promising results on few-shot gaze estimation (competitive results can be achieved with as few as <= 100 calibration samples), cross-dataset gaze estimation, gaze network pretraining, and another task (head pose estimation) demonstrate the validity of our framework.
count=1
* 4D Association Graph for Realtime Multi-Person Motion Capture Using Multiple Video Cameras
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_4D_Association_Graph_for_Realtime_Multi-Person_Motion_Capture_Using_Multiple_CVPR_2020_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_4D_Association_Graph_for_Realtime_Multi-Person_Motion_Capture_Using_Multiple_CVPR_2020_paper.pdf)]
    * Title: 4D Association Graph for Realtime Multi-Person Motion Capture Using Multiple Video Cameras
    * Year: `2020`
    * Authors: Yuxiang Zhang,  Liang An,  Tao Yu,  Xiu Li,  Kun Li,  Yebin Liu
    * Abstract: his paper contributes a novel realtime multi-person motion capture algorithm using multiview video inputs. Due to the heavy occlusions and closely interacting motions in each view, joint optimization on the multiview images and multiple temporal frames is indispensable, which brings up the essential challenge of realtime efficiency. To this end, for the first time, we unify per-view parsing, cross-view matching, and temporal tracking into a single optimization framework, i.e., a 4D association graph that each dimension (image space, viewpoint and time) can be treated equally and simultaneously. To solve the 4D association graph efficiently, we further contribute the idea of 4D limb bundle parsing based on heuristic searching, followed with limb bundle assembling by proposing a bundle Kruskal's algorithm. Our method enables a realtime motion capture system running at 30fps using 5 cameras on a 5-person scene. Benefiting from the unified parsing, matching and tracking constraints, our method is robust to noisy detection due to severe occlusions and close interacting motions, and achieves high-quality online pose reconstruction quality. The proposed method outperforms state-of-the-art methods quantitatively without using high-level appearance information.
count=1
* Learning to Reconstruct People in Clothing From a Single RGB Camera
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Alldieck_Learning_to_Reconstruct_People_in_Clothing_From_a_Single_RGB_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Alldieck_Learning_to_Reconstruct_People_in_Clothing_From_a_Single_RGB_CVPR_2019_paper.pdf)]
    * Title: Learning to Reconstruct People in Clothing From a Single RGB Camera
    * Year: `2019`
    * Authors: Thiemo Alldieck,  Marcus Magnor,  Bharat Lal Bhatnagar,  Christian Theobalt,  Gerard Pons-Moll
    * Abstract: We present Octopus, a learning-based model to infer the personalized 3D shape of people from a few frames (1-8) of a monocular video in which the person is moving with a reconstruction accuracy of 4 to 5mm, while being orders of magnitude faster than previous methods. From semantic segmentation images, our Octopus model reconstructs a 3D shape, including the parameters of SMPL plus clothing and hair in 10 seconds or less. The model achieves fast and accurate predictions based on two key design choices. First, by predicting shape in a canonical T-pose space, the network learns to encode the images of the person into pose-invariant latent codes, where the information is fused. Second, based on the observation that feed-forward predictions are fast but do not always align with the input images, we predict using both, bottom-up and top-down streams (one per view) allowing information to flow in both directions. Learning relies only on synthetic 3D data. Once learned, Octopus can take a variable number of frames as input, and is able to reconstruct shapes even from a single image with an accuracy of 5mm. Results on 3 different datasets demonstrate the efficacy and accuracy of our approach.
count=1
* Exploiting Temporal Context for 3D Human Pose Estimation in the Wild
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Arnab_Exploiting_Temporal_Context_for_3D_Human_Pose_Estimation_in_the_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Arnab_Exploiting_Temporal_Context_for_3D_Human_Pose_Estimation_in_the_CVPR_2019_paper.pdf)]
    * Title: Exploiting Temporal Context for 3D Human Pose Estimation in the Wild
    * Year: `2019`
    * Authors: Anurag Arnab,  Carl Doersch,  Andrew Zisserman
    * Abstract: We present a bundle-adjustment-based algorithm for recovering accurate 3D human pose and meshes from monocular videos. Unlike previous algorithms which operate on single frames, we show that reconstructing a person over an entire sequence gives extra constraints that can resolve ambiguities. This is because videos often give multiple views of a person, yet the overall body shape does not change and 3D positions vary slowly. Our method improves not only on standard mocap-based datasets like Human 3.6M -- where we show quantitative improvements -- but also on challenging in-the-wild datasets such as Kinetics. Building upon our algorithm, we present a new dataset of more than 3 million frames of YouTube videos from Kinetics with automatically generated 3D poses and meshes. We show that retraining a single-frame 3D pose estimator on this data improves accuracy on both real-world and mocap data by evaluating on the 3DPW and HumanEVA datasets.
count=1
* Scan2CAD: Learning CAD Model Alignment in RGB-D Scans
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Avetisyan_Scan2CAD_Learning_CAD_Model_Alignment_in_RGB-D_Scans_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Avetisyan_Scan2CAD_Learning_CAD_Model_Alignment_in_RGB-D_Scans_CVPR_2019_paper.pdf)]
    * Title: Scan2CAD: Learning CAD Model Alignment in RGB-D Scans
    * Year: `2019`
    * Authors: Armen Avetisyan,  Manuel Dahnert,  Angela Dai,  Manolis Savva,  Angel X. Chang,  Matthias Niessner
    * Abstract: We present Scan2CAD, a novel data-driven method that learns to align clean 3D CAD models from a shape database to the noisy and incomplete geometry of a commodity RGB-D scan. For a 3D reconstruction of an indoor scene, our method takes as input a set of CAD models, and predicts a 9DoF pose that aligns each model to the underlying scan geometry. To tackle this problem, we create a new scan-to-CAD alignment dataset based on 1506 ScanNet scans with 97607 annotated keypoint pairs between 14225 CAD models from ShapeNet and their counterpart objects in the scans. Our method selects a set of representative keypoints in a 3D scan for which we find correspondences to the CAD geometry. To this end, we design a novel 3D CNN architecture that learns a joint embedding between real and synthetic objects, and from this predicts a correspondence heatmap. Based on these correspondence heatmaps, we formulate a variational energy minimization that aligns a given set of CAD models to the reconstruction. We evaluate our approach on our newly introduced Scan2CAD benchmark where we outperform both handcrafted feature descriptor as well as state-of-the-art CNN based methods by 21.39%.
count=1
* Pushing the Envelope for RGB-Based Dense 3D Hand Pose Estimation via Neural Rendering
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Baek_Pushing_the_Envelope_for_RGB-Based_Dense_3D_Hand_Pose_Estimation_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Baek_Pushing_the_Envelope_for_RGB-Based_Dense_3D_Hand_Pose_Estimation_CVPR_2019_paper.pdf)]
    * Title: Pushing the Envelope for RGB-Based Dense 3D Hand Pose Estimation via Neural Rendering
    * Year: `2019`
    * Authors: Seungryul Baek,  Kwang In Kim,  Tae-Kyun Kim
    * Abstract: Estimating 3D hand meshes from single RGB images is challenging, due to intrinsic 2D-3D mapping ambiguities and limited training data. We adopt a compact parametric 3D hand model that represents deformable and articulated hand meshes. To achieve the model fitting to RGB images, we investigate and contribute in three ways: 1) Neural rendering: inspired by recent work on human body, our hand mesh estimator (HME) is implemented by a neural network and a differentiable renderer, supervised by 2D segmentation masks and 3D skeletons. HME demonstrates good performance for estimating diverse hand shapes and improves pose estimation accuracies. 2) Iterative testing refinement: Our fitting function is differentiable. We iteratively refine the initial estimate using the gradients, in the spirit of iterative model fitting methods like ICP. The idea is supported by the latest research on human body. 3) Self-data augmentation: collecting sized RGB-mesh (or segmentation mask)-skeleton triplets for training is a big hurdle. Once the model is successfully fitted to input RGB images, its meshes i.e. shapes and articulations, are realistic, and we augment view-points on top of estimated dense hand poses. Experiments using three RGB-based benchmarks show that our framework offers beyond state-of-the-art accuracy in 3D pose estimation, as well as recovers dense 3D hand shapes. Each technical component above meaningfully improves the accuracy in the ablation study.
count=1
* Spectral Metric for Dataset Complexity Assessment
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Branchaud-Charron_Spectral_Metric_for_Dataset_Complexity_Assessment_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Branchaud-Charron_Spectral_Metric_for_Dataset_Complexity_Assessment_CVPR_2019_paper.pdf)]
    * Title: Spectral Metric for Dataset Complexity Assessment
    * Year: `2019`
    * Authors: Frederic Branchaud-Charron,  Andrew Achkar,  Pierre-Marc Jodoin
    * Abstract: In this paper, we propose a new measure to gauge the complexity of image classification problems. Given an annotated image dataset, our method computes a complexity measure called the cumulative spectral gradient (CSG) which strongly correlates with the test accuracy of convolutional neural networks (CNN). The CSG measure is derived from the probabilistic divergence between classes in a spectral clustering framework. We show that this metric correlates with the overall separability of the dataset and thus its inherent complexity. As will be shown, our metric can be used for dataset reduction, to assess which classes are more difficult to disentangle, and approximate the accuracy one could expect to get with a CNN. Results obtained on 11 datasets and three CNN models reveal that our method is more accurate and faster than previous complexity measures.
count=1
* Where's Wally Now? Deep Generative and Discriminative Embeddings for Novelty Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Burlina_Wheres_Wally_Now_Deep_Generative_and_Discriminative_Embeddings_for_Novelty_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Burlina_Wheres_Wally_Now_Deep_Generative_and_Discriminative_Embeddings_for_Novelty_CVPR_2019_paper.pdf)]
    * Title: Where's Wally Now? Deep Generative and Discriminative Embeddings for Novelty Detection
    * Year: `2019`
    * Authors: Philippe Burlina,  Neil Joshi,  I-Jeng Wang
    * Abstract: We develop a framework for novelty detection (ND) methods relying on deep embeddings, either discriminative or generative, and also propose a novel framework for assessing their performance. While much progress was made recently in these approaches, it has been accompanied by certain limitations: most methods were tested on relatively simple problems (low resolution images / small number of classes) or involved non-public data; comparative performance has often proven inconclusive because of lacking statistical significance; and evaluation has generally been done on non-canonical problem sets of differing complexity, making apples-to-apples comparative performance evaluation difficult. This has led to a relative confusing state of affairs. We address these challenges via the following contributions: We make a proposal for a novel framework to measure the performance of novelty detection methods using a trade-space demonstrating performance (measured by ROCAUC) as a function of problem complexity. We also make several proposals to formally characterize problem complexity. We conduct experiments with problems of higher complexity (higher image resolution / number of classes). To this end we design several canonical datasets built from CIFAR-10 and ImageNet (IN-125) which we make available to perform future benchmarks for novelty detection as well as other related tasks including semantic zero/adaptive shot and unsupervised learning. Finally, we demonstrate, as one of the methods in our ND framework, a generative novelty detection method whose performance exceeds that of all recent best-in-class generative ND methods.
count=1
* Hybrid Scene Compression for Visual Localization
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Camposeco_Hybrid_Scene_Compression_for_Visual_Localization_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Camposeco_Hybrid_Scene_Compression_for_Visual_Localization_CVPR_2019_paper.pdf)]
    * Title: Hybrid Scene Compression for Visual Localization
    * Year: `2019`
    * Authors: Federico Camposeco,  Andrea Cohen,  Marc Pollefeys,  Torsten Sattler
    * Abstract: Localizing an image w.r.t. a 3D scene model represents a core task for many computer vision applications. An increasing number of real-world applications of visual localization on mobile devices, e.g., Augmented Reality or autonomous robots such as drones or self-driving cars, demand localization approaches to minimize storage and bandwidth requirements. Compressing the 3D models used for localization thus becomes a practical necessity. In this work, we introduce a new hybrid compression algorithm that uses a given memory limit in a more effective way. Rather than treating all 3D points equally, it represents a small set of points with full appearance information and an additional, larger set of points with compressed information. This enables our approach to obtain a more complete scene representation without increasing the memory requirements, leading to a superior performance compared to previous compression schemes. As part of our contribution, we show how to handle ambiguous matches arising from point compression during RANSAC. Besides outperforming previous compression techniques in terms of pose accuracy under the same memory constraints, our compression scheme itself is also more efficient. Furthermore, the localization rates and accuracy obtained with our approach are comparable to state-of-the-art feature-based methods, while using a small fraction of the memory.
count=1
* End-To-End Learned Random Walker for Seeded Image Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Cerrone_End-To-End_Learned_Random_Walker_for_Seeded_Image_Segmentation_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Cerrone_End-To-End_Learned_Random_Walker_for_Seeded_Image_Segmentation_CVPR_2019_paper.pdf)]
    * Title: End-To-End Learned Random Walker for Seeded Image Segmentation
    * Year: `2019`
    * Authors: Lorenzo Cerrone,  Alexander Zeilmann,  Fred A. Hamprecht
    * Abstract: We present an end-to-end learned algorithm for seeded segmentation. Our method is based on the Random Walker algorithm, where we predict the edge weights of the un- derlying graph using a convolutional neural network. This can be interpreted as learning context-dependent diffusiv- ities for a linear diffusion process. After calculating the exact gradient for optimizing these diffusivities, we pro- pose simplifications that sparsely sample the gradient while still maintaining competitive results. The proposed method achieves the currently best results on the seeded CREMI neuron segmentation challenge.
count=1
* Joint Face Detection and Facial Motion Retargeting for Multiple Faces
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Chaudhuri_Joint_Face_Detection_and_Facial_Motion_Retargeting_for_Multiple_Faces_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Chaudhuri_Joint_Face_Detection_and_Facial_Motion_Retargeting_for_Multiple_Faces_CVPR_2019_paper.pdf)]
    * Title: Joint Face Detection and Facial Motion Retargeting for Multiple Faces
    * Year: `2019`
    * Authors: Bindita Chaudhuri,  Noranart Vesdapunt,  Baoyuan Wang
    * Abstract: Facial motion retargeting is an important problem in both computer graphics and vision, which involves capturing the performance of a human face and transferring it to another 3D character. Learning 3D morphable model (3DMM) parameters from 2D face images using convolutional neural networks is common in 2D face alignment, 3D face reconstruction etc. However, existing methods either require an additional face detection step before retargeting or use a cascade of separate networks to perform detection followed by retargeting in a sequence. In this paper, we present a single end-to-end network to jointly predict the bounding box locations and 3DMM parameters for multiple faces. First, we design a novel multitask learning framework that learns a disentangled representation of 3DMM parameters for a single face. Then, we leverage the trained single face model to generate ground truth 3DMM parameters for multiple faces to train another network that performs joint face detection and motion retargeting for images with multiple faces. Experimental results show that our joint detection and retargeting network has high face detection accuracy and is robust to extreme expressions and poses while being faster than state-of-the-art methods.
count=1
* Learning Multi-Class Segmentations From Single-Class Datasets
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Dmitriev_Learning_Multi-Class_Segmentations_From_Single-Class_Datasets_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Dmitriev_Learning_Multi-Class_Segmentations_From_Single-Class_Datasets_CVPR_2019_paper.pdf)]
    * Title: Learning Multi-Class Segmentations From Single-Class Datasets
    * Year: `2019`
    * Authors: Konstantin Dmitriev,  Arie E. Kaufman
    * Abstract: Multi-class segmentation has recently achieved significant performance in natural images and videos. This achievement is due primarily to the public availability of large multi-class datasets. However, there are certain domains, such as biomedical images, where obtaining sufficient multi-class annotations is a laborious and often impossible task and only single-class datasets are available. While existing segmentation research in such domains use private multi-class datasets or focus on single-class segmentations, we propose a unified highly efficient framework for robust simultaneous learning of multi-class segmentations by combining single-class datasets and utilizing a novel way of conditioning a convolutional network for the purpose of segmentation. We demonstrate various ways of incorporating the conditional information, perform an extensive evaluation, and show compelling multi-class segmentation performance on biomedical images, which outperforms current state-of-the-art solutions (up to 2.7%). Unlike current solutions, which are meticulously tailored for particular single-class datasets, we utilize datasets from a variety of sources. Furthermore, we show the applicability of our method also to natural images and evaluate it on the Cityscapes dataset. We further discuss other possible applications of our proposed framework.
count=1
* CrossInfoNet
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Du_CrossInfoNet_Multi-Task_Information_Sharing_Based_Hand_Pose_Estimation_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Du_CrossInfoNet_Multi-Task_Information_Sharing_Based_Hand_Pose_Estimation_CVPR_2019_paper.pdf)]
    * Title: CrossInfoNet: Multi-Task Information Sharing Based Hand Pose Estimation
    * Year: `2019`
    * Authors: Kuo Du,  Xiangbo Lin,  Yi Sun,  Xiaohong Ma
    * Abstract: This paper focuses on the topic of vision based hand pose estimation from single depth map using convolutional neural network (CNN). Our main contributions lie in designing a new pose regression network architecture named CrossInfoNet. The proposed CrossInfoNet decomposes hand pose estimation task into palm pose estimation sub-task and finger pose estimation sub-task, and adopts two-branch crossconnection structure to share the beneficial complementary information between the sub-tasks. Our work is inspired by multi-task information sharing mechanism, which has been few discussed in hand pose estimation using depth data in previous publications. In addition, we propose a heat-map guided feature extraction structure to get better feature maps, and train the complete network end-to-end. The effectiveness of the proposed CrossInfoNet is evaluated with extensively self-comparative experiments and in comparison with state-of-the-art methods on four public hand pose datasets. The code is available.
count=1
* Group-Wise Correlation Stereo Network
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Guo_Group-Wise_Correlation_Stereo_Network_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_Group-Wise_Correlation_Stereo_Network_CVPR_2019_paper.pdf)]
    * Title: Group-Wise Correlation Stereo Network
    * Year: `2019`
    * Authors: Xiaoyang Guo,  Kai Yang,  Wukui Yang,  Xiaogang Wang,  Hongsheng Li
    * Abstract: Stereo matching estimates the disparity between a rectified image pair, which is of great importance to depth sensing, autonomous driving, and other related tasks. Previous works built cost volumes with cross-correlation or concatenation of left and right features across all disparity levels, and then a 2D or 3D convolutional neural network is utilized to regress the disparity maps. In this paper, we propose to construct the cost volume by group-wise correlation. The left features and the right features are divided into groups along the channel dimension, and correlation maps are computed among each group to obtain multiple matching cost proposals, which are then packed into a cost volume. Group-wise correlation provides efficient representations for measuring feature similarities and will not lose too much information like full correlation. It also preserves better performance when reducing parameters compared with previous methods. The 3D stacked hourglass network proposed in previous works is improved to boost the performance and decrease the inference computational cost. Experiment results show that our method outperforms previous methods on Scene Flow, KITTI 2012, and KITTI 2015 datasets.
count=1
* Unsupervised Learning of Dense Shape Correspondence
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Halimi_Unsupervised_Learning_of_Dense_Shape_Correspondence_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Halimi_Unsupervised_Learning_of_Dense_Shape_Correspondence_CVPR_2019_paper.pdf)]
    * Title: Unsupervised Learning of Dense Shape Correspondence
    * Year: `2019`
    * Authors: Oshri Halimi,  Or Litany,  Emanuele Rodola,  Alex M. Bronstein,  Ron Kimmel
    * Abstract: We introduce the first completely unsupervised correspondence learning approach for deformable 3D shapes. Key to our model is the understanding that natural deformations (such as changes in pose) approximately preserve the metric structure of the surface, yielding a natural criterion to drive the learning process toward distortion-minimizing predictions. On this basis, we overcome the need for annotated data and replace it by a purely geometric criterion. The resulting learning model is class-agnostic, and is able to leverage any type of deformable geometric data for the training phase. In contrast to existing supervised approaches which specialize on the class seen at training time, we demonstrate stronger generalization as well as applicability to a variety of challenging settings. We showcase our method on a wide selection of correspondence benchmarks, where we outperform other methods in terms of accuracy, generalization, and efficiency.
count=1
* SAIL-VOS: Semantic Amodal Instance Level Video Object Segmentation - A Synthetic Dataset and Baselines
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Hu_SAIL-VOS_Semantic_Amodal_Instance_Level_Video_Object_Segmentation_-_A_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_SAIL-VOS_Semantic_Amodal_Instance_Level_Video_Object_Segmentation_-_A_CVPR_2019_paper.pdf)]
    * Title: SAIL-VOS: Semantic Amodal Instance Level Video Object Segmentation - A Synthetic Dataset and Baselines
    * Year: `2019`
    * Authors: Yuan-Ting Hu,  Hong-Shuo Chen,  Kexin Hui,  Jia-Bin Huang,  Alexander G. Schwing
    * Abstract: We introduce SAIL-VOS (Semantic Amodal Instance Level Video Object Segmentation), a new dataset aiming to stimulate semantic amodal segmentation research. Humans can effortlessly recognize partially occluded objects and reliably estimate their spatial extent beyond the visible. However, few modern computer vision techniques are capable of reasoning about occluded parts of an object. This is partly due to the fact that very few image datasets and no video dataset exist which permit development of those methods. To address this issue, we present a synthetic dataset extracted from the photo-realistic game GTA-V. Each frame is accompanied with densely annotated, pixel-accurate visible and amodal segmentation masks with semantic labels. More than 1.8M objects are annotated resulting in 100 times more annotations than existing datasets. We demonstrate the challenges of the dataset by quantifying the performance of several baselines. Data and additional material is available at http://sailvos.web.illinois.edu.
count=1
* Convolutional Mesh Regression for Single-Image Human Shape Reconstruction
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Kolotouros_Convolutional_Mesh_Regression_for_Single-Image_Human_Shape_Reconstruction_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolotouros_Convolutional_Mesh_Regression_for_Single-Image_Human_Shape_Reconstruction_CVPR_2019_paper.pdf)]
    * Title: Convolutional Mesh Regression for Single-Image Human Shape Reconstruction
    * Year: `2019`
    * Authors: Nikos Kolotouros,  Georgios Pavlakos,  Kostas Daniilidis
    * Abstract: This paper addresses the problem of 3D human pose and shape estimation from a single image. Previous approaches consider a parametric model of the human body, SMPL, and attempt to regress the model parameters that give rise to a mesh consistent with image evidence. This parameter regression has been a very challenging task, with model-based approaches underperforming compared to nonparametric solutions in terms of pose estimation. In our work, we propose to relax this heavy reliance on the model's parameter space. We still retain the topology of the SMPL template mesh, but instead of predicting model parameters, we directly regress the 3D location of the mesh vertices. This is a heavy task for a typical network, but our key insight is that the regression becomes significantly easier using a Graph-CNN. This architecture allows us to explicitly encode the template mesh structure within the network and leverage the spatial locality the mesh has to offer. Image-based features are attached to the mesh vertices and the Graph-CNN is responsible to process them on the mesh structure, while the regression target for each vertex is its 3D location. Having recovered the complete 3D geometry of the mesh, if we still require a specific model parametrization, this can be reliably regressed from the vertices locations. We demonstrate the flexibility and the effectiveness of our proposed graph-based mesh regression by attaching different types of features on the mesh vertices. In all cases, we outperform the comparable baselines relying on model parameter regression, while we also achieve state-of-the-art results among model-based pose estimation approaches.
count=1
* Do Better ImageNet Models Transfer Better?
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Kornblith_Do_Better_ImageNet_Models_Transfer_Better_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Kornblith_Do_Better_ImageNet_Models_Transfer_Better_CVPR_2019_paper.pdf)]
    * Title: Do Better ImageNet Models Transfer Better?
    * Year: `2019`
    * Authors: Simon Kornblith,  Jonathon Shlens,  Quoc V. Le
    * Abstract: Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.
count=1
* Im2Pencil: Controllable Pencil Illustration From Photographs
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Im2Pencil_Controllable_Pencil_Illustration_From_Photographs_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Im2Pencil_Controllable_Pencil_Illustration_From_Photographs_CVPR_2019_paper.pdf)]
    * Title: Im2Pencil: Controllable Pencil Illustration From Photographs
    * Year: `2019`
    * Authors: Yijun Li,  Chen Fang,  Aaron Hertzmann,  Eli Shechtman,  Ming-Hsuan Yang
    * Abstract: We propose a high-quality photo-to-pencil translation method with fine-grained control over the drawing style. This is a challenging task due to multiple stroke types (e.g., outline and shading), structural complexity of pencil shading (e.g., hatching), and the lack of aligned training data pairs. To address these challenges, we develop a two-branch model that learns separate filters for generating sketchy outlines and tonal shading from a collection of pencil drawings. We create training data pairs by extracting clean outlines and tonal illustrations from original pencil drawings using image filtering techniques, and we manually label the drawing styles. In addition, our model creates different pencil styles (e.g., line sketchiness and shading style) in a user-controllable manner. Experimental results on different types of pencil drawings show that the proposed algorithm performs favorably against existing methods in terms of quality, diversity and user evaluations.
count=1
* Orthogonal Decomposition Network for Pixel-Wise Binary Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Orthogonal_Decomposition_Network_for_Pixel-Wise_Binary_Classification_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Orthogonal_Decomposition_Network_for_Pixel-Wise_Binary_Classification_CVPR_2019_paper.pdf)]
    * Title: Orthogonal Decomposition Network for Pixel-Wise Binary Classification
    * Year: `2019`
    * Authors: Chang Liu,  Fang Wan,  Wei Ke,  Zhuowei Xiao,  Yuan Yao,  Xiaosong Zhang,  Qixiang Ye
    * Abstract: The weight sharing scheme and spatial pooling operations in Convolutional Neural Networks (CNNs) introduce semantic correlation to neighboring pixels on feature maps and therefore deteriorate their pixel-wise classification performance. In this paper, we implement an Orthogonal Decomposition Unit (ODU) that transforms a convolutional feature map into orthogonal bases targeting at de-correlating neighboring pixels on convolutional features. In theory, complete orthogonal decomposition produces orthogonal bases which can perfectly reconstruct any binary mask (ground-truth). In practice, we further design incomplete orthogonal decomposition focusing on de-correlating local patches which balances the reconstruction performance and computational cost. Fully Convolutional Networks (FCNs) implemented with ODUs, referred to as Orthogonal Decomposition Networks (ODNs), learn de-correlated and complementary convolutional features and fuse such features in a pixel-wise selective manner. Over pixel-wise binary classification tasks for two-dimensional image processing, specifically skeleton detection, edge detection, and saliency detection, and one-dimensional keypoint detection, specifically S-wave arrival time detection for earthquake localization, ODNs consistently improves the state-of-the-arts with significant margins.
count=1
* ContextDesc: Local Descriptor Augmentation With Cross-Modality Context
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Luo_ContextDesc_Local_Descriptor_Augmentation_With_Cross-Modality_Context_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Luo_ContextDesc_Local_Descriptor_Augmentation_With_Cross-Modality_Context_CVPR_2019_paper.pdf)]
    * Title: ContextDesc: Local Descriptor Augmentation With Cross-Modality Context
    * Year: `2019`
    * Authors: Zixin Luo,  Tianwei Shen,  Lei Zhou,  Jiahui Zhang,  Yao Yao,  Shiwei Li,  Tian Fang,  Long Quan
    * Abstract: Most existing studies on learning local features focus on the patch-based descriptions of individual keypoints, whereas neglecting the spatial relations established from their keypoint locations. In this paper, we go beyond the local detail representation by introducing context awareness to augment off-the-shelf local feature descriptors. Specifically, we propose a unified learning framework that leverages and aggregates the cross-modality contextual information, including (i) visual context from high-level image representation, and (ii) geometric context from 2D keypoint distribution. Moreover, we propose an effective N-pair loss that eschews the empirical hyper-parameter search and improves the convergence. The proposed augmentation scheme is lightweight compared with the raw local feature description, meanwhile improves remarkably on several large-scale benchmarks with diversified scenes, which demonstrates both strong practicality and generalization ability in geometric matching applications.
count=1
* Cross-Classification Clustering: An Efficient Multi-Object Tracking Technique for 3-D Instance Segmentation in Connectomics
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Meirovitch_Cross-Classification_Clustering_An_Efficient_Multi-Object_Tracking_Technique_for_3-D_Instance_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Meirovitch_Cross-Classification_Clustering_An_Efficient_Multi-Object_Tracking_Technique_for_3-D_Instance_CVPR_2019_paper.pdf)]
    * Title: Cross-Classification Clustering: An Efficient Multi-Object Tracking Technique for 3-D Instance Segmentation in Connectomics
    * Year: `2019`
    * Authors: Yaron Meirovitch,  Lu Mi,  Hayk Saribekyan,  Alexander Matveev,  David Rolnick,  Nir Shavit
    * Abstract: Pixel-accurate tracking of objects is a key element in many computer vision applications, often solved by iterated individual object tracking or instance segmentation followed by object matching. Here we introduce cross-classification clustering (3C), a technique that simultaneously tracks complex, interrelated objects in an image stack. The key idea in cross-classification is to efficiently turn a clustering problem into a classification problem by running a logarithmic number of independent classifications per image, letting the cross-labeling of these classifications uniquely classify each pixel to the object labels. We apply the 3C mechanism to achieve state-of-the-art accuracy in connectomics -- the nanoscale mapping of neural tissue from electron microscopy volumes. Our reconstruction system increases scalability by an order of magnitude over existing single-object tracking methods (such as flood-filling networks). This scalability is important for the deployment of connectomics pipelines, since currently the best performing techniques require computing infrastructures that are beyond the reach of most laboratories. Our algorithm may offer benefits in other domains that require pixel-accurate tracking of multiple objects, such as segmentation of videos and medical imagery.
count=1
* LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Meyer_LaserNet_An_Efficient_Probabilistic_3D_Object_Detector_for_Autonomous_Driving_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Meyer_LaserNet_An_Efficient_Probabilistic_3D_Object_Detector_for_Autonomous_Driving_CVPR_2019_paper.pdf)]
    * Title: LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving
    * Year: `2019`
    * Authors: Gregory P. Meyer,  Ankit Laddha,  Eric Kee,  Carlos Vallespi-Gonzalez,  Carl K. Wellington
    * Abstract: In this paper, we present LaserNet, a computationally efficient method for 3D object detection from LiDAR data for autonomous driving. The efficiency results from processing LiDAR data in the native range view of the sensor, where the input data is naturally compact. Operating in the range view involves well known challenges for learning, including occlusion and scale variation, but it also provides contextual information based on how the sensor data was captured. Our approach uses a fully convolutional network to predict a multimodal distribution over 3D boxes for each point and then it efficiently fuses these distributions to generate a prediction for each object. Experiments show that modeling each detection as a distribution rather than a single deterministic box leads to better overall detection performance. Benchmark results show that this approach has significantly lower runtime than other recent detectors and that it achieves state-of-the-art performance when compared on a large dataset that has enough data to overcome the challenges of training on the range view.
count=1
* PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Peng_PVNet_Pixel-Wise_Voting_Network_for_6DoF_Pose_Estimation_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Peng_PVNet_Pixel-Wise_Voting_Network_for_6DoF_Pose_Estimation_CVPR_2019_paper.pdf)]
    * Title: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation
    * Year: `2019`
    * Authors: Sida Peng,  Yuan Liu,  Qixing Huang,  Xiaowei Zhou,  Hujun Bao
    * Abstract: This paper addresses the challenge of 6DoF pose estimation from a single RGB image under severe occlusion or truncation. Many recent works have shown that a two-stage approach, which first detects keypoints and then solves a Perspective-n-Point (PnP) problem for pose estimation, achieves remarkable performance. However, most of these methods only localize a set of sparse keypoints by regressing their image coordinates or heatmaps, which are sensitive to occlusion and truncation. Instead, we introduce a Pixel-wise Voting Network (PVNet) to regress pixel-wise vectors pointing to the keypoints and use these vectors to vote for keypoint locations. This creates a flexible representation for localizing occluded or truncated keypoints. Another important feature of this representation is that it provides uncertainties of keypoint locations that can be further leveraged by the PnP solver. Experiments show that the proposed approach outperforms the state of the art on the LINEMOD, Occlusion LINEMOD and YCB-Video datasets by a large margin, while being efficient for real-time pose estimation. We further create a Truncation LINEMOD dataset to validate the robustness of our approach against truncation. The code is available at https://zju3dv.github.io/pvnet/.
count=1
* JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds With Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Pham_JSIS3D_Joint_Semantic-Instance_Segmentation_of_3D_Point_Clouds_With_Multi-Task_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Pham_JSIS3D_Joint_Semantic-Instance_Segmentation_of_3D_Point_Clouds_With_Multi-Task_CVPR_2019_paper.pdf)]
    * Title: JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds With Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields
    * Year: `2019`
    * Authors: Quang-Hieu Pham,  Thanh Nguyen,  Binh-Son Hua,  Gemma Roig,  Sai-Kit Yeung
    * Abstract: Deep learning techniques have become the to-go models for most vision-related tasks on 2D images. However, their power has not been fully realised on several tasks in 3D space, e.g., 3D scene understanding. In this work, we jointly address the problems of semantic and instance segmentation of 3D point clouds. Specifically, we develop a multi-task pointwise network that simultaneously performs two tasks: predicting the semantic classes of 3D points and embedding the points into high-dimensional vectors so that points of the same object instance are represented by similar embeddings. We then propose a multi-value conditional random field model to incorporate the semantic and instance labels and formulate the problem of semantic and instance segmentation as jointly optimising labels in the field model. The proposed method is thoroughly evaluated and compared with existing methods on different indoor scene datasets including S3DIS and SceneNN. Experimental results showed the robustness of the proposed joint semantic-instance segmentation scheme over its single components. Our method also achieved state-of-the-art performance on semantic segmentation.
count=1
* Seamless Scene Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Porzi_Seamless_Scene_Segmentation_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Porzi_Seamless_Scene_Segmentation_CVPR_2019_paper.pdf)]
    * Title: Seamless Scene Segmentation
    * Year: `2019`
    * Authors: Lorenzo Porzi,  Samuel Rota Bulo,  Aleksander Colovic,  Peter Kontschieder
    * Abstract: In this work we introduce a novel, CNN-based architecture that can be trained end-to-end to deliver seamless scene segmentation results. Our goal is to predict consistent semantic segmentation and detection results by means of a panoptic output format, going beyond the simple combination of independently trained segmentation and detection models. The proposed architecture takes advantage of a novel segmentation head that seamlessly integrates multi-scale features generated by a Feature Pyramid Network with contextual information conveyed by a light-weight DeepLab-like module. As additional contribution we review the panoptic metric and propose an alternative that overcomes its limitations when evaluating non-instance categories. Our proposed network architecture yields state-of-the-art results on three challenging street-level datasets, i.e. Cityscapes, Indian Driving Dataset and Mapillary Vistas.
count=1
* Scene Categorization From Contours: Medial Axis Based Salience Measures
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Rezanejad_Scene_Categorization_From_Contours_Medial_Axis_Based_Salience_Measures_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Rezanejad_Scene_Categorization_From_Contours_Medial_Axis_Based_Salience_Measures_CVPR_2019_paper.pdf)]
    * Title: Scene Categorization From Contours: Medial Axis Based Salience Measures
    * Year: `2019`
    * Authors: Morteza Rezanejad,  Gabriel Downs,  John Wilder,  Dirk B. Walther,  Allan Jepson,  Sven Dickinson,  Kaleem Siddiqi
    * Abstract: The computer vision community has witnessed recent advances in scene categorization from images, with the state of the art systems now achieving impressive recognition rates on challenging benchmarks. Such systems have been trained on photographs which include color, texture and shading cues. The geometry of shapes and surfaces, as conveyed by scene contours, is not explicitly considered for this task. Remarkably, humans can accurately recognize natural scenes from line drawings, which consist solely of contour-based shape cues. Here we report the first computer vision study on scene categorization of line drawings derived from popular databases including an artist scene database, MIT67 and Places365. Specifically, we use off-the-shelf pre-trained Convolutional Neural Networks (CNNs) to perform scene classification given only contour information as input, and find performance levels well above chance. We also show that medial-axis based contour salience methods can be used to select more informative subsets of contour pixels, and that the variation in CNN classification performance on various choices for these subsets is qualitatively similar to that observed in human performance. Moreover, when the salience measures are used to weight the contours, we find that these weights boost our CNN performance above that for unweighted contour input. That is, the medial axis based salience weights appear to add useful information that is not available when CNNs are trained to use contours alone.
count=1
* Inverse Cooking: Recipe Generation From Food Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Salvador_Inverse_Cooking_Recipe_Generation_From_Food_Images_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Salvador_Inverse_Cooking_Recipe_Generation_From_Food_Images_CVPR_2019_paper.pdf)]
    * Title: Inverse Cooking: Recipe Generation From Food Images
    * Year: `2019`
    * Authors: Amaia Salvador,  Michal Drozdzal,  Xavier Giro-i-Nieto,  Adriana Romero
    * Abstract: People enjoy food photography because they appreciate food. Behind each meal there is a story described in a complex recipe and, unfortunately, by simply looking at a food image we do not have access to its preparation process. Therefore, in this paper we introduce an inverse cooking system that recreates cooking recipes given food images. Our system predicts ingredients as sets by means of a novel architecture, modeling their dependencies without imposing any order, and then generates cooking instructions by attending to both image and its inferred ingredients simultaneously. We extensively evaluate the whole system on the large-scale Recipe1M dataset and show that (1) we improve performance w.r.t. previous baselines for ingredient prediction; (2) we are able to obtain high quality recipes by leveraging both image and ingredients; (3) our system is able to produce more compelling recipes than retrieval-based approaches according to human judgment. We make code and models publicly available.
count=1
* Understanding the Limitations of CNN-Based Absolute Camera Pose Regression
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Sattler_Understanding_the_Limitations_of_CNN-Based_Absolute_Camera_Pose_Regression_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Sattler_Understanding_the_Limitations_of_CNN-Based_Absolute_Camera_Pose_Regression_CVPR_2019_paper.pdf)]
    * Title: Understanding the Limitations of CNN-Based Absolute Camera Pose Regression
    * Year: `2019`
    * Authors: Torsten Sattler,  Qunjie Zhou,  Marc Pollefeys,  Laura Leal-Taixe
    * Abstract: Visual localization is the task of accurate camera pose estimation in a known scene. It is a key problem in computer vision and robotics, with applications including self-driving cars, Structure-from-Motion, SLAM, and Mixed Reality. Traditionally, the localization problem has been tackled using 3D geometry. Recently, end-to-end approaches based on convolutional neural networks have become popular. These methods learn to directly regress the camera pose from an input image. However, they do not achieve the same level of pose accuracy as 3D structure-based methods. To understand this behavior, we develop a theoretical model for camera pose regression. We use our model to predict failure cases for pose regression techniques and verify our predictions through experiments. We furthermore use our model to show that pose regression is more closely related to pose approximation via image retrieval than to accurate pose estimation via 3D structure. A key result is that current approaches do not consistently outperform a handcrafted image retrieval baseline. This clearly shows that additional research is needed before pose regression algorithms are ready to compete with structure-based methods.
count=1
* Residual Regression With Semantic Prior for Crowd Counting
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Wan_Residual_Regression_With_Semantic_Prior_for_Crowd_Counting_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wan_Residual_Regression_With_Semantic_Prior_for_Crowd_Counting_CVPR_2019_paper.pdf)]
    * Title: Residual Regression With Semantic Prior for Crowd Counting
    * Year: `2019`
    * Authors: Jia Wan,  Wenhan Luo,  Baoyuan Wu,  Antoni B. Chan,  Wei Liu
    * Abstract: Crowd counting is a challenging task due to factors such as large variations in crowdedness and severe occlusions. Although recent deep learning based counting algorithms have achieved a great progress, the correlation knowledge among samples and the semantic prior have not yet been fully exploited. In this paper, a residual regression framework is proposed for crowd counting utilizing the correlation information among samples. By incorporating such information into our network, we discover that more intrinsic characteristics can be learned by the network which thus generalizes better to unseen scenarios. Besides, we show how to effectively leverage the semantic prior to improve the performance of crowd counting. We also observe that the adversarial loss can be used to improve the quality of predicted density maps, thus leading to an improvement in crowd counting. Experiments on public datasets demonstrate the effectiveness and generalization ability of the proposed method.
count=1
* Visual Localization by Learning Objects-Of-Interest Dense Match Regression
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Weinzaepfel_Visual_Localization_by_Learning_Objects-Of-Interest_Dense_Match_Regression_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Weinzaepfel_Visual_Localization_by_Learning_Objects-Of-Interest_Dense_Match_Regression_CVPR_2019_paper.pdf)]
    * Title: Visual Localization by Learning Objects-Of-Interest Dense Match Regression
    * Year: `2019`
    * Authors: Philippe Weinzaepfel,  Gabriela Csurka,  Yohann Cabon,  Martin Humenberger
    * Abstract: We introduce a novel CNN-based approach for visual localization from a single RGB image that relies on densely matching a set of Objects-of-Interest (OOIs). In this paper, we focus on planar objects which are highly descriptive in an environment, such as paintings in museums or logos and storefronts in malls or airports. For each OOI, we define a reference image for which 3D world coordinates are available. Given a query image, our CNN model detects the OOIs, segments them and finds a dense set of 2D-2D matches between each detected OOI and its corresponding reference image. Given these 2D-2D matches, together with the 3D world coordinates of each reference image, we obtain a set of 2D-3D matches from which solving a Perspective-n-Point problem gives a pose estimate. We show that 2D-3D matches for reference images, as well as OOI annotations can be obtained for all training images from a single instance annotation per OOI by leveraging Structure-from-Motion reconstruction. We introduce a novel synthetic dataset, VirtualGallery, which targets challenges such as varying lighting conditions and different occlusion levels. Our results show that our method achieves high precision and is robust to these challenges. We also experiment using the Baidu localization dataset captured in a shopping mall. Our approach is the first deep regression-based method to scale to such a larger environment.
count=1
* Attentive Region Embedding Network for Zero-Shot Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Xie_Attentive_Region_Embedding_Network_for_Zero-Shot_Learning_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Xie_Attentive_Region_Embedding_Network_for_Zero-Shot_Learning_CVPR_2019_paper.pdf)]
    * Title: Attentive Region Embedding Network for Zero-Shot Learning
    * Year: `2019`
    * Authors: Guo-Sen Xie,  Li Liu,  Xiaobo Jin,  Fan Zhu,  Zheng Zhang,  Jie Qin,  Yazhou Yao,  Ling Shao
    * Abstract: Zero-shot learning (ZSL) aims to classify images from unseen categories, by merely utilizing seen class images as the training data. Existing works on ZSL mainly leverage the global features or learn the global regions, from which, to construct the embeddings to the semantic space. However, few of them study the discrimination power implied in local image regions (parts), which, in some sense, correspond to semantic attributes, have stronger discrimination than attributes, and can thus assist the semantic transfer between seen/unseen classes. In this paper, to discover (semantic) regions, we propose the attentive region embedding network (AREN), which is tailored to advance the ZSL task. Specifically, AREN is end-to-end trainable and consists of two network branches, i.e., the attentive region embedding (ARE) stream, and the attentive compressed second-order embedding (ACSE) stream. ARE is capable of discovering multiple part regions under the guidance of the attention and the compatibility loss. Moreover, a novel adaptive thresholding mechanism is proposed for suppressing redundant (such as background) attention regions. To further guarantee more stable semantic transfer from the perspective of second-order collaboration, ACSE is incorporated into the AREN. In the comprehensive evaluations on four benchmarks, our models achieve state-of-the-art performances under ZSL setting, and compelling results under generalized ZSL setting.
count=1
* FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose Estimation From a Single Image
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Yang_FSA-Net_Learning_Fine-Grained_Structure_Aggregation_for_Head_Pose_Estimation_From_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_FSA-Net_Learning_Fine-Grained_Structure_Aggregation_for_Head_Pose_Estimation_From_CVPR_2019_paper.pdf)]
    * Title: FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose Estimation From a Single Image
    * Year: `2019`
    * Authors: Tsun-Yi Yang,  Yi-Ting Chen,  Yen-Yu Lin,  Yung-Yu Chuang
    * Abstract: This paper proposes a method for head pose estimation from a single image. Previous methods often predict head poses through landmark or depth estimation and would require more computation than necessary. Our method is based on regression and feature aggregation. For having a compact model, we employ the soft stagewise regression scheme. Existing feature aggregation methods treat inputs as a bag of features and thus ignore their spatial relationship in a feature map. We propose to learn a fine-grained structure mapping for spatially grouping features before aggregation. The fine-grained structure provides part-based information and pooled values. By utilizing learnable and non-learnable importance over the spatial location, different model variants can be generated and form a complementary ensemble. Experiments show that our method outperforms the state-of-the-art methods including both the landmark-free ones and the ones based on landmark or depth estimation. With only a single RGB frame as input, our method even outperforms methods utilizing multi-modality information (RGB-D, RGB-Time) on estimating the yaw angle. Furthermore, the memory overhead of our model is 100 times smaller than those of previous methods.
count=1
* AE2-Nets: Autoencoder in Autoencoder Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_AE2-Nets_Autoencoder_in_Autoencoder_Networks_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_AE2-Nets_Autoencoder_in_Autoencoder_Networks_CVPR_2019_paper.pdf)]
    * Title: AE2-Nets: Autoencoder in Autoencoder Networks
    * Year: `2019`
    * Authors: Changqing Zhang,  Yeqing Liu,  Huazhu Fu
    * Abstract: Learning on data represented with multiple views (e.g., multiple types of descriptors or modalities) is a rapidly growing direction in machine learning and computer vision. Although effectiveness achieved, most existing algorithms usually focus on classification or clustering tasks. Differently, in this paper, we focus on unsupervised representation learning and propose a novel framework termed Autoencoder in Autoencoder Networks (AE^2-Nets), which integrates information from heterogeneous sources into an intact representation by the nested autoencoder framework. The proposed method has the following merits: (1) our model jointly performs view-specific representation learning (with the inner autoencoder networks) and multi-view information encoding (with the outer autoencoder networks) in a unified framework; (2) due to the degradation process from the latent representation to each single view, our model flexibly balances the complementarity and consistence among multiple views. The proposed model is efficiently solved by the alternating direction method (ADM), and demonstrates the effectiveness compared with state-of-the-art algorithms.
count=1
* MetaCleaner: Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_MetaCleaner_Learning_to_Hallucinate_Clean_Representations_for_Noisy-Labeled_Visual_Recognition_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_MetaCleaner_Learning_to_Hallucinate_Clean_Representations_for_Noisy-Labeled_Visual_Recognition_CVPR_2019_paper.pdf)]
    * Title: MetaCleaner: Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition
    * Year: `2019`
    * Authors: Weihe Zhang,  Yali Wang,  Yu Qiao
    * Abstract: Deep Neural Networks (DNNs) have achieved remarkable successes in large-scale visual recognition. However, they often suffer from overfitting under noisy labels. To alleviate this problem, we propose a conceptually simple but effective MetaCleaner, which can learn to hallucinate a clean representation of an object category, according to a small noisy subset from the same category. Specially, MetaCleaner consists of two flexible submodules. The first submodule, namely Noisy Weighting, can estimate the confidence scores of all the images in the noisy subset, by analyzing their deep features jointly. The second submodule, namely Clean Hallucinating, can generate a clean representation from the noisy subset, by summarizing the noisy images with their confidence scores. Via MetaCleaner, DNNs can strengthen its robustness to noisy labels, as well as enhance its generalization capacity with richer data diversity. Moreover, MetaCleaner can be easily integrated into the standard training procedure of DNNs, which promotes its value for real-life applications. We conduct extensive experiments on two popular benchmarks in noisy-labeled recognition, i.e., Food-101N and Clothing1M. For both datasets, our MetaCleaner significantly outperforms baselines, and achieves the state-of-the-art performance.
count=1
* Mind Your Neighbours: Image Annotation With Metadata Neighbourhood Graph Co-Attention Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Mind_Your_Neighbours_Image_Annotation_With_Metadata_Neighbourhood_Graph_Co-Attention_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Mind_Your_Neighbours_Image_Annotation_With_Metadata_Neighbourhood_Graph_Co-Attention_CVPR_2019_paper.pdf)]
    * Title: Mind Your Neighbours: Image Annotation With Metadata Neighbourhood Graph Co-Attention Networks
    * Year: `2019`
    * Authors: Junjie Zhang,  Qi Wu,  Jian Zhang,  Chunhua Shen,  Jianfeng Lu
    * Abstract: As the visual reflections of our daily lives, images are frequently shared on the social network, which generates the abundant 'metadata' that records user interactions with images. Due to the diverse contents and complex styles, some images can be challenging to recognise when neglecting the context. Images with the similar metadata, such as 'relevant topics and textual descriptions', 'common friends of users' and 'nearby locations', form a neighbourhood for each image, which can be used to assist the annotation. In this paper, we propose a Metadata Neighbourhood Graph Co-Attention Network (MangoNet) to model the correlations between each target image and its neighbours. To accurately capture the visual clues from the neighbourhood, a co-attention mechanism is introduced to embed the target image and its neighbours as graph nodes, while the graph edges capture the node pair correlations. By reasoning on the neighbourhood graph, we obtain the graph representation to help annotate the target image. Experimental results on three benchmark datasets indicate that our proposed model achieves the best performance compared to the state-of-the-art methods.
count=1
* Wide-Area Crowd Counting via Ground-Plane Density Maps and Multi-View Fusion CNNs
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Wide-Area_Crowd_Counting_via_Ground-Plane_Density_Maps_and_Multi-View_Fusion_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Wide-Area_Crowd_Counting_via_Ground-Plane_Density_Maps_and_Multi-View_Fusion_CVPR_2019_paper.pdf)]
    * Title: Wide-Area Crowd Counting via Ground-Plane Density Maps and Multi-View Fusion CNNs
    * Year: `2019`
    * Authors: Qi Zhang,  Antoni B. Chan
    * Abstract: Crowd counting in single-view images has achieved outstanding performance on existing counting datasets. However, single-view counting is not applicable to large and wide scenes (e.g., public parks, long subway platforms, or event spaces) because a single camera cannot capture the whole scene in adequate detail for counting, e.g., when the scene is too large to fit into the field-of-view of the camera, too long so that the resolution is too low on faraway crowds, or when there are too many large objects that occlude large portions of the crowd. Therefore, to solve the wide-area counting task requires multiple cameras with overlapping fields-of-view. In this paper, we propose a deep neural network framework for multi-view crowd counting, which fuses information from multiple camera views to predict a scene-level density map on the ground-plane of the 3D world. We consider 3 versions of the fusion framework: the late fusion model fuses camera-view density map; the naive early fusion model fuses camera-view feature maps; and the multi-view multi-scale early fusion model favors that features aligned to the same ground-plane point have consistent scales. We test our 3 fusion models on 3 multi-view counting datasets, PETS2009, DukeMTMC, and a newly collected multi-view counting dataset containing a crowded street intersection. Our methods achieve state-of-the-art results compared to other multi-view counting baselines.
count=1
* Leveraging Heterogeneous Auxiliary Tasks to Assist Crowd Counting
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Leveraging_Heterogeneous_Auxiliary_Tasks_to_Assist_Crowd_Counting_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_Leveraging_Heterogeneous_Auxiliary_Tasks_to_Assist_Crowd_Counting_CVPR_2019_paper.pdf)]
    * Title: Leveraging Heterogeneous Auxiliary Tasks to Assist Crowd Counting
    * Year: `2019`
    * Authors: Muming Zhao,  Jian Zhang,  Chongyang Zhang,  Wenjun Zhang
    * Abstract: Crowd counting is a challenging task in the presence of drastic scale variations, the clutter background, and severe occlusions, etc. Existing CNN-based counting methods tackle these challenges mainly by fusing either multi-scale or multi-context features to generate robust representations. In this paper, we propose to address these issues by leveraging the heterogeneous attributes compounded in the density map. We identify three geometric/semantic/numeric attributes essentially important to the density estimation, and demonstrate how to effectively utilize these heterogeneous attributes to assist the crowd counting by formulating them into multiple auxiliary tasks. With the multi-fold regularization effects induced by the auxiliary tasks, the backbone CNN model is driven to embed desired properties explicitly and thus gains robust representations towards more accurate density estimation. Extensive experiments on three challenging crowd counting datasets have demonstrated the effectiveness of the proposed approach.
count=1
* Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Binary_Ensemble_Neural_Network_More_Bits_per_Network_or_More_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Binary_Ensemble_Neural_Network_More_Bits_per_Network_or_More_CVPR_2019_paper.pdf)]
    * Title: Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?
    * Year: `2019`
    * Authors: Shilin Zhu,  Xin Dong,  Hao Su
    * Abstract: Binary neural networks (BNN) have been studied extensively since they run dramatically faster at lower memory and power consumption than floating-point networks, thanks to the efficiency of bit operations. However, contemporary BNNs whose weights and activations are both single bits suffer from severe accuracy degradation. To understand why, we investigate the representation ability, speed and bias/variance of BNNs through extensive experiments. We conclude that the error of BNNs is predominantly caused by intrinsic instability (training time) and non-robustness (train & test time). Inspired by this investigation, we propose the Binary Ensemble Neural Network (BENN) which leverages ensemble methods to improve the performance of BNNs with limited efficiency cost. While ensemble techniques have been broadly believed to be only marginally helpful for strong classifiers such as deep neural networks, our analysis and experiments show that they are naturally a perfect fit to boost BNNs. We find that our BENN, which is faster and more robust than state-of-the-art binary networks, can even surpass the accuracy of the full-precision floating number network with the same architecture.
count=1
* Robust Facial Landmark Detection via Occlusion-Adaptive Deep Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Robust_Facial_Landmark_Detection_via_Occlusion-Adaptive_Deep_Networks_CVPR_2019_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Robust_Facial_Landmark_Detection_via_Occlusion-Adaptive_Deep_Networks_CVPR_2019_paper.pdf)]
    * Title: Robust Facial Landmark Detection via Occlusion-Adaptive Deep Networks
    * Year: `2019`
    * Authors: Meilu Zhu,  Daming Shi,  Mingjie Zheng,  Muhammad Sadiq
    * Abstract: In this paper, we present a simple and effective framework called Occlusion-adaptive Deep Networks (ODN) with the purpose of solving the occlusion problem for facial landmark detection. In this model, the occlusion probability of each position in high-level features are inferred by a distillation module that can be learnt automatically in the process of estimating the relationship between facial appearance and facial shape. The occlusion probability serves as the adaptive weight on high-level features to reduce the impact of occlusion and obtain clean feature representation. Nevertheless, the clean feature representation cannot represent the holistic face due to the missing semantic features. To obtain exhaustive and complete feature representation, it is vital that we leverage a low-rank learning module to recover lost features. Considering that facial geometric characteristics are conducive to the low-rank module to recover lost features, we propose a geometry-aware module to excavate geometric relationships between different facial components. Depending on the synergistic effect of three modules, the proposed network achieves better performance in comparison to state-of-the-art methods on challenging benchmark datasets.
count=1
* Augmented Skeleton Space Transfer for Depth-Based Hand Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Baek_Augmented_Skeleton_Space_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Baek_Augmented_Skeleton_Space_CVPR_2018_paper.pdf)]
    * Title: Augmented Skeleton Space Transfer for Depth-Based Hand Pose Estimation
    * Year: `2018`
    * Authors: Seungryul Baek, Kwang In Kim, Tae-Kyun Kim
    * Abstract: Crucial to the success of training a depth-based 3D hand pose estimator (HPE) is the availability of comprehensive datasets covering diverse camera perspectives, shapes, and pose variations. However, collecting such annotated datasets is challenging. We propose to complete existing databases by generating new database entries. The key idea is to synthesize data in the skeleton space (instead of doing so in the depth-map space) which enables an easy and intuitive way of manipulating data entries. Since the skeleton entries generated in this way do not have the corresponding depth map entries, we exploit them by training a separate hand pose generator (HPG) which synthesizes the depth map from the skeleton entries. By training the HPG and HPE in a single unified optimization framework enforcing that 1) the HPE agrees with the paired depth and skeleton entries; and 2) the HPG-HPE combination satisfies the cyclic consistency (both the input and the output of HPG-HPE are skeletons) observed via the newly generated unpaired skeletons, our algorithm constructs a HPE which is robust to variations that go beyond the coverage of the existing database. Our training algorithm adopts the generative adversarial networks (GAN) training process. As a by-product, we obtain a hand pose discriminator (HPD) that is capable of picking out realistic hand poses. Our algorithm exploits this capability to refine the initial skeleton estimates in testing, further improving the accuracy. We test our algorithm on four challenging benchmark datasets (ICVL, MSRA, NYU and Big Hand 2.2M datasets) and demonstrate that our approach outperforms or is on par with state-of-the-art methods quantitatively and qualitatively.
count=1
* Visual Feature Attribution Using Wasserstein GANs
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Baumgartner_Visual_Feature_Attribution_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Baumgartner_Visual_Feature_Attribution_CVPR_2018_paper.pdf)]
    * Title: Visual Feature Attribution Using Wasserstein GANs
    * Year: `2018`
    * Authors: Christian F. Baumgartner, Lisa M. Koch, Kerem Can Tezcan, Jia Xi Ang, Ender Konukoglu
    * Abstract: Attributing the pixels of an input image to a certain category is an important and well-studied problem in computer vision, with applications ranging from weakly supervised localisation to understanding hidden effects in the data. In recent years, approaches based on interpreting a previously trained neural network classifier have become the de facto state-of-the-art and are commonly used on medical as well as natural image datasets. In this paper, we discuss a limitation of these approaches which may lead to only a subset of the category specific features being detected. To address this problem we develop a novel feature attribution technique based on Wasserstein Generative Adversarial Networks (WGAN), which does not suffer from this limitation. We show that our proposed method performs substantially better than the state-of-the-art for visual attribution on a synthetic dataset and on real 3D neuroimaging data from patients with mild cognitive impairment (MCI) and Alzheimer's disease (AD). For AD patients the method produces compellingly realistic disease effect maps which are very close to the observed effects.
count=1
* Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.pdf)]
    * Title: Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation
    * Year: `2018`
    * Authors: Piotr Bilinski, Victor Prisacariu
    * Abstract: We propose a novel end-to-end trainable, deep, encoder-decoder architecture for single-pass semantic segmentation. Our approach is based on a cascaded architecture with feature-level long-range skip connections. The encoder incorporates the structure of ResNeXt's residual building blocks and adopts the strategy of repeating a building block that aggregates a set of transformations with the same topology. The decoder features a novel architecture, consisting of blocks, that (i) capture context information, (ii) generate semantic features, and (iii) enable fusion between different output resolutions. Crucially, we introduce dense decoder shortcut connections to allow decoder blocks to use semantic feature maps from all previous decoder levels, i.e. from all higher-level feature maps. The dense decoder connections allow for effective information propagation from one decoder block to another, as well as for multi-level feature fusion that significantly improves the accuracy. Importantly, these connections allow our method to obtain state-of-the-art performance on several challenging datasets, without the need of time-consuming multi-scale averaging of previous works.
count=1
* Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Cui_Large_Scale_Fine-Grained_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Cui_Large_Scale_Fine-Grained_CVPR_2018_paper.pdf)]
    * Title: Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning
    * Year: `2018`
    * Authors: Yin Cui, Yang Song, Chen Sun, Andrew Howard, Serge Belongie
    * Abstract: Transferring the knowledge learned from large scale datasets (e.g., ImageNet) via fine-tuning offers an effective solution for domain-specific fine-grained visual categorization (FGVC) tasks (e.g., recognizing bird species or car make & model). In such scenarios, data annotation often calls for specialized domain knowledge and thus is difficult to scale. In this work, we first tackle a problem in large scale FGVC. Our method won first place in iNaturalist 2017 large scale species classification challenge. Central to the success of our approach is a training scheme that uses higher image resolution and deals with the long-tailed distribution of training data. Next, we study transfer learning via fine-tuning from large scale datasets to small scale, domain-specific FGVC datasets. We propose a measure to estimate domain similarity via Earth Mover's Distance and demonstrate that transfer learning benefits from pre-training on a source domain that is similar to the target domain by this measure. Our proposed transfer learning outperforms ImageNet pre-training and obtains state-of-the-art results on multiple commonly used FGVC datasets.
count=1
* Wing Loss for Robust Facial Landmark Localisation With Convolutional Neural Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Feng_Wing_Loss_for_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Feng_Wing_Loss_for_CVPR_2018_paper.pdf)]
    * Title: Wing Loss for Robust Facial Landmark Localisation With Convolutional Neural Networks
    * Year: `2018`
    * Authors: Zhen-Hua Feng, Josef Kittler, Muhammad Awais, Patrik Huber, Xiao-Jun Wu
    * Abstract: We present a new loss function, namely Wing loss, for robust facial landmark localisation with Convolutional Neural Networks (CNNs). We first compare and analyse different loss functions including L2, L1 and smooth L1. The analysis of these loss functions suggests that, for the training of a CNN-based localisation model, more attention should be paid to small and medium range errors. To this end, we design a piece-wise loss function. The new loss amplifies the impact of errors from the interval (-w, w) by switching from L1 loss to a modified logarithm function. To address the problem of under-representation of samples with large out-of-plane head rotations in the training set, we propose a simple but effective boosting strategy, referred to as pose-based data balancing. In particular, we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation, bounding box translation and other data augmentation approaches. Last, the proposed approach is extended to create a two-stage framework for robust facial landmark localisation. The experimental results obtained on AFLW and 300W demonstrate the merits of the Wing loss function, and prove the superiority of the proposed method over the state-of-the-art approaches.
count=1
* DiverseNet: When One Right Answer Is Not Enough
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Firman_DiverseNet_When_One_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Firman_DiverseNet_When_One_CVPR_2018_paper.pdf)]
    * Title: DiverseNet: When One Right Answer Is Not Enough
    * Year: `2018`
    * Authors: Michael Firman, Neill D. F. Campbell, Lourdes Agapito, Gabriel J. Brostow
    * Abstract: Many structured prediction tasks in machine vision have a collection of acceptable answers, instead of one definitive ground truth answer. Segmentation of images, for example, is subject to human labeling bias. Similarly, there are multiple possible pixel values that could plausibly complete occluded image regions. State-of-the art supervised learning methods are typically optimized to make a single test-time prediction for each query, failing to find other modes in the output space. Existing methods that allow for sampling often sacrifice speed or accuracy. We introduce a simple method for training a neural network, which enables diverse structured predictions to be made for each test-time query. For a single input, we learn to predict a range of possible answers. We compare favorably to methods that seek diversity through an ensemble of networks. Such stochastic multiple choice learning faces mode collapse, where one or more ensemble members fail to receive any training signal. Our best performing solution can be deployed for various tasks, and just involves small modifications to the existing single-mode architecture, loss function, and training regime. We demonstrate that our method results in quantitative improvements across three challenging tasks: 2D image completion, 3D volume estimation, and flow prediction.
count=1
* First-Person Hand Action Benchmark With RGB-D Videos and 3D Hand Pose Annotations
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Garcia-Hernando_First-Person_Hand_Action_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Garcia-Hernando_First-Person_Hand_Action_CVPR_2018_paper.pdf)]
    * Title: First-Person Hand Action Benchmark With RGB-D Videos and 3D Hand Pose Annotations
    * Year: `2018`
    * Authors: Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, Tae-Kyun Kim
    * Abstract: In this work we study the use of 3D hand poses to recognize first-person dynamic hand actions interacting with 3D objects. Towards this goal, we collected RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations. To obtain hand pose annotations, we used our own mo-cap system that automatically infers the 3D location of each of the 21 joints of a hand model via 6 magnetic sensors and inverse kinematics. Additionally, we recorded the 6D object poses and provide 3D object models for a subset of hand-object interaction sequences. To the best of our knowledge, this is the first benchmark that enables the study of first-person hand actions with the use of 3D hand poses. We present an extensive experimental evaluation of RGB-D and pose-based action recognition by 18 baselines/state-of-the-art approaches. The impact of using appearance features, poses, and their combinations are measured, and the different training/testing protocols are evaluated. Finally, we assess how ready the 3D hand pose estimation field is when hands are severely occluded by objects in egocentric views and its influence on action recognition. From the results, we see clear benefits of using hand pose as a cue for action recognition compared to other data modalities. Our dataset and experiments can be of interest to communities of 3D hand pose estimation, 6D object pose, and robotics as well as action recognition.
count=1
* Going From Image to Video Saliency: Augmenting Image Salience With Dynamic Attentional Push
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Gorji_Going_From_Image_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gorji_Going_From_Image_CVPR_2018_paper.pdf)]
    * Title: Going From Image to Video Saliency: Augmenting Image Salience With Dynamic Attentional Push
    * Year: `2018`
    * Authors: Siavash Gorji, James J. Clark
    * Abstract: We present a novel method to incorporate the recent advent in static saliency models to predict the saliency in videos. Our model augments the static saliency models with the Attentional Push effect of the photographer and the scene actors in a shared attention setting. We demonstrate that not only it is imperative to use static Attentional Push cues, noticeable performance improvement is achievable by learning the time-varying nature of Attentional Push. We propose a multi-stream Convolutional Long Short-Term Memory network (ConvLSTM) structure which augments state-of-the-art in static saliency models with dynamic Attentional Push. Our network contains four pathways, a saliency pathway and three Attentional Push pathways. The multi-pathway structure is followed by an augmenting convnet that learns to combine the complementary and time-varying outputs of the ConvLSTMs by minimizing the relative entropy between the augmented saliency and viewers fixation patterns on videos. We evaluate our model by comparing the performance of several augmented static saliency models with state-of-the-art in spatiotemporal saliency on three largest dynamic eye tracking datasets, HOLLYWOOD2, UCF-Sport and DIEM. Experimental results illustrates that solid performance gain is achievable using the proposed methodology.
count=1
* Image Super-Resolution via Dual-State Recurrent Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Han_Image_Super-Resolution_via_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Han_Image_Super-Resolution_via_CVPR_2018_paper.pdf)]
    * Title: Image Super-Resolution via Dual-State Recurrent Networks
    * Year: `2018`
    * Authors: Wei Han, Shiyu Chang, Ding Liu, Mo Yu, Michael Witbrock, Thomas S. Huang
    * Abstract: Advances in image super-resolution (SR) have recently benefited significantly from rapid developments in deep neural networks. Inspired by these recent discoveries, we note that many state-of-the-art deep SR architectures can be reformulated as a single-state recurrent neural network (RNN) with finite unfoldings. In this paper, we explore new structures for SR based on this compact RNN view, leading us to a dual-state design, the Dual-State Recurrent Network (DSRN). Compared to its single-state counterparts that op- erate at a fixed spatial resolution, DSRN exploits both low- resolution (LR) and high-resolution (HR) signals jointly. Recurrent signals are exchanged between these states in both directions (both LR to HR and HR to LR) via de- layed feedback. Extensive quantitative and qualitative eval- uations on benchmark datasets and on a recent challenge demonstrate that the proposed DSRN performs favorably against state-of-the-art algorithms in terms of both mem- ory consumption and predictive accuracy.
count=1
* Fast and Accurate Single Image Super-Resolution via Information Distillation Network
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Hui_Fast_and_Accurate_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Hui_Fast_and_Accurate_CVPR_2018_paper.pdf)]
    * Title: Fast and Accurate Single Image Super-Resolution via Information Distillation Network
    * Year: `2018`
    * Authors: Zheng Hui, Xiumei Wang, Xinbo Gao
    * Abstract: Recently, deep convolutional neural networks (CNNs) have been demonstrated remarkable progress on single image super-resolution. However, as the depth and width of the networks increase, CNN-based super-resolution methods have been faced with the challenges of computational complexity and memory consumption in practice. In order to solve the above questions, we propose a deep but compact convolutional network to directly reconstruct the high resolution image from the original low resolution image. In general, the proposed model consists of three parts, which are feature extraction block, stacked information distillation blocks and reconstruction block respectively. By combining an enhancement unit with a compression unit into a distillation block, the local long and short-path features can be effectively extracted. Specifically, the proposed enhancement unit mixes together two different types of features and the compression unit distills more useful information for the sequential blocks. In addition, the proposed network has the advantage of fast execution due to the comparatively few number of filters per layer and the use of group convolution. Experimental results demonstrate that the proposed method is superior to the state-of-the-art methods, especially in terms of time performance.
count=1
* Left-Right Comparative Recurrent Model for Stereo Matching
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Jie_Left-Right_Comparative_Recurrent_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Jie_Left-Right_Comparative_Recurrent_CVPR_2018_paper.pdf)]
    * Title: Left-Right Comparative Recurrent Model for Stereo Matching
    * Year: `2018`
    * Authors: Zequn Jie, Pengfei Wang, Yonggen Ling, Bo Zhao, Yunchao Wei, Jiashi Feng, Wei Liu
    * Abstract: Leveraging the disparity information from both left and right views is crucial for stereo disparity estimation. Left-right consistency check is an effective way to enhance the disparity estimation by referring to the information from the opposite view. However, the conventional left-right consistency check is an isolated post-processing step and heavily hand-crafted. This paper proposes a novel left-right comparative recurrent model to perform left-right consistency checking jointly with disparity estimation. At each recurrent step, the model produces disparity results for both views, and then performs online left-right comparison to identify the mismatched regions which may probably contain erroneously labeled pixels. A soft attention mechanism is introduced, which employs the learned error maps for better guiding the model to selectively focus on refining the unreliable regions at the next recurrent step. In this way, the generated disparity maps are progressively improved by the proposed recurrent model. Extensive evaluations on KITTI 2015, Scene Flow and Middlebury benchmarks validate the effectiveness of our model, demonstrating that state-of-the-art stereo disparity estimation results can be achieved by this new model.
count=1
* End-to-End Recovery of Human Shape and Pose
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper.pdf)]
    * Title: End-to-End Recovery of Human Shape and Pose
    * Year: `2018`
    * Authors: Angjoo Kanazawa, Michael J. Black, David W. Jacobs, Jitendra Malik
    * Abstract: We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation.
count=1
* A Deeper Look at Power Normalizations
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Koniusz_A_Deeper_Look_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Koniusz_A_Deeper_Look_CVPR_2018_paper.pdf)]
    * Title: A Deeper Look at Power Normalizations
    * Year: `2018`
    * Authors: Piotr Koniusz, Hongguang Zhang, Fatih Porikli
    * Abstract: Power Normalizations (PN) are very useful non-linear operators in the context of Bag-of-Words data representations as they tackle problems such as feature imbalance. In this paper, we reconsider these operators in the deep learning setup by introducing a novel layer that implements PN for non-linear pooling of feature maps. Specifically, by using a kernel formulation, our layer combines the feature vectors and their respective spatial locations in the feature maps produced by the last convolutional layer of CNN. Linearization of such a kernel results in a positive definite matrix capturing the second-order statistics of the feature vectors, to which PN operators are applied. We study two types of PN functions, namely (i) MaxExp and (ii) Gamma, addressing their role and meaning in the context of non-linear pooling. We also provide a probabilistic interpretation of these operators and derive their surrogates with well-behaved gradients for end-to-end CNN learning. We apply our theory to practice by implementing the PN layer on a ResNet-50 model and showcase experiments on four benchmarks for fine-grained recognition, scene recognition, and material classification. Our results demonstrate state-of-the-part performance across all these tasks.
count=1
* Learning Intelligent Dialogs for Bounding Box Annotation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Konyushkova_Learning_Intelligent_Dialogs_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Konyushkova_Learning_Intelligent_Dialogs_CVPR_2018_paper.pdf)]
    * Title: Learning Intelligent Dialogs for Bounding Box Annotation
    * Year: `2018`
    * Authors: Ksenia Konyushkova, Jasper Uijlings, Christoph H. Lampert, Vittorio Ferrari
    * Abstract: We introduce Intelligent Annotation Dialogs for bounding box annotation. We train an agent to automatically choose a sequence of actions for a human annotator to produce a bounding box in a minimal amount of time. Specifically, we consider two actions: box verification, where the annotator verifies a box generated by an object detector, and manual box drawing. We explore two kinds of agents, one based on predicting the probability that a box will be positively verified, and the other based on reinforcement learning. We demonstrate that (1) our agents are able to learn efficient annotation strategies in several scenarios, automatically adapting to the image difficulty, the desired quality of the boxes, and the detector strength; (2) in all scenarios the resulting annotation dialogs speed up annotation compared to manual box drawing alone and box verification alone, while also outperforming any fixed combination of verification and drawing in most scenarios; (3) in a realistic scenario where the detector is iteratively re-trained, our agents evolve a series of strategies that reflect the shifting trade-off between verification and drawing as the detector grows stronger.
count=1
* PointGrid: A Deep Network for 3D Shape Understanding
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Le_PointGrid_A_Deep_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf)]
    * Title: PointGrid: A Deep Network for 3D Shape Understanding
    * Year: `2018`
    * Authors: Truc Le, Ye Duan
    * Abstract: This paper presents a new deep learning architecture called PointGrid that is designed for 3D model recognition from unorganized point clouds. The new architecture embeds the input point cloud into a 3D grid by a simple, yet effective, sampling strategy and directly learns transformations and features from their raw coordinates. The proposed method is an integration of point and grid, a hybrid model, that leverages the simplicity of grid-based approaches such as VoxelNet while avoid its information loss. PointGrid learns better global information compared with PointNet and is much simpler than PointNet++, Kd-Net, Oct-Net and O-CNN, yet provides comparable recognition accuracy. With experiments on popular shape recognition benchmarks, PointGrid demonstrates competitive performance over existing deep learning methods on both classification and segmentation.
count=1
* CleanNet: Transfer Learning for Scalable Image Classifier Training With Label Noise
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_CleanNet_Transfer_Learning_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_CleanNet_Transfer_Learning_CVPR_2018_paper.pdf)]
    * Title: CleanNet: Transfer Learning for Scalable Image Classifier Training With Label Noise
    * Year: `2018`
    * Authors: Kuang-Huei Lee, Xiaodong He, Lei Zhang, Linjun Yang
    * Abstract: In this paper, we study the problem of learning image classification models with label noise. Existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is time-consuming, whereas approaches not relying on human supervision are scalable but less effective. To reduce the amount of human supervision for label noise cleaning, we introduce CleanNet, a joint neural embedding network, which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network classifier into one framework for image classification learning. We demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets. Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images verified on an image classification task. Source code and dataset will be available at kuanghuei.github.io/CleanNetProject.
count=1
* Deformable Shape Completion With Graph Convolutional Autoencoders
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Litany_Deformable_Shape_Completion_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Litany_Deformable_Shape_Completion_CVPR_2018_paper.pdf)]
    * Title: Deformable Shape Completion With Graph Convolutional Autoencoders
    * Year: `2018`
    * Authors: Or Litany, Alex Bronstein, Michael Bronstein, Ameesh Makadia
    * Abstract: The availability of affordable and portable depth sensors has made scanning objects and people simpler than ever. However, dealing with occlusions and missing parts is still a significant challenge. The problem of reconstructing a (possibly non-rigidly moving) 3D object from a single or multiple partial scans has received increasing attention in recent years. In this work, we propose a novel learningbased method for the completion of partial shapes. Unlike the majority of existing approaches, our method focuses on objects that can undergo non-rigid deformations. The core of our method is a variational autoencoder with graph convolutional operations that learns a latent space for complete realistic shapes. At inference, we optimize to find the representation in this latent space that best fits the generated shape to the known partial input. The completed shape exhibits a realistic appearance on the unknown part. We show promising results towards the completion of synthetic and real scans of human body and face meshes exhibiting different styles of articulation and partiality.
count=1
* Recognizing Human Actions as the Evolution of Pose Estimation Maps
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Recognizing_Human_Actions_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Recognizing_Human_Actions_CVPR_2018_paper.pdf)]
    * Title: Recognizing Human Actions as the Evolution of Pose Estimation Maps
    * Year: `2018`
    * Authors: Mengyuan Liu, Junsong Yuan
    * Abstract: Most video-based action recognition approaches choose to extract features from the whole video to recognize actions. The cluttered background and non-action motions limit the performances of these methods, since they lack the explicit modeling of human body movements. With recent advances of human pose estimation, this work presents a novel method to recognize human action as the evolution of pose estimation maps. Instead of relying on the inaccurate human poses estimated from videos, we observe that pose estimation maps, the byproduct of pose estimation, preserve richer cues of human body to benefit action recognition. Specifically, the evolution of pose estimation maps can be decomposed as an evolution of heatmaps, e.g., probabilistic maps, and an evolution of estimated 2D human poses, which denote the changes of body shape and body pose, respectively. Considering the sparse property of heatmap, we develop spatial rank pooling to aggregate the evolution of heatmaps as a body shape evolution image. As body shape evolution image does not differentiate body parts, we design body guided sampling to aggregate the evolution of poses as a body pose evolution image. The complementary properties between both types of images are explored by deep convolutional neural networks to predict action label. Experiments on NTU RGB+D, UTD-MHAD and PennAction datasets verify the effectiveness of our method, which outperforms most state-of-the-art methods.
count=1
* Direct Shape Regression Networks for End-to-End Face Alignment
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Miao_Direct_Shape_Regression_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Miao_Direct_Shape_Regression_CVPR_2018_paper.pdf)]
    * Title: Direct Shape Regression Networks for End-to-End Face Alignment
    * Year: `2018`
    * Authors: Xin Miao, Xiantong Zhen, Xianglong Liu, Cheng Deng, Vassilis Athitsos, Heng Huang
    * Abstract: Face alignment has been extensively studied in computer vision community due to its fundamental role in facial analysis, but it remains an unsolved problem. The major challenges lie in the highly nonlinear relationship between face images and associated facial shapes, which is coupled by underlying correlation of landmarks. Existing methods mainly rely on cascaded regression, suffering from intrinsic shortcomings, e.g., strong dependency on initialization and failure to exploit landmark correlations. In this paper, we propose the direct shape regression network (DSRN) for end-to-end face alignment by jointly handling the aforementioned challenges in a unified framework. Specifically, by deploying doubly convolutional layer and by using the Fourier feature pooling layer proposed in this paper, DSRN efficiently constructs strong representations to disentangle highly nonlinear relationships between images and shapes; by incorporating a linear layer of low-rank learning, DSRN effectively encodes correlations of landmarks to improve performance. DSRN leverages the strengths of kernels for nonlinear feature extraction and neural networks for structured prediction, and provides the first end-to-end learning architecture for direct face alignment. Its effectiveness and generality are validated by extensive experiments on five benchmark datasets, including AFLW, 300W, CelebA, MAFL, and 300VW. All empirical results demonstrate that DSRN consistently produces high performance and in most cases surpasses state-of-the-art.
count=1
* Blind Predicting Similar Quality Map for Image Quality Assessment
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Pan_Blind_Predicting_Similar_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Blind_Predicting_Similar_CVPR_2018_paper.pdf)]
    * Title: Blind Predicting Similar Quality Map for Image Quality Assessment
    * Year: `2018`
    * Authors: Da Pan, Ping Shi, Ming Hou, Zefeng Ying, Sizhe Fu, Yuan Zhang
    * Abstract: A key problem in blind image quality assessment (BIQA) is how to effectively model the properties of human visual system in a data-driven manner. In this paper, we propose a simple and efficient BIQA model based on a novel framework which consists of a fully convolutional neural network (FCNN) and a pooling network to solve this problem. In principle, FCNN is capable of predicting a pixel-by-pixel similar quality map only from a distorted image by using the intermediate similarity maps derived from conventional full-reference image quality assessment methods. The predicted pixel-by-pixel quality maps have good consistency with the distortion correlations between the reference and distorted images. Finally, a deep pooling network regresses the quality map into a score. Experiments have demonstrated that our predictions outperform many state-of-the-art BIQA methods.
count=1
* Learning to Estimate 3D Human Pose and Shape From a Single Color Image
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Pavlakos_Learning_to_Estimate_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Pavlakos_Learning_to_Estimate_CVPR_2018_paper.pdf)]
    * Title: Learning to Estimate 3D Human Pose and Shape From a Single Color Image
    * Year: `2018`
    * Authors: Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, Kostas Daniilidis
    * Abstract: This work addresses the problem of estimating the full body 3D human pose and shape from a single color image. This is a task where iterative optimization-based solutions have typically prevailed, while Convolutional Networks (ConvNets) have suffered because of the lack of training data and their low resolution 3D predictions. Our work aims to bridge this gap and proposes an efficient and effective direct prediction method based on ConvNets. Central part to our approach is the incorporation of a parametric statistical body shape model (SMPL) within our end-to-end framework. This allows us to get very detailed 3D mesh results, while requiring estimation only of a small number of parameters, making it friendly for direct network prediction. Interestingly, we demonstrate that these parameters can be predicted reliably only from 2D keypoints and masks. These are typical outputs of generic 2D human analysis ConvNets, allowing us to relax the massive requirement that images with 3D shape ground truth are available for training. Simultaneously, by maintaining differentiability, at training time we generate the 3D mesh from the estimated parameters and optimize explicitly for the surface using a 3D per-vertex loss. Finally, a differentiable renderer is employed to project the 3D mesh to the image, which enables further refinement of the network, by optimizing for the consistency of the projection with 2D annotations (i.e., 2D keypoints or masks). The proposed approach outperforms previous baselines on this task and offers an attractive solution for direct prediction of 3D shape from a single color image.
count=1
* Ordinal Depth Supervision for 3D Human Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Pavlakos_Ordinal_Depth_Supervision_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Pavlakos_Ordinal_Depth_Supervision_CVPR_2018_paper.pdf)]
    * Title: Ordinal Depth Supervision for 3D Human Pose Estimation
    * Year: `2018`
    * Authors: Georgios Pavlakos, Xiaowei Zhou, Kostas Daniilidis
    * Abstract: Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural images. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these ordinal relations in different settings, always achieving competitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the potential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This extension allows us to present quantitative and qualitative evaluation in non-studio conditions. Simultaneously, these ordinal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose.
count=1
* PieAPP: Perceptual Image-Error Assessment Through Pairwise Preference
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper.pdf)]
    * Title: PieAPP: Perceptual Image-Error Assessment Through Pairwise Preference
    * Year: `2018`
    * Authors: Ekta Prashnani, Hong Cai, Yasamin Mostofi, Pradeep Sen
    * Abstract: The ability to estimate the perceptual error between images is an important problem in computer vision with many applications. Although it has been studied extensively, however, no method currently exists that can robustly predict visual differences like humans. Some previous approaches used hand-coded models, but they fail to model the complexity of the human visual system. Others used machine learning to train models on human-labeled datasets, but creating large, high-quality datasets is difficult because people are unable to assign consistent error labels to distorted images. In this paper, we present a new learning-based method that is the first to predict perceptual image error like human observers. Since it is much easier for people to compare two given images and identify the one more similar to a reference than to assign quality scores to each, we propose a new, large-scale dataset labeled with the probability that humans will prefer one image over another. We then train a deep-learning model using a novel, pairwise-learning framework to predict the preference of one distorted image over the other. Our key observation is that our trained network can then be used separately with only one distorted image and a reference to predict its perceptual error, without ever being trained on explicit human perceptual-error labels. The perceptual error estimated by our new metric, PieAPP, is well-correlated with human opinion. Furthermore, it significantly outperforms existing algorithms, beating the state-of-the-art by almost 3x on our test set in terms of binary error rate, while also generalizing to new kinds of distortions, unlike previous learning-based methods.
count=1
* Soccer on Your Tabletop
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Rematas_Soccer_on_Your_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Rematas_Soccer_on_Your_CVPR_2018_paper.pdf)]
    * Title: Soccer on Your Tabletop
    * Year: `2018`
    * Authors: Konstantinos Rematas, Ira Kemelmacher-Shlizerman, Brian Curless, Steve Seitz
    * Abstract: We present a system that transforms a monocular video of a soccer game into a moving 3D reconstruction, in which the players and field can be rendered interactively with a 3D viewer or through an Augmented Reality device. At the heart of our paper is an approach to estimate the depth map of each player, using a CNN that is trained on 3D player data extracted from soccer video games. We compare with state of the art body pose and depth estimation techniques, and show results on both synthetic ground truth benchmarks, and real YouTube soccer footage.
count=1
* Gated Fusion Network for Single Image Dehazing
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Ren_Gated_Fusion_Network_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_Gated_Fusion_Network_CVPR_2018_paper.pdf)]
    * Title: Gated Fusion Network for Single Image Dehazing
    * Year: `2018`
    * Authors: Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, Ming-Hsuan Yang
    * Abstract: In this paper, we propose an efficient algorithm to directly restore a clear image from a hazy input. The proposed algorithm hinges on an end-to-end trainable neural network that consists of an encoder and a decoder. The encoder is exploited to capture the context of the derived input images, while the decoder is employed to estimate the contribution of each input to the final dehazed result using the learned representations attributed to the encoder. The constructed network adopts a novel fusion-based strategy which derives three inputs from an original hazy image by applying White Balance (WB), Contrast Enhancing (CE), and Gamma Correction (GC). We compute pixel-wise confidence maps based on the appearance differences between these different inputs to blend the information of the derived inputs and preserve the regions with pleasant visibility. The final dehazed image is yielded by gating the important features of the derived inputs. To train the network, we introduce a multi-scale based approach so that the halo artifacts can be avoided. Extensive experimental results on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against the state-of-the-art algorithms.
count=1
* Exploiting Transitivity for Learning Person Re-Identification Models on a Budget
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.pdf)]
    * Title: Exploiting Transitivity for Learning Person Re-Identification Models on a Budget
    * Year: `2018`
    * Authors: Sourya Roy, Sujoy Paul, Neal E. Young, Amit K. Roy-Chowdhury
    * Abstract: Minimization of labeling effort for person re-identification in camera networks is an important problem as most of the existing popular methods are supervised and they require large amount of manual annotations, acquiring which is a tedious job. In this work, we focus on this labeling effort minimization problem and approach it as a subset selection task where the objective is to select an optimal subset of image-pairs for labeling without compromising performance. Towards this goal, our proposed scheme first represents any camera network (with k number of cameras) as an edge weighted complete k-partite graph where each vertex denotes a person and similarity scores between persons are used as edge-weights. Then in the second stage, our algorithm selects an optimal subset of pairs by solving a triangle free subgraph maximization problem on the k-partite graph. This sub-graph weight maximization problem is NP-hard (at least for k > = 4) which means for large datasets the optimization problem becomes intractable. In order to make our framework scalable, we propose two polynomial time approximately-optimal algorithms. The first algorithm is a 1/2-approximation algorithm which runs in linear time in the number of edges. The second algorithm is a greedy algorithm with sub-quadratic (in number of edges) time-complexity. Experiments on three state-of-the-art datasets depict that the proposed approach requires on an average only 8-15 % manually labeled pairs in order to achieve the performance when all the pairs are manually annotated.
count=1
* Crowd Counting With Deep Negative Correlation Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Shi_Crowd_Counting_With_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Crowd_Counting_With_CVPR_2018_paper.pdf)]
    * Title: Crowd Counting With Deep Negative Correlation Learning
    * Year: `2018`
    * Authors: Zenglin Shi, Le Zhang, Yun Liu, Xiaofeng Cao, Yangdong Ye, Ming-Ming Cheng, Guoyan Zheng
    * Abstract: Deep convolutional networks (ConvNets) have achieved unprecedented performances on many computer vision tasks. However, their adaptations to crowd counting on single images are still in their infancy and suffer from severe over-fitting. Here we propose a new learning strategy to produce generalizable features by way of deep negative correlation learning (NCL). More specifically, we deeply learn a pool of decorrelated regressors with sound generalization capabilities through managing their intrinsic diversities. Our proposed method, named decorrelated ConvNet (D-ConvNet), is end-to-end-trainable and independent of the backbone fully-convolutional network architectures. Extensive experiments on very deep VGGNet as well as our customized network structure indicate the superiority of D-ConvNet when compared with several state-of-the-art methods. Our implementation will be released at https://github.com/shizenglin/Deep-NCL
count=1
* HATS: Histograms of Averaged Time Surfaces for Robust Event-Based Object Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Sironi_HATS_Histograms_of_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sironi_HATS_Histograms_of_CVPR_2018_paper.pdf)]
    * Title: HATS: Histograms of Averaged Time Surfaces for Robust Event-Based Object Classification
    * Year: `2018`
    * Authors: Amos Sironi, Manuele Brambilla, Nicolas Bourdis, Xavier Lagorce, Ryad Benosman
    * Abstract: Event-based cameras have recently drawn the attention of the Computer Vision community thanks to their advantages in terms of high temporal resolution, low power consumption and high dynamic range, compared to traditional frame-based cameras. These properties make event-based cameras an ideal choice for autonomous vehicles, robot navigation or UAV vision, among others. However, the accuracy of event-based object classification algorithms, which is of crucial importance for any reliable system working in real-world conditions, is still far behind their frame-based counterparts. Two main reasons for this performance gap are: 1. The lack of effective low-level representations and architectures for event-based object classification and 2. The absence of large real-world event-based datasets. In this paper we address both problems. First, we introduce a novel event-based feature representation together with a new machine learning architecture. Compared to previous approaches, we use local memory units to efficiently leverage past temporal information and build a robust event-based representation. Second, we release the first large real-world event-based dataset for object classification. We compare our method to the state-of-the-art with extensive experiments, showing better classification performance and real-time computation.
count=1
* High-Speed Tracking With Multi-Kernel Correlation Filters
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Tang_High-Speed_Tracking_With_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Tang_High-Speed_Tracking_With_CVPR_2018_paper.pdf)]
    * Title: High-Speed Tracking With Multi-Kernel Correlation Filters
    * Year: `2018`
    * Authors: Ming Tang, Bin Yu, Fan Zhang, Jinqiao Wang
    * Abstract: Correlation filter (CF) based trackers are currently ranked top in terms of their performances. Nevertheless, only some of them, such as KCF [henriques12&15] and MKCF[tang&Feng15}, are able to exploit the powerful discriminability of non-linear kernels. Although MKCF achieves more powerful discriminability than KCF through introducing multi-kernel learning (MKL) into KCF, its improvement over KCF is quite limited and its computational burden increases significantly in comparison with KCF. In this paper, we will introduce the MKL into KCF in a different way than MKCF. We reformulate the MKL version of CF objective function with its upper bound, alleviating the negative mutual interference of different kernels significantly. Our novel MKCF tracker, MKCFup, outperforms KCF and MKCF with large margins and can still work at very high fps. Extensive experiments on public data sets show that our method is superior to state-of-the-art algorithms for target objects of small move at very high speed.
count=1
* Dense 3D Regression for Hand Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Wan_Dense_3D_Regression_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wan_Dense_3D_Regression_CVPR_2018_paper.pdf)]
    * Title: Dense 3D Regression for Hand Pose Estimation
    * Year: `2018`
    * Authors: Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao
    * Abstract: We present a simple and effective method for 3D hand pose estimation from a single depth frame. As opposed to previous state-of-arts based on holistic 3D regression, our method works on dense pixel-wise estimation. This is achieved by careful design choices in pose parameterization, which leverages both 2D and 3D properties of depth map. Specifically, we decompose the pose parameters into a set of per-pixel estimations, i.e., 2D heat maps, 3D heat maps and unit 3D direction vector fields. The 2D/3D joint heat maps and 3D joint offsets are estimated via multi-task network cascades, which is trained end-to-end. The pixel-wise estimations can be directly translated into a vote casting scheme. A variant of mean shift is then used to aggregate local votes and explicitly handles the global 3D estimation in consensus with pixel-wise 2D and 3D estimations. Our method is efficient and highly accurate. On MSRA and NYU hand dataset, our method outperforms all previous state-of-arts by a large margin. On ICVL hand dataset, our method achieves similar accuracy compared to the state-of-art which is nearly saturated and outperforms other state-of-arts. Code will be made available.
count=1
* Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Recovering_Realistic_Texture_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Recovering_Realistic_Texture_CVPR_2018_paper.pdf)]
    * Title: Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform
    * Year: `2018`
    * Authors: Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy
    * Abstract: Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN and EnhanceNet.
count=1
* Clinical Skin Lesion Diagnosis Using Representations Inspired by Dermatologist Criteria
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Clinical_Skin_Lesion_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Clinical_Skin_Lesion_CVPR_2018_paper.pdf)]
    * Title: Clinical Skin Lesion Diagnosis Using Representations Inspired by Dermatologist Criteria
    * Year: `2018`
    * Authors: Jufeng Yang, Xiaoxiao Sun, Jie Liang, Paul L. Rosin
    * Abstract: The skin is the largest organ in human body. Around 30%-70% of individuals worldwide have skin related health problems, for whom effective and efficient diagnosis is necessary. Recently, computer aided diagnosis (CAD) systems have been successfully applied to the recognition of skin cancers in dermatoscopic images. However, little work has concentrated on the commonly encountered skin diseases in clinical images captured by easily-accessed cameras or mobile phones. Meanwhile, for a CAD system, the representations of skin lesions are required to be understandable for dermatologists so that the predictions are convincing. To address this problem, we present effective representations inspired by the accepted dermatological criteria for diagnosing clinical skin lesions. We demonstrate that the dermatological criteria are highly correlated with measurable visual components. Accordingly, we design six medical representations considering different criteria for the recognition of skin lesions, and construct a diagnosis system for clinical skin disease images. Experimental results show that the proposed medical representations can not only capture the manifestations of skin lesions effectively, and consistently with the dermatological criteria, but also improve the prediction performance with respect to the state-of-the-art methods based on uninterpretable features.
count=1
* Multiple Granularity Group Interaction Prediction
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Yao_Multiple_Granularity_Group_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Yao_Multiple_Granularity_Group_CVPR_2018_paper.pdf)]
    * Title: Multiple Granularity Group Interaction Prediction
    * Year: `2018`
    * Authors: Taiping Yao, Minsi Wang, Bingbing Ni, Huawei Wei, Xiaokang Yang
    * Abstract: Most human activity analysis works (i.e., recognition or　prediction) only focus on a single granularity, i.e., either　modelling global motion based on the coarse level movement such as human trajectories or　forecasting future detailed action based on body parts’ movement such as skeleton motion. In contrast, in this work, we propose a multi-granularity interaction prediction network which integrates　both global motion and detailed local action. Built on a bi- directional LSTM network, the　proposed method possesses　between granularities links which encourage feature sharing as well as cross-feature consistency between both global　and local granularity (e.g., trajectory or local action), and in turn predict long-term global location and local dynamics of each individual. We validate our method on several　public datasets with promising performance.
count=1
* Deep Layer Aggregation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Deep_Layer_Aggregation_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Deep_Layer_Aggregation_CVPR_2018_paper.pdf)]
    * Title: Deep Layer Aggregation
    * Year: `2018`
    * Authors: Fisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell
    * Abstract: Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been ``shallow'' themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes.
count=1
* DoubleFusion: Real-Time Capture of Human Performances With Inner Body Shapes From a Single Depth Sensor
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_DoubleFusion_Real-Time_Capture_CVPR_2018_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_DoubleFusion_Real-Time_Capture_CVPR_2018_paper.pdf)]
    * Title: DoubleFusion: Real-Time Capture of Human Performances With Inner Body Shapes From a Single Depth Sensor
    * Year: `2018`
    * Authors: Tao Yu, Zerong Zheng, Kaiwen Guo, Jianhui Zhao, Qionghai Dai, Hao Li, Gerard Pons-Moll, Yebin Liu
    * Abstract: We propose DoubleFusion, a new real-time system that combines volumetric dynamic reconstruction with data-driven template fitting to simultaneously reconstruct detailed geometry, non-rigid motion and the inner human body shape from a single depth camera. One of the key contributions of this method is a double layer representation consisting of a complete parametric body shape inside and a gradually fused outer surface layer. A pre-defined node graph on the body surface parameterizes the non-rigid deformations near the body and a free-form dynamically changing graph parameterizes the outer surface layer far from the body allowing more general reconstruction. We further propose a joint motion tracking method based on the double layer representation to enable robust and fast motion tracking performance. Moreover, the inner body shape is optimized online and forced to fit inside the outer surface layer. Overall, our method enables increasingly denoised, detailed and complete surface reconstructions, fast motion tracking performance and plausible inner body shape reconstruction in real-time. In particular, experiments show improved fast motion tracking and loop closure performance on more challenging scenarios.
count=1
* Infinite Variational Autoencoder for Semi-Supervised Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Abbasnejad_Infinite_Variational_Autoencoder_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Abbasnejad_Infinite_Variational_Autoencoder_CVPR_2017_paper.pdf)]
    * Title: Infinite Variational Autoencoder for Semi-Supervised Learning
    * Year: `2017`
    * Authors: M. Ehsan Abbasnejad, Anthony Dick, Anton van den Hengel
    * Abstract: This paper presents an infinite variational autoencoder (VAE) whose capacity adapts to suit the input data. This is achieved using a mixture model where the mixing coefficients are modeled by a Dirichlet process, allowing us to integrate over the coefficients when performing inference. Critically, this then allows us to automatically vary the number of autoencoders in the mixture based on the data. Experiments show the flexibility of our method, particularly for semi-supervised learning, where only a small number of training samples are available.
count=1
* POSEidon: Face-From-Depth for Driver Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Borghi_POSEidon_Face-From-Depth_for_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Borghi_POSEidon_Face-From-Depth_for_CVPR_2017_paper.pdf)]
    * Title: POSEidon: Face-From-Depth for Driver Pose Estimation
    * Year: `2017`
    * Authors: Guido Borghi, Marco Venturelli, Roberto Vezzani, Rita Cucchiara
    * Abstract: Fast and accurate upper-body and head pose estimation is a key task for automatic monitoring of driver attention, a challenging context characterized by severe illumination changes, occlusions and extreme poses. In this work, we present a new deep learning framework for head localization and pose estimation on depth images. The core of the proposal is a regressive neural network, called POSEidon, which is composed of three independent convolutional nets followed by a fusion layer, specially conceived for understanding the pose by depth. In addition, to recover the intrinsic value of face appearance for understanding head position and orientation, we propose a new Face-from-Depth model for learning image faces from depth. Results in face reconstruction are qualitatively impressive. We test the proposed framework on two public datasets, namely Biwi Kinect Head Pose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by the automotive setup. Results show that our method overcomes all recent state-of-art works, running in real time at more than 30 frames per second.
count=1
* Deep Representation Learning for Human Motion Prediction and Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Butepage_Deep_Representation_Learning_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Butepage_Deep_Representation_Learning_CVPR_2017_paper.pdf)]
    * Title: Deep Representation Learning for Human Motion Prediction and Classification
    * Year: `2017`
    * Authors: Judith Butepage, Michael J. Black, Danica Kragic, Hedvig Kjellstrom
    * Abstract: Generative models of 3D human motion are often restricted to a small number of activities and can therefore not generalize well to novel movements or applications. In this work we propose a deep learning framework for human motion capture data that learns a generic representation from a large corpus of motion capture data and generalizes well to new, unseen, motions. Using an encoding-decoding network that learns to predict future 3D poses from the most recent past, we extract a feature representation of human motion. Most work on deep learning for sequence prediction focuses on video and speech. Since skeletal data has a different structure, we present and evaluate different network architectures that make different assumptions about time dependencies and limb correlations. To quantify the learned features, we use the output of different layers for action classification and visualize the receptive fields of the network units. Our method outperforms the recent state of the art in skeletal motion prediction even though these use action specific training data. Our results show that deep feedforward networks, trained from a generic mocap database, can successfully be used for feature extraction from human motion data and that this representation can be used as a foundation for classification and prediction.
count=1
* Multi-View 3D Object Detection Network for Autonomous Driving
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Multi-View_3D_Object_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf)]
    * Title: Multi-View 3D Object Detection Network for Autonomous Driving
    * Year: `2017`
    * Authors: Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, Tian Xia
    * Abstract: This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.
count=1
* Generalized Rank Pooling for Activity Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Cherian_Generalized_Rank_Pooling_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Cherian_Generalized_Rank_Pooling_CVPR_2017_paper.pdf)]
    * Title: Generalized Rank Pooling for Activity Recognition
    * Year: `2017`
    * Authors: Anoop Cherian, Basura Fernando, Mehrtash Harandi, Stephen Gould
    * Abstract: Most popular deep models for action recognition split video sequences into short sub-sequences consisting of a few frames; frame-based features are then pooled for recognizing the activity. Usually, this pooling step discards the temporal order of the frames, which could otherwise be used for better recognition. Towards this end, we propose a novel pooling method, generalized rank pooling (GRP), that takes as input, features from the intermediate layers of a CNN that is trained on tiny sub-sequences, and produces as output the parameters of a subspace which (i) provides a low-rank approximation to the features and (ii) preserves their temporal order. We propose to use these parameters as a compact representation for the video sequence, which is then used in a classification setup. We formulate an objective for computing this subspace as a Riemannian optimization problem on the Grassmann manifold, and propose an efficient conjugate gradient scheme for solving it. Experiments on several activity recognition datasets show that our scheme leads to state-of-the-art performance.
count=1
* VidLoc: A Deep Spatio-Temporal Model for 6-DoF Video-Clip Relocalization
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Clark_VidLoc_A_Deep_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Clark_VidLoc_A_Deep_CVPR_2017_paper.pdf)]
    * Title: VidLoc: A Deep Spatio-Temporal Model for 6-DoF Video-Clip Relocalization
    * Year: `2017`
    * Authors: Ronald Clark, Sen Wang, Andrew Markham, Niki Trigoni, Hongkai Wen
    * Abstract: Machine learning techniques, namely convolutional neural networks (CNN) and regression forests, have recently shown great promise in performing 6-DoF localization of monocular images. However, in most cases image-sequences, rather only single images, are readily available. To this extent, none of the proposed learning-based approaches exploit the valuable constraint of temporal smoothness, often leading to situations where the per-frame error is larger than the camera motion. In this paper we propose a recurrent model for performing 6-DoF localization of video-clips. We find that, even by considering only short sequences (20 frames), the pose estimates are smoothed and the localization error can be drastically reduced. Finally, we consider means of obtaining probabilistic pose estimates from our model. We evaluate our method on openly-available real-world autonomous driving and indoor localization datasets.
count=1
* Fast Boosting Based Detection Using Scale Invariant Multimodal Multiresolution Filtered Features
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Costea_Fast_Boosting_Based_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Costea_Fast_Boosting_Based_CVPR_2017_paper.pdf)]
    * Title: Fast Boosting Based Detection Using Scale Invariant Multimodal Multiresolution Filtered Features
    * Year: `2017`
    * Authors: Arthur Daniel Costea, Robert Varga, Sergiu Nedevschi
    * Abstract: In this paper we propose a novel boosting-based sliding window solution for object detection which can keep up with the precision of the state-of-the art deep learning approaches, while being 10 to 100 times faster. The solution takes advantage of multisensorial perception and exploits information from color, motion and depth. We introduce multimodal multiresolution filtering of signal intensity, gradient magnitude and orientation channels, in order to capture structure at multiple scales and orientations. To achieve scale invariant classification features, we analyze the effect of scale change on features for different filter types and propose a correction scheme. To improve recognition we incorporate 2D and 3D context by generating spatial, geometric and symmetrical channels. Finally, we evaluate the proposed solution on multiple benchmarks for the detection of pedestrians, cars and bicyclists. We achieve competitive results at over 25 frames per second.
count=1
* Kernel Pooling for Convolutional Neural Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Cui_Kernel_Pooling_for_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Cui_Kernel_Pooling_for_CVPR_2017_paper.pdf)]
    * Title: Kernel Pooling for Convolutional Neural Networks
    * Year: `2017`
    * Authors: Yin Cui, Feng Zhou, Jiang Wang, Xiao Liu, Yuanqing Lin, Serge Belongie
    * Abstract: Convolutional Neural Networks (CNNs) with Bilinear Pooling, initially in their full form and later using compact representations, have yielded impressive performance gains on a wide range of visual tasks, including fine-grained visual categorization, visual question answering, face recognition, and description of texture and style. The key to their success lies in the spatially invariant modeling of pairwise (2nd order) feature interactions. In this work, we propose a general pooling framework that captures higher order interactions of features in the form of kernels. We demonstrate how to approximate kernels such as Gaussian RBF up to a given order using compact explicit feature maps in a parameter-free manner. Combined with CNNs, the composition of the kernel can be learned from data in an end-to-end fashion via error back-propagation. The proposed kernel pooling scheme is evaluated in terms of both kernel approximation error and visual recognition accuracy. Experimental evaluations demonstrate state-of-the-art performance on commonly used fine-grained recognition datasets.
count=1
* Human Shape From Silhouettes Using Generative HKS Descriptors and Cross-Modal Neural Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Dibra_Human_Shape_From_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Dibra_Human_Shape_From_CVPR_2017_paper.pdf)]
    * Title: Human Shape From Silhouettes Using Generative HKS Descriptors and Cross-Modal Neural Networks
    * Year: `2017`
    * Authors: Endri Dibra, Himanshu Jain, Cengiz Oztireli, Remo Ziegler, Markus Gross
    * Abstract: In this work, we present a novel method for capturing human body shape from a single scaled silhouette. We combine deep correlated features capturing different 2D views, and embedding spaces based on 3D cues in a novel convolutional neural network (CNN) based architecture. We first train a CNN to find a richer body shape representation space from pose invariant 3D human shape descriptors. Then, we learn a mapping from silhouettes to this representation space, with the help of a novel architecture that exploits correlation of multi-view data during training time, to improve prediction at test time. We extensively validate our results on synthetic and real data, demonstrating significant improvements in accuracy as compared to the state-of-the-art, and providing a practical system for detailed human body measurements from a single image.
count=1
* Spatio-Temporal Vector of Locally Max Pooled Features for Action Recognition in Videos
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Duta_Spatio-Temporal_Vector_of_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Duta_Spatio-Temporal_Vector_of_CVPR_2017_paper.pdf)]
    * Title: Spatio-Temporal Vector of Locally Max Pooled Features for Action Recognition in Videos
    * Year: `2017`
    * Authors: Ionut Cosmin Duta, Bogdan Ionescu, Kiyoharu Aizawa, Nicu Sebe
    * Abstract: We introduce Spatio-Temporal Vector of Locally Max Pooled Features (ST-VLMPF), a super vector-based encoding method specifically designed for local deep features encoding. The proposed method addresses an important problem of video understanding: how to build a video representation that incorporates the CNN features over the entire video. Feature assignment is carried out at two levels, by using the similarity and spatio-temporal information. For each assignment we build a specific encoding, focused on the nature of deep features, with the goal to capture the highest feature responses from the highest neuron activation of the network. Our ST-VLMPF clearly provides a more reliable video representation than some of the most widely used and powerful encoding approaches (Improved Fisher Vectors and Vector of Locally Aggregated Descriptors), while maintaining a low computational complexity. We conduct experiments on three action recognition datasets: HMDB51, UCF50 and UCF101. Our pipeline obtains state-of-the-art results.
count=1
* Dynamic Facial Analysis: From Bayesian Filtering to Recurrent Neural Network
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Gu_Dynamic_Facial_Analysis_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Gu_Dynamic_Facial_Analysis_CVPR_2017_paper.pdf)]
    * Title: Dynamic Facial Analysis: From Bayesian Filtering to Recurrent Neural Network
    * Year: `2017`
    * Authors: Jinwei Gu, Xiaodong Yang, Shalini De Mello, Jan Kautz
    * Abstract: Facial analysis in videos, including head pose estimation and facial landmark localization, is key for many applications such as facial animation capture, human activity recognition, and human-computer interaction. In this paper, we propose to use a recurrent neural network (RNN) for joint estimation and tracking of facial features in videos. We are inspired by the fact that the computation performed in an RNN bears resemblance to Bayesian filters, which have been used for tracking in many previous methods for facial analysis from videos. Bayesian filters used in these methods, however, require complicated, problem-specific design and tuning. In contrast, our proposed RNN-based method avoids such tracker-engineering by learning from training data, similar to how a convolutional neural network (CNN) avoids feature-engineering for image classification. As an end-to-end network, the proposed RNN-based method provides a generic and holistic solution for joint estimation and tracking of various types of facial features from consecutive video frames. Extensive experimental results on head pose estimation and facial landmark localization from videos demonstrate that the proposed RNN-based method outperforms frame-wise models and Bayesian filtering. In addition, we create a large-scale synthetic dataset for head pose estimation, with which we achieve state-of-the-art performance on a benchmark dataset.
count=1
* STD2P: RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven Pooling
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/He_STD2P_RGBD_Semantic_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/He_STD2P_RGBD_Semantic_CVPR_2017_paper.pdf)]
    * Title: STD2P: RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven Pooling
    * Year: `2017`
    * Authors: Yang He, Wei-Chen Chiu, Margret Keuper, Mario Fritz
    * Abstract: We propose a novel superpixel-based multi-view convolutional neural network for semantic image segmentation. The proposed network produces a high quality segmentation of a single image by leveraging information from additional views of the same scene. Particularly in indoor videos such as captured by robotic platforms or handheld and bodyworn RGBD cameras, nearby video frames provide diverse viewpoints and additional context of objects and scenes. To leverage such information, we first compute region correspondences by optical flow and image boundary-based superpixels. Given these region correspondences, we propose a novel spatio-temporal pooling layer to aggregate information over space and time. We evaluate our approach on the NYU-Depth-V2 and the SUN3D datasets and compare it to various state-of-the-art single-view and multi-view approaches. Besides a general improvement over the state-of-the-art, we also show the benefits of making use of unlabeled frames during training for multi-view as well as single-view prediction.
count=1
* Soft-Margin Mixture of Regressions
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Soft-Margin_Mixture_of_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Soft-Margin_Mixture_of_CVPR_2017_paper.pdf)]
    * Title: Soft-Margin Mixture of Regressions
    * Year: `2017`
    * Authors: Dong Huang, Longfei Han, Fernando De la Torre
    * Abstract: Nonlinear regression is a common statistical tool to solve many computer vision problems (e.g., age estimation, pose estimation). Existing approaches to nonlinear regression fall into two main categories: (1) The universal approach provides an implicit or explicit homogeneous feature mapping (e.g., kernel ridge regression, Gaussian process regression, neural networks). These approaches may fail when data is heterogeneous or discontinuous. (2) Divide-and-conquer approaches partition a heterogeneous input feature space and learn multiple local regressors. However, existing divide-and-conquer approaches fail to deal with discontinuities between partitions (e.g., Gaussian mixture of regressions) and they cannot guarantee that the partitioned input space will be homogeneously modeled by local regressors (e.g., ordinal regression). To address these issues, this paper proposes Soft-Margin Mixture of Regressions (SMMR), a method that directly learns homogeneous partitions of the input space and is able to deal with discontinuities. SMMR outperforms the state-of-the-art methods on three popular computer vision tasks: age estimation, crowd counting and viewpoint estimation from images.
count=1
* Co-Occurrence Filter
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Jevnisek_Co-Occurrence_Filter_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Jevnisek_Co-Occurrence_Filter_CVPR_2017_paper.pdf)]
    * Title: Co-Occurrence Filter
    * Year: `2017`
    * Authors: Roy J. Jevnisek, Shai Avidan
    * Abstract: Co-occurrence Filter (CoF) is a boundary preserving filter. It is based on the Bilateral Filter (BF) but instead of using a Gaussian on the range values to preserve edges it relies on a co-occurrence matrix. Pixel values that co-occur frequently in the image (i.e., inside textured regions) will have a high weight in the co-occurrence matrix. This, in turn, means that such pixel pairs will be averaged and hence smoothed, regardless of their intensity differences. On the other hand, pixel values that rarely co-occur (i.e., across texture boundaries) will have a low weight in the co-occurrence matrix. As a result, they will not be averaged and the boundary between them will be preserved. The CoF therefore extends the BF to deal with boundaries, not just edges. It learns co-occurrences directly from the image. We can achieve various filtering results by directing it to learn the co-occurrence matrix from a part of the image, or a different image. We give the definition of the filter, discuss how to use it with color images and show several use cases.
count=1
* Detangling People: Individuating Multiple Close People and Their Body Parts via Region Assembly
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Jiang_Detangling_People_Individuating_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Jiang_Detangling_People_Individuating_CVPR_2017_paper.pdf)]
    * Title: Detangling People: Individuating Multiple Close People and Their Body Parts via Region Assembly
    * Year: `2017`
    * Authors: Hao Jiang, Kristen Grauman
    * Abstract: Today's person detection methods work best when people are in common upright poses and appear reasonably well spaced out in the image. However, in many real images, that's not what people do. People often appear quite close to each other, e.g., with limbs linked or heads touching, and their poses are often not pedestrian-like. We propose an approach to detangle people in multi-person images. We formulate the task as a region assembly problem. Starting from a large set of overlapping regions from body part semantic segmentation and generic object proposals, our optimization approach reassembles those pieces together into multiple person instances. Since optimal region assembly is a challenging combinatorial problem, we present a Lagrangian relaxation method to accelerate the lower bound estimation, thereby enabling a fast branch and bound solution for the global optimum. As output, our method produces a pixel-level map indicating both 1) the body part labels (arm, leg, torso, and head), and 2) which parts belong to which individual person. Our results on challenging datasets show our method is robust to clutter, occlusion, and complex poses. It outperforms a variety of competing methods, including existing detector CRF methods and region CNN approaches. In addition, we demonstrate its impact on a proxemics recognition task, which demands a precise representation of "whose body part is where" in crowded images.
count=1
* 3D Shape Segmentation With Projective Convolutional Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Kalogerakis_3D_Shape_Segmentation_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Kalogerakis_3D_Shape_Segmentation_CVPR_2017_paper.pdf)]
    * Title: 3D Shape Segmentation With Projective Convolutional Networks
    * Year: `2017`
    * Authors: Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji, Siddhartha Chaudhuri
    * Abstract: This paper introduces a deep architecture for segmenting 3D objects into their labeled semantic parts. Our architecture combines image-based Fully Convolutional Networks (FCNs) and surface-based Conditional Random Fields (CRFs) to yield coherent segmentations of 3D shapes. The image-based FCNs are used for efficient view-based reasoning about 3D object parts. Through a special projection layer, FCN outputs are effectively aggregated across multiple views and scales, then are projected onto the 3D object surfaces. Finally, a surface-based CRF combines the projected outputs with geometric consistency cues to yield coherent segmentations. The whole architecture (multi-view FCNs and CRF) is trained end-to-end. Our approach significantly outperforms the existing state-of-the-art methods in the currently largest segmentation benchmark (ShapeNet). Finally, we demonstrate promising segmentation results on noisy 3D shapes acquired from consumer-grade depth cameras.
count=1
* Fine-Grained Recognition of Thousands of Object Categories With Single-Example Training
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Karlinsky_Fine-Grained_Recognition_of_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Karlinsky_Fine-Grained_Recognition_of_CVPR_2017_paper.pdf)]
    * Title: Fine-Grained Recognition of Thousands of Object Categories With Single-Example Training
    * Year: `2017`
    * Authors: Leonid Karlinsky, Joseph Shtok, Yochay Tzur, Asaf Tzadok
    * Abstract: We approach the problem of fast detection and recognition of a large number (thousands) of object categories while training on a very limited amount of examples, usually one per category. Examples of this task include: (i) detection of retail products, where we have only one studio image of each product available for training; (ii) detection of brand logos; and (iii) detection of 3D objects and their respective poses within a static 2D image, where only a sparse subset of (partial) object views is available for training, with a single example for each view. Building a detector based on so few examples presents a significant challenge for the current top-performing (deep) learning based techniques, which require large amounts of data to train. Our approach for this task is based on a non-parametric probabilistic model for initial detection, CNN-based refinement and temporal integration where applicable. We successfully demonstrate its usefulness in a variety of experiments on both existing and our own benchmarks achieving state-of-the-art performance.
count=1
* SRN: Side-output Residual Network for Object Symmetry Detection in the Wild
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Ke_SRN_Side-output_Residual_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Ke_SRN_Side-output_Residual_CVPR_2017_paper.pdf)]
    * Title: SRN: Side-output Residual Network for Object Symmetry Detection in the Wild
    * Year: `2017`
    * Authors: Wei Ke, Jie Chen, Jianbin Jiao, Guoying Zhao, Qixiang Ye
    * Abstract: In this paper, we establish a baseline for object symmetry detection in complex backgrounds by presenting a new benchmark and an end-to-end deep learning approach, opening up a promising direction for symmetry detection in the wild. The new benchmark, named Sym-PASCAL, spans challenges including object diversity, multi-objects, part-invisibility, and various complex backgrounds that are far beyond those in existing datasets. The proposed symmetry detection approach, named Side-output Residual Network (SRN), leverages output Residual Units (RUs) to fit the errors between the object symmetry ground-truth and the outputs of RUs. By stacking RUs in a deep-to-shallow manner, SRN exploits the 'flow' of errors among multiple scales to ease the problems of fitting complex outputs with limited layers, suppressing the complex backgrounds, and effectively matching object symmetry of different scales. Experimental results validate both the benchmark and its challenging aspects related to real-world images, and the state-of-the-art performance of our symmetry detection approach. The benchmark and the code for SRN are publicly available at https://github.com/KevinKecc/SRN.
count=1
* Real-Time 3D Model Tracking in Color and Depth on a Single CPU Core
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Kehl_Real-Time_3D_Model_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Kehl_Real-Time_3D_Model_CVPR_2017_paper.pdf)]
    * Title: Real-Time 3D Model Tracking in Color and Depth on a Single CPU Core
    * Year: `2017`
    * Authors: Wadim Kehl, Federico Tombari, Slobodan Ilic, Nassir Navab
    * Abstract: We present a novel method to track 3D models in color and depth data. To this end, we introduce approximations that accelerate the state-of-the-art in region-based tracking by an order of magnitude while retaining similar accuracy. Furthermore, we show how the method can be made more robust in the presence of depth data and consequently formulate a new joint contour and ICP tracking energy. We present better results than the state-of-the-art while being much faster then most other methods and achieving all of the above on a single CPU core.
count=1
* Deep Learning of Human Visual Sensitivity in Image Quality Assessment Framework
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Kim_Deep_Learning_of_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Kim_Deep_Learning_of_CVPR_2017_paper.pdf)]
    * Title: Deep Learning of Human Visual Sensitivity in Image Quality Assessment Framework
    * Year: `2017`
    * Authors: Jongyoo Kim, Sanghoon Lee
    * Abstract: Since human observers are the ultimate receivers of digital images, image quality metrics should be designed from a human-oriented perspective. Conventionally, a number of full-reference image quality assessment (FR-IQA) methods adopted various computational models of the human visual system (HVS) from psychological vision science research. In this paper, we propose a novel convolutional neural networks (CNN) based FR-IQA model, named Deep Image Quality Assessment (DeepQA), where the behavior of the HVS is learned from the underlying data distribution of IQA databases. Different from previous studies, our model seeks the optimal visual weight based on understanding of database information itself without any prior knowledge of the HVS. Through the experiments, we show that the predicted visual sensitivity maps agree with the human subjective opinions. In addition, DeepQA achieves the state-of-the-art prediction accuracy among FR-IQA models.
count=1
* Unite the People: Closing the Loop Between 3D and 2D Human Representations
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Lassner_Unite_the_People_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Lassner_Unite_the_People_CVPR_2017_paper.pdf)]
    * Title: Unite the People: Closing the Loop Between 3D and 2D Human Representations
    * Year: `2017`
    * Authors: Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael J. Black, Peter V. Gehler
    * Abstract: 3D models provide a common ground for different representations of human bodies. In turn, robust 2D estimation has proven to be a powerful tool to obtain 3D fits "in-the-wild". However, depending on the level of detail, it can be hard to impossible to acquire labeled data for training 2D estimators on large scale. We propose a hybrid approach to this problem: with an extended version of the recently introduced SMPLify method, we obtain high quality 3D body model fits for multiple human pose datasets. Human annotators solely sort good and bad fits. This procedure leads to an initial dataset, UP-3D, with rich annotations. With a comprehensive set of experiments, we show how this data can be used to train discriminative models that produce results with an unprecedented level of detail: our models predict 31 segments and 91 landmark locations on the body. Using the 91 landmark pose estimator, we present state-of-the art results for 3D human pose and shape estimation using an order of magnitude less training data and without assumptions about gender or pose in the fitting procedure. We show that UP-3D can be enhanced with these improved fits to grow in quantity and quality, which makes the system deployable on large scale. The data, code and models are available for research purposes.
count=1
* Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf)]
    * Title: Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network
    * Year: `2017`
    * Authors: Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi
    * Abstract: Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.
count=1
* On Human Motion Prediction Using Recurrent Neural Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Martinez_On_Human_Motion_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Martinez_On_Human_Motion_CVPR_2017_paper.pdf)]
    * Title: On Human Motion Prediction Using Recurrent Neural Networks
    * Year: `2017`
    * Authors: Julieta Martinez, Michael J. Black, Javier Romero
    * Abstract: Human motion modelling is a classical problem at the intersection of graphics and computer vision, with applications spanning human-computer interaction, motion synthesis, and motion prediction for virtual and augmented reality. Following the success of deep learning methods in several computer vision tasks, recent work has focused on using deep recurrent neural networks (RNNs) to model human motion, with the goal of learning time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion synthesis. We examine recent work, with a focus on the evaluation methodologies commonly used in the literature, and show that, surprisingly, state of the art performance can be achieved by a simple baseline that does not attempt to model motion at all. We investigate this result, and analyze recent RNN methods by looking at the architectures, loss functions, and training procedures used in state-of-the-art approaches. We propose three changes to the standard RNN models typically used for human motion, which results in a simple and scalable RNN architecture that obtains state-of-the-art performance on human motion prediction.
count=1
* Semantically Consistent Regularization for Zero-Shot Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Morgado_Semantically_Consistent_Regularization_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Morgado_Semantically_Consistent_Regularization_CVPR_2017_paper.pdf)]
    * Title: Semantically Consistent Regularization for Zero-Shot Recognition
    * Year: `2017`
    * Authors: Pedro Morgado, Nuno Vasconcelos
    * Abstract: The role of semantics in zero-shot learning is considered. The effectiveness of previous approaches is analyzed according to the form of supervision provided. While some learn semantics independently, others only supervise the semantic subspace explained by training classes. Thus, the former is able to constrain the whole space but lacks the ability to model semantic correlations. The latter addresses this issue but leaves part of the semantic space unsupervised. This complementarity is exploited in a new convolutional neural network (CNN) framework, which proposes the use of semantics as constraints for recognition.Although a CNN trained for classification has no transfer ability, this can be encouraged by learning an hidden semantic layer together with a semantic code for classification. Two forms of semantic constraints are then introduced. The first is a loss-based regularizer that introduces a generalization constraint on each semantic predictor. The second is a codeword regularizer that favors semantic-to-class mappings consistent with prior semantic knowledge while allowing these to be learned from data. Significant improvements over the state-of-the-art are achieved on several datasets.
count=1
* Jointly Learning Energy Expenditures and Activities Using Egocentric Multimodal Signals
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Nakamura_Jointly_Learning_Energy_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Nakamura_Jointly_Learning_Energy_CVPR_2017_paper.pdf)]
    * Title: Jointly Learning Energy Expenditures and Activities Using Egocentric Multimodal Signals
    * Year: `2017`
    * Authors: Katsuyuki Nakamura, Serena Yeung, Alexandre Alahi, Li Fei-Fei
    * Abstract: Physiological signals such as heart rate can provide valuable information about an individual's state and activity. However, existing work on computer vision has not yet explored leveraging these signals to enhance egocentric video understanding. In this work, we propose a model for reasoning on multimodal data to jointly predict activities and energy expenditures. We use heart rate signals as privileged self-supervision to derive energy expenditure in a training stage. A multitask objective is used to jointly optimize the two tasks. Additionally, we introduce a dataset that contains 31 hours of egocentric video augmented with heart rate and acceleration signals. This study can lead to new applications such as a visual calorie counter.
count=1
* Harvesting Multiple Views for Marker-Less 3D Human Pose Annotations
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Pavlakos_Harvesting_Multiple_Views_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Pavlakos_Harvesting_Multiple_Views_CVPR_2017_paper.pdf)]
    * Title: Harvesting Multiple Views for Marker-Less 3D Human Pose Annotations
    * Year: `2017`
    * Authors: Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas Daniilidis
    * Abstract: Recent advances with Convolutional Networks (ConvNets) have shifted the bottleneck for many computer vision tasks to annotated data collection. In this paper, we present a geometry-driven approach to automatically collect annotations for human pose prediction tasks. Starting from a generic ConvNet for 2D human pose, and assuming a multi-view setup, we describe an automatic way to collect accurate 3D human pose annotations. We capitalize on constraints offered by the 3D geometry of the camera setup and the 3D structure of the human body to probabilistically combine per view 2D ConvNet predictions into a globally optimal 3D pose. This 3D pose is used as the basis for harvesting annotations. The benefit of the annotations produced automatically with our approach is demonstrated in two challenging settings: (i) fine-tuning a generic ConvNet-based 2D pose predictor to capture the discriminative aspects of a subject's appearance (i.e.,"personalization"), and (ii) training a ConvNet from scratch for single view 3D human pose prediction without leveraging 3D pose groundtruth. The proposed multi-view pose estimator achieves state-of-the-art results on standard benchmarks, demonstrating the effectiveness of our method in exploiting the available multi-view information.
count=1
* Deep Multitask Architecture for Integrated 2D and 3D Human Sensing
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Popa_Deep_Multitask_Architecture_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Popa_Deep_Multitask_Architecture_CVPR_2017_paper.pdf)]
    * Title: Deep Multitask Architecture for Integrated 2D and 3D Human Sensing
    * Year: `2017`
    * Authors: Alin-Ionut Popa, Mihai Zanfir, Cristian Sminchisescu
    * Abstract: We propose a deep multitask architecture for fully automatic 2d and 3d human sensing (DMHS), including recognition and reconstruction, in monocular images. The system computes the figure-ground segmentation, semantically identifies the human body parts at pixel level, and estimates the 2d and 3d pose of the person. The model supports the joint training of all components by means of multi-task losses where early processing stages recursively feed into advanced ones for increasingly complex calculations, accuracy and robustness. The design allows us to tie a complete training protocol, by taking advantage of multiple datasets that would otherwise restrictively cover only some of the model components: complex 2d image data with no body part labeling and without associated 3d ground truth, or complex 3d data with limited 2d background variability. In detailed experiments based on several challenging 2d and 3d datasets (LSP, HumanEva, Human3.6M), we evaluate the sub-structures of the model, the effect of various types of training data in the multitask loss, and demonstrate that state-of-the-art results can be achieved at all processing levels. We also show that in the wild our monocular RGB architecture is perceptually competitive to a state-of-the art (commercial) Kinect system based on RGB-D data.
count=1
* DeshadowNet: A Multi-Context Embedding Deep Network for Shadow Removal
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Qu_DeshadowNet_A_Multi-Context_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qu_DeshadowNet_A_Multi-Context_CVPR_2017_paper.pdf)]
    * Title: DeshadowNet: A Multi-Context Embedding Deep Network for Shadow Removal
    * Year: `2017`
    * Authors: Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, Rynson W. H. Lau
    * Abstract: Shadow removal is a challenging task as it requires the detection/annotation of shadows as well as semantic understanding of the scene. In this paper, we propose an automatic and end-to-end deep neural network (DeshadowNet) to tackle these problems in a unified manner. DeshadowNet is designed with a multi-context architecture, where the output shadow matte is predicted by embedding information from three different perspectives. The first global network extracts shadow features from a global view. Two levels of features are derived from the global network and transferred to two parallel networks. While one extracts the appearance of the input image, the other one involves semantic understanding for final prediction. These two complementary networks generate multi-context features to obtain the shadow matte with fine local details. To evaluate the performance of the proposed method, we construct the first large scale benchmark with 3088 image pairs. Extensive experiments on two publicly available benchmarks and our large-scale benchmark show that the proposed method performs favorably against several state-of-the-art methods.
count=1
* Learning Cross-Modal Embeddings for Cooking Recipes and Food Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Salvador_Learning_Cross-Modal_Embeddings_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Salvador_Learning_Cross-Modal_Embeddings_CVPR_2017_paper.pdf)]
    * Title: Learning Cross-Modal Embeddings for Cooking Recipes and Food Images
    * Year: `2017`
    * Authors: Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Ofli, Ingmar Weber, Antonio Torralba
    * Abstract: In this paper, we introduce Recipe1M, a new large-scale, structured corpus of over 1m cooking recipes and 800k food images. As the largest publicly available collection of recipe data, Recipe1M affords the ability to train high-capacity models on aligned, multi-modal data. Accordingly, we train a neural network to find a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task. Additionally, we demonstrate that regularization via the addition of a high-level, semantic classification objective improves performance to rival that of humans and enables semantic vector arithmetic. We postulate that these embeddings will provide a basis for further exploration of the Recipe1M dataset and food and cooking in general.
count=1
* Weakly Supervised Affordance Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Sawatzky_Weakly_Supervised_Affordance_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Sawatzky_Weakly_Supervised_Affordance_CVPR_2017_paper.pdf)]
    * Title: Weakly Supervised Affordance Detection
    * Year: `2017`
    * Authors: Johann Sawatzky, Abhilash Srikantha, Juergen Gall
    * Abstract: Localizing functional regions of objects or affordances is an important aspect of scene understanding and relevant for many robotics applications. In this work, we introduce a pixel-wise annotated affordance dataset of 3090 images containing 9916 object instances. Since parts of an object can have multiple affordances, we address this by a convo- lutional neural network for multilabel affordance segmen- tation. We also propose an approach to train the network from very few keypoint annotations. Our approach achieves a higher affordance detection accuracy than other weakly supervised methods that also rely on keypoint annotations or image annotations as weak supervision.
count=1
* Comparative Evaluation of Hand-Crafted and Learned Local Features
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Schonberger_Comparative_Evaluation_of_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Schonberger_Comparative_Evaluation_of_CVPR_2017_paper.pdf)]
    * Title: Comparative Evaluation of Hand-Crafted and Learned Local Features
    * Year: `2017`
    * Authors: Johannes L. Schonberger, Hans Hardmeier, Torsten Sattler, Marc Pollefeys
    * Abstract: Matching local image descriptors is a key step in many computer vision applications. For more than a decade, hand-crafted descriptors such as SIFT have been used for this task. Recently, multiple new descriptors learned from data have been proposed and shown to improve on SIFT in terms of discriminative power. This paper is dedicated to an extensive experimental evaluation of learned local features to establish a single evaluation protocol that ensures comparable results. In terms of matching performance, we evaluate the different descriptors regarding standard criteria. However, considering matching performance in isolation only provides an incomplete measure of a descriptor's quality. For example, finding additional correct matches between similar images does not necessarily lead to a better performance when trying to match images under extreme viewpoint or illumination changes. Besides pure descriptor matching, we thus also evaluate the different descriptors in the context of image-based reconstruction. This enables us to study the descriptor performance on a set of more practical criteria including image retrieval, the ability to register images under strong viewpoint and illumination changes, and the accuracy and completeness of the reconstructed cameras and scenes. To facilitate future research, the full evaluation pipeline is made publicly available.
count=1
* Learning From Simulated and Unsupervised Images Through Adversarial Training
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Shrivastava_Learning_From_Simulated_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Shrivastava_Learning_From_Simulated_CVPR_2017_paper.pdf)]
    * Title: Learning From Simulated and Unsupervised Images Through Adversarial Training
    * Year: `2017`
    * Authors: Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, Russell Webb
    * Abstract: With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.
count=1
* Learning to Detect Salient Objects With Image-Level Supervision
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Learning_to_Detect_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Learning_to_Detect_CVPR_2017_paper.pdf)]
    * Title: Learning to Detect Salient Objects With Image-Level Supervision
    * Year: `2017`
    * Authors: Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, Xiang Ruan
    * Abstract: Deep Neural Networks (DNNs) have substantially improved the state-of-the-art in salient object detection. However, training DNNs requires costly pixel-level annotations. In this paper, we leverage the observation that image-level tags provide important cues of foreground salient objects, and develop a weakly supervised learning method for saliency detection using image-level tags only. The Foreground Inference Network (FIN) is introduced for this challenging task. In the first stage of our training method, FIN is jointly trained with a fully convolutional network (FCN) for image-level tag prediction. A global smooth pooling layer is proposed, enabling FCN to assign object category tags to corresponding object regions, while FIN is capable of capturing all potential foreground regions with the predicted saliency maps. In the second stage, FIN is fine-tuned with its predicted saliency maps as ground truth. For refinement of ground truth, an iterative Conditional Random Field is developed to enforce spatial label consistency and further boost performance. Our method alleviates annotation efforts and allows the usage of existing large scale training sets with image-level tags. Our model runs at 60 FPS, outperforms unsupervised ones with a large margin, and achieves comparable or even superior performance than fully supervised counterparts.
count=1
* Recurrent Modeling of Interaction Context for Collective Activity Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Recurrent_Modeling_of_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Recurrent_Modeling_of_CVPR_2017_paper.pdf)]
    * Title: Recurrent Modeling of Interaction Context for Collective Activity Recognition
    * Year: `2017`
    * Authors: Minsi Wang, Bingbing Ni, Xiaokang Yang
    * Abstract: Modeling of high order interactional context, e.g., group interaction, lies in the central of collective/group activity recognition. However, most of the previous activity recognition methods do not offer a flexible and scalable scheme to handle the high order context modeling problem. To explicitly address this fundamental bottleneck, we propose a recurrent interactional context modeling scheme based on LSTM network. By utilizing the information propagation/aggregation capability of LSTM, the proposed scheme unifies the interactional feature modeling process for single person dynamics, intra-group (e.g., persons within a group) and inter-group(e.g., group to group)interactions. The proposed high order context modeling scheme produces more discriminative/descriptive interactional features. It is very flexible to handle a varying number of input instances (e.g., different number of persons in a group or different number of groups) and linearly scalable to high order context modeling problem. Extensive experiments on two benchmark collective/group activity datasets demonstrate the effectiveness of the proposed method.
count=1
* Predicting Ground-Level Scene Layout From Aerial Imagery
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Zhai_Predicting_Ground-Level_Scene_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhai_Predicting_Ground-Level_Scene_CVPR_2017_paper.pdf)]
    * Title: Predicting Ground-Level Scene Layout From Aerial Imagery
    * Year: `2017`
    * Authors: Menghua Zhai, Zachary Bessinger, Scott Workman, Nathan Jacobs
    * Abstract: We introduce a novel strategy for learning to extract semantically meaningful features from aerial imagery. Instead of manually labeling the aerial imagery, we propose to predict (noisy) semantic features automatically extracted from co-located ground imagery. Our network architecture takes an aerial image as input, extracts features using a convolutional neural network, and then applies an adaptive transformation to map these features into the ground-level perspective. We use an end-to-end learning approach to minimize the difference between the semantic segmentation extracted directly from the ground image and the semantic segmentation predicted solely based on the aerial image. We show that a model learned using this strategy, with no additional training, is already capable of rough semantic labeling of aerial imagery. Furthermore, we demonstrate that by finetuning this model we can achieve more accurate semantic segmentation than two baseline initialization strategies. We use our network to address the task of estimating the geolocation and geo-orientation of a ground image. Finally, we show how features extracted from an aerial image can be used to hallucinate a plausible ground-level panorama.
count=1
* 4D Light Field Superpixel and Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/html/Zhu_4D_Light_Field_CVPR_2017_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_4D_Light_Field_CVPR_2017_paper.pdf)]
    * Title: 4D Light Field Superpixel and Segmentation
    * Year: `2017`
    * Authors: Hao Zhu, Qi Zhang, Qing Wang
    * Abstract: Superpixel segmentation of 2D image has been widely used in many computer vision tasks. However, limited to the Gaussian imaging principle, there is not a thorough segmentation solution to the ambiguity in defocus and occlusion boundary areas. In this paper, we consider the essential element of image pixel, i.e., rays in the light space and propose light field superpixel (LFSP) segmentation to eliminate the ambiguity. The LFSP is first defined mathematically and then a refocus-invariant metric named LFSP self-similarity is proposed to evaluate the segmentation performance. By building a clique system containing 80 neighbors in light field, a robust refocus-invariant LFSP segmentation algorithm is developed. Experimental results on both synthetic and real light field datasets demonstrate the advantages over the state-of-the-arts in terms of traditional evaluation metrics. Additionally the LFSP self-similarity evaluation under different light field refocus levels shows the refocus-invariance of the proposed algorithm.
count=1
* GIFT: A Real-Time and Scalable 3D Shape Search Engine
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Bai_GIFT_A_Real-Time_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Bai_GIFT_A_Real-Time_CVPR_2016_paper.pdf)]
    * Title: GIFT: A Real-Time and Scalable 3D Shape Search Engine
    * Year: `2016`
    * Authors: Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, Longin Jan Latecki
    * Abstract: Projective analysis is an important solution for 3D shape retrieval, since human visual perceptions of 3D shapes rely on various 2D observations from different view points. Although multiple informative and discriminative views are utilized, most projection-based retrieval systems suffer from heavy computational cost, thus cannot satisfy the basic requirement of scalability for search engines. In this paper, we present a real-time 3D shape search engine based on the projective images of 3D shapes. The real-time property of our search engine results from the following aspects: (1) efficient projection and view feature extraction using GPU acceleration; (2) the first inverted file, referred as F-IF, is utilized to speed up the procedure of multi-view matching; (3) the second inverted file (S-IF), which captures a local distribution of 3D shapes in the feature manifold, is adopted for efficient context-based reranking. As a result, for each query the retrieval task can be finished within one second despite the necessary cost of IO overhead. We name the proposed 3D shape search engine, which combines GPU acceleration and Inverted File (Twice), as GIFT. Besides its high efficiency, GIFT also outperforms the state-of-the-art methods significantly in retrieval accuracy on various shape benchmarks and competitions
count=1
* Semantic Segmentation With Boundary Neural Fields
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Bertasius_Semantic_Segmentation_With_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Bertasius_Semantic_Segmentation_With_CVPR_2016_paper.pdf)]
    * Title: Semantic Segmentation With Boundary Neural Fields
    * Year: `2016`
    * Authors: Gedas Bertasius, Jianbo Shi, Lorenzo Torresani
    * Abstract: The state-of-the-art in semantic segmentation is currently represented by fully convolutional networks (FCNs). However, FCNs use large receptive fields and many pooling layers, both of which cause blurring and low spatial resolution in the deep layers. As a result FCNs tend to produce segmentations that are poorly localized around object boundaries. Prior work has attempted to address this issue in post-processing steps, for example using a color-based CRF on top of the FCN predictions. However, these approaches require additional parameters and low-level features that are difficult to tune and integrate into the original network architecture. Additionally, most CRFs use color-based pixel affinities, which are not well suited for semantic segmentation and lead to spatially disjoint predictions. To overcome these problems, we introduce a Boundary Neural Field (BNF), which is a global energy model integrating FCN predictions with boundary cues. The boundary information is used to enhance semantic segment coherence and to improve object localization. Specifically, we first show that the convolutional filters of semantic FCNs provide good features for boundary detection. We then employ the predicted boundaries to define pairwise potentials in our energy. Finally, we show that our energy decomposes semantic segmentation into multiple binary problems, which can be relaxed for efficient global optimization. We report extensive experiments demonstrating that minimization of our global boundary-based energy yields results superior to prior globalization methods, both quantitatively as well as qualitatively.
count=1
* Staple: Complementary Learners for Real-Time Tracking
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Bertinetto_Staple_Complementary_Learners_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Bertinetto_Staple_Complementary_Learners_CVPR_2016_paper.pdf)]
    * Title: Staple: Complementary Learners for Real-Time Tracking
    * Year: `2016`
    * Authors: Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej Miksik, Philip H. S. Torr
    * Abstract: Correlation Filter-based trackers have recently achieved excellent performance, showing great robustness to challenging situations exhibiting motion blur and illumination changes. However, since the model that they learn depends strongly on the spatial layout of the tracked object, they are notoriously sensitive to deformation. Models based on colour statistics have complementary traits: they cope well with variation in shape, but suffer when illumination is not consistent throughout a sequence. Moreover, colour distributions alone can be insufficiently discriminative. In this paper, we show that a simple tracker combining complementary cues in a ridge regression framework can operate faster than 80 FPS and outperform not only all entries in the popular VOT14 competition, but also recent and far more sophisticated trackers according to multiple benchmarks.
count=1
* RIFD-CNN: Rotation-Invariant and Fisher Discriminative Convolutional Neural Networks for Object Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Cheng_RIFD-CNN_Rotation-Invariant_and_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Cheng_RIFD-CNN_Rotation-Invariant_and_CVPR_2016_paper.pdf)]
    * Title: RIFD-CNN: Rotation-Invariant and Fisher Discriminative Convolutional Neural Networks for Object Detection
    * Year: `2016`
    * Authors: Gong Cheng, Peicheng Zhou, Junwei Han
    * Abstract: Thanks to the powerful feature representations obtained through deep convolutional neural network (CNN), the performance of object detection has recently been substantially boosted. Despite the remarkable success, the problems of object rotation, within-class variability, and between-class similarity remain several major challenges. To address these problems, this paper proposes a novel and effective method to learn a rotation-invariant and Fisher discriminative CNN (RIFD-CNN) model. This is achieved by introducing and learning a rotation-invariant layer and a Fisher discriminative layer, respectively, on the basis of the existing high-capacity CNN architectures. Specifically, the rotation-invariant layer is trained by imposing an explicit regularization constraint on the objective function that enforces invariance on the CNN features before and after rotating. The Fisher discriminative layer is trained by imposing the Fisher discrimination criterion on the CNN features so that they have small within-class scatter but large between-class separation. In the experiments, we comprehensively evaluate the proposed method for object detection task on a public available aerial image dataset and the PASCAL VOC 2007 dataset. State-of-the-art results are achieved compared with the existing baseline methods.
count=1
* Canny Text Detector: Fast and Robust Scene Text Localization Algorithm
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Cho_Canny_Text_Detector_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Cho_Canny_Text_Detector_CVPR_2016_paper.pdf)]
    * Title: Canny Text Detector: Fast and Robust Scene Text Localization Algorithm
    * Year: `2016`
    * Authors: Hojin Cho, Myungchul Sung, Bongjin Jun
    * Abstract: This paper presents a novel scene text detection algorithm, Canny Text Detector, which takes advantage of the similarity between image edge and text for effective text localization with improved recall rate. As closely related edge pixels construct the structural information of an object, we observe that cohesive characters compose a meaningful word/sentence sharing similar properties such as spatial location, size, color, and stroke width regardless of language. However, prevalent scene text detection approaches have not fully utilized such similarity, but mostly rely on the characters classified with high confidence, leading to low recall rate. By exploiting the similarity, our approach can quickly and robustly localize a variety of texts. Inspired by the original Canny edge detector, our algorithm makes use of double threshold and hysteresis tracking to detect texts of low confidence. Experimental results on public datasets demonstrate that our algorithm outperforms the state-of-the-art scene text detection methods in terms of detection rate.
count=1
* Fine-Grained Categorization and Dataset Bootstrapping Using Deep Metric Learning With Humans in the Loop
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Cui_Fine-Grained_Categorization_and_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Cui_Fine-Grained_Categorization_and_CVPR_2016_paper.pdf)]
    * Title: Fine-Grained Categorization and Dataset Bootstrapping Using Deep Metric Learning With Humans in the Loop
    * Year: `2016`
    * Authors: Yin Cui, Feng Zhou, Yuanqing Lin, Serge Belongie
    * Abstract: Existing fine-grained visual categorization methods often suffer from three challenges: lack of training data, large number of fine-grained categories, and high intra-class vs. low inter-class variance. In this work we propose a generic iterative framework for fine-grained categorization and dataset bootstrapping that handles these three challenges. Using deep metric learning with humans in the loop, we learn a low dimensional feature embedding with anchor points on manifolds for each category. These anchor points capture intra-class variances and remain discriminative between classes. In each round, images with high confidence scores from our model are sent to humans for labeling. By comparing with exemplar images, labelers mark each candidate image as either a "true positive" or a "false positive." True positives are added into our current dataset and false positives are regarded as "hard negatives" for our metric learning model. Then the model is re-trained with an expanded dataset and hard negatives for the next round. To demonstrate the effectiveness of the proposed framework, we bootstrap a fine-grained flower dataset with 620 categories from Instagram images. The proposed deep metric learning scheme is evaluated on both our dataset and the CUB-200-2001 Birds dataset. Experimental evaluations show significant performance gain using dataset bootstrapping and demonstrate state-of-the-art results achieved by the proposed deep metric learning methods.
count=1
* Solving Temporal Puzzles
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Dicle_Solving_Temporal_Puzzles_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Dicle_Solving_Temporal_Puzzles_CVPR_2016_paper.pdf)]
    * Title: Solving Temporal Puzzles
    * Year: `2016`
    * Authors: Caglayan Dicle, Burak Yilmaz, Octavia Camps, Mario Sznaier
    * Abstract: Many physical phenomena, within short time windows, can be explained by low order differential relations. In a discrete world, these relations can be described using low order difference equations or equivalently low order auto regressive (AR) models. In this paper, based on this intuition, we propose an algorithm for solving time-sort temporal puzzles, defined as scrambled time series that need to be sorted out. We frame this highly combinatorial problem using a mixed-integer semi definite programming formulation and show how to turn it into a mixed-integer linear programming problem by using the recently introduced atomic norm framework. Our experiments show the effectiveness and generality of our approach in different scenarios.
count=1
* 3D Shape Attributes
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Fouhey_3D_Shape_Attributes_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Fouhey_3D_Shape_Attributes_CVPR_2016_paper.pdf)]
    * Title: 3D Shape Attributes
    * Year: `2016`
    * Authors: David F. Fouhey, Abhinav Gupta, Andrew Zisserman
    * Abstract: In this paper we investigate 3D attributes as a means to understand the shape of an object in a single image. To this end, we make a number of contributions: (i) we introduce and define a set of 3D Shape attributes, including planarity, symmetry and occupied space; (ii) we show that such properties can be successfully inferred from a single image using a Convolutional Neural Network (CNN); (iii) we introduce a 143K image dataset of sculptures with 2197 works over 242 artists for training and evaluating the CNN; (iv) we show that the 3D attributes trained on this dataset generalize to images of other (non-sculpture) object classes; and furthermore (v) we show that the CNN also provides a shape embedding that can be used to match previously unseen sculptures largely independent of viewpoint.
count=1
* Detecting Repeating Objects Using Patch Correlation Analysis
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Huberman_Detecting_Repeating_Objects_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Huberman_Detecting_Repeating_Objects_CVPR_2016_paper.pdf)]
    * Title: Detecting Repeating Objects Using Patch Correlation Analysis
    * Year: `2016`
    * Authors: Inbar Huberman, Raanan Fattal
    * Abstract: In this paper we describe a new method for detecting and counting a repeating object in an image. While the method relies on a fairly sophisticated deformable part model, unlike existing techniques it estimates the model parameters in an unsupervised fashion thus alleviating the need for a user-annotated training data and avoiding the associated specificity. This automatic fitting process is carried out by exploiting the recurrence of small image patches associated with the repeating object and analyzing their spatial correlation. The analysis allows us to reject outlier patches, recover the visual and shape parameters of the part model, and detect the object instances efficiently. In order to achieve a practical system which is able to cope with diverse images, we describe a simple and intuitive active-learning procedure that updates the object classification by querying the user on very few carefully chosen marginal classifications. Evaluation of the new method against the state-of-the-art techniques demonstrates its ability to achieve higher accuracy through a better user experience.
count=1
* A Hierarchical Deep Temporal Model for Group Activity Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Ibrahim_A_Hierarchical_Deep_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Ibrahim_A_Hierarchical_Deep_CVPR_2016_paper.pdf)]
    * Title: A Hierarchical Deep Temporal Model for Group Activity Recognition
    * Year: `2016`
    * Authors: Mostafa S. Ibrahim, Srikanth Muralidharan, Zhiwei Deng, Arash Vahdat, Greg Mori
    * Abstract: In group activity recognition, the temporal dynamics of the whole activity can be inferred based on the dynamics of the individual people representing the activity. We build a deep model to capture these dynamics based on LSTM (long short-term memory) models. To make use of these observations, we present a 2-stage deep temporal model for the group activity recognition problem. In our model, a LSTM model is designed to represent action dynamics of individual people in a sequence and another LSTM model is designed to aggregate person-level information for whole activity understanding. We evaluate our model over two datasets: the Collective Activity Dataset and a new volleyball dataset. Experimental results demonstrate that our proposed model improves group activity recognition performance compared to baseline methods.
count=1
* Active Image Segmentation Propagation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Jain_Active_Image_Segmentation_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Jain_Active_Image_Segmentation_CVPR_2016_paper.pdf)]
    * Title: Active Image Segmentation Propagation
    * Year: `2016`
    * Authors: Suyog Dutt Jain, Kristen Grauman
    * Abstract: We propose a semi-automatic method to obtain foreground object masks for a large set of related images. We develop a stagewise active approach to propagation: in each stage, we actively determine the images that appear most valuable for human annotation, then revise the foreground estimates in all unlabeled images accordingly. In order to identify images that, once annotated, will propagate well to other examples, we introduce an active selection procedure that operates on the joint segmentation graph over all images. It prioritizes human intervention for those images that are uncertain and influential in the graph, while also mutually diverse. We apply our method to obtain foreground masks for over 1 million images. Our method yields state-of-the-art accuracy on the ImageNet and MIT Object Discovery datasets, and it focuses human attention more effectively than existing propagation strategies.
count=1
* Accurate Image Super-Resolution Using Very Deep Convolutional Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.pdf)]
    * Title: Accurate Image Super-Resolution Using Very Deep Convolutional Networks
    * Year: `2016`
    * Authors: Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee
    * Abstract: We present a highly accurate single image superresolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification [19]. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.
count=1
* Deeply-Recursive Convolutional Network for Image Super-Resolution
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper.pdf)]
    * Title: Deeply-Recursive Convolutional Network for Image Super-Resolution
    * Year: `2016`
    * Authors: Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee
    * Abstract: We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/ vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive supervision and skip-connection. Our method outperforms previous methods by a large margin.
count=1
* Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Koller_Deep_Hand_How_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Koller_Deep_Hand_How_CVPR_2016_paper.pdf)]
    * Title: Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled
    * Year: `2016`
    * Authors: Oscar Koller, Hermann Ney, Richard Bowden
    * Abstract: This work presents a new approach to learning a frame-based classifier on weakly labelled sequence data by embedding a CNN within an iterative EM algorithm. This allows the CNN to be trained on a vast number of example images when only loose sequence level information is available for the source videos. Although we demonstrate this in the context of hand shape recognition, the approach has wider application to any video recognition task where frame level labelling is not available. The iterative EM algorithm leverages the discriminative ability of the CNN to iteratively refine the frame level annotation and subsequent training of the CNN. By embedding the classifier within an EM framework the CNN can easily be trained on 1 million hand images. We demonstrate that the final classifier generalises over both individuals and data sets. The algorithm is evaluated on over 3000 manually labelled hand shape images of 60 different classes which will be released to the community. Furthermore, we demonstrate its use in continuous sign language recognition on two publicly available large sign language data sets, where it outperforms the current state-of-the-art by a large margin. To our knowledge no previous work has explored expectation maximization without Gaussian mixture models to exploit weak sequence labels for sign language recognition.
count=1
* Eye Tracking for Everyone
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Krafka_Eye_Tracking_for_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Krafka_Eye_Tracking_for_CVPR_2016_paper.pdf)]
    * Title: Eye Tracking for Everyone
    * Year: `2016`
    * Authors: Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Harini Kannan, Suchendra Bhandarkar, Wojciech Matusik, Antonio Torralba
    * Abstract: From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2:5M frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device. Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results. The code, data, and models are available at http://gazecapture.csail.mit.edu.
count=1
* Efficient Globally Optimal 2D-To-3D Deformable Shape Matching
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Lahner_Efficient_Globally_Optimal_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Lahner_Efficient_Globally_Optimal_CVPR_2016_paper.pdf)]
    * Title: Efficient Globally Optimal 2D-To-3D Deformable Shape Matching
    * Year: `2016`
    * Authors: Zorah Lahner, Emanuele Rodola, Frank R. Schmidt, Michael M. Bronstein, Daniel Cremers
    * Abstract: We propose the first algorithm for non-rigid 2D-to-3D shape matching, where the input is a 2D query shape as well as a 3D target shape and the output is a continuous matching curve represented as a closed contour on the 3D shape. We cast the problem as finding the shortest circular path on the product 3-manifold of the two shapes. We prove that the optimal matching can be computed in polynomial time with a (worst-case) complexity of O(m*n^2*log(n)), where m and n denote the number of vertices on the 2D and the 3D shape respectively. Quantitative evaluation confirms that the method provides excellent results for sketch-based deformable 3D shape retrieval.
count=1
* Conditional Graphical Lasso for Multi-Label Image Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Conditional_Graphical_Lasso_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Conditional_Graphical_Lasso_CVPR_2016_paper.pdf)]
    * Title: Conditional Graphical Lasso for Multi-Label Image Classification
    * Year: `2016`
    * Authors: Qiang Li, Maoying Qiao, Wei Bian, Dacheng Tao
    * Abstract: Multi-label image classification aims to predict multiple labels for a single image which contains diverse content. By utilizing label correlations, various techniques have been developed to improve classification performance. However, current existing methods either neglect image features when exploiting label correlations or lack the ability to learn image-dependent conditional label structures. In this paper, we develop conditional graphical Lasso (CGL) to handle these challenges. CGL provides a unified Bayesian framework for structure and parameter learning conditioned on image features. We formulate the multi-label prediction as CGL inference problem, which is solved by a mean field variational approach. Meanwhile, CGL learning is efficient due to a tailored proximal gradient procedure by applying the maximum a posterior (MAP) methodology. CGL performs competitively for multi-label image classification on benchmark datasets MULAN scene, PASCAL VOC 2007 and PASCAL VOC 2012, compared with the state-of-the-art multi-label classification algorithms.
count=1
* TGIF: A New Dataset and Benchmark on Animated GIF Description
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Li_TGIF_A_New_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_TGIF_A_New_CVPR_2016_paper.pdf)]
    * Title: TGIF: A New Dataset and Benchmark on Animated GIF Description
    * Year: `2016`
    * Authors: Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, Jiebo Luo
    * Abstract: With the recent popularity of animated GIFs on social media, there is need for ways to index them with rich metadata. To advance research on animated GIF understanding, we collected a new dataset, Tumblr GIF (TGIF), with 100K animated GIFs from Tumblr and 120K natural language descriptions obtained via crowdsourcing. The motivation for this work is to develop a testbed for image sequence description systems, where the task is to generate natural language descriptions for animated GIFs or video clips. To ensure a high quality dataset, we developed a series of novel quality controls to validate free-form text input from crowdworkers. We show that there is unambiguous association between visual content and natural language descriptions in our dataset, making it an ideal benchmark for the visual content captioning task. We perform extensive statistical analyses to compare our dataset to existing image and video description datasets. Next, we provide baseline results on the animated GIF description task, using three representative techniques: nearest neighbor, statistical machine translation, and recurrent neural networks. Finally, we show that models fine-tuned from our animated GIF description dataset can be helpful for automatic movie description.
count=1
* Geospatial Correspondences for Multimodal Registration
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Marcos_Geospatial_Correspondences_for_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Marcos_Geospatial_Correspondences_for_CVPR_2016_paper.pdf)]
    * Title: Geospatial Correspondences for Multimodal Registration
    * Year: `2016`
    * Authors: Diego Marcos, Raffay Hamid, Devis Tuia
    * Abstract: The growing availability of very high resolution (<1 m/pixel) satellite and aerial images has opened up unprecedented opportunities to monitor and analyze the evolution of land-cover and land-use across the world. To do so, images of the same geographical areas acquired at different times and, potentially, with different sensors must be efficiently parsed to update maps and detect land-cover changes. However, a naive transfer of ground truth labels from one location in the source image to the corresponding location in the target image is not generally feasible, as these images are often only loosely registered (with up to +- 50m of non-uniform errors). Furthermore, land-cover changes in an area over time must be taken into account for an accurate ground truth transfer. To tackle these challenges, we propose a mid-level sensor-invariant representation that encodes image regions in terms of the spatial distribution of their spectral neighbors. We incorporate this representation in a Markov Random Field to simultaneously account for nonlinear mis-registrations and enforce locality priors to find matches between multi-sensor images. We show how our approach can be used to assist in several multimodal land-cover update and change detection problems.
count=1
* Reinforcement Learning for Visual Object Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Mathe_Reinforcement_Learning_for_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Mathe_Reinforcement_Learning_for_CVPR_2016_paper.pdf)]
    * Title: Reinforcement Learning for Visual Object Detection
    * Year: `2016`
    * Authors: Stefan Mathe, Aleksis Pirinen, Cristian Sminchisescu
    * Abstract: One of the most widely used strategies for visual object detection is based on exhaustive spatial hypothesis search. While methods like sliding windows have been successful and effective for many years, they are still brute-force, independent of the image content and the visual category being searched. In this paper we present formally rigorous sequential models that accumulate evidence collected at a small set of image locations in order to detect visual objects effectively. By formulating sequential search as reinforcement learning of the search policy (including the stopping condition), our fully trainable model can explicitly balance for each class, specifically, the conflicting goals of exploration -- sampling more image regions for better accuracy --, and exploitation -- stopping the search efficiently when sufficiently confident in the target's location. The methodology is general and applicable to any detector response function. We report encouraging results in the PASCAL VOC 2012 object detection test set showing that the proposed methodology achieves almost two orders of magnitude speed-up over sliding window methods.
count=1
* Active Learning for Delineation of Curvilinear Structures
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Mosinska-Domanska_Active_Learning_for_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Mosinska-Domanska_Active_Learning_for_CVPR_2016_paper.pdf)]
    * Title: Active Learning for Delineation of Curvilinear Structures
    * Year: `2016`
    * Authors: Agata Mosinska-Domanska, Raphael Sznitman, Przemyslaw Glowacki, Pascal Fua
    * Abstract: Many recent delineation techniques owe much of their increased effectiveness to path classification algorithms that make it possible to distinguish promising paths from others. The downside of this development is that they require annotated training data, which is tedious to produce. In this paper, we propose an Active Learning approach that considerably speeds up the annotation process. Unlike standard ones, it takes advantage of the specificities of the delineation problem. It operates on a graph and can reduce the training set size by up to 80% without compromising the reconstruction quality. We will show that our approach outperforms conventional ones on various biomedical and natural image datasets, thus showing that it is broadly applicable.
count=1
* Determining Occlusions From Space and Time Image Reconstructions
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Perez-Rua_Determining_Occlusions_From_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Perez-Rua_Determining_Occlusions_From_CVPR_2016_paper.pdf)]
    * Title: Determining Occlusions From Space and Time Image Reconstructions
    * Year: `2016`
    * Authors: Juan-Manuel Perez-Rua, Tomas Crivelli, Patrick Bouthemy, Patrick Perez
    * Abstract: The problem of localizing occlusions between consecutive frames of a video is important but rarely tackled on its own. In most works, it is tightly interleaved with the computation of accurate optical flows, which leads to a delicate chicken-and-egg problem. With this in mind, we propose a novel approach to occlusion detection where visibility or not of a point in next frame is formulated in terms of visual reconstruction. The key issue is now to determine how well a pixel in the first image can be "recon- structed" from co-located colors in the next image. We first exploit this reasoning at the pixel level with a new detection criterion. Contrary to the ubiquitous displaced-frame-difference and forward-backward flow vector matching, the proposed alternative does not critically depend on a precomputed, dense displacement field, while being shown to be more effective. We then leverage this local modeling within an energy-minimization framework that delivers occlusion maps. An easy-to-obtain collection of parametric motion models is exploited within the energy to provide the required level of motion information. Our approach outperforms state-of-the-art detection methods on the challenging MPI Sintel dataset.
count=1
* Less Is More: Zero-Shot Learning From Online Textual Documents With Noise Suppression
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Qiao_Less_Is_More_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Qiao_Less_Is_More_CVPR_2016_paper.pdf)]
    * Title: Less Is More: Zero-Shot Learning From Online Textual Documents With Noise Suppression
    * Year: `2016`
    * Authors: Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton van den Hengel
    * Abstract: Classifying a visual concept merely from its associated online textual source, such as a Wikipedia article, is an attractive research topic in zero-shot learning because it alleviates the burden of manually collecting semantic attributes. Several recent works have pursued this approach by exploring various ways of connecting the visual and text domains. This paper revisits this idea by stepping further to consider one important factor: the textual representation is usually too noisy for the zero-shot learning application. This consideration motivates us to design a simple-but-effective zero-shot learning method capable of suppressing noise in the text. More specifically, we propose an l_2,1-norm based objective function which can simultaneously suppress the noisy signal in the text and learn a function to match the text document and visual features. We also develop an optimization algorithm to efficiently solve the resulting problem. By conducting experiments on two large datasets, we demonstrate that the proposed method significantly outperforms the competing methods which rely on online information sources but without explicit noise suppression. We further make an in-depth analysis of the proposed method and provide insight as to what kind of information in documents is useful for zero-shot learning.
count=1
* The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf)]
    * Title: The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes
    * Year: `2016`
    * Authors: German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, Antonio M. Lopez
    * Abstract: Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images; thus, having a sufficient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for semantic segmentation -- in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show how the inclusion of SYNTHIA in the training stage significantly improves performance on the semantic segmentation task.
count=1
* Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Shi_Real-Time_Single_Image_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Shi_Real-Time_Single_Image_CVPR_2016_paper.pdf)]
    * Title: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network
    * Year: `2016`
    * Authors: Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel Rueckert, Zehan Wang
    * Abstract: Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.
count=1
* Predicting the Where and What of Actors and Actions Through Online Action Localization
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Soomro_Predicting_the_Where_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Soomro_Predicting_the_Where_CVPR_2016_paper.pdf)]
    * Title: Predicting the Where and What of Actors and Actions Through Online Action Localization
    * Year: `2016`
    * Authors: Khurram Soomro, Haroon Idrees, Mubarak Shah
    * Abstract: This paper proposes a novel approach to tackle the challenging problem of 'online action localization' which entails predicting actions and their locations as they happen in a video. Typically, action localization or recognition is performed in an offline manner where all the frames in the video are processed together and action labels are not predicted for the future. This dis-allows timely localization of actions - an important consideration for surveillance tasks. In our approach, given a batch of frames from the immediate past in a video, we estimate pose and over- segment the current frame into superpixels. Next, we discriminatively train an actor foreground model on the superpixels using the pose bounding boxes. A Conditional Random Field with superpixels as nodes, and edges connecting spatio-temporal neighbors is used to obtain action segments. The action confidence is predicted using dynamic programming on SVM scores obtained on short segments of the video, thereby capturing sequential information of the actions. The issue of visual drift is handled by updating the appearance model and pose refinement in an online manner. Lastly, we introduce a new measure to quantify the performance of action prediction (i.e. online action localization), which analyzes how the prediction accuracy varies as a function of observed portion of the video. Our experiments suggest that despite using only a few frames to localize actions at each time instant, we are able to predict the action and obtain competitive results to state-of-the-art offline methods.
count=1
* Convolutional Pose Machines
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Wei_Convolutional_Pose_Machines_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Wei_Convolutional_Pose_Machines_CVPR_2016_paper.pdf)]
    * Title: Convolutional Pose Machines
    * Year: `2016`
    * Authors: Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh
    * Abstract: Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.
count=1
* Dense Human Body Correspondences Using Convolutional Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Wei_Dense_Human_Body_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Wei_Dense_Human_Body_CVPR_2016_paper.pdf)]
    * Title: Dense Human Body Correspondences Using Convolutional Networks
    * Year: `2016`
    * Authors: Lingyu Wei, Qixing Huang, Duygu Ceylan, Etienne Vouga, Hao Li
    * Abstract: We propose a deep learning approach for finding dense correspondences between 3D scans of people. Our method requires only partial geometric information in the form of two depth maps or partial reconstructed surfaces, works for humans in arbitrary poses and wearing any clothing, does not require the two people to be scanned from similar viewpoints, and runs in real time. We use a deep convolutional neural network to train a feature descriptor on depth map pixels, but crucially, rather than training the network to solve the shape correspondence problem directly, we train it to solve a body region classification problem, modified to increase the smoothness of the learned descriptors near region boundaries. This approach ensures that nearby points on the human body are nearby in feature space, and vice versa, rendering the feature descriptor suitable for computing dense correspondences between the scans. We validate our method on real and synthetic data for both clothed and unclothed humans, and show that our correspondences are more robust than is possible with state-of-the-art unsupervised methods, and more accurate to those found using methods that require full watertight 3D geometry.
count=1
* Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Xie_Semantic_Instance_Annotation_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_Semantic_Instance_Annotation_CVPR_2016_paper.pdf)]
    * Title: Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer
    * Year: `2016`
    * Authors: Jun Xie, Martin Kiefel, Ming-Ting Sun, Andreas Geiger
    * Abstract: This supplementary material provides additional illustrations, visualizations and experiments. We start by showing the color coding and label mapping used for the semantic and instance label results in the paper. Then we provide more details about the 3D fold/curb detection and parameter settings that are used in the paper. Next, we provide additional quantitative and qualitative semi-dense inference results for both semantic and instance segmentation. Finally, we show the ability of our method to annotate 3D point clouds with semantic and instance labels which is a byproduct of our approach.
count=1
* Top-Push Video-Based Person Re-Identification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/You_Top-Push_Video-Based_Person_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/You_Top-Push_Video-Based_Person_CVPR_2016_paper.pdf)]
    * Title: Top-Push Video-Based Person Re-Identification
    * Year: `2016`
    * Authors: Jinjie You, Ancong Wu, Xiang Li, Wei-Shi Zheng
    * Abstract: Most existing person re-identification (re-id) models focus on matching still person images across disjoint camera views using the setting of either single-shot or multi-shot. Since limited information can be exploited from still images, it is hard (if not impossible) to overcome the occlusion, pose and camera-view change, and lighting variation problems. In comparison, video-based re-id methods can utilize extra space-time information, which contains much more rich cues for matching to overcome the mentioned problems. However, in this work, we find that when using video-based representation, some inter-class difference can be much more obscure than the one when using still-image-based representation, because different people could not only have similar appearance but also may have similar motions and actions which are hard to align. To solve this problem, we propose a top-push distance learning model (TDL), in which we integrate a top-push constrain, for matching video features of persons. The top-push constraint enforces the optimization on top-rank matching in re-id, so as to make the matching model more effective towards selecting more discriminative features to distinguish different persons. Our experiments show that the proposed video-based re-id framework outperforms the state-of-the-art video-based re-id methods.
count=1
* Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Yu_Video_Paragraph_Captioning_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Yu_Video_Paragraph_Captioning_CVPR_2016_paper.pdf)]
    * Title: Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks
    * Year: `2016`
    * Authors: Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, Wei Xu
    * Abstract: We present an approach that exploits hierarchical Recurrent Neural Networks (RNNs) to tackle the video captioning problem, i.e., generating one or multiple sentences to describe a realistic video. Our hierarchical framework contains a sentence generator and a paragraph generator. The sentence generator produces one simple short sentence that describes a specific short video interval. It exploits both temporal- and spatial-attention mechanisms to selectively focus on visual elements during generation. The paragraph generator captures the inter-sentence dependency by taking as input the sentential embedding produced by the sentence generator, combining it with the paragraph history, and outputting the new initial state for the sentence generator. We evaluate our approach on two large-scale benchmark datasets: YouTubeClips and TACoS-MultiLevel. The experiments demonstrate that our approach significantly outperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and 0.305 respectively.
count=1
* Detecting Vanishing Points Using Global Image Context in a Non-Manhattan World
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Zhai_Detecting_Vanishing_Points_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhai_Detecting_Vanishing_Points_CVPR_2016_paper.pdf)]
    * Title: Detecting Vanishing Points Using Global Image Context in a Non-Manhattan World
    * Year: `2016`
    * Authors: Menghua Zhai, Scott Workman, Nathan Jacobs
    * Abstract: We propose a novel method for detecting horizontal vanishing points and the zenith vanishing point in man-made environments. The dominant trend in existing methods is to first find candidate vanishing points, then remove outliers by enforcing mutual orthogonality. Our method reverses this process: we propose a set of horizon line candidates and score each based on the vanishing points it contains. A key element of our approach is the use of global image context, extracted with a deep convolutional network, to constrain the set of candidates under consideration. Our method does not make a Manhattan-world assumption and can operate effectively on scenes with only a single horizontal vanishing point. We evaluate our approach on three benchmark datasets and achieve state-of-the-art performance on each. In addition, our approach is significantly faster than the previous best method.
count=1
* Beyond F-Formations: Determining Social Involvement in Free Standing Conversing Groups From Static Images
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Beyond_F-Formations_Determining_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Beyond_F-Formations_Determining_CVPR_2016_paper.pdf)]
    * Title: Beyond F-Formations: Determining Social Involvement in Free Standing Conversing Groups From Static Images
    * Year: `2016`
    * Authors: Lu Zhang, Hayley Hung
    * Abstract: In this paper, we present the first attempt to analyse differing levels of social involvement in free standing conversing groups (or the so-called F-formations) from static images. In addition, we enrich state-of-the-art F-formation modelling by learning a frustum of attention that accounts for the spatial context. That is, F-formation configurations vary with respect to the arrangement of furniture and the non-uniform crowdedness in the space during mingling scenarios. The majority of prior works have considered the labelling of conversing group as an objective task, requiring only a single annotator. However, we show that by embracing the subjectivity of social involvement, we not only generate a richer model of the social interactions in a scene but also significantly improve F-formation detection. We carry out extensive experimental validation of our proposed approach by collecting a novel set of multi-annotator labels of involvement on the publicly available Idiap Poster Data; the only multi-annotator labelled database of free standing conversing groups that is currently available.
count=1
* Embedding Label Structures for Fine-Grained Feature Representation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Embedding_Label_Structures_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Embedding_Label_Structures_CVPR_2016_paper.pdf)]
    * Title: Embedding Label Structures for Fine-Grained Feature Representation
    * Year: `2016`
    * Authors: Xiaofan Zhang, Feng Zhou, Yuanqing Lin, Shaoting Zhang
    * Abstract: Recent algorithms in convolutional neural networks (CNN) considerably advance the fine-grained image classification, which aims to differentiate the subtle differences among subordinate classes. However, previous studies have rarely focused on learning a fined-grained and structured feature representation that is able to locate relevant images at different levels of relevance, e.g., discovering cars from the same make or the same model, both of which require high precision. In this paper, we propose two main contributions to tackle this problem. 1) A multi-task learning framework is designed to effectively learn fine-grained feature representations by jointly optimizing both classification and similarity constraints. 2) To model the multi-level relevance, label structures such as hierarchy or shared attributes are seamlessly embedded into the framework by generalizing the triplet loss. Extensive and thorough experiments have been conducted on three fine-grained datasets, i.e., the Stanford car, the car-333, and the food datasets, which contain either hierarchical labels or shared attributes. Our proposed method has achieved very competitive performance, i.e., among state-of-the-art classification accuracy. More importantly, it significantly outperforms previous fine-grained feature representations for image retrieval at different levels of relevance
count=1
* Zero-Shot Learning via Joint Latent Similarity Embedding
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Zero-Shot_Learning_via_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Zero-Shot_Learning_via_CVPR_2016_paper.pdf)]
    * Title: Zero-Shot Learning via Joint Latent Similarity Embedding
    * Year: `2016`
    * Authors: Ziming Zhang, Venkatesh Saligrama
    * Abstract: Zero-shot recognition (ZSR) deals with the problem of predicting class labels for target domain instances based on source domain side information (e.g. attributes) of unseen classes. We formulate ZSR as a binary prediction problem. Our resulting classifier is class-independent. It takes an arbitrary pair of source and target domain instances as input and predicts whether or not they come from the same class, i.e. whether there is a match. We model the posterior probability of a match since it is a sufficient statistic and propose a latent probabilistic model in this context. We develop a joint discriminative learning framework based on dictionary learning to jointly learn the parameters of our model for both domains, which ultimately leads to our class-independent classifier. Many of the existing embedding methods can be viewed as special cases of our probabilistic model. On ZSR our method shows 4.90% improvement over the state-of-the-art in accuracy averaged across four benchmark datasets. We also adapt ZSR method for zero-shot retrieval and show 22.45% improvement accordingly in mean average precision (mAP).
count=1
* Fine-Grained Image Classification by Exploring Bipartite-Graph Labels
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Fine-Grained_Image_Classification_CVPR_2016_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Fine-Grained_Image_Classification_CVPR_2016_paper.pdf)]
    * Title: Fine-Grained Image Classification by Exploring Bipartite-Graph Labels
    * Year: `2016`
    * Authors: Feng Zhou, Yuanqing Lin
    * Abstract: Given a food image, can a fine-grained object recognition engine tell "which restaurant which dish" the food belongs to? Such ultra-fine grained image recognition is the key for many applications like search by images, but it is very challenging because it needs to discern subtle difference between classes while dealing with the scarcity of training data. Fortunately, the ultra-fine granularity naturally brings rich relationships among object classes. This paper proposes a novel approach to exploit the rich relationships through bipartite-graph labels (BGL). We show how to model BGL in an overall convolutional neural networks and the resulting system can be optimized through back-propagation. We also show that it is computationally efficient in inference thanks to the bipartite structure. To facilitate the study, we construct a new food benchmark dataset, which consists of 37,885 food images collected from 6 restaurants and totally 975 menus. Experimental results on this new food and three other datasets demonstrate BGL advances previous works in fine-grained object recognition. An online demo is available at http://www.f-zhou.com/fg_demo/.
count=1
* DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Bertasius_DeepEdge_A_Multi-Scale_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Bertasius_DeepEdge_A_Multi-Scale_2015_CVPR_paper.pdf)]
    * Title: DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection
    * Year: `2015`
    * Authors: Gedas Bertasius, Jianbo Shi, Lorenzo Torresani
    * Abstract: Contour detection has been a fundamental component in many image segmentation and object detection systems. Most previous work utilizes low-level features such as texture or saliency to detect contours and then use them as cues for a higher-level task such as object detection. However, we claim that recognizing objects and predicting contours are two mutually related tasks. Contrary to traditional approaches, we show that we can invert the commonly established pipeline: instead of detecting contours with low-level cues for a higher-level recognition task, we exploit object-related features as high-level cues for contour detection. We achieve this goal by means of a multi-scale deep network that consists of five convolutional layers and a bifurcated fully-connected sub-network. The section from the input layer to the fifth convolutional layer is fixed and directly lifted from a pre-trained network optimized over a large-scale object classification task. This section of the network is applied to four different scales of the image input. These four parallel and identical streams are then attached to a bifurcated sub-network consisting of two independently-trained branches. One branch learns to predict the contour likelihood (with a classification objective) whereas the other branch is trained to learn the fraction of human labelers agreeing about the contour presence at a given point (with a regression criterion). We show that without any feature engineering our multi-scale deep learning approach achieves state-of-the-art results in contour detection.
count=1
* Metric Imitation by Manifold Transfer for Efficient Vision Applications
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Dai_Metric_Imitation_by_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Dai_Metric_Imitation_by_2015_CVPR_paper.pdf)]
    * Title: Metric Imitation by Manifold Transfer for Efficient Vision Applications
    * Year: `2015`
    * Authors: Dengxin Dai, Till Kroeger, Radu Timofte, Luc Van Gool
    * Abstract: Metric learning has proved very successful. However, human annotations are necessary. In this paper, we propose an unsupervised method, dubbed Metric Imitation (MI), where metrics over one cheap feature (target features, TFs) are learned by imitating the standard metrics over another sophisticated, off-the-shelf feature (source features, SFs) by transferring the view-independent property manifold structures. In particular, MI consists of: 1) quantifying the properties of source metrics as manifold geometry, 2) transferring the manifold from source domain to target domain, and 3) learning a mapping of TFs so that the manifold is approximated as well as possible in the mapped feature domain. MI is useful in at least two scenarios where: 1) TFs are more efficient computationally and in terms of memory than SFs; and 2) SFs contain privileged information, but they are not available during testing. For the former, MI is evaluated on image clustering, category-based image retrieval, and instance-based object retrieval, with three SFs and three TFs. For the latter, MI is tested on the task of example-based image super-resolution, where high-resolution patches are taken as SFs and low-resolution patches as TFs. Experiments show that MI is able to provide good metrics while avoiding expensive data labeling efforts and that it achieves state-of-the-art performance for image super-resolution. In addition, manifold transfer is an interesting direction of transfer learning.
count=1
* A Statistical Model of Riemannian Metric Variation for Deformable Shape Analysis
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Gasparetto_A_Statistical_Model_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Gasparetto_A_Statistical_Model_2015_CVPR_paper.pdf)]
    * Title: A Statistical Model of Riemannian Metric Variation for Deformable Shape Analysis
    * Year: `2015`
    * Authors: Andrea Gasparetto, Andrea Torsello
    * Abstract: The analysis of deformable 3D shape is often cast in terms of the shape's intrinsic geometry due to its invariance to a wide range of non-rigid deformations. However, object's plasticity in non-rigid transformation often results in transformations that are not completely isometric in the surface's geometry and whose mode of deviation from isometry is an identifiable characteristic of the shape and its deformation modes. In this paper, we propose a novel generative model of the variations of the intrinsic metric of deformable shapes, based on the spectral decomposition of the Laplace-Beltrami operator. To this end, we assume two independent models for the eigenvectors and the eigenvalues of the graph-Laplacian of a 3d mesh which are learned in a supervised way from a set of shapes belonging to the same class. We show how this model can be efficiently learned given a set of 3D meshes, and evaluate the performance of the resulting generative model in shape classification and retrieval tasks. Comparison with state-of-the-art solutions for these problems confirm the validity of the approach.
count=1
* Image Parsing With a Wide Range of Classes and Scene-Level Context
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/George_Image_Parsing_With_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/George_Image_Parsing_With_2015_CVPR_paper.pdf)]
    * Title: Image Parsing With a Wide Range of Classes and Scene-Level Context
    * Year: `2015`
    * Authors: Marian George
    * Abstract: This paper presents a nonparametric scene parsing approach that improves the overall accuracy, as well as the coverage of foreground classes in scene images. We first improve the label likelihood estimates at superpixels by merging likelihood scores from different probabilistic classifiers. This boosts the classification performance and enriches the representation of less-represented classes. Our second contribution consists of incorporating semantic context in the parsing process through global label costs. Our method does not rely on image retrieval sets but rather assigns a global likelihood estimate to each label, which is plugged into the overall energy function. We evaluate our system on two large-scale datasets, SIFTflow and LMSun. We achieve state-of-the-art performance on the SIFTflow dataset and near-record results on LMSun.
count=1
* A Maximum Entropy Feature Descriptor for Age Invariant Face Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Gong_A_Maximum_Entropy_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Gong_A_Maximum_Entropy_2015_CVPR_paper.pdf)]
    * Title: A Maximum Entropy Feature Descriptor for Age Invariant Face Recognition
    * Year: `2015`
    * Authors: Dihong Gong, Zhifeng Li, Dacheng Tao, Jianzhuang Liu, Xuelong Li
    * Abstract: In this paper, we propose a new approach to overcome the representation and matching problems in age invariant face recognition. First, a new maximum entropy feature descriptor (MEFD) is developed that encodes the microstructure of facial images into a set of discrete codes in terms of maximum entropy. By densely sampling the encoded face image, sufficient discriminatory and expressive information can be extracted for further analysis. A new matching method is also developed, called identity factor analysis (IFA), to estimate the probability that two faces have the same underlying identity. The effectiveness of the framework is confirmed by extensive experimentation on two face aging datasets, MORPH (the largest public-domain face aging dataset) and FGNET. We also conduct experiments on the famous LFW dataset to demonstrate the excellent generalizability of our new approach.
count=1
* Hierarchical Sparse Coding With Geometric Prior For Visual Geo-Location
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Gopalan_Hierarchical_Sparse_Coding_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Gopalan_Hierarchical_Sparse_Coding_2015_CVPR_paper.pdf)]
    * Title: Hierarchical Sparse Coding With Geometric Prior For Visual Geo-Location
    * Year: `2015`
    * Authors: Raghuraman Gopalan
    * Abstract: We address the problem of estimating location information of an image using principles from automated representation learning. We pursue a hierarchical sparse coding approach that learns features useful in discriminating images across locations, by initializing it with a geometric prior corresponding to transformations between image appearance space and their corresponding location grouping space using the notion of parallel transport on manifolds. We then extend this approach to account for the availability of heterogeneous data modalities such as geo-tags and videos pertaining to different locations, and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations. We evaluate our approach on several standard datasets such as im2gps, San Francisco and MediaEval2010, and obtain state-of-the-art results.
count=1
* Supervised Mid-Level Features for Word Image Representation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Gordo_Supervised_Mid-Level_Features_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Gordo_Supervised_Mid-Level_Features_2015_CVPR_paper.pdf)]
    * Title: Supervised Mid-Level Features for Word Image Representation
    * Year: `2015`
    * Authors: Albert Gordo
    * Abstract: This paper addresses the problem of learning word image representations: given the cropped image of a word, we are interested in finding a descriptive, robust, and compact fixed-length representation. Machine learning techniques can then be supplied with these representations to produce models useful for word retrieval or recognition tasks. Although many works have focused on the machine learning aspect once a global representation has been produced, little work has been devoted to the construction of those base image representations: most works use standard coding and aggregation techniques directly on top of standard computer vision features such as SIFT or HOG. We propose to learn local mid-level features suitable for building word image representations. These features are learnt by leveraging character bounding box annotations on a small set of training images. However, contrary to other approaches that use character bounding box information, our approach does not rely on detecting the individual characters explicitly at testing time. Our local mid-level features can then be aggregated to produce a global word image signature. When pairing these features with the recent word attributes framework of Almaz\'an et al., we obtain results comparable with or better than the state-of-the-art on matching and recognition tasks using global descriptors of only 96 dimensions.
count=1
* Aligning 3D Models to RGB-D Images of Cluttered Scenes
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Gupta_Aligning_3D_Models_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Gupta_Aligning_3D_Models_2015_CVPR_paper.pdf)]
    * Title: Aligning 3D Models to RGB-D Images of Cluttered Scenes
    * Year: `2015`
    * Authors: Saurabh Gupta, Pablo Arbelaez, Ross Girshick, Jitendra Malik
    * Abstract: The goal of this work is to represent objects in an RGB-D scene with corresponding 3D models from a library. We approach this problem by first detecting and segmenting object instances in the scene and then using a convolutional neural network (CNN) to predict the pose of the object. This CNN is trained using pixel surface normals in images containing renderings of synthetic objects. When tested on real data, our method outperforms alternative algorithms trained on real data. We then use this coarse pose estimate along with the inferred pixel support to align a small number of prototypical models to the data, and place into the scene the model that fits best. We observe a 48% relative improvement in performance at the task of 3D detection over the current state-of-the-art, while being an order of magnitude faster.
count=1
* Mapping Visual Features to Semantic Profiles for Retrieval in Medical Imaging
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Hofmanninger_Mapping_Visual_Features_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Hofmanninger_Mapping_Visual_Features_2015_CVPR_paper.pdf)]
    * Title: Mapping Visual Features to Semantic Profiles for Retrieval in Medical Imaging
    * Year: `2015`
    * Authors: Johannes Hofmanninger, Georg Langs
    * Abstract: Content based image retrieval is highly relevant in medical imaging, since it makes vast amounts of imaging data accessible for comparison during diagnosis. Finding image similarity measures that reflect diagnostically relevant relationships is challenging, since the overall appearance variability is high compared to often subtle signatures of diseases. To learn models that capture the relationship between semantic clinical information and image elements at scale, we have to rely on data generated during clinical routine (images and radiology reports), since expert annotation is prohibitively costly. Here we show that re-mapping visual features extracted from medical imaging data based on weak labels that can be found in corresponding radiology reports creates descriptions of local image content capturing clinically relevant information. We show that these semantic profiles enable higher recall and precision during retrieval compared to visual features, and that we can even map semantic terms describing clinical findings from radiology reports to localized image volume areas.
count=1
* Jointly Learning Heterogeneous Features for RGB-D Activity Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Hu_Jointly_Learning_Heterogeneous_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Hu_Jointly_Learning_Heterogeneous_2015_CVPR_paper.pdf)]
    * Title: Jointly Learning Heterogeneous Features for RGB-D Activity Recognition
    * Year: `2015`
    * Authors: Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, Jianguo Zhang
    * Abstract: In this paper, we focus on heterogeneous feature learning for RGB-D activity recognition. Considering that features from different channels could share some similar hidden structures, we propose a joint learning model to simultaneously explore the shared and feature-specific components as an instance of heterogenous multi-task learning. The proposed model in an unified framework is capable of: 1) jointly mining a set of subspaces with the same dimensionality to enable the multi-task classifier learning, and 2) meanwhile, quantifying the shared and feature-specific components of features in the subspaces. To efficiently train the joint model, a three-step iterative optimization algorithm is proposed, followed by two inference models. Extensive results on three activity datasets have demonstrated the efficacy of the proposed method. In addition, a novel RGB-D activity dataset focusing on human-object interaction is collected for evaluating the proposed method, which will be made available to the community for RGB-D activity benchmarking and analysis.
count=1
* Learning Hypergraph-Regularized Attribute Predictors
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Huang_Learning_Hypergraph-Regularized_Attribute_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Huang_Learning_Hypergraph-Regularized_Attribute_2015_CVPR_paper.pdf)]
    * Title: Learning Hypergraph-Regularized Attribute Predictors
    * Year: `2015`
    * Authors: Sheng Huang, Mohamed Elhoseiny, Ahmed Elgammal, Dan Yang
    * Abstract: We present a novel attribute learning framework named Hypergraph-based Attribute Predictor (HAP). In HAP, a hypergraph is leveraged to depict the attribute relations in the data. Then the attribute prediction problem is casted as a regularized hypergraph cut problem, in which a collection of attribute projections is jointly learnt from the feature space to a hypergraph embedding space aligned with the attributes. The learned projections directly act as attribute classifiers (linear and kernelized). This formulation leads to a very efficient approach. By considering our model as a multi-graph cut task, our framework can flexibly incorporate other available information, in particular class label. We apply our approach to attribute prediction, Zero-shot and N-shot learning tasks. The results on AWA, USAA and CUB databases demonstrate the value of our methods in comparison with the state-of-the-art approaches.
count=1
* Separating Objects and Clutter in Indoor Scenes
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Khan_Separating_Objects_and_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Khan_Separating_Objects_and_2015_CVPR_paper.pdf)]
    * Title: Separating Objects and Clutter in Indoor Scenes
    * Year: `2015`
    * Authors: Salman H. Khan, Xuming He, Mohammed Bennamoun, Ferdous Sohel, Roberto Togneri
    * Abstract: Objects' spatial layout estimation and clutter identification are two important tasks to understand indoor scenes. We propose to solve both of these problems in a joint framework using RGBD images of indoor scenes. In contrast to recent approaches which focus on either one of these two problems, we perform `fine grained structure categorization' by predicting all the major objects and simultaneously labeling the cluttered regions. A conditional random field model is proposed to incorporate a rich set of local appearance, geometric features and interactions between the scene elements. We take a structural learning approach with a loss of 3D localisation to estimate the model parameters from a large annotated RGBD dataset, and a mixed integer linear programming formulation for inference. We demonstrate that our approach is able to detect cuboids and estimate cluttered regions across many different object and scene categories in the presence of occlusion, illumination and appearance variations.
count=1
* A MRF Shape Prior for Facade Parsing With Occlusions
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Kozinski_A_MRF_Shape_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Kozinski_A_MRF_Shape_2015_CVPR_paper.pdf)]
    * Title: A MRF Shape Prior for Facade Parsing With Occlusions
    * Year: `2015`
    * Authors: Mateusz Kozinski, Raghudeep Gadde, Sergey Zagoruyko, Guillaume Obozinski, Renaud Marlet
    * Abstract: We present a new shape prior formalism for segmentation of rectified facade images. It combines the simplicity of split grammars with unprecedented expressive power: the capability of encoding simultaneous alignment in two dimensions, facade occlusions and irregular boundaries between facade elements. Our method simultaneously segments the visible and occluding objects and recovers the structure of the occluded facade. We formulate the task of finding the most likely image segmentation conforming to a prior of the proposed form as a MAP-MRF problem over the standard 4-connected pixel grid with hard constraints on the classes of neighboring pixels, and propose an efficient optimization algorithm for solving it. We demonstrate state of the art results on a number of facade segmentation datasets.
count=1
* Transformation-Invariant Convolutional Jungles
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Laptev_Transformation-Invariant_Convolutional_Jungles_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Laptev_Transformation-Invariant_Convolutional_Jungles_2015_CVPR_paper.pdf)]
    * Title: Transformation-Invariant Convolutional Jungles
    * Year: `2015`
    * Authors: Dmitry Laptev, Joachim M. Buhmann
    * Abstract: Many Computer Vision problems arise from information processing of data sources with nuisance variances like scale, orientation, contrast, perspective foreshortening or - in medical imaging - staining and local warping. In most cases these variances can be stated a priori and can be used to improve the generalization of recognition algorithms. We propose a novel supervised feature learning approach, which efficiently extracts information from these constraints to produce interpretable, transformation-invariant features. The proposed method can incorporate a large class of transformations, e.g., shifts, rotations, change of scale, morphological operations, non-linear distortions, photometric transformations, etc. These features boost the discrimination power of a novel image classification and segmentation method, which we call Transformation-Invariant Convolutional Jungles (TICJ). We test the algorithm on two benchmarks in face recognition and medical imaging, where it achieves state of the art results, while being computationally significantly more efficient than Deep Neural Networks.
count=1
* Face Alignment Using Cascade Gaussian Process Regression Trees
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Lee_Face_Alignment_Using_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Lee_Face_Alignment_Using_2015_CVPR_paper.pdf)]
    * Title: Face Alignment Using Cascade Gaussian Process Regression Trees
    * Year: `2015`
    * Authors: Donghoon Lee, Hyunsin Park, Chang D. Yoo
    * Abstract: In this paper, we propose a face alignment method that uses cascade Gaussian process regression trees (cGPRT) constructed by combining Gaussian process regression trees (GPRT) in a cascade stage-wise manner. Here, GPRT is a Gaussian process with a kernel defined by a set of trees. The kernel measures the similarity between two inputs as the number of trees where the two inputs fall in the same leaves. Without increasing prediction time, the prediction of cGPRT can be performed in the same framework as the cascade regression trees (CRT) but with better generalization. Features for GPRT are designed using shape-indexed difference of Gaussian (DoG) filter responses sampled from local retinal patterns to increase stability and to attain robustness against geometric variances. Compared with the previous CRT-based face alignment methods that have shown state-of-the-art performances, cGPRT using shape-indexed DoG features performed best on the HELEN and 300-W datasets which are the most challenging dataset today.
count=1
* Multiple Random Walkers and Their Application to Image Cosegmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Lee_Multiple_Random_Walkers_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Lee_Multiple_Random_Walkers_2015_CVPR_paper.pdf)]
    * Title: Multiple Random Walkers and Their Application to Image Cosegmentation
    * Year: `2015`
    * Authors: Chulwoo Lee, Won-Dong Jang, Jae-Young Sim, Chang-Su Kim
    * Abstract: A graph-based system to simulate the movements and interactions of multiple random walkers (MRW) is proposed in this work. In the MRW system, multiple agents traverse a single graph simultaneously. To achieve desired interactions among those agents, a restart rule can be designed, which determines the restart distribution of each agent according to the probability distributions of all agents. In particular, we develop the repulsive rule for data clustering. We illustrate that the MRW clustering can segment real images reliably. Furthermore, we propose a novel image cosegmentation algorithm based on the MRW clustering. Specifically, the proposed algorithm consists of two steps: inter-image concurrence computation and intra-image MRW clustering. Experimental results demonstrate that the proposed algorithm provides promising cosegmentation performance.
count=1
* Mid-Level Deep Pattern Mining
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Li_Mid-Level_Deep_Pattern_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Li_Mid-Level_Deep_Pattern_2015_CVPR_paper.pdf)]
    * Title: Mid-Level Deep Pattern Mining
    * Year: `2015`
    * Authors: Yao Li, Lingqiao Liu, Chunhua Shen, Anton van den Hengel
    * Abstract: Mid-level visual element discovery aims to find clusters of image patches that are both representative and discriminative. In this work, we study this problem from the prospective of pattern mining while relying on the recently popularized Convolutional Neural Networks (CNNs). Specifically, we find that for an image patch, activation extracted from the first fully-connected layer of a CNN have two appealing properties which enable its seamless integration with pattern mining. Patterns are then discovered from a large number of CNN activations of image patches through the well-known association rule mining. When we retrieve and visualize image patches with the same pattern (See Fig. 1), surprisingly, they are not only visually similar but also semantically consistent. We apply our approach to scene and object classification tasks, and demonstrate that our approach outperforms all previous works on mid-level visual element discovery by a sizeable margin with far fewer elements being used. Our approach also outperforms or matches recent works using CNN for these tasks. Source code of the complete system is available online.
count=1
* Visual Saliency Based on Multiscale Deep Features
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Li_Visual_Saliency_Based_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Li_Visual_Saliency_Based_2015_CVPR_paper.pdf)]
    * Title: Visual Saliency Based on Multiscale Deep Features
    * Year: `2015`
    * Authors: Guanbin Li, Yizhou Yu
    * Abstract: Visual saliency is a fundamental problem in both cognitive and computational sciences, including computer vision. In this paper, we discover that a high-quality visual saliency model can be learned from multiscale features extracted using deep convolutional neural networks(CNN), which have had many successes in visual recognition tasks. For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for extracting features at three different scales. Our learned saliency model is capable of achieving state-of-the-art performance on all public benchmarks. We then propose a refinement method to enhance the spatial coherence of our saliency results. Finally, we point out that aggregating multiple saliency maps computed for different levels of image segmentation can further boost the performance, yielding saliency maps better than those generated from a single region decomposition. To promote further research and evaluation of visual saliency models, we also construct a large database of 4447 challenging images and their pixelwise saliency annotation. Experimental results demonstrate that our proposed method significantly outperforms all existing saliency estimation techniques, improving the F-Measure by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset, and lowering the mean absolute error by 5.7% and 35.1% respectively on the same two datasets.
count=1
* Small Instance Detection by Integer Programming on Object Density Maps
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Ma_Small_Instance_Detection_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Ma_Small_Instance_Detection_2015_CVPR_paper.pdf)]
    * Title: Small Instance Detection by Integer Programming on Object Density Maps
    * Year: `2015`
    * Authors: Zheng Ma, Lei Yu, Antoni B. Chan
    * Abstract: We propose a novel object detection framework for partially-occluded small instances, such as pedestrians in low resolution surveillance video, cells under a microscope, flocks of small animals (e.g. birds, fishes), or even tiny insects like honeybees and flies. These scenarios are very challenging for traditional detectors, which are typically trained on individual instances. In our approach, we first estimate the object density map of the input image, and then divide it into local regions. For each region, a sliding window (ROI) is passed over the density map to calculate the instance count within each ROI. 2D integer programming is used to recover the locations of object instances from the set of ROI counts, and the global count estimate of the density map is used as a constraint to regularize the detection performance. Finally, the bounding box for each instance is estimated using the local density map. Compared with current small-instance detection methods, our proposed approach achieves state-of-the-art performance on several challenging datasets including fluorescence microscopy cell images, UCSD pedestrians, small animals and insects.
count=1
* Real-Time 3D Head Pose and Facial Landmark Estimation From Depth Images Using Triangular Surface Patch Features
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Papazov_Real-Time_3D_Head_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Papazov_Real-Time_3D_Head_2015_CVPR_paper.pdf)]
    * Title: Real-Time 3D Head Pose and Facial Landmark Estimation From Depth Images Using Triangular Surface Patch Features
    * Year: `2015`
    * Authors: Chavdar Papazov, Tim K. Marks, Michael Jones
    * Abstract: We present a real-time system for 3D head pose estimation and facial landmark localization using a commodity depth sensor. We introduce a novel triangular surface patch (TSP) descriptor, which encodes the shape of the 3D surface of the face within a triangular area. The proposed descriptor is viewpoint invariant, and it is robust to noise and to variations in the data resolution. Using a fast nearest neighbor lookup, TSP descriptors from an input depth map are matched to the most similar ones that were computed from synthetic head models in a training phase. The matched triangular surface patches in the training set are used to compute estimates of the 3D head pose and facial landmark positions in the input depth map. By sampling many TSP descriptors, many votes for pose and landmark positions are generated which together yield robust final estimates. We evaluate our approach on the publicly available Biwi Kinect Head Pose Database to compare it against state-of-the-art methods. Our results show a significant improvement in the accuracy of both pose and landmark location estimates while maintaining real-time speed.
count=1
* Graph-Based Simplex Method for Pairwise Energy Minimization With Binary Variables
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Prusa_Graph-Based_Simplex_Method_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Prusa_Graph-Based_Simplex_Method_2015_CVPR_paper.pdf)]
    * Title: Graph-Based Simplex Method for Pairwise Energy Minimization With Binary Variables
    * Year: `2015`
    * Authors: Daniel Prusa
    * Abstract: We show how the simplex algorithm can be tailored to the linear programming relaxation of pairwise energy minimization with binary variables. A special structure formed by basic and nonbasic variables in each stage of the algorithm is identified and utilized to perform the whole iterative process combinatorially over the input energy minimization graph rather than algebraically over the simplex tableau. This leads to a new efficient solver. We demonstrate that for some computer vision instances it performs even better than methods reducing binary energy minimization to finding maximum flow in a network.
count=1
* Discriminative Shape From Shading in Uncalibrated Illumination
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Richter_Discriminative_Shape_From_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Richter_Discriminative_Shape_From_2015_CVPR_paper.pdf)]
    * Title: Discriminative Shape From Shading in Uncalibrated Illumination
    * Year: `2015`
    * Authors: Stephan R. Richter, Stefan Roth
    * Abstract: Estimating surface normals from just a single image is challenging. To simplify the problem, previous work focused on special cases, including directional lighting, known reflectance maps, etc., making shape from shading impractical outside the lab. To cope with more realistic settings, shading cues need to be combined and generalized to natural illumination. This significantly increases the complexity of the approach, as well as the number of parameters that require tuning. Enabled by a new large-scale dataset for training and analysis, we address this with a discriminative learning approach to shape from shading, which uses regression forests for efficient pixel-independent prediction and fast learning. Von Mises-Fisher distributions in the leaves of each tree enable the estimation of surface normals. To account for their expected spatial regularity, we introduce spatial features, including texton and silhouette features. The proposed silhouette features are computed from the occluding contours of the surface and provide scale-invariant context. Aside from computational efficiency, they enable good generalization to unseen data and importantly allow for a robust estimation of the reflectance map, extending our approach to the uncalibrated setting. Experiments show that our discriminative approach outperforms state-of-the-art methods on synthetic and real-world datasets.
count=1
* Classifier Adaptation at Prediction Time
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Royer_Classifier_Adaptation_at_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Royer_Classifier_Adaptation_at_2015_CVPR_paper.pdf)]
    * Title: Classifier Adaptation at Prediction Time
    * Year: `2015`
    * Authors: Amelie Royer, Christoph H. Lampert
    * Abstract: Classifiers for object categorization are usually evaluated by their accuracy on a set of i.i.d. test examples. This provides us with an estimate of the expected error when applying the classifiers to a single new image. In real application, however, classifiers are rarely only used for a single image and then discarded. Instead, they are applied sequentially to many images, and these are typically not i.i.d. samples from a fixed data distribution, but they carry dependencies and their class distribution varies over time. In this work, we argue that the phenomenon of correlated data at prediction time is not a nuisance, but a blessing in disguise. We describe a probabilistic method for adapting classifiers at prediction time without having to retrain them. We also introduce a framework for creating realistically distributed image sequences, which offers a way to benchmark classifier adaptation methods, such as the one we propose. Experiments on the ILSVRC2010 and ILSVRC2012 datasets show that adapting object classification systems at prediction time can significantly reduce their error rate, even with no additional human feedback.
count=1
* SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Song_SUN_RGB-D_A_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Song_SUN_RGB-D_A_2015_CVPR_paper.pdf)]
    * Title: SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite
    * Year: `2015`
    * Authors: Shuran Song, Samuel P. Lichtenberg, Jianxiong Xiao
    * Abstract: Although RGB-D sensors have enabled major breakthroughs for several vision tasks, such as 3D reconstruction, we have not attained the same level of success in high-level scene understanding. Perhaps one of the main reasons is the lack of a large-scale benchmark with 3D annotations and 3D evaluation metrics. In this paper, we introduce an RGB-D benchmark suite for the goal of advancing the state-of-the-arts in all major scene understanding tasks. Our dataset is captured by four different sensors and contains 10,335 RGB-D images, at a similar scale as PASCAL VOC. The whole dataset is densely annotated and includes 146,617 2D polygons and 64,595 3D bounding boxes with accurate object orientations, as well as a 3D room layout and scene category for each image. This dataset enables us to train data-hungry algorithms for scene-understanding tasks, evaluate them using meaningful 3D metrics, avoid overfitting to a small testing set, and study cross-sensor bias.
count=1
* TVSum: Summarizing Web Videos Using Titles
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Song_TVSum_Summarizing_Web_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf)]
    * Title: TVSum: Summarizing Web Videos Using Titles
    * Year: `2015`
    * Authors: Yale Song, Jordi Vallmitjana, Amanda Stent, Alejandro Jaimes
    * Abstract: Video summarization is a challenging problem in part because knowing which part of a video is important requires prior knowledge about its main topic. We present TVSum, an unsupervised video summarization framework that uses title-based image search results to find visually important shots. We observe that a video title is often carefully chosen to be maximally descriptive of its main topic, and hence images related to the title can serve as a proxy for important visual concepts of the main topic. However, because titles are free-formed, unconstrained, and often written ambiguously, images searched using the title can contain noise (images irrelevant to video content) and variance (images of different topics). To deal with this challenge, we developed a novel co-archetypal analysis technique that learns canonical visual concepts shared between video and images, but not in either alone, by finding a joint-factorial representation of two data sets. We introduce a new benchmark dataset, TVSum50, that contains 50 videos and their shot-level importance scores annotated via crowdsourcing. Experimental results on two datasets, SumMe and TVSum50, suggest our approach produces superior quality summaries compared to several recently proposed approaches.
count=1
* 24/7 Place Recognition by View Synthesis
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Torii_247_Place_Recognition_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Torii_247_Place_Recognition_2015_CVPR_paper.pdf)]
    * Title: 24/7 Place Recognition by View Synthesis
    * Year: `2015`
    * Authors: Akihiko Torii, Relja Arandjelovic, Josef Sivic, Masatoshi Okutomi, Tomas Pajdla
    * Abstract: We address the problem of large-scale visual place recognition for situations where the scene undergoes a major change in appearance, for example, due to illumination (day/night), change of seasons, aging, or structural modifications over time such as buildings built or destroyed. Such situations represent a major challenge for current large-scale place recognition methods. This work has the following three principal contributions. First, we demonstrate that matching across large changes in the scene appearance becomes much easier when both the query image and the database image depict the scene from approximately the same viewpoint. Second, based on this observation, we develop a new place recognition approach that combines (i) an efficient synthesis of novel views with (ii) a compact indexable image representation. Third, we introduce a new challenging dataset of 1,125 camera-phone query images of Tokyo that contain major changes in illumination (day, sunset, night) as well as structural changes in the scene. We demonstrate that the proposed approach significantly outperforms other large-scale place recognition techniques on this challenging data.
count=1
* Representing 3D Texture on Mesh Manifolds for Retrieval and Recognition Applications
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Werghi_Representing_3D_Texture_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Werghi_Representing_3D_Texture_2015_CVPR_paper.pdf)]
    * Title: Representing 3D Texture on Mesh Manifolds for Retrieval and Recognition Applications
    * Year: `2015`
    * Authors: Naoufel Werghi, Claudio Tortorici, Stefano Berretti, Alberto Del Bimbo
    * Abstract: In this paper, we present and experiment a novel approach for representing texture of 3D mesh manifolds using local binary patterns (LBP). Using a recently proposed framework [37], we compute LBP directly on the mesh surface, either using geometric or photometric appearance. Compared to its depth-image counterpart, our approach is distinguished by the following features: a) inherits the intrinsic advantages of mesh surface (e.g., preservation of the full geometry); b) does not require normalization; c) can accommodate partial matching. In addition, it allows early-level fusion of the geometry and photometric texture modalities. Through experiments conducted on two application scenarios, namely, 3D texture retrieval and 3D face recognition, we assess the effectiveness of the proposed solution with respect to state of the art approaches.
count=1
* Learning Descriptors for Object Recognition and 3D Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Wohlhart_Learning_Descriptors_for_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Wohlhart_Learning_Descriptors_for_2015_CVPR_paper.pdf)]
    * Title: Learning Descriptors for Object Recognition and 3D Pose Estimation
    * Year: `2015`
    * Authors: Paul Wohlhart, Vincent Lepetit
    * Abstract: Detecting poorly textured objects and estimating their 3D pose reliably is still a very challenging problem. We introduce a simple but powerful approach to computing descriptors for object views that efficiently capture both the object identity and 3D pose. By contrast with previous manifold-based approaches, we can rely on the Euclidean distance to evaluate the similarity between descriptors, and therefore use scalable Nearest Neighbor search methods to efficiently handle a large number of objects under a large range of poses. To achieve this, we train a Convolutional Neural Network to compute these descriptors by enforcing simple similarity and dissimilarity constraints between the descriptors. We show that our constraints nicely untangle the images from different objects and different views into clusters that are not only well-separated but also structured as the corresponding sets of poses: The Euclidean distance between descriptors is large when the descriptors are from different objects, and directly related to the distance between the poses when the descriptors are from the same object. These important properties allow us to outperform state-of-the-art object views representations on challenging RGB and RGB-D data.
count=1
* Complexity-Adaptive Distance Metric for Object Proposals Generation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Xiao_Complexity-Adaptive_Distance_Metric_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Xiao_Complexity-Adaptive_Distance_Metric_2015_CVPR_paper.pdf)]
    * Title: Complexity-Adaptive Distance Metric for Object Proposals Generation
    * Year: `2015`
    * Authors: Yao Xiao, Cewu Lu, Efstratios Tsougenis, Yongyi Lu, Chi-Keung Tang
    * Abstract: Distance metric plays a key role in grouping superpixels to produce object proposals for object detection. We observe that existing distance metrics work primarily for low complexity cases. In this paper, we develop a novel distance metric for grouping two superpixels in high-complexity scenarios. Combining them, a complexity-adaptive distance measure is produced that achieves improved grouping in different levels of complexity. Our extensive experimentation shows that our method can achieve good results in the PASCAL VOC 2012 dataset surpassing the latest state-of-the-art methods.
count=1
* Grasp Type Revisited: A Modern Perspective on a Classical Feature for Vision
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Yang_Grasp_Type_Revisited_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Yang_Grasp_Type_Revisited_2015_CVPR_paper.pdf)]
    * Title: Grasp Type Revisited: A Modern Perspective on a Classical Feature for Vision
    * Year: `2015`
    * Authors: Yezhou Yang, Cornelia Fermuller, Yi Li, Yiannis Aloimonos
    * Abstract: The grasp type provides crucial information about human action. However, recognizing the grasp type in unconstrained scenes is challenging because of the large variations in appearance, occlusions and geometric distortions. In this paper, first we present a convolutional neural network to classify functional hand grasp types. Experiments on a public static scene hand data set validate good performance of the presented method. Then we present two applications utilizing grasp type classification: (a) inference of human action intention and (b) fine level manipulation action segmentation. Experiments on both tasks demonstrate the usefulness of grasp type as a cognitive feature for computer vision. This study shows that the grasp type is a powerful symbolic representation for action understanding, and thus opens new avenues for future research.
count=1
* Mirror, Mirror on the Wall, Tell Me, Is the Error Small?
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Yang_Mirror_Mirror_on_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Yang_Mirror_Mirror_on_2015_CVPR_paper.pdf)]
    * Title: Mirror, Mirror on the Wall, Tell Me, Is the Error Small?
    * Year: `2015`
    * Authors: Heng Yang, Ioannis Patras
    * Abstract: Do object part localization methods produce bilaterally symmetric results on mirror images? Surprisingly not, even though state of the art methods augment the training set with mirrored images. In this paper we take a closer look into this issue. We first introduce the concept of mirrorability as the ability of a model to produce symmetric results in mirrored images and introduce a corresponding measure, namely the \textit{mirror error} that is defined as the difference between the detection result on an image and the mirror of the detection result on its mirror image. We evaluate the mirrorability of several state of the art algorithms in two of the most intensively studied problems, namely human pose estimation and face alignment. Our experiments lead to several interesting findings: 1) Most of state of the art methods struggle to preserve the mirror symmetry, despite the fact that they do have very similar overall performance on the original and mirror images; 2) the low mirrorability is not caused by training or testing sample bias - all algorithms are trained on both the original images and their mirrored versions; 3) the mirror error is strongly correlated to the localization/alignment error (with correlation coefficients around 0.7). Since the mirror error is calculated without knowledge of the ground truth, we show two interesting applications - in the first it is used to guide the selection of difficult samples and in the second to give feedback in a popular Cascaded Pose Regression method for face alignment.
count=1
* PatchCut: Data-Driven Object Segmentation via Local Shape Transfer
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Yang_PatchCut_Data-Driven_Object_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Yang_PatchCut_Data-Driven_Object_2015_CVPR_paper.pdf)]
    * Title: PatchCut: Data-Driven Object Segmentation via Local Shape Transfer
    * Year: `2015`
    * Authors: Jimei Yang, Brian Price, Scott Cohen, Zhe Lin, Ming-Hsuan Yang
    * Abstract: Object segmentation is highly desirable for image understanding and editing. Current interactive tools require a great deal of user effort while automatic methods are usually limited to images of special object categories or with high color contrast. In this paper, we propose a data-driven algorithm that uses examples to break through these limits. As similar objects tend to share similar local shapes, we match query image patches with example images in multiscale to enable local shape transfer. The transferred local shape masks constitute a patch-level segmentation solution space and we thus develop a novel cascade algorithm, PatchCut, for coarse-to-fine object segmentation. In each stage of the cascade, local shape mask candidates are selected to refine the estimated segmentation of the previous stage iteratively with color models. Experimental results on various datasets (Weizmann Horse, Fashionista, Object Discovery and PASCAL) demonstrate the effectiveness and robustness of our algorithm.
count=1
* Cross-Scene Crowd Counting via Deep Convolutional Neural Networks
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Zhang_Cross-Scene_Crowd_Counting_2015_CVPR_paper.pdf)]
    * Title: Cross-Scene Crowd Counting via Deep Convolutional Neural Networks
    * Year: `2015`
    * Authors: Cong Zhang, Hongsheng Li, Xiaogang Wang, Xiaokang Yang
    * Abstract: Cross-scene crowd counting is a challenging task where no laborious data annotation is required for counting people in new target surveillance crowd scenes unseen in the training set. The performance of most existing crowd counting methods drops significantly when they are applied to an unseen scene. To address this problem, we propose a deep convolutional neural network (CNN) for crowd counting, and it is trained alternatively with two related learning objectives, crowd density and crowd count. This proposed switchable learning approach is able to obtain better local optimum for both objectives. To handle an unseen target crowd scene, we present a data-driven method to fine-tune the trained CNN model for the target scene. A new dataset including 108 crowd scenes with nearly 200,000 head annotations is introduced to better evaluate the accuracy of cross-scene crowd counting methods. Extensive experiments on the proposed and another two existing datasets demonstrate the effectiveness and reliability of our approach.
count=1
* Improving Object Detection With Deep Convolutional Networks via Bayesian Optimization and Structured Prediction
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Zhang_Improving_Object_Detection_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Zhang_Improving_Object_Detection_2015_CVPR_paper.pdf)]
    * Title: Improving Object Detection With Deep Convolutional Networks via Bayesian Optimization and Structured Prediction
    * Year: `2015`
    * Authors: Yuting Zhang, Kihyuk Sohn, Ruben Villegas, Gang Pan, Honglak Lee
    * Abstract: Object detection systems based on the deep convolutional neural network (CNN) have recently made ground- breaking advances on several object detection benchmarks. While the features learned by these high-capacity neural networks are discriminative for categorization, inaccurate localization is still a major source of error for detection. Building upon high-capacity CNN architectures, we address the localization problem by 1) using a search algorithm based on Bayesian optimization that sequentially proposes candidate regions for an object bounding box, and 2) training the CNN with a structured loss that explicitly penalizes the localization inaccuracy. In experiments, we demonstrated that each of the proposed methods improves the detection performance over the baseline method on PASCAL VOC 2007 and 2012 datasets. Furthermore, two methods are complementary and significantly outperform the previous state-of-the-art when combined.
count=1
* Fixation Bank: Learning to Reweight Fixation Candidates
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/html/Zhao_Fixation_Bank_Learning_2015_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2015/papers/Zhao_Fixation_Bank_Learning_2015_CVPR_paper.pdf)]
    * Title: Fixation Bank: Learning to Reweight Fixation Candidates
    * Year: `2015`
    * Authors: Jiaping Zhao, Christian Siagian, Laurent Itti
    * Abstract: Predicting where humans will fixate in a scene has many practical applications. Biologically-inspired saliency models decompose visual stimuli into feature maps across multiple scales, and then integrate different feature channels, e.g., in a linear, MAX, or MAP. However, to date there is no universally accepted feature integration mechanism. Here, we propose a new a data-driven solution: We first build a "fixation bank" by mining training samples, which maintains the association between local patterns of activation, in 4 feature channels (color, intensity, orientation, motion) around a given location, and corresponding human fixation density at that location. During testing, we decompose feature maps into blobs, extract local activation patterns around each blob, match those patterns against the fixation bank by group lasso, and determine weights of blobs based on reconstruction errors. Our final saliency map is the weighted sum of all blobs. Our system thus incorporates some amount of spatial and featural context information into the location-dependent weighting mechanism. Tested on two standard data sets (DIEM for training and test, and CRCNS for test only; total of 23,670 training and 15,793 + 4,505 test frames), our model slightly but significantly outperforms 7 state-of-the-art saliency models.
count=1
* Cut, Glue & Cut: A Fast, Approximate Solver for Multicut Partitioning
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Beier_Cut_Glue__2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Beier_Cut_Glue__2014_CVPR_paper.pdf)]
    * Title: Cut, Glue & Cut: A Fast, Approximate Solver for Multicut Partitioning
    * Year: `2014`
    * Authors: Thorsten Beier, Thorben Kroeger, Jorg H. Kappes, Ullrich Kothe, Fred A. Hamprecht
    * Abstract: Recently, unsupervised image segmentation has become increasingly popular. Starting from a superpixel segmentation, an edge-weighted region adjacency graph is constructed. Amongst all segmentations of the graph, the one which best conforms to the given image evidence, as measured by the sum of cut edge weights, is chosen. Since this problem is NP-hard, we propose a new approximate solver based on the move-making paradigm: first, the graph is recursively partitioned into small regions (cut phase). Then, for any two adjacent regions, we consider alternative cuts of these two regions defining possible moves (glue & cut phase). For planar problems, the optimal move can be found, whereas for non-planar problems, efficient approximations exist. We evaluate our algorithm on published and new benchmark datasets, which we make available here. The proposed algorithm finds segmentations that, as measured by a loss function, are as close to the ground-truth as the global optimum found by exact solvers. It does so significantly faster then existing approximate methods, which is important for large-scale problems.
count=1
* 3D Pictorial Structures for Multiple Human Pose Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Belagiannis_3D_Pictorial_Structures_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Belagiannis_3D_Pictorial_Structures_2014_CVPR_paper.pdf)]
    * Title: 3D Pictorial Structures for Multiple Human Pose Estimation
    * Year: `2014`
    * Authors: Vasileios Belagiannis, Sikandar Amin, Mykhaylo Andriluka, Bernt Schiele, Nassir Navab, Slobodan Ilic
    * Abstract: In this work, we address the problem of 3D pose estimation of multiple humans from multiple views. This is a more challenging problem than single human 3D pose estimation due to the much larger state space, partial occlusions as well as across view ambiguities when not knowing the identity of the humans in advance. To address these problems, we first create a reduced state space by triangulation of corresponding body joints obtained from part detectors in pairs of camera views. In order to resolve the ambiguities of wrong and mixed body parts of multiple humans after triangulation and also those coming from false positive body part detections, we introduce a novel 3D pictorial structures (3DPS) model. Our model infers 3D human body configurations from our reduced state space. The 3DPS model is generic and applicable to both single and multiple human pose estimation. In order to compare to the state-of-the art, we first evaluate our method on single human 3D pose estimation on HumanEva-I [22] and KTH Multiview Football Dataset II [8] datasets. Then, we introduce and evaluate our method on two datasets for multiple human 3D pose estimation.
count=1
* Head Pose Estimation Based on Multivariate Label Distribution
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Geng_Head_Pose_Estimation_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Geng_Head_Pose_Estimation_2014_CVPR_paper.pdf)]
    * Title: Head Pose Estimation Based on Multivariate Label Distribution
    * Year: `2014`
    * Authors: Xin Geng, Yu Xia
    * Abstract: Accurate ground truth pose is essential to the training of most existing head pose estimation algorithms. However, in many cases, the "ground truth" pose is obtained in rather subjective ways, such as asking the human subjects to stare at different markers on the wall. In such case, it is better to use soft labels rather than explicit hard labels. Therefore, this paper proposes to associate a multivariate label distribution (MLD) to each image. An MLD covers a neighborhood around the original pose. Labeling the images with MLD can not only alleviate the problem of inaccurate pose labels, but also boost the training examples associated to each pose without actually increasing the total amount of training examples. Two algorithms are proposed to learn from the MLD by minimizing the weighted Jeffrey's divergence between the predicted MLD and the ground truth MLD. Experimental results show that the MLD-based methods perform significantly better than the compared state-of-the-art head pose estimation algorithms.
count=1
* Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)]
    * Title: Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation
    * Year: `2014`
    * Authors: Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik
    * Abstract: Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.
count=1
* Multi-Output Learning for Camera Relocalization
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Guzman-Rivera_Multi-Output_Learning_for_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Guzman-Rivera_Multi-Output_Learning_for_2014_CVPR_paper.pdf)]
    * Title: Multi-Output Learning for Camera Relocalization
    * Year: `2014`
    * Authors: Abner Guzman-Rivera, Pushmeet Kohli, Ben Glocker, Jamie Shotton, Toby Sharp, Andrew Fitzgibbon, Shahram Izadi
    * Abstract: We address the problem of estimating the pose of a cam- era relative to a known 3D scene from a single RGB-D frame. We formulate this problem as inversion of the generative rendering procedure, i.e., we want to find the camera pose corresponding to a rendering of the 3D scene model that is most similar with the observed input. This is a non-convex optimization problem with many local optima. We propose a hybrid discriminative-generative learning architecture that consists of: (i) a set of M predictors which generate M camera pose hypotheses; and (ii) a 'selector' or 'aggregator' that infers the best pose from the multiple pose hypotheses based on a similarity function. We are interested in predictors that not only produce good hypotheses but also hypotheses that are different from each other. Thus, we propose and study methods for learning 'marginally relevant' predictors, and compare their performance when used with different selection procedures. We evaluate our method on a recently released 3D reconstruction dataset with challenging camera poses, and scene variability. Experiments show that our method learns to make multiple predictions that are marginally relevant and can effectively select an accurate prediction. Furthermore, our method outperforms the state-of-the-art discriminative approach for camera relocalization.
count=1
* RIGOR: Reusing Inference in Graph Cuts for Generating Object Regions
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Humayun_RIGOR_Reusing_Inference_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Humayun_RIGOR_Reusing_Inference_2014_CVPR_paper.pdf)]
    * Title: RIGOR: Reusing Inference in Graph Cuts for Generating Object Regions
    * Year: `2014`
    * Authors: Ahmad Humayun, Fuxin Li, James M. Rehg
    * Abstract: Popular figure-ground segmentation algorithms generate a pool of boundary-aligned segment proposals that can be used in subsequent object recognition engines. These algorithms can recover most image objects with high accuracy, but are usually computationally intensive since many graph cuts are computed with different enumerations of segment seeds. In this paper we propose an algorithm, RIGOR, for efficiently generating a pool of overlapping segment proposals in images. By precomputing a graph which can be used for parametric min-cuts over different seeds, we speed up the generation of the segment pool. In addition, we have made design choices that avoid extensive computations without losing performance. In particular, we demonstrate that the segmentation performance of our algorithm is slightly better than the state-of-the-art on the PASCAL VOC dataset, while being an order of magnitude faster.
count=1
* NMF-KNN: Image Annotation using Weighted Multi-view Non-negative Matrix Factorization
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Kalayeh_NMF-KNN_Image_Annotation_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Kalayeh_NMF-KNN_Image_Annotation_2014_CVPR_paper.pdf)]
    * Title: NMF-KNN: Image Annotation using Weighted Multi-view Non-negative Matrix Factorization
    * Year: `2014`
    * Authors: Mahdi M. Kalayeh, Haroon Idrees, Mubarak Shah
    * Abstract: The real world image databases such as Flickr are characterized by continuous addition of new images. The recent approaches for image annotation, i.e. the problem of assigning tags to images, have two major drawbacks. First, either models are learned using the entire training data, or to handle the issue of dataset imbalance, tag-specific discriminative models are trained. Such models become obsolete and require relearning when new images and tags are added to database. Second, the task of feature-fusion is typically dealt using ad-hoc approaches. In this paper, we present a weighted extension of Multi-view Non-negative Matrix Factorization (NMF) to address the aforementioned drawbacks. The key idea is to learn query-specific generative model on the features of nearest-neighbors and tags using the proposed NMF-KNN approach which imposes consensus constraint on the coefficient matrices across different features. This results in coefficient vectors across features to be consistent and, thus, naturally solves the problem of feature fusion, while the weight matrices introduced in the proposed formulation alleviate the issue of dataset imbalance. Furthermore, our approach, being query-specific, is unaffected by addition of images and tags in a database. We tested our method on two datasets used for evaluation of image annotation and obtained competitive results.
count=1
* The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Kuehne_The_Language_of_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Kuehne_The_Language_of_2014_CVPR_paper.pdf)]
    * Title: The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities
    * Year: `2014`
    * Authors: Hilde Kuehne, Ali Arslan, Thomas Serre
    * Abstract: This paper describes a framework for modeling human activities as temporally structured processes. Our approach is motivated by the inherently hierarchical nature of human activities and the close correspondence between human actions and speech: We model action units using Hidden Markov Models, much like words in speech. These action units then form the building blocks to model complex human activities as sentences using an action grammar. To evaluate our approach, we collected a large dataset of daily cooking activities: The dataset includes a total of 52 participants, each performing a total of 10 cooking activities in multiple real-life kitchens, resulting in over 77 hours of video footage. We evaluate the HTK toolkit, a state-of-the-art speech recognition engine, in combination with multiple video feature descriptors, for both the recognition of cooking activities (e.g., making pancakes) as well as the semantic parsing of videos into action units (e.g., cracking eggs). Our results demonstrate the benefits of structured temporal generative approaches over existing discriminative approaches in coping with the complexity of human daily life activities.
count=1
* The Secrets of Salient Object Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Li_The_Secrets_of_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Li_The_Secrets_of_2014_CVPR_paper.pdf)]
    * Title: The Secrets of Salient Object Segmentation
    * Year: `2014`
    * Authors: Yin Li, Xiaodi Hou, Christof Koch, James M. Rehg, Alan L. Yuille
    * Abstract: In this paper we provide an extensive evaluation of fixation prediction and salient object segmentation algorithms as well as statistics of major datasets. Our analysis identifies serious design flaws of existing salient object benchmarks, called the dataset design bias, by over emphasising the stereotypical concepts of saliency. The dataset design bias does not only create the discomforting disconnection between fixations and salient object segmentation, but also misleads the algorithm designing. Based on our analysis, we propose a new high quality dataset that offers both fixation and salient object segmentation ground-truth. With fixations and salient object being presented simultaneously, we are able to bridge the gap between fixations and salient objects, and propose a novel method for salient object segmentation. Finally, we report significant benchmark progress on 3 existing datasets of segmenting salient objects.
count=1
* Two-Class Weather Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Lu_Two-Class_Weather_Classification_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Lu_Two-Class_Weather_Classification_2014_CVPR_paper.pdf)]
    * Title: Two-Class Weather Classification
    * Year: `2014`
    * Authors: Cewu Lu, Di Lin, Jiaya Jia, Chi-Keung Tang
    * Abstract: Given a single outdoor image, this paper proposes a collaborative learning approach for labeling it as either sunny or cloudy. Never adequately addressed, this twoclass classification problem is by no means trivial given the great variety of outdoor images. Our weather feature combines special cues after properly encoding them into feature vectors. They then work collaboratively in synergy under a unified optimization framework that is aware of the presence (or absence) of a given weather cue during learning and classification. Extensive experiments and comparisons are performed to verify our method. We build a new weather image dataset consisting of 10K sunny and cloudy images, which is available online together with the executable.
count=1
* A Compact and Discriminative Face Track Descriptor
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Parkhi_A_Compact_and_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Parkhi_A_Compact_and_2014_CVPR_paper.pdf)]
    * Title: A Compact and Discriminative Face Track Descriptor
    * Year: `2014`
    * Authors: Omkar M. Parkhi, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman
    * Abstract: Our goal is to learn a compact, discriminative vector representation of a face track, suitable for the face recognition tasks of verification and classification. To this end, we propose a novel face track descriptor, based on the Fisher Vector representation, and demonstrate that it has a number of favourable properties. First, the descriptor is suitable for tracks of both frontal and profile faces, and is insensitive to their pose. Second, the descriptor is compact due to discriminative dimensionality reduction, and it can be further compressed using binarization. Third, the descriptor can be computed quickly (using hard quantization) and its compact size and fast computation render it very suitable for large scale visual repositories. Finally, the descriptor demonstrates good generalization when trained on one dataset and tested on another, reflecting its tolerance to the dataset bias. In the experiments we show that the descriptor exceeds the state of the art on both face verification task (YouTube Faces without outside training data, and INRIA-Buffy benchmarks), and face classification task (using the Oxford-Buffy dataset).
count=1
* SeamSeg: Video Object Segmentation using Patch Seams
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Ramakanth_SeamSeg_Video_Object_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Ramakanth_SeamSeg_Video_Object_2014_CVPR_paper.pdf)]
    * Title: SeamSeg: Video Object Segmentation using Patch Seams
    * Year: `2014`
    * Authors: S. Avinash Ramakanth, R. Venkatesh Babu
    * Abstract: In this paper, we propose a technique for video object segmentation using patch seams across frames. Typically, seams, which are connected paths of low energy, are utilised for retargeting, where the primary aim is to reduce the image size while preserving the salient image contents. Here, we adapt the formulation of seams for temporal label propagation. The energy function associated with the proposed video seams provides temporal linking of patches across frames, to accurately segment the object. The proposed energy function takes into account the similarity of patches along the seam, temporal consistency of motion and spatial coherency of seams. Label propagation is achieved with high fidelity in the critical boundary regions, utilising the proposed patch seams. To achieve this without additional overheads, we curtail the error propagation by formulating boundary regions as rough-sets. The proposed approach out-perform state-of-the-art supervised and unsupervised algorithms, on benchmark datasets.
count=1
* Feature-Independent Action Spotting Without Human Localization, Segmentation or Frame-wise Tracking
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Sun_Feature-Independent_Action_Spotting_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Sun_Feature-Independent_Action_Spotting_2014_CVPR_paper.pdf)]
    * Title: Feature-Independent Action Spotting Without Human Localization, Segmentation or Frame-wise Tracking
    * Year: `2014`
    * Authors: Chuan Sun, Marshall Tappen, Hassan Foroosh
    * Abstract: In this paper, we propose an unsupervised framework for action spotting in videos that does not depend on any specific feature (e.g. HOG/HOF, STIP, silhouette, bag-of-words, etc.). Furthermore, our solution requires no human localization, segmentation, or framewise tracking. This is achieved by treating the problem holistically as that of extracting the internal dynamics of video cuboids by modeling them in their natural form as multilinear tensors. To extract their internal dynamics, we devised a novel Two-Phase Decomposition (TP-Decomp) of a tensor that generates very compact and discriminative representations that are robust to even heavily perturbed data. Technically, a Rank-based Tensor Core Pyramid (Rank-TCP) descriptor is generated by combining multiple tensor cores under multiple ranks, allowing to represent video cuboids in a hierarchical tensor pyramid. The problem then reduces to a template matching problem, which is solved efficiently by using two boosting strategies: (1) to reduce search space, we filter the dense trajectory cloud extracted from the target video; (2) to boost the matching speed, we perform matching in an iterative coarse-to-fine manner. Experiments on 5 benchmarks show that our method outperforms current state-of-the-art under various challenging conditions. We also created a challenging dataset called Heavily Perturbed Video Array (HPVA) to validate the robustness of our framework under heavily perturbed situations.
count=1
* Local Layering for Joint Motion Estimation and Occlusion Detection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Sun_Local_Layering_for_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Sun_Local_Layering_for_2014_CVPR_paper.pdf)]
    * Title: Local Layering for Joint Motion Estimation and Occlusion Detection
    * Year: `2014`
    * Authors: Deqing Sun, Ce Liu, Hanspeter Pfister
    * Abstract: Most motion estimation algorithms (optical flow, layered models) cannot handle large amount of occlusion in textureless regions, as motion is often initialized with no occlusion assumption despite that occlusion may be included in the final objective. To handle such situations, we propose a local layering model where motion and occlusion relationships are inferred jointly. In particular, the uncertainties of occlusion relationships are retained so that motion is inferred by considering all the possibilities of local occlusion relationships. In addition, the local layering model handles articulated objects with self-occlusion. We demonstrate that the local layering model can handle motion and occlusion well for both challenging synthetic and real sequences.
count=1
* A Hierarchical Context Model for Event Recognition in Surveillance Video
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Wang_A_Hierarchical_Context_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Wang_A_Hierarchical_Context_2014_CVPR_paper.pdf)]
    * Title: A Hierarchical Context Model for Event Recognition in Surveillance Video
    * Year: `2014`
    * Authors: Xiaoyang Wang, Qiang Ji
    * Abstract: Due to great challenges such as tremendous intra-class variations and low image resolution, context information has been playing a more and more important role for accurate and robust event recognition in surveillance videos. The context information can generally be divided into the feature level context, the semantic level context, and the prior level context. These three levels of context provide crucial bottom-up, middle level, and top down information that can benefit the recognition task itself. Unlike existing researches that generally integrate the context information at one of the three levels, we propose a hierarchical context model that simultaneously exploits contexts at all three levels and systematically incorporate them into event recognition. To tackle the learning and inference challenges brought in by the model hierarchy, we develop complete learning and inference algorithms for the proposed hierarchical context model based on variational Bayes method. Experiments on VIRAT 1.0 and 2.0 Ground Datasets demonstrate the effectiveness of the proposed hierarchical context model for improving the event recognition performance even under great challenges like large intra-class variations and low image resolution.
count=1
* Orientational Pyramid Matching for Recognizing Indoor Scenes
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Xie_Orientational_Pyramid_Matching_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Xie_Orientational_Pyramid_Matching_2014_CVPR_paper.pdf)]
    * Title: Orientational Pyramid Matching for Recognizing Indoor Scenes
    * Year: `2014`
    * Authors: Lingxi Xie, Jingdong Wang, Baining Guo, Bo Zhang, Qi Tian
    * Abstract: Scene recognition is a basic task towards image understanding. Spatial Pyramid Matching (SPM) has been shown to be an efficient solution for spatial context modeling. In this paper, we introduce an alternative approach, Orientational Pyramid Matching (OPM), for orientational context modeling. Our approach is motivated by the observation that the 3D orientations of objects are a crucial factor to discriminate indoor scenes. The novelty lies in that OPM uses the 3D orientations to form the pyramid and produce the pooling regions, which is unlike SPM that uses the spatial positions to form the pyramid. Experimental results on challenging scene classification tasks show that OPM achieves the performance comparable with SPM and that OPM and SPM make complementary contributions so that their combination gives the state-of-the-art performance.
count=1
* Are Cars Just 3D Boxes? - Jointly Estimating the 3D Shape of Multiple Objects
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/html/Zia_Are_Cars_Just_2014_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2014/papers/Zia_Are_Cars_Just_2014_CVPR_paper.pdf)]
    * Title: Are Cars Just 3D Boxes? - Jointly Estimating the 3D Shape of Multiple Objects
    * Year: `2014`
    * Authors: Muhammad Zeeshan Zia, Michael Stark, Konrad Schindler
    * Abstract: Current systems for scene understanding typically represent objects as 2D or 3D bounding boxes. While these representations have proven robust in a variety of applications, they provide only coarse approximations to the true 2D and 3D extent of objects. As a result, object-object interactions, such as occlusions or ground-plane contact, can be represented only superficially. In this paper, we approach the problem of scene understanding from the perspective of 3D shape modeling, and design a 3D scene representation that reasons jointly about the 3D shape of multiple objects. This representation allows to express 3D geometry and occlusion on the fine detail level of individual vertices of 3D wireframe models, and makes it possible to treat dependencies between objects, such as occlusion reasoning, in a deterministic way. In our experiments, we demonstrate the benefit of jointly estimating the 3D shape of multiple objects in a scene over working with coarse boxes, on the recently proposed KITTI dataset of realistic street scenes.
count=1
* POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Berg_POOF_Part-Based_One-vs.-One_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Berg_POOF_Part-Based_One-vs.-One_2013_CVPR_paper.pdf)]
    * Title: POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation
    * Year: `2013`
    * Authors: Thomas Berg, Peter N. Belhumeur
    * Abstract: From a set of images in a particular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs-One Features (POOFs). Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. We demonstrate the particular usefulness of these features for fine-grained visual categorization with new state-of-the-art results on bird species identification using the Caltech UCSD Birds (CUB) dataset and parity with the best existing results in face verification on the Labeled Faces in the Wild (LFW) dataset. Finally, we demonstrate the particular advantage of POOFs when training data is scarce.
count=1
* Multipath Sparse Coding Using Hierarchical Matching Pursuit
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Bo_Multipath_Sparse_Coding_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Bo_Multipath_Sparse_Coding_2013_CVPR_paper.pdf)]
    * Title: Multipath Sparse Coding Using Hierarchical Matching Pursuit
    * Year: `2013`
    * Authors: Liefeng Bo, Xiaofeng Ren, Dieter Fox
    * Abstract: Complex real-world signals, such as images, contain discriminative structures that differ in many aspects including scale, invariance, and data channel. While progress in deep learning shows the importance of learning features through multiple layers, it is equally important to learn features through multiple paths. We propose Multipath Hierarchical Matching Pursuit (M-HMP), a novel feature learning architecture that combines a collection of hierarchical sparse features for image classification to capture multiple aspects of discriminative structures. Our building blocks are MI-KSVD, a codebook learning algorithm that balances the reconstruction error and the mutual incoherence of the codebook, and batch orthogonal matching pursuit (OMP); we apply them recursively at varying layers and scales. The result is a highly discriminative image representation that leads to large improvements to the state-of-the-art on many standard benchmarks, e.g., Caltech-101, Caltech-256, MITScenes, Oxford-IIIT Pet and Caltech-UCSD Bird-200.
count=1
* Efficient Large-Scale Structured Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Branson_Efficient_Large-Scale_Structured_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Branson_Efficient_Large-Scale_Structured_2013_CVPR_paper.pdf)]
    * Title: Efficient Large-Scale Structured Learning
    * Year: `2013`
    * Authors: Steve Branson, Oscar Beijbom, Serge Belongie
    * Abstract: We introduce an algorithm, SVM-IS, for structured SVM learning that is computationally scalable to very large datasets and complex structural representations. We show that structured learning is at least as fast-and often much faster-than methods based on binary classification for problems such as deformable part models, object detection, and multiclass classification, while achieving accuracies that are at least as good. Our method allows problem-specific structural knowledge to exploited for faster optimization by integrating with a user-defined importance sampling function. We demonstrate fast train times on two challenging largescale datasets for two very different problems: ImageNet for multiclass classification and CUB-200-2011 for deformable part model training. Our method is shown to be 10-50 times faster than SVMstruct for cost-sensitive multiclass classification while being about as fast as the fastest 1-vs-all methods for multiclass classification. For deformable part model training, it is shown to be 50-1000 times faster than methods based on SVMstruct, mining hard negatives, and Pegasos-style stochastic gradient descent. Source code of our method is publicly available.
count=1
* Dense Reconstruction Using 3D Object Shape Priors
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Dame_Dense_Reconstruction_Using_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Dame_Dense_Reconstruction_Using_2013_CVPR_paper.pdf)]
    * Title: Dense Reconstruction Using 3D Object Shape Priors
    * Year: `2013`
    * Authors: Amaury Dame, Victor A. Prisacariu, Carl Y. Ren, Ian Reid
    * Abstract: We propose a formulation of monocular SLAM which combines live dense reconstruction with shape priors-based 3D tracking and reconstruction. Current live dense SLAM approaches are limited to the reconstruction of visible surfaces. Moreover, most of them are based on the minimisation of a photo-consistency error, which usually makes them sensitive to specularities. In the 3D pose recovery literature, problems caused by imperfect and ambiguous image information have been dealt with by using prior shape knowledge. At the same time, the success of depth sensors has shown that combining joint image and depth information drastically increases the robustness of the classical monocular 3D tracking and 3D reconstruction approaches. In this work we link dense SLAM to 3D object pose and shape recovery. More specifically, we automatically augment our SLAM system with object specific identity, together with 6D pose and additional shape degrees of freedom for the object(s) of known class in the scene, combining image data and depth information for the pose and shape recovery. This leads to a system that allows for full scaled 3D reconstruction with the known object(s) segmented from the scene. The segmentation enhances the clarity, accuracy and completeness of the maps built by the dense SLAM system, while the dense 3D data aids the segmentation process, yielding faster and more reliable convergence than when using 2D image data alone.
count=1
* Fine-Grained Crowdsourcing for Fine-Grained Recognition
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Deng_Fine-Grained_Crowdsourcing_for_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Deng_Fine-Grained_Crowdsourcing_for_2013_CVPR_paper.pdf)]
    * Title: Fine-Grained Crowdsourcing for Fine-Grained Recognition
    * Year: `2013`
    * Authors: Jia Deng, Jonathan Krause, Li Fei-Fei
    * Abstract: Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, fine-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called "Bubbles" that reveals discriminative features humans use. The player's goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions ("bubbles"), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the "BubbleBank" algorithm that uses the human selected bubbles to improve machine recognition performance. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks.
count=1
* Boundary Cues for 3D Object Shape Recovery
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Karsch_Boundary_Cues_for_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Karsch_Boundary_Cues_for_2013_CVPR_paper.pdf)]
    * Title: Boundary Cues for 3D Object Shape Recovery
    * Year: `2013`
    * Authors: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem
    * Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked "boundary" cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).
count=1
* Adaptive Active Learning for Image Classification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Li_Adaptive_Active_Learning_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Li_Adaptive_Active_Learning_2013_CVPR_paper.pdf)]
    * Title: Adaptive Active Learning for Image Classification
    * Year: `2013`
    * Authors: Xin Li, Yuhong Guo
    * Abstract: Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision, object recognition and scene recognition, demonstrate the efficacy of the proposed approach.
count=1
* Non-parametric Filtering for Geometric Detail Extraction and Material Representation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Liao_Non-parametric_Filtering_for_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Liao_Non-parametric_Filtering_for_2013_CVPR_paper.pdf)]
    * Title: Non-parametric Filtering for Geometric Detail Extraction and Material Representation
    * Year: `2013`
    * Authors: Zicheng Liao, Jason Rock, Yang Wang, David Forsyth
    * Abstract: Geometric detail is a universal phenomenon in real world objects. It is an important component in object modeling, but not accounted for in current intrinsic image works. In this work, we explore using a non-parametric method to separate geometric detail from intrinsic image components. We further decompose an image as albedo * (coarse-scale shading + shading detail). Our decomposition offers quantitative improvement in albedo recovery and material classification.Our method also enables interesting image editing activities, including bump removal, geometric detail smoothing/enhancement and material transfer.
count=1
* Crossing the Line: Crowd Counting by Integer Programming with Local Features
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Ma_Crossing_the_Line_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Ma_Crossing_the_Line_2013_CVPR_paper.pdf)]
    * Title: Crossing the Line: Crowd Counting by Integer Programming with Local Features
    * Year: `2013`
    * Authors: Zheng Ma, Antoni B. Chan
    * Abstract: We propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a video sequence. Through a line sampling process, the video is first converted into a temporal slice image. Next, the number of people is estimated in a set of overlapping sliding windows on the temporal slice image, using a regression function that maps from local features to a count. Given that count in a sliding window is the sum of the instantaneous counts in the corresponding time interval, an integer programming method is proposed to recover the number of pedestrians crossing the line of interest in each frame. Integrating over a specific time interval yields the cumulative count of pedestrian crossing the line. Compared with current methods for line counting, our proposed approach achieves state-of-the-art performance on several challenging crowd video datasets.
count=1
* Local Fisher Discriminant Analysis for Pedestrian Re-identification
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Pedagadi_Local_Fisher_Discriminant_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Pedagadi_Local_Fisher_Discriminant_2013_CVPR_paper.pdf)]
    * Title: Local Fisher Discriminant Analysis for Pedestrian Re-identification
    * Year: `2013`
    * Authors: Sateesh Pedagadi, James Orwell, Sergio Velastin, Boghos Boghossian
    * Abstract: Metric learning methods, for person re-identification, estimate a scaling for distances in a vector space that is optimized for picking out observations of the same individual. This paper presents a novel approach to the pedestrian re-identification problem that uses metric learning to improve the state-of-the-art performance on standard public datasets. Very high dimensional features are extracted from the source color image. A first processing stage performs unsupervised PCA dimensionality reduction, constrained to maintain the redundancy in color-space representation. A second stage further reduces the dimensionality, using a Local Fisher Discriminant Analysis defined by a training set. A regularization step is introduced to avoid singular matrices during this stage. The experiments conducted on three publicly available datasets confirm that the proposed method outperforms the state-of-the-art performance, including all other known metric learning methods. Furthermore, the method is an effective way to process observations comprising multiple shots, and is non-iterative: the computation times are relatively modest. Finally, a novel statistic is derived to characterize the Match Characteristic: the normalized entropy reduction can be used to define the 'Proportion of Uncertainty Removed' (P UR ). This measure is invariant to test set size and provides an intuitive indication of performance.
count=1
* Learning Video Saliency from Human Gaze Using Candidate Selection
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Rudoy_Learning_Video_Saliency_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Rudoy_Learning_Video_Saliency_2013_CVPR_paper.pdf)]
    * Title: Learning Video Saliency from Human Gaze Using Candidate Selection
    * Year: `2013`
    * Authors: Dmitry Rudoy, Dan B. Goldman, Eli Shechtman, Lihi Zelnik-Manor
    * Abstract: During recent years remarkable progress has been made in visual saliency modeling. Our interest is in video saliency. Since videos are fundamentally different from still images, they are viewed differently by human observers. For example, the time each video frame is observed is a fraction of a second, while a still image can be viewed leisurely. Therefore, video saliency estimation methods should differ substantially from image saliency methods. In this paper we propose a novel method for video saliency estimation, which is inspired by the way people watch videos. We explicitly model the continuity of the video by predicting the saliency map of a given frame, conditioned on the map from the previous frame. Furthermore, accuracy and computation speed are improved by restricting the salient locations to a carefully selected candidate set. We validate our method using two gaze-tracked video datasets and show we outperform the state-of-the-art.
count=1
* Pedestrian Detection with Unsupervised Multi-stage Feature Learning
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Sermanet_Pedestrian_Detection_with_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Sermanet_Pedestrian_Detection_with_2013_CVPR_paper.pdf)]
    * Title: Pedestrian Detection with Unsupervised Multi-stage Feature Learning
    * Year: `2013`
    * Authors: Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann Lecun
    * Abstract: Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.
count=1
* Vantage Feature Frames for Fine-Grained Categorization
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Sfar_Vantage_Feature_Frames_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Sfar_Vantage_Feature_Frames_2013_CVPR_paper.pdf)]
    * Title: Vantage Feature Frames for Fine-Grained Categorization
    * Year: `2013`
    * Authors: Asma Rejeb Sfar, Nozha Boujemaa, Donald Geman
    * Abstract: We study fine-grained categorization, the task of distinguishing among (sub)categories of the same generic object class (e.g., birds), focusing on determining botanical species (leaves and orchids) from scanned images. The strategy is to focus attention around several vantage points, which is the approach taken by botanists, but using features dedicated to the individual categories. Our implementation of the strategy is based on vantage feature frames, a novel object representation consisting of two components: a set of coordinate systems centered at the most discriminating local viewpoints for the generic object class and a set of category-dependent features computed in these frames. The features are pooled over frames to build the classifier. Categorization then proceeds from coarse-grained (finding the frames) to fine-grained (finding the category), and hence the vantage feature frames must be both detectable and discriminating. The proposed method outperforms state-of-the art algorithms, in particular those using more distributed representations, on standard databases of leaves.
count=1
* A Joint Model for 2D and 3D Pose Estimation from a Single Image
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Simo-Serra_A_Joint_Model_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Simo-Serra_A_Joint_Model_2013_CVPR_paper.pdf)]
    * Title: A Joint Model for 2D and 3D Pose Estimation from a Single Image
    * Year: `2013`
    * Authors: Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, Francesc Moreno-Noguer
    * Abstract: We introduce a novel approach to automatically recover 3D human pose from a single image. Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.
count=1
* Exemplar-Based Face Parsing
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Smith_Exemplar-Based_Face_Parsing_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Smith_Exemplar-Based_Face_Parsing_2013_CVPR_paper.pdf)]
    * Title: Exemplar-Based Face Parsing
    * Year: `2013`
    * Authors: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang
    * Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.
count=1
* Optimizing 1-Nearest Prototype Classifiers
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Wohlhart_Optimizing_1-Nearest_Prototype_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Wohlhart_Optimizing_1-Nearest_Prototype_2013_CVPR_paper.pdf)]
    * Title: Optimizing 1-Nearest Prototype Classifiers
    * Year: `2013`
    * Authors: Paul Wohlhart, Martin Kostinger, Michael Donoser, Peter M. Roth, Horst Bischof
    * Abstract: The development of complex, powerful classifiers and their constant improvement have contributed much to the progress in many fields of computer vision. However, the trend towards large scale datasets revived the interest in simpler classifiers to reduce runtime. Simple nearest neighbor classifiers have several beneficial properties, such as low complexity and inherent multi-class handling, however, they have a runtime linear in the size of the database. Recent related work represents data samples by assigning them to a set of prototypes that partition the input feature space and afterwards applies linear classifiers on top of this representation to approximate decision boundaries locally linear. In this paper, we go a step beyond these approaches and purely focus on 1-nearest prototype classification, where we propose a novel algorithm for deriving optimal prototypes in a discriminative manner from the training samples. Our method is implicitly multi-class capable, parameter free, avoids noise overfitting and, since during testing only comparisons to the derived prototypes are required, highly efficient. Experiments demonstrate that we are able to outperform related locally linear methods, while even getting close to the results of more complex classifiers.
count=1
* Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Zhang_Probabilistic_Graphlet_Cut_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Zhang_Probabilistic_Graphlet_Cut_2013_CVPR_paper.pdf)]
    * Title: Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation
    * Year: `2013`
    * Authors: Luming Zhang, Mingli Song, Zicheng Liu, Xiao Liu, Jiajun Bu, Chun Chen
    * Abstract: Weakly supervised image segmentation is a challenging problem in computer vision field. In this paper, we present a new weakly supervised image segmentation algorithm by learning the distribution of spatially structured superpixel sets from image-level labels. Specifically, we first extract graphlets from each image where a graphlet is a smallsized graph consisting of superpixels as its nodes and it encapsulates the spatial structure of those superpixels. Then, a manifold embedding algorithm is proposed to transform graphlets of different sizes into equal-length feature vectors. Thereafter, we use GMM to learn the distribution of the post-embedding graphlets. Finally, we propose a novel image segmentation algorithm, called graphlet cut, that leverages the learned graphlet distribution in measuring the homogeneity of a set of spatially structured superpixels. Experimental results show that the proposed approach outperforms state-of-the-art weakly supervised image segmentation methods, and its performance is comparable to those of the fully supervised segmentation models.
count=1
* Structure Preserving Object Tracking
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Zhang_Structure_Preserving_Object_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Zhang_Structure_Preserving_Object_2013_CVPR_paper.pdf)]
    * Title: Structure Preserving Object Tracking
    * Year: `2013`
    * Authors: Lu Zhang, Laurens van der Maaten
    * Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation of our structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.
count=1
* Explicit Occlusion Modeling for 3D Object Class Representations
    [[abs-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/html/Zia_Explicit_Occlusion_Modeling_2013_CVPR_paper.html)]
    [[pdf-CVPR](https://openaccess.thecvf.com/content_cvpr_2013/papers/Zia_Explicit_Occlusion_Modeling_2013_CVPR_paper.pdf)]
    * Title: Explicit Occlusion Modeling for 3D Object Class Representations
    * Year: `2013`
    * Authors: M. Zeeshan Zia, Michael Stark, Konrad Schindler
    * Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.
count=1
* A Retrospect to Multi-prompt Learning across Vision and Language
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Chen_A_Retrospect_to_Multi-prompt_Learning_across_Vision_and_Language_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_A_Retrospect_to_Multi-prompt_Learning_across_Vision_and_Language_ICCV_2023_paper.pdf)]
    * Title: A Retrospect to Multi-prompt Learning across Vision and Language
    * Year: `2023`
    * Authors: Ziliang Chen, Xin Huang, Quanlong Guan, Liang Lin, Weiqi Luo
    * Abstract: The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing research milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.
count=1
* Distribution-Aware Prompt Tuning for Vision-Language Models
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Cho_Distribution-Aware_Prompt_Tuning_for_Vision-Language_Models_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_Distribution-Aware_Prompt_Tuning_for_Vision-Language_Models_ICCV_2023_paper.pdf)]
    * Title: Distribution-Aware Prompt Tuning for Vision-Language Models
    * Year: `2023`
    * Authors: Eulrang Cho, Jooyeon Kim, Hyunwoo J Kim
    * Abstract: Pre-trained vision-language models (VLMs) have shown impressive performance on various downstream tasks by utilizing knowledge learned from large data. In general, the performance of VLMs on target tasks can be further improved by prompt tuning, which adds context to the input image or text. By leveraging data from target tasks, various prompt-tuning methods have been studied in the literature. A key to prompt tuning is the feature space alignment between two modalities via learnable vectors with model parameters fixed. We observed that the alignment becomes more effective when embeddings of each modality are 'well-arranged' in the latent space. Inspired by this observation, we proposed distribution-aware prompt tuning (DAPT) for vision-language models, which is simple yet effective. Specifically, the prompts are learned by maximizing inter-dispersion, the distance between classes, as well as minimizing the intra-dispersion measured by the distance between embeddings from the same class. Our extensive experiments on 11 benchmark datasets demonstrate that our method significantly improves generalizability. The code is available at https://github.com/mlvlab/DAPT.
count=1
* Towards Inadequately Pre-trained Models in Transfer Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Deng_Towards_Inadequately_Pre-trained_Models_in_Transfer_Learning_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_Towards_Inadequately_Pre-trained_Models_in_Transfer_Learning_ICCV_2023_paper.pdf)]
    * Title: Towards Inadequately Pre-trained Models in Transfer Learning
    * Year: `2023`
    * Authors: Andong Deng, Xingjian Li, Di Hu, Tianyang Wang, Haoyi Xiong, Cheng-Zhong Xu
    * Abstract: Transfer learning has been a popular learning paradigm in the deep learning era, especially in annotation-insufficient scenarios. Better ImageNet pre-trained models have been demonstrated, from the perspective of architecture, by previous research to have better transferability to downstream tasks. However, in this paper, we find that during the same pre-training process, models at middle epochs, which are inadequately pre-trained, can outperform fully trained models when used as feature extractors (FE), while the fine-tuning (FT) performance still grows with the source performance. This reveals that there is not a solid positive correlation between top-1 accuracy on ImageNet and the transferring result on target data. Based on the contradictory phenomenon between FE and FT that a better feature extractor fails to be fine-tuned better accordingly, we conduct comprehensive analyses on features before the softmax layer to provide insightful explanations. Our discoveries suggest that, during pre-training, models tend to first learn spectral components corresponding to large singular values and the residual components contribute more when fine-tuning.
count=1
* Bayesian Prompt Learning for Image-Language Model Generalization
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Derakhshani_Bayesian_Prompt_Learning_for_Image-Language_Model_Generalization_ICCV_2023_paper.pdf)]
    * Title: Bayesian Prompt Learning for Image-Language Model Generalization
    * Year: `2023`
    * Authors: Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor G. Turrisi da Costa, Cees G.M. Snoek, Georgios Tzimiropoulos, Brais Martinez
    * Abstract: Foundational image-language models have generated considerable interest due to their efficient adaptation to downstream tasks by prompt learning. Prompt learning treats part of the language model input as trainable while freezing the rest, and optimizes an Empirical Risk Minimization objective. However, Empirical Risk Minimization is known to suffer from distributional shifts which hurt generalizability to prompts unseen during training. By leveraging the regularization ability of Bayesian methods, we frame prompt learning from the Bayesian perspective and formulate it as a variational inference problem. Our approach regularizes the prompt space, reduces overfitting to the seen prompts and improves the prompt generalization on unseen prompts. Our framework is implemented by modeling the input prompt space in a probabilistic manner, as an a priori distribution which makes our proposal compatible with prompt learning approaches that are unconditional or conditional on the image. We demonstrate empirically on 15 benchmarks that Bayesian prompt learning provides an appropriate coverage of the prompt space, prevents learning spurious features, and exploits transferable invariant features. This results in better generalization of unseen prompts, even across different datasets and domains. Code available at: https://github.com/saic-fi/Bayesian Prompt-Learning
count=1
* Semi-Supervised Learning via Weight-Aware Distillation under Class Distribution Mismatch
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Du_Semi-Supervised_Learning_via_Weight-Aware_Distillation_under_Class_Distribution_Mismatch_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Du_Semi-Supervised_Learning_via_Weight-Aware_Distillation_under_Class_Distribution_Mismatch_ICCV_2023_paper.pdf)]
    * Title: Semi-Supervised Learning via Weight-Aware Distillation under Class Distribution Mismatch
    * Year: `2023`
    * Authors: Pan Du, Suyun Zhao, Zisen Sheng, Cuiping Li, Hong Chen
    * Abstract: Semi-Supervised Learning (SSL) under class distribution mismatch aims to tackle a challenging problem wherein unlabeled data contain lots of unknown categories unseen in the labeled ones. In such mismatch scenarios, traditional SSL suffers severe performance damage due to the harmful invasion of the instances with unknown categories into the target classifier. In this study, by strict mathematical reasoning, we reveal that the SSL error under class distribution mismatch is composed of pseudo-labeling error and invasion error, both of which jointly bound the SSL population risk. To alleviate the SSL error, we propose a robust SSL framework called Weight-Aware Distillation (WAD) that, by weights, selectively transfers knowledge beneficial to the target task from unsupervised contrastive representation to the target classifier. Specifically, WAD captures adaptive weights and high-quality pseudo-labels to target instances by exploring point mutual information (PMI) in representation space to maximize the role of unlabeled data and filter unknown categories. Theoretically, we prove that WAD has a tight upper bound of population risk under class distribution mismatch. Experimentally, extensive results demonstrate that WAD outperforms five state-of-the-art SSL approaches and one standard baseline on two benchmark datasets, CIFAR10 and CIFAR100, and an artificial cross-dataset. The code is available at https://github.com/RUC-DWBI-ML/research/tree/main/WAD-master.
count=1
* Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Faghri_Reinforce_Data_Multiply_Impact_Improved_Model_Accuracy_and_Robustness_with_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Faghri_Reinforce_Data_Multiply_Impact_Improved_Model_Accuracy_and_Robustness_with_ICCV_2023_paper.pdf)]
    * Title: Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement
    * Year: `2023`
    * Authors: Fartash Faghri, Hadi Pouransari, Sachin Mehta, Mehrdad Farajtabar, Ali Farhadi, Mohammad Rastegari, Oncel Tuzel
    * Abstract: We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the ImageNet validation set is also reduced by 9.9%. Using this backbone with Mask-RCNN for object detection on MS-COCO, the mean average precision improves by 0.8%. We reach similar gains for MobileNets, ViTs, and Swin-Transformers. For MobileNetV3 and Swin-Tiny, we observe significant improvements on ImageNet-R/A/C of up to 20% improved robustness. Models pretrained on ImageNet+ and fine-tuned on CIFAR-100+, Flowers-102+, and Food-101+, reach up to 3.4% improved accuracy. The code, datasets, and pretrained models are available at https://github.com/apple/ml-dr.
count=1
* Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Diverse_Data_Augmentation_with_Diffusions_for_Effective_Test-time_Prompt_Tuning_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_Diverse_Data_Augmentation_with_Diffusions_for_Effective_Test-time_Prompt_Tuning_ICCV_2023_paper.pdf)]
    * Title: Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning
    * Year: `2023`
    * Authors: Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, Wangmeng Zuo
    * Abstract: Benefiting from prompt tuning, recent years have witnessed the promising performance of pre-trained vision-language models, e.g., CLIP, on versatile downstream tasks. In this paper, we focus on a particular setting of learning adaptive prompts on the fly for each test sample from an unseen new domain, which is known as test-time prompt tuning (TPT). Existing TPT methods typically rely on data augmentation and confidence selection. However, conventional data augmentation techniques, e.g., random resized crops, suffers from the lack of data diversity, while entropy-based confidence selection alone is not sufficient to guarantee prediction fidelity. To address these issues, we propose a novel TPT method, named DiffTPT, which leverages pre-trained diffusion models to generate diverse and informative new data. Specifically, we incorporate augmented data by both conventional method and pre-trained stable diffusion to exploit their respective merits, improving the model's ability to adapt to unknown new test data. Moreover, to ensure the prediction fidelity of generated data, we introduce a cosine similarity-based filtration technique to select the generated data with higher similarity to the single test sample. Our experiments on test datasets with distribution shifts and unseen categories demonstrate that DiffTPT improves the zero-shot accuracy by an average of 5.13% compared to the state-of-the-art TPT method.
count=1
* Controllable Visual-Tactile Synthesis
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Controllable_Visual-Tactile_Synthesis_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Controllable_Visual-Tactile_Synthesis_ICCV_2023_paper.pdf)]
    * Title: Controllable Visual-Tactile Synthesis
    * Year: `2023`
    * Authors: Ruihan Gao, Wenzhen Yuan, Jun-Yan Zhu
    * Abstract: Deep generative models have various content creation applications such as graphic design, e-commerce, and virtual try-on. However, current works mainly focus on synthesizing realistic visual outputs, often ignoring other sensory modalities, such as touch, which limits physical interaction with users. In this work, we leverage deep generative models to create a multi-sensory experience where users can touch and see the synthesized object when sliding their fingers on a haptic surface. The main challenges lie in the significant scale discrepancy between vision and touch sensing and the lack of explicit mapping from touch sensing data to a haptic rendering device. To bridge this gap, we collect high-resolution tactile data with a GelSight sensor and create a new visuotactile clothing dataset. We then develop a conditional generative model that synthesizes both visual and tactile outputs from a single sketch. We evaluate our method regarding image quality and tactile rendering accuracy. Finally, we introduce a pipeline to render high-quality visual and tactile outputs on an electroadhesion-based haptic device for an immersive experience, allowing for challenging materials and editable sketch inputs.
count=1
* ETran: Energy-Based Transferability Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Gholami_ETran_Energy-Based_Transferability_Estimation_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Gholami_ETran_Energy-Based_Transferability_Estimation_ICCV_2023_paper.pdf)]
    * Title: ETran: Energy-Based Transferability Estimation
    * Year: `2023`
    * Authors: Mohsen Gholami, Mohammad Akbari, Xinglu Wang, Behnam Kamranian, Yong Zhang
    * Abstract: This paper addresses the problem of ranking pre-trained models for object detection and image classification. Selecting the best pre-trained model by fine-tuning is an expensive and time-consuming task. Previous works have proposed transferability estimation based on features extracted by the pre-trained models. We argue that quantifying whether the target dataset is in-distribution (IND) or out-of-distribution (OOD) for the pre-trained model is an important factor in the transferability estimation. To this end, we propose ETran, an energy-based transferability assessment metric, which includes three scores: 1) energy score, 2) classification score, and 3) regression score. We use energy-based models to determine whether the target dataset is OOD or IND for the pre-trained model. In contrast to the prior works, ETran is applicable to a wide range of tasks including classification, regression, and object detection (classification+regression). This is the first work that proposes transferability estimation for object detection task. Our extensive experiments on four benchmarks and two tasks show that ETran outperforms previous works on object detection and classification benchmarks by an average of 21% and 12%, respectively, and achieves SOTA in transferability assessment.
count=1
* STEERER: Resolving Scale Variations for Counting and Localization via Selective Inheritance Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Han_STEERER_Resolving_Scale_Variations_for_Counting_and_Localization_via_Selective_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_STEERER_Resolving_Scale_Variations_for_Counting_and_Localization_via_Selective_ICCV_2023_paper.pdf)]
    * Title: STEERER: Resolving Scale Variations for Counting and Localization via Selective Inheritance Learning
    * Year: `2023`
    * Authors: Tao Han, Lei Bai, Lingbo Liu, Wanli Ouyang
    * Abstract: Scale variation is a deep-rooted problem in object counting, which has not been effectively addressed by existing scale-aware algorithms. An important factor is that they typically involve cooperative learning across multi-resolutions, which could be suboptimal for learning the most discriminative features from each scale. In this paper, we propose a novel method termed STEERER (SelecTivE inhERitance lEaRning) that addresses the issue of scale variations in object counting. STEERER selects the most suitable scale for patch objects to boost feature extraction and only inherits discriminative features from lower to higher resolution progressively. The main insights of STEERER are a dedicated Feature Selection and Inheritance Adaptor (FSIA), which selectively forwards scale-customized features at each scale, and a Masked Selection and Inheritance Loss (MSIL) that helps to achieve high-quality density maps across all scales. Our experimental results on nine datasets with counting and localization tasks demonstrate the unprecedented scale generalization ability of STEERER. Code is available at https://github.com/taohan10200/STEERER.
count=1
* Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Open-domain_Visual_Entity_Recognition_Towards_Recognizing_Millions_of_Wikipedia_Entities_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Open-domain_Visual_Entity_Recognition_Towards_Recognizing_Millions_of_Wikipedia_Entities_ICCV_2023_paper.pdf)]
    * Title: Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities
    * Year: `2023`
    * Authors: Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, Ming-Wei Chang
    * Abstract: Large-scale multi-modal pre-training models such as CLIP and PaLI exhibit strong generalization on various visual domains and tasks. However, existing image classification benchmarks often evaluate recognition on a specific domain (e.g., outdoor images) or a specific task (e.g., classifying plant species), which falls short of evaluating whether pre-trained foundational models are universal visual recognizers. To address this, we formally present the task of Open-domain Visual Entity recognitioN (OVEN), where a model need to link an image onto a Wikipedia entity with respect to a text query. We construct OVEN by re-purposing 14 existing datasets with all labels grounded onto one single label space: Wikipedia entities. OVEN challenges models to select among six million possible Wikipedia entities, making it a general visual recognition benchmark with largest number of labels. Our study on state-of-the-art pre-trained models reveals large headroom in generalizing to the massive-scale label space. We show that a PaLI-based auto-regressive visual recognition model performs surprisingly well, even on Wikipedia entities that have never been seen during fine-tuning. We also find existing pre-trained models yield different unique strengths: while PaLI-based models obtains higher overall performance, CLIP-based models are better at recognizing tail entities.
count=1
* DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Huang_DiffDis_Empowering_Generative_Diffusion_Model_with_Cross-Modal_Discrimination_Capability_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_DiffDis_Empowering_Generative_Diffusion_Model_with_Cross-Modal_Discrimination_Capability_ICCV_2023_paper.pdf)]
    * Title: DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability
    * Year: `2023`
    * Authors: Runhui Huang, Jianhua Han, Guansong Lu, Xiaodan Liang, Yihan Zeng, Wei Zhang, Hang Xu
    * Abstract: Recently, large-scale diffusion models, e.g., Stable diffusion and DallE2, have shown remarkable results on image synthesis. On the other hand, large-scale cross-modal pre-trained models (e.g., CLIP, ALIGN, and FILIP) are competent for various downstream tasks by learning to align vision and language embeddings. In this paper, we explore the possibility of jointly modeling generation and discrimination. Specifically, we propose DiffDis to unify the cross-modal generative and discriminative pretraining into one single framework under the diffusion process. DiffDis first formulates the image-text discriminative problem as a generative diffusion process of the text embedding from the text encoder conditioned on the image. Then, we propose a novel dual-stream network architecture, which fuses the noisy text embedding with the knowledge of latent images from different scales for image-text discriminative learning. Moreover, the generative and discriminative tasks can efficiently share the image-branch network structure in the multi-modality model. Benefiting from diffusion-based unified training, DiffDis achieves both better generation ability and cross-modal semantic alignment in one architecture. Experimental results show that DiffDis outperforms single-task models on both the image generation and the image-text discriminative tasks, e.g., 1.65% improvement on average accuracy of zero-shot classification over 12 datasets and 2.42 improvement on FID of zero-shot image synthesis.
count=1
* Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Jie_Revisiting_the_Parameter_Efficiency_of_Adapters_from_the_Perspective_of_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Jie_Revisiting_the_Parameter_Efficiency_of_Adapters_from_the_Perspective_of_ICCV_2023_paper.pdf)]
    * Title: Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy
    * Year: `2023`
    * Authors: Shibo Jie, Haoqing Wang, Zhi-Hong Deng
    * Abstract: Current state-of-the-art results in computer vision depend in part on fine-tuning large pre-trained vision models. However, with the exponential growth of model sizes, the conventional full fine-tuning, which needs to store a individual network copy for each tasks, leads to increasingly huge storage and transmission overhead. Adapter-based Parameter-Efficient Tuning (PET) methods address this challenge by tuning lightweight adapters inserted into the frozen pre-trained models. In this paper, we investigate how to make adapters even more efficient, reaching a new minimum size required to store a task-specific fine-tuned network. Inspired by the observation that the parameters of adapters converge at flat local minima, we find that adapters are resistant to noise in parameter space, which means they are also resistant to low numerical precision. To train low-precision adapters, we propose a computational-efficient quantization method which minimizes the quantization error. Through extensive experiments, we find that low-precision adapters exhibit minimal performance degradation, and even 1-bit precision is sufficient for adapters. The results of the experiments demonstrate that 1-bit adapters outperform all other PET methods on both the VTAB-1K benchmark and few-shot FGVC datasets, while requiring the smallest storage size. Our findings show, for the first time, the significant potential of quantization techniques in PET, providing a general solution to enhance the parameter efficiency of adapter-based PET methods.
count=1
* Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Kan_Knowledge-Aware_Prompt_Tuning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf)]
    * Title: Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models
    * Year: `2023`
    * Authors: Baoshuo Kan, Teng Wang, Wenpeng Lu, Xiantong Zhen, Weili Guan, Feng Zheng
    * Abstract: Pre-trained vision-language models, e.g., CLIP, working with manually designed prompts have demonstrated great effectiveness in transfer learning. Recently, learnable prompts achieve state-of-the-art performance, which however are prone to overfit to seen classes while failing to generalize to unseen classes. In this paper, we propose a Knowledge-Aware Prompt Tuning (KAPT) framework for vision-language models. Our approach takes the inspiration from human intelligence in which external knowledge is usually incorporated into recognizing novel categories of objects. Specifically, we design two complementary types of knowledge-aware prompts for the text encoder to leverage the distinctive characteristics of category-related external knowledge. The discrete prompt extracts the key information from descriptions of an object category, and the learned continuous prompt captures overall contexts. We further design an adaptation head for the visual encoder to aggregate salient attentive visual cues, which establishes discriminative and task-aware visual representations. We conduct extensive experiments on 11 widely-used benchmark datasets and the results verify the effectiveness in few-shot image classification, especially in generalizing to unseen categories. Compared with the state-of-the-art CoCoOp method, KAPT exhibits favorable performance and achieves an absolute gain of 3.22% on new classes and 2.57% in terms of harmonic mean.
count=1
* Self-regulating Prompts: Foundational Model Adaptation without Forgetting
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Khattak_Self-regulating_Prompts_Foundational_Model_Adaptation_without_Forgetting_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Khattak_Self-regulating_Prompts_Foundational_Model_Adaptation_without_Forgetting_ICCV_2023_paper.pdf)]
    * Title: Self-regulating Prompts: Foundational Model Adaptation without Forgetting
    * Year: `2023`
    * Authors: Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan
    * Abstract: Prompt learning has emerged as an efficient alternative for fine-tuning foundational models, such as CLIP, for various downstream tasks. Conventionally trained using the task-specific objective, i.e., cross-entropy loss, prompts tend to overfit downstream data distributions and find it challenging to capture task-agnostic general features from the frozen CLIP. This leads to the loss of the model's original generalization capability. To address this issue, our work introduces a self-regularization framework for prompting called PromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the prompts to optimize for both task-specific and task-agnostic general representations using a three-pronged approach by: (a) regulating prompted representations via mutual agreement maximization with the frozen model, (b) regulating with self-ensemble of prompts over the training trajectory to encode their complementary strengths, and (c) regulating with textual diversity to mitigate sample diversity imbalance with the visual branch. To the best of our knowledge, this is the first regularization framework for prompt learning that avoids overfitting by jointly attending to pre-trained model features, the training trajectory during prompting, and the textual diversity. PromptSRC explicitly steers the prompts to learn a representation space that maximizes performance on downstream tasks without compromising CLIP generalization. We perform extensive experiments on 4 benchmarks where PromptSRC overall performs favorably well compared to the existing methods. Our code and pre-trained models are publicly available.
count=1
* CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Kim_CRN_Camera_Radar_Net_for_Accurate_Robust_Efficient_3D_Perception_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_CRN_Camera_Radar_Net_for_Accurate_Robust_Efficient_3D_Perception_ICCV_2023_paper.pdf)]
    * Title: CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception
    * Year: `2023`
    * Authors: Youngseok Kim, Juyeb Shin, Sanmin Kim, In-Jae Lee, Jun Won Choi, Dongsuk Kum
    * Abstract: Autonomous driving requires an accurate and fast 3D perception system that includes 3D object detection, tracking, and segmentation. Although recent low-cost camera-based approaches have shown promising results, they are susceptible to poor illumination or bad weather conditions and have a large localization error. Hence, fusing camera with low-cost radar, which provides precise long-range measurement and operates reliably in all environments, is promising but has not yet been thoroughly investigated. In this paper, we propose Camera Radar Net (CRN), a novel camera-radar fusion framework that generates a semantically rich and spatially accurate bird's-eye-view (BEV) feature map for various tasks. To overcome the lack of spatial information in an image, we transform perspective view image features to BEV with the help of sparse but accurate radar points. We further aggregate image and radar feature maps in BEV using multi-modal deformable attention designed to tackle the spatial misalignment between inputs. CRN with real-time setting operates at 20 FPS while achieving comparable performance to LiDAR detectors on nuScenes, and even outperforms at a far distance on 100m setting. Moreover, CRN with offline setting yields 62.4% NDS, 57.5% mAP on nuScenes test set and ranks first among all camera and camera-radar 3D object detectors.
count=1
* Misalign, Contrast then Distill: Rethinking Misalignments in Language-Image Pre-training
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Kim_Misalign_Contrast_then_Distill_Rethinking_Misalignments_in_Language-Image_Pre-training_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Misalign_Contrast_then_Distill_Rethinking_Misalignments_in_Language-Image_Pre-training_ICCV_2023_paper.pdf)]
    * Title: Misalign, Contrast then Distill: Rethinking Misalignments in Language-Image Pre-training
    * Year: `2023`
    * Authors: Bumsoo Kim, Yeonsik Jo, Jinhyung Kim, Seunghwan Kim
    * Abstract: Contrastive Language-Image Pretraining has emerged as a prominent approach for training vision and text encoders with uncurated image-text pairs from the web. To enhance data-efficiency, recent efforts have introduced additional supervision terms that involve random-augmented views of the image. However, since the image augmentation process is unaware of its text counterpart, this procedure could cause various degrees of image-text misalignments during training. Prior methods either disregarded this discrepancy or introduced external models to mitigate the impact of misalignments during training. In contrast, we propose a novel metric learning approach that capitalizes on these misalignments as an additional training source, which we term "Misalign, Contrast then Distill (MCD)". Unlike previous methods that treat augmented images and their text counterparts as simple positive pairs, MCD predicts the continuous scales of misalignment caused by the augmentation. Our extensive experimental results show that our proposed MCD achieves state-of-the-art transferability in multiple classification and retrieval downstream datasets.
count=1
* Adaptive Similarity Bootstrapping for Self-Distillation Based Representation Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Lebailly_Adaptive_Similarity_Bootstrapping_for_Self-Distillation_Based_Representation_Learning_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Lebailly_Adaptive_Similarity_Bootstrapping_for_Self-Distillation_Based_Representation_Learning_ICCV_2023_paper.pdf)]
    * Title: Adaptive Similarity Bootstrapping for Self-Distillation Based Representation Learning
    * Year: `2023`
    * Authors: Tim Lebailly, Thomas Stegmüller, Behzad Bozorgtabar, Jean-Philippe Thiran, Tinne Tuytelaars
    * Abstract: Most self-supervised methods for representation learning leverage a cross-view consistency objective i.e., they maximize the representation similarity of a given image's augmented views. Recent work NNCLR goes beyond the cross-view paradigm and uses positive pairs from different images obtained via nearest neighbor bootstrapping in a contrastive setting. We empirically show that as opposed to the contrastive learning setting which relies on negative samples, incorporating nearest neighbor bootstrapping in a self-distillation scheme can lead to a performance drop or even collapse. We scrutinize the reason for this unexpected behavior and provide a solution. We propose to adaptively bootstrap neighbors based on the estimated quality of the latent space. We report consistent improvements compared to the naive bootstrapping approach and the original baselines. Our approach leads to performance improvements for various self-distillation method/backbone combinations and standard downstream tasks. Our code is publicly available at https://github.com/tileb1/AdaSim.
count=1
* Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Mitigating_Adversarial_Vulnerability_through_Causal_Parameter_Estimation_by_Adversarial_Double_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Mitigating_Adversarial_Vulnerability_through_Causal_Parameter_Estimation_by_Adversarial_Double_ICCV_2023_paper.pdf)]
    * Title: Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning
    * Year: `2023`
    * Authors: Byung-Kwan Lee, Junho Kim, Yong Man Ro
    * Abstract: Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision process of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across targets and certain vulnerabilities remain prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods. To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adversarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial perturbations per se and mitigate negative effects that can potentially damage robustness, bridging a causal perspective into the adversarial vulnerability. Through extensive experiments on various CNN and Transformer architectures, we corroborate that ADML improves adversarial robustness with large margins and relieve the empirical observation.
count=1
* Read-only Prompt Optimization for Vision-Language Few-shot Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Read-only_Prompt_Optimization_for_Vision-Language_Few-shot_Learning_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Read-only_Prompt_Optimization_for_Vision-Language_Few-shot_Learning_ICCV_2023_paper.pdf)]
    * Title: Read-only Prompt Optimization for Vision-Language Few-shot Learning
    * Year: `2023`
    * Authors: Dongjun Lee, Seokwon Song, Jihee Suh, Joonmyeong Choi, Sanghyeok Lee, Hyunwoo J. Kim
    * Abstract: In recent years, prompt tuning has proven effective in adapting pre-trained vision-language models to down- stream tasks. These methods aim to adapt the pre-trained models by introducing learnable prompts while keeping pre- trained weights frozen. However, learnable prompts can affect the internal representation within the self-attention module, which may negatively impact performance vari- ance and generalization, especially in data-deficient set- tings. To address these issues, we propose a novel ap- proach, Read-only Prompt Optimization (RPO). RPO lever- ages masked attention to prevent the internal representa- tion shift in the pre-trained model. Further, to facilitate the optimization of RPO, the read-only prompts are ini- tialized based on special tokens of the pre-trained model. Our extensive experiments demonstrate that RPO outper- forms CLIP and CoCoOp in base-to-new generalization and domain generalization while displaying better robust- ness. Also, the proposed method achieves better generaliza- tion on extremely data-deficient settings, while improving parameter efficiency and computational overhead. Code is available at https://github.com/mlvlab/RPO.
count=1
* 2D3D-MATR: 2D-3D Matching Transformer for Detection-Free Registration Between Images and Point Clouds
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Li_2D3D-MATR_2D-3D_Matching_Transformer_for_Detection-Free_Registration_Between_Images_and_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_2D3D-MATR_2D-3D_Matching_Transformer_for_Detection-Free_Registration_Between_Images_and_ICCV_2023_paper.pdf)]
    * Title: 2D3D-MATR: 2D-3D Matching Transformer for Detection-Free Registration Between Images and Point Clouds
    * Year: `2023`
    * Authors: Minhao Li, Zheng Qin, Zhirui Gao, Renjiao Yi, Chenyang Zhu, Yulan Guo, Kai Xu
    * Abstract: The commonly adopted detect-then-match approach to registration finds difficulties in the cross-modality cases due to the incompatible keypoint detection and inconsistent feature description. We propose, 2D3D-MATR, a detection-free method for accurate and robust registration between images and point clouds. Our method adopts a coarse-to-fine pipeline where it first computes coarse correspondences between downsampled patches of the input image and the point cloud and then extends them to form dense correspondences between pixels and points within the patch region. The coarse-level patch matching is based on transformer which jointly learns global contextual constraints with self-attention and cross-modality correlations with cross-attention. To resolve the scale ambiguity in patch matching, we construct a multi-scale pyramid for each image patch and learn to find for each point patch the best matching image patch at a proper resolution level. Extensive experiments on two public benchmarks demonstrate that 2D3D-MATR outperforms the previous state-of-the-art P2-Net by around 20 percentage points on inlier ratio and over 10 points on registration recall. Our code and models will be publicly released.
count=1
* Distilling Large Vision-Language Model with Out-of-Distribution Generalizability
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Li_Distilling_Large_Vision-Language_Model_with_Out-of-Distribution_Generalizability_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Distilling_Large_Vision-Language_Model_with_Out-of-Distribution_Generalizability_ICCV_2023_paper.pdf)]
    * Title: Distilling Large Vision-Language Model with Out-of-Distribution Generalizability
    * Year: `2023`
    * Authors: Xuanlin Li, Yunhao Fang, Minghua Liu, Zhan Ling, Zhuowen Tu, Hao Su
    * Abstract: Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a small- or mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enriching the teacher's language representations with informative and finegrained semantic attributes to effectively distinguish between different labels. We propose several metrics and conduct extensive experiments to investigate their techniques. The results demonstrate significant improvements in zero-shot and few-shot student performance on open-vocabulary out-of-distribution classification, highlighting the effectiveness of our proposed approaches.
count=1
* Exploring Model Transferability through the Lens of Potential Energy
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Li_Exploring_Model_Transferability_through_the_Lens_of_Potential_Energy_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Exploring_Model_Transferability_through_the_Lens_of_Potential_Energy_ICCV_2023_paper.pdf)]
    * Title: Exploring Model Transferability through the Lens of Potential Energy
    * Year: `2023`
    * Authors: Xiaotong Li, Zixuan Hu, Yixiao Ge, Ying Shan, Ling-Yu Duan
    * Abstract: Transfer learning has become crucial in computer vision tasks due to the vast availability of pre-trained deep learning models. However, selecting the optimal pre-trained model from a diverse pool for a specific downstream task remains a challenge. Existing methods for measuring the transferability of pre-trained models rely on statistical correlations between encoded static features and task labels, but they overlook the impact of underlying representation dynamics during fine-tuning, leading to unreliable results, especially for self-supervised models. In this paper, we present an insightful physics-inspired approach named PED to address these challenges. We reframe the challenge of model selection through the lens of potential energy and directly model the interaction forces that influence fine-tuning dynamics. By capturing the motion of dynamic representations to decline the potential energy within a force-driven physical model, we can acquire an enhanced and more stable observation for estimating transferability. The experimental results on 10 downstream tasks and 12 self-supervised models demonstrate that our approach can seamlessly integrate into existing ranking techniques and enhance their performances, revealing its effectiveness for the model selection task and its potential for understanding the mechanism in transfer learning. Code is available at https://github.com/lixiaotong97/PED.
count=1
* Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Li_Gradient-Regulated_Meta-Prompt_Learning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Gradient-Regulated_Meta-Prompt_Learning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf)]
    * Title: Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models
    * Year: `2023`
    * Authors: Juncheng Li, Minghe Gao, Longhui Wei, Siliang Tang, Wenqiao Zhang, Mengze Li, Wei Ji, Qi Tian, Tat-Seng Chua, Yueting Zhuang
    * Abstract: Prompt tuning, a recently emerging paradigm, enables the powerful vision-language pre-training models to adapt to downstream tasks in a parameter- and data- efficient way, by learning the "soft prompts" to condition frozen pre-training models. Though effective, it is particularly problematic in the few-shot scenario, where prompt tuning performance is sensitive to the initialization and requires a time-consuming process to find a good initialization, thus restricting the fast adaptation ability of the pre-training models. In addition, prompt tuning could undermine the generalizability of the pre-training models, because the learnable prompt tokens are easy to overfit to the limited training samples. To address these issues, we introduce a novel Gradient-RegulAted Meta-prompt learning (GRAM) framework that jointly meta-learns an efficient soft prompt initialization for better adaptation and a lightweight gradient regulating function for strong cross-domain generalizability in a meta-learning paradigm using only the unlabeled image-text pre-training data. Rather than designing a specific prompt tuning method, our GRAM can be easily incorporated into various prompt tuning methods in a model-agnostic way, and comprehensive experiments show that GRAM brings about consistent improvement for them in several settings (i.e., few-shot learning, cross-domain generalization, cross-dataset generalization, etc.) over 11 datasets. Further, experiments show that GRAM enables the orthogonal methods of textual and visual prompt tuning to work in a mutually-enhanced way, offering better generalizability beyond the uni-modal prompt tuning methods.
count=1
* Your Diffusion Model is Secretly a Zero-Shot Classifier
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Li_Your_Diffusion_Model_is_Secretly_a_Zero-Shot_Classifier_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Your_Diffusion_Model_is_Secretly_a_Zero-Shot_Classifier_ICCV_2023_paper.pdf)]
    * Title: Your Diffusion Model is Secretly a Zero-Shot Classifier
    * Year: `2023`
    * Authors: Alexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, Deepak Pathak
    * Abstract: The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has stronger multimodal compositional reasoning abilities than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. These models approach the performance of SOTA discriminative classifiers and exhibit strong "effective robustness" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks.
count=1
* MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Lin_MMST-ViT_Climate_Change-aware_Crop_Yield_Prediction_via_Multi-Modal_Spatial-Temporal_Vision_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_MMST-ViT_Climate_Change-aware_Crop_Yield_Prediction_via_Multi-Modal_Spatial-Temporal_Vision_ICCV_2023_paper.pdf)]
    * Title: MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer
    * Year: `2023`
    * Authors: Fudong Lin, Summer Crawford, Kaleb Guillot, Yihe Zhang, Yan Chen, Xu Yuan, Li Chen, Shelby Williams, Robert Minvielle, Xiangming Xiao, Drew Gholson, Nicolas Ashwell, Tri Setiyono, Brenda Tubana, Lu Peng, Magdy Bayoumi, Nian-Feng Tzeng
    * Abstract: Precise crop yield prediction provides valuable information for agricultural planning and decision-making processes. However, timely predicting crop yields remains challenging as crop growth is sensitive to growing season weather variation and climate change. In this work, we develop a deep learning-based solution, namely Multi-Modal Spatial-Temporal Vision Transformer (MMST-ViT), for predicting crop yields at the county level across the United States, by considering the effects of short-term meteorological variations during the growing season and the long-term climate change on crops. Specifically, our MMST-ViT consists of a Multi-Modal Transformer, a Spatial Transformer, and a Temporal Transformer. The Multi-Modal Transformer leverages both visual remote sensing data and short-term meteorological data for modeling the effect of growing season weather variations on crop growth. The Spatial Transformer learns the high-resolution spatial dependency among counties for accurate agricultural tracking. The Temporal Transformer captures the long-range temporal dependency for learning the impact of long-term climate change on crops. Meanwhile, we also devise a novel multi-modal contrastive learning technique to pre-train our model without extensive human supervision. Hence, our MMST-ViT captures the impacts of both short-term weather variations and long-term climate change on crops by leveraging both satellite images and meteorological data. We have conducted extensive experiments on over 200 counties in the United States, with the experimental results exhibiting that our MMST-ViT outperforms its counterparts under three performance metrics of interest.
count=1
* SKiT: a Fast Key Information Video Transformer for Online Surgical Phase Recognition
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Liu_SKiT_a_Fast_Key_Information_Video_Transformer_for_Online_Surgical_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_SKiT_a_Fast_Key_Information_Video_Transformer_for_Online_Surgical_ICCV_2023_paper.pdf)]
    * Title: SKiT: a Fast Key Information Video Transformer for Online Surgical Phase Recognition
    * Year: `2023`
    * Authors: Yang Liu, Jiayu Huo, Jingjing Peng, Rachel Sparks, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin
    * Abstract: This paper introduces SKiT, a fast Key information Transformer for phase recognition of videos. Unlike previous methods that rely on complex models to capture long-term temporal information, SKiT accurately recognizes high-level stages of videos using an efficient key pooling operation. This operation records important key information by retaining the maximum value recorded from the beginning up to the current video frame, with a time complexity of O(1). Experimental results on Cholec80 and AutoLaparo surgical datasets demonstrate the ability of our model to recognize phases in an online manner. SKiT achieves higher performance than state-of-the-art methods with an accuracy of 92.5% and 82.9% on Cholec80 and AutoLaparo, respectively, while running the temporal model eight times faster ( 7ms v.s. 55ms) than LoViT, which uses ProbSparse to capture global information. We highlight that the inference time of SKiT is constant, and independent from the input length, making it a stable choice for keeping a record of important global information, that appears on long surgical videos, essential for phase recognition. To sum up, we propose an effective and efficient model for surgical phase recognition that leverages key global information. This has an intrinsic value when performing this task in an online manner on long surgical videos for stable real-time surgical recognition systems.
count=1
* Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Long_Task-Oriented_Multi-Modal_Mutual_Leaning_for_Vision-Language_Models_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Long_Task-Oriented_Multi-Modal_Mutual_Leaning_for_Vision-Language_Models_ICCV_2023_paper.pdf)]
    * Title: Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models
    * Year: `2023`
    * Authors: Sifan Long, Zhen Zhao, Junkun Yuan, Zichang Tan, Jiangjiang Liu, Luping Zhou, Shengsheng Wang, Jingdong Wang
    * Abstract: Prompt learning has become one of the most efficient paradigms for adapting large pre-trained vision-language models to downstream tasks. Current state-of-the-art methods, like CoOp and ProDA, tend to adopt soft prompts to learn an appropriate prompt for each specific task. Recent CoCoOp further boosts the base-to-new generalization performance via an image-conditional prompt. However, it directly fuses identical image semantics to prompts of different labels and significantly weakens the discrimination among different classes as shown in our experiments. Motivated by this observation, we first propose a class-aware text prompt (CTP) to enrich generated prompts with label-related image information. Unlike CoCoOp, CTP can effectively involve image semantics and avoid introducing extra ambiguities into different prompts. On the other hand, instead of reserving the complete image representations, we propose text-guided feature tuning (TFT) to make the image branch attend to class-related representation. A contrastive loss is employed to align such augmented text and image representations on downstream tasks. In this way, the image-to-text CTP and text-to-image TFT can be mutually promoted to enhance the adaptation of VLMs for downstream tasks. Extensive experiments demonstrate that our method outperforms the existing methods by a significant margin. Especially, compared to CoCoOp, we achieve an average improvement of 4.03% on new classes and 3.19% on harmonic-mean over eleven classification benchmarks.
count=1
* LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Sparse Retrieval
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Luo_LexLIP_Lexicon-Bottlenecked_Language-Image_Pre-Training_for_Large-Scale_Image-Text_Sparse_Retrieval_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_LexLIP_Lexicon-Bottlenecked_Language-Image_Pre-Training_for_Large-Scale_Image-Text_Sparse_Retrieval_ICCV_2023_paper.pdf)]
    * Title: LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Sparse Retrieval
    * Year: `2023`
    * Authors: Ziyang Luo, Pu Zhao, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang
    * Abstract: Image-text retrieval (ITR) aims to retrieve images or texts that match a query originating from the other modality. The conventional dense retrieval paradigm relies on encoding images and texts into dense representations with dual-stream encoders. However, this approach is limited by slow retrieval speeds in large-scale scenarios. To address this issue, we propose a novel sparse retrieval paradigm for ITR that exploits sparse representations in the vocabulary space for images and texts. This paradigm enables us to leverage bag-of-words models and efficient inverted indexes, significantly reducing retrieval latency. A critical gap emerges from representing continuous image data in a sparse vocabulary space. To bridge this gap, we introduce a novel pre-training framework, Lexicon-Bottlenecked Language-Image Pre-Training (LexLIP), that learns importance-aware lexicon representations. By using lexicon-bottlenecked modules between the dual-stream encoders and weakened text decoders, we are able to construct continuous bag-of-words bottlenecks and learn lexicon-importance distributions. Upon pre-training with same-scale data, our LexLIP achieves state-of-the-art performance on two ITR benchmarks, MSCOCO and Flickr30k. Furthermore, in large-scale retrieval scenarios, LexLIP outperforms CLIP with 5.8x faster retrieval speed and 19.1x less index storage memory. Beyond this, LexLIP surpasses CLIP across 8 out of 10 zero-shot image classification tasks.
count=1
* Borrowing Knowledge From Pre-trained Language Model: A New Data-efficient Visual Learning Paradigm
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Ma_Borrowing_Knowledge_From_Pre-trained_Language_Model_A_New_Data-efficient_Visual_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Borrowing_Knowledge_From_Pre-trained_Language_Model_A_New_Data-efficient_Visual_ICCV_2023_paper.pdf)]
    * Title: Borrowing Knowledge From Pre-trained Language Model: A New Data-efficient Visual Learning Paradigm
    * Year: `2023`
    * Authors: Wenxuan Ma, Shuang Li, JinMing Zhang, Chi Harold Liu, Jingxuan Kang, Yulin Wang, Gao Huang
    * Abstract: The development of vision models for real-world applications is hindered by the challenge of annotated data scarcity, which has necessitated the adoption of data-efficient visual learning techniques such as semi-supervised learning. Unfortunately, the prevalent cross-entropy supervision is limited by its focus on category discrimination while disregarding the semantic connection between concepts, which ultimately results in the suboptimal exploitation of scarce labeled data. To address this issue, this paper presents a novel approach that seeks to leverage linguistic knowledge for data-efficient visual learning. The proposed approac, BorLan, Borrows knowledge from off-the-shelf pretrained Language models that are already endowed with rich semantics extracted from large corpora, to compensate the semantic deficiency due to limited annotation in visual training. Specifically, we design a distribution alignment objective, which guides the vision model to learn both semantic-aware and domain-agnostic representations for the task through linguistic knowledge. One significant advantage of this paradigm is its flexibility in combining various visual and linguistic models. Extensive experiments on semi-supervised learning, single domain generalization and few-shot learning validate its effectiveness.
count=1
* Representation Uncertainty in Self-Supervised Learning as Variational Inference
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Nakamura_Representation_Uncertainty_in_Self-Supervised_Learning_as_Variational_Inference_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Nakamura_Representation_Uncertainty_in_Self-Supervised_Learning_as_Variational_Inference_ICCV_2023_paper.pdf)]
    * Title: Representation Uncertainty in Self-Supervised Learning as Variational Inference
    * Year: `2023`
    * Authors: Hiroki Nakamura, Masashi Okada, Tadahiro Taniguchi
    * Abstract: In this study, a novel self-supervised learning (SSL) method is proposed, which considers SSL in terms of variational inference to learn not only representation but also representation uncertainties. SSL is a method of learning representations without labels by maximizing the similarity between image representations of different augmented views of an image. Meanwhile, variational autoencoder (VAE) is an unsupervised representation learning method that trains a probabilistic generative model with variational inference. Both VAE and SSL can learn representations without labels, but their relationship has not been investigated in the past. Herein, the theoretical relationship between SSL and variational inference has been clarified. Furthermore, a novel method, namely variational inference SimSiam (VI-SimSiam), has been proposed. VI-SimSiam can predict the representation uncertainty by interpreting SimSiam with variational inference and defining the latent space distribution. The present experiments qualitatively show that VI-SimSiam could learn uncertainty by comparing input images and predicted uncertainties. Additionally, we described a relationship between estimated uncertainty and classification accuracy.
count=1
* Black Box Few-Shot Adaptation for Vision-Language Models
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.pdf)]
    * Title: Black Box Few-Shot Adaptation for Vision-Language Models
    * Year: `2023`
    * Authors: Yassine Ouali, Adrian Bulat, Brais Matinez, Georgios Tzimiropoulos
    * Abstract: Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple linear approach for V-L re-alignment in the target domain. LFA is initialized from a closed-form solution to a least-squares problem and then it is iteratively updated by minimizing a re-ranking loss. Despite its simplicity, our approach can even surpass soft-prompt learning methods as shown by extensive experiments on 11 image and 2 video datasets.
count=1
* Teaching CLIP to Count to Ten
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Paiss_Teaching_CLIP_to_Count_to_Ten_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Paiss_Teaching_CLIP_to_Count_to_Ten_ICCV_2023_paper.pdf)]
    * Title: Teaching CLIP to Count to Ten
    * Year: `2023`
    * Authors: Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, Tali Dekel
    * Abstract: Large vision-language models, such as CLIP, learn robust representations of text and images, facilitating advances in many downstream tasks, including zero-shot classification and text-to-image generation. However, these models have several well-documented limitations. They fail to encapsulate compositional concepts, such as counting. To the best of our knowledge, this work is the first to extend CLIP to handle object counting. We introduce a simple yet effective method to improve the quantitative understanding of vision-language models, while maintaining their overall performance on common benchmarks. Our method automatically augments image captions to create hard negative samples that differ from the original captions by only the number of objects. For example, an image of three dogs can be contrasted with the negative caption "Six dogs playing in the yard". A dedicated loss encourages discrimination between the correct caption and its negative variant. In addition, we introduce CountBench, a new benchmark for evaluating a model's understanding of object counting, and demonstrate significant improvement over baseline models on this task. Furthermore, we leverage our improved CLIP representations for text-conditioned image generation, and show that our model can produce specific counts of objects more reliably than existing ones.
count=1
* What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Pratt_What_Does_a_Platypus_Look_Like_Generating_Customized_Prompts_for_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Pratt_What_Does_a_Platypus_Look_Like_Generating_Customized_Prompts_for_ICCV_2023_paper.pdf)]
    * Title: What Does a Platypus Look Like? Generating Customized Prompts for Zero-Shot Image Classification
    * Year: `2023`
    * Authors: Sarah Pratt, Ian Covert, Rosanne Liu, Ali Farhadi
    * Abstract: Open-vocabulary models are a promising new paradigm for image classification. Unlike traditional classification models, open-vocabulary models classify among any arbitrary set of categories specified with natural language during inference. This natural language, called "prompts", typically consists of a set of hand-written templates (e.g., "a photo of a ") which are completed with each of the category names. This work introduces a simple method to generate higher accuracy prompts, without relying on any explicit knowledge of the task domain and with far fewer hand-constructed sentences. To achieve this, we combine open-vocabulary models with large language models (LLMs) to create Customized Prompts via Language models (CuPL, pronounced "couple"). In particular, we leverage the knowledge contained in LLMs in order to generate many descriptive sentences that contain important discriminating characteristics of the image categories. This allows the model to place a greater importance on these regions in the image when making predictions. We find that this straightforward and general approach improves accuracy on a range of zero-shot image classification benchmarks, including over one percentage point gain on ImageNet. Finally, this simple baseline requires no additional training and remains completely zero-shot. Code available at https://github.com/sarahpratt/CuPL.
count=1
* Waffling Around for Performance: Visual Classification with Random Words and Broad Concepts
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.pdf)]
    * Title: Waffling Around for Performance: Visual Classification with Random Words and Broad Concepts
    * Year: `2023`
    * Authors: Karsten Roth, Jae Myung Kim, A. Sophia Koepke, Oriol Vinyals, Cordelia Schmid, Zeynep Akata
    * Abstract: The visual classification performance of vision-language models such as CLIP has been shown to benefit from additional semantic knowledge from large language models (LLMs) such as GPT-3. In particular, averaging over LLM-generated class descriptors, e.g. "waffle, which has a round shape", can notably improve generalization performance. In this work, we critically study this behavior and propose WaffleCLIP, a framework for zero-shot visual classification which simply replaces LLM-generated descriptors with random character and word descriptors. Without querying external models, we achieve comparable performance gains on a large number of visual classification tasks. This allows WaffleCLIP to both serve as a low-cost alternative, as well as a sanity check for any future LLM-based vision-language model extensions. We conduct an extensive experimental study on the impact and shortcomings of additional semantics introduced with LLM-generated descriptors, and showcase how - if available - semantic context is better leveraged by querying LLMs for high-level concepts, which we show can be done to jointly resolve potential class name ambiguities. Code is available here: https://github.com/ExplainableML/WaffleCLIP.
count=1
* LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Shi_LoGoPrompt_Synthetic_Text_Images_Can_Be_Good_Visual_Prompts_for_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_LoGoPrompt_Synthetic_Text_Images_Can_Be_Good_Visual_Prompts_for_ICCV_2023_paper.pdf)]
    * Title: LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models
    * Year: `2023`
    * Authors: Cheng Shi, Sibei Yang
    * Abstract: Prompt engineering is a powerful tool used to enhance the performance of pre-trained models on downstream tasks. For example, providing the prompt "Let's think step by step" improved GPT-3's reasoning accuracy to 63% on MutiArith while prompting "a photo of" filled with a class name enables CLIP to achieve 80% zero-shot accuracy on ImageNet. While previous research has explored prompt learning for the visual modality, analyzing what constitutes a good visual prompt specifically for image recognition is limited. In addition, existing visual prompt tuning methods' generalization ability is worse than text-only prompting tuning. This paper explores our key insight: synthetic text images are good visual prompts for vision-language models! To achieve that, we propose our LoGoPrompt, which reformulates the classification objective to the visual prompt selection and addresses the chicken-and-egg challenge of first adding synthetic text images as class-wise visual prompts or predicting the class first. Without any trainable visual prompt parameters, experimental results on 16 datasets demonstrate that our method consistently outperforms state-of-the-art methods in few-shot learning, base-to-new generalization, and domain generalization. The code will be publicly available upon publication.
count=1
* Group Ordering Constraints (GroCo)
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Shvetsova_Learning_by_Sorting_Self-supervised_Learning_with_Group_Ordering_Constraints_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Shvetsova_Learning_by_Sorting_Self-supervised_Learning_with_Group_Ordering_Constraints_ICCV_2023_paper.pdf)]
    * Title: Learning by Sorting: Self-supervised Learning with Group Ordering Constraints
    * Year: `2023`
    * Authors: Nina Shvetsova, Felix Petersen, Anna Kukleva, Bernt Schiele, Hilde Kuehne
    * Abstract: Contrastive learning has become an important tool in learning representations from unlabeled data mainly relying on the idea of minimizing distance between positive data pairs, e.g., views from the same images, and maximizing distance between negative data pairs, e.g., views from different images. This paper proposes a new variation of the contrastive learning objective, Group Ordering Constraints (GroCo), that leverages the idea of sorting the distances of positive and negative pairs and computing the respective loss based on how many positive pairs have a larger distance than the negative pairs, and thus are not ordered correctly. To this end, the GroCo loss is based on differentiable sorting networks, which enable training with sorting supervision by matching a differentiable permutation matrix, which is produced by sorting a given set of scores, to a respective ground truth permutation matrix. Applying this idea to groupwise pre-ordered inputs of multiple positive and negative pairs allows introducing the GroCo loss with implicit emphasis on strong positives and negatives, leading to better optimization of the local neighborhood. We evaluate the proposed formulation on various self-supervised learning benchmarks and show that it not only leads to improved results compared to vanilla contrastive learning but also shows competitive performance to comparable methods in linear probing and outperforms current methods in k-NN performance.
count=1
* The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Singh_The_Effectiveness_of_MAE_Pre-Pretraining_for_Billion-Scale_Pretraining_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Singh_The_Effectiveness_of_MAE_Pre-Pretraining_for_Billion-Scale_Pretraining_ICCV_2023_paper.pdf)]
    * Title: The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining
    * Year: `2023`
    * Authors: Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron Adcock, Armand Joulin, Piotr Dollar, Christoph Feichtenhofer, Ross Girshick, Rohit Girdhar, Ishan Misra
    * Abstract: This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of labels). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video recognition, object detection, low-shot classification and zero-shot recognition. Our largest model achieves new state-of-the-art results on iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on Food-101 (96.2%). Our study reveals that model initialization plays a significant role, even for web-scale pretraining with billions of images.
count=1
* Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Communication-Efficient_Vertical_Federated_Learning_with_Limited_Overlapping_Samples_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Communication-Efficient_Vertical_Federated_Learning_with_Limited_Overlapping_Samples_ICCV_2023_paper.pdf)]
    * Title: Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples
    * Year: `2023`
    * Authors: Jingwei Sun, Ziyue Xu, Dong Yang, Vishwesh Nath, Wenqi Li, Can Zhao, Daguang Xu, Yiran Chen, Holger R. Roth
    * Abstract: Federated learning is a popular collaborative learning approach that enables clients to train a global model without sharing their local data. Vertical federated learning (VFL) deals with scenarios in which the data on clients have different feature spaces but share some overlapping samples. Existing VFL approaches suffer from high communication costs and cannot deal efficiently with limited overlapping samples commonly seen in the real world. We propose a practical vertical federated learning (VFL) framework called one-shot VFL that can solve the communication bottleneck and the problem of limited overlapping samples simultaneously based on semi-supervised learning. We also propose few-shot VFL to improve the accuracy further with just one more communication round between the server and the clients. In our proposed framework, the clients only need to communicate with the server once or only a few times. We evaluate the proposed VFL framework on both image and tabular datasets. Our methods can improve the accuracy by more than 46.5% and reduce the communication cost by more than 330xcompared with state-of-the-art VFL methods when evaluated on CIFAR-10. Our code is publicly available.
count=1
* ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Tang_ElasticViT_Conflict-aware_Supernet_Training_for_Deploying_Fast_Vision_Transformer_on_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_ElasticViT_Conflict-aware_Supernet_Training_for_Deploying_Fast_Vision_Transformer_on_ICCV_2023_paper.pdf)]
    * Title: ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices
    * Year: `2023`
    * Authors: Chen Tang, Li Lyna Zhang, Huiqiang Jiang, Jiahang Xu, Ting Cao, Quanlu Zhang, Yuqing Yang, Zhi Wang, Mao Yang
    * Abstract: Neural Architecture Search (NAS) has shown promising performance in the automatic design of vision transformers (ViT) exceeding 1G FLOPs. However, designing lightweight and low-latency ViT models for diverse mobile devices remains a big challenge. In this work, we propose ElasticViT, a two-stage NAS approach that trains a high-quality ViT supernet over a very large search space for covering a wide range of mobile devices, and then searches an optimal sub-network (subnet) for direct deployment. However, current supernet training methods that rely on uniform sampling suffer from the gradient conflict issue: the sampled subnets can have vastly different model sizes (e.g., 50M vs. 2G FLOPs), leading to different optimization directions and inferior performance. To address this challenge, we propose two novel sampling techniques: complexity-aware sampling and performance-aware sampling. Complexity-aware sampling limits the FLOPs difference among the subnets sampled across adjacent training steps, while covering different-sized subnets in the search space. Performance-aware sampling further selects subnets that have good accuracy, which can reduce gradient conflicts and improve supernet quality. Our discovered models, ElasticViT models, achieve top-1 accuracy from 67.2% to 80.0% on ImageNet from 60M to 800M FLOPs without extra retraining, outperforming all prior CNNs and ViTs in terms of accuracy and latency. Our tiny and small models are also the first ViT models that surpass state-of-the-art CNNs with significantly lower latency on mobile devices. For instance, ElasticViT-S1 runs 2.62x faster than EfficientNet-B0 with 0.1% higher accuracy.
count=1
* SuS-X: Training-Free Name-Only Transfer of Vision-Language Models
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Udandarao_SuS-X_Training-Free_Name-Only_Transfer_of_Vision-Language_Models_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Udandarao_SuS-X_Training-Free_Name-Only_Transfer_of_Vision-Language_Models_ICCV_2023_paper.pdf)]
    * Title: SuS-X: Training-Free Name-Only Transfer of Vision-Language Models
    * Year: `2023`
    * Authors: Vishaal Udandarao, Ankush Gupta, Samuel Albanie
    * Abstract: Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval performance on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to circumvent this need for fine-tuning still require access to images from the target distribution. In this paper, we pursue a different approach and explore the regime of training-free "name-only transfer" in which the only knowledge we possess about downstream tasks comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks--"SuS" and "TIP-X", that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art (SoTA) zero-shot classification results on 19 benchmark datasets. We further show the utility of TIP-X in the training-free few-shot setting, where we again achieve SoTA results over strong training-free baselines.
count=1
* Get the Best of Both Worlds: Improving Accuracy and Transferability by Grassmann Class Representation
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Get_the_Best_of_Both_Worlds_Improving_Accuracy_and_Transferability_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Get_the_Best_of_Both_Worlds_Improving_Accuracy_and_Transferability_ICCV_2023_paper.pdf)]
    * Title: Get the Best of Both Worlds: Improving Accuracy and Transferability by Grassmann Class Representation
    * Year: `2023`
    * Authors: Haoqi Wang, Zhizhong Li, Wayne Zhang
    * Abstract: We generalize the class vectors found in neural networks to linear subspaces (i.e., points in the Grassmann manifold) and show that the Grassmann Class Representation (GCR) enables simultaneous improvement in accuracy and feature transferability. In GCR, each class is a subspace, and the logit is defined as the norm of the projection of a feature onto the class subspace. We integrate Riemannian SGD into deep learning frameworks such that class subspaces in a Grassmannian are jointly optimized with the rest model parameters. Compared to the vector form, the representative capability of subspaces is more powerful. We show that on ImageNet-1K, the top-1 errors of ResNet50-D, ResNeXt50, Swin-T, and Deit3-S are reduced by 5.6%, 4.5%, 3.0%, and 3.5%, respectively. Subspaces also provide freedom for features to vary, and we observed that the intra-class feature variability grows when the subspace dimension increases. Consequently, we found the quality of GCR features is better for downstream tasks. For ResNet50-D, the average linear transfer accuracy across 6 datasets improves from 77.98% to 79.70% compared to the strong baseline of vanilla softmax. For Swin-T, it improves from 81.5% to 83.4% and for Deit3, it improves from 73.8% to 81.4%. With these encouraging results, we believe that more applications could benefit from the Grassmann class representation. Code is released at https://github.com/innerlee/GCR.
count=1
* How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_How_Far_Pre-trained_Models_Are_from_Neural_Collapse_on_the_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_How_Far_Pre-trained_Models_Are_from_Neural_Collapse_on_the_ICCV_2023_paper.pdf)]
    * Title: How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability
    * Year: `2023`
    * Authors: Zijian Wang, Yadan Luo, Liang Zheng, Zi Huang, Mahsa Baktashmotlagh
    * Abstract: This paper focuses on model transferability estimation, i.e., assessing the performance of pre-trained models on a downstream task without performing fine-tuning. Motivated by the neural collapse (NC) that reveals the feature geometry at the terminal stage of training, our method considers the model transferability as how far the target activations obtained by pre-trained models are from their hypothetical state in the terminal phase of the fine-tuned model. We propose a metric that computes this proximity based on three phenomena of NC: within-class variability collapse, simplex encoded label interpolation geometry structure is formed, and the nearest center classifier becomes optimal on training data. Extensive experiments on 11 benchmark datasets demonstrate the effectiveness and efficiency of the proposed method over the existing SOTA approaches. Particularly, our method achieves SOTA transferability estimation accuracy with approximately 10xwall-clock time speed up compared to the existing approaches
count=1
* Improved Visual Fine-tuning with Natural Language Supervision
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Improved_Visual_Fine-tuning_with_Natural_Language_Supervision_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Improved_Visual_Fine-tuning_with_Natural_Language_Supervision_ICCV_2023_paper.pdf)]
    * Title: Improved Visual Fine-tuning with Natural Language Supervision
    * Year: `2023`
    * Authors: Junyang Wang, Yuanhong Xu, Juhua Hu, Ming Yan, Jitao Sang, Qi Qian
    * Abstract: Fine-tuning a visual pre-trained model can leverage the semantic information from large-scale pre-training data and mitigate the over-fitting problem on downstream vision tasks with limited training examples. While the problem of catastrophic forgetting in pre-trained backbone has been extensively studied for fine-tuning, its potential bias from the corresponding pre-training task and data, attracts less attention. In this work, we investigate this problem by demonstrating that the obtained classifier after fine-tuning will be close to that induced by the pre-trained model. To reduce the bias in the classifier effectively, we introduce a reference distribution obtained from a fixed text classifier, which can help regularize the learned vision classifier. The proposed method, Text Supervised fine-tuning (TeS), is evaluated with diverse pre-trained vision models including ResNet and ViT, and text encoders including BERT and CLIP, on 11 downstream tasks. The consistent improvement with a clear margin over distinct scenarios confirms the effectiveness of our proposal. Code is available at https://github.com/idstcv/TeS.
count=1
* Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation for Anomaly Detection
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Unilaterally_Aggregated_Contrastive_Learning_with_Hierarchical_Augmentation_for_Anomaly_Detection_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Unilaterally_Aggregated_Contrastive_Learning_with_Hierarchical_Augmentation_for_Anomaly_Detection_ICCV_2023_paper.pdf)]
    * Title: Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation for Anomaly Detection
    * Year: `2023`
    * Authors: Guodong Wang, Yunhong Wang, Jie Qin, Dongming Zhang, Xiuguo Bao, Di Huang
    * Abstract: Anomaly detection (AD), aiming to find samples that deviate from the training distribution, is essential in safety-critical applications. Though recent self-supervised learning based attempts achieve promising results by creating virtual outliers, their training objectives are less faithful to AD which requires a concentrated inlier distribution as well as a dispersive outlier distribution. In this paper, we propose Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation (UniCon-HA), taking into account both the requirements above. Specifically, we explicitly encourage the concentration of inliers and the dispersion of virtual outliers via supervised and unsupervised contrastive losses, respectively. Considering that standard contrastive data augmentation for generating positive views may induce outliers, we additionally introduce a soft mechanism to re-weight each augmented inlier according to its deviation from the inlier distribution, to ensure a purified concentration. Moreover, to prompt a higher concentration, inspired by curriculum learning, we adopt an easy-to-hard hierarchical augmentation strategy and perform contrastive aggregation at different depths of the network based on the strengths of data augmentation. Our method is evaluated under three AD settings including unlabeled one-class, unlabeled multi-class, and labeled multi-class, demonstrating its consistent superiority over other competitors.
count=1
* Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Why_Is_Prompt_Tuning_for_Vision-Language_Models_Robust_to_Noisy_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Why_Is_Prompt_Tuning_for_Vision-Language_Models_Robust_to_Noisy_ICCV_2023_paper.pdf)]
    * Title: Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?
    * Year: `2023`
    * Authors: Cheng-En Wu, Yu Tian, Haichao Yu, Heng Wang, Pedro Morgado, Yu Hen Hu, Linjie Yang
    * Abstract: Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1. the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2. the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting.
count=1
* Learning Concise and Descriptive Attributes for Visual Recognition
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Yan_Learning_Concise_and_Descriptive_Attributes_for_Visual_Recognition_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Learning_Concise_and_Descriptive_Attributes_for_Visual_Recognition_ICCV_2023_paper.pdf)]
    * Title: Learning Concise and Descriptive Attributes for Visual Recognition
    * Year: `2023`
    * Authors: An Yan, Yu Wang, Yiwu Zhong, Chengyu Dong, Zexue He, Yujie Lu, William Yang Wang, Jingbo Shang, Julian McAuley
    * Abstract: Recent advances in foundation models present new opportunities for interpretable visual recognition -- one can first query Large Language Models (LLMs) to obtain a set of attributes that describe each class, then apply vision-language models to classify images via these attributes. Pioneering work shows that querying thousands of attributes can achieve performance competitive with image features. However, our further investigation on 8 datasets reveals that LLM-generated attributes in a large quantity perform almost the same as random words. This surprising finding suggests that significant noise may be present in these attributes. We hypothesize that there exist subsets of attributes that can maintain the classification performance with much smaller sizes, and propose a novel learning-to-search method to discover those concise sets of attributes. As a result, on the CUB dataset, our method achieves performance close to that of massive LLM-generated attributes (e.g., 10k attributes for CUB), yet using only 32 attributes in total to distinguish 200 bird species. Furthermore, our new paradigm demonstrates several additional benefits: higher interpretability and interactivity for humans, and the ability to summarize knowledge for a recognition task.
count=1
* AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Yang_AIDE_A_Vision-Driven_Multi-View_Multi-Modal_Multi-Tasking_Dataset_for_Assistive_Driving_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_AIDE_A_Vision-Driven_Multi-View_Multi-Modal_Multi-Tasking_Dataset_for_Assistive_Driving_ICCV_2023_paper.pdf)]
    * Title: AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception
    * Year: `2023`
    * Authors: Dingkang Yang, Shuai Huang, Zhi Xu, Zhenpeng Li, Shunli Wang, Mingcheng Li, Yuzheng Wang, Yang Liu, Kun Yang, Zhaoyu Chen, Yan Wang, Jing Liu, Peixuan Zhang, Peng Zhai, Lihua Zhang
    * Abstract: Driver distraction has become a significant cause of severe traffic accidents over the past decade. Despite the growing development of vision-driven driver monitoring systems, the lack of comprehensive perception datasets restricts road safety and traffic security. In this paper, we present an AssIstive Driving pErception dataset (AIDE) that considers context information both inside and outside the vehicle in naturalistic scenarios. AIDE facilitates holistic driver monitoring through three distinctive characteristics, including multi-view settings of driver and scene, multi-modal annotations of face, body, posture, and gesture, and four pragmatic task designs for driving understanding. To thoroughly explore AIDE, we provide experimental benchmarks on three kinds of baseline frameworks via extensive methods. Moreover, two fusion strategies are introduced to give new insights into learning effective multi-stream/modal representations. We also systematically investigate the importance and rationality of the key components in AIDE and benchmarks. The project link is https://github.com/ydk122024/AIDE.
count=1
* ALIP: Adaptive Language-Image Pre-Training with Synthetic Caption
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Yang_ALIP_Adaptive_Language-Image_Pre-Training_with_Synthetic_Caption_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_ALIP_Adaptive_Language-Image_Pre-Training_with_Synthetic_Caption_ICCV_2023_paper.pdf)]
    * Title: ALIP: Adaptive Language-Image Pre-Training with Synthetic Caption
    * Year: `2023`
    * Authors: Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, Tongliang Liu
    * Abstract: Contrastive Language-Image Pre-training (CLIP) has significantly boosted the performance of various vision-language tasks by scaling up the dataset with image-text pairs collected from the web. However, the presence of intrinsic noise and unmatched image-text pairs in web data can potentially affect the performance of representation learning. To address this issue, we first utilize the OFA model to generate synthetic captions that focus on the image content. The generated captions contain complementary information that is beneficial for pre-training. Then, we propose an Adaptive Language-Image Pre-training (ALIP), a bi-path model that integrates supervision from both raw text and synthetic caption. As the core components of ALIP, the Language Consistency Gate (LCG) and Description Consistency Gate (DCG) dynamically adjust the weights of samples and image-text/caption pairs during the training process. Meanwhile, the adaptive contrastive loss can effectively reduce the impact of noise data and enhances the efficiency of pre-training data. We validate ALIP with experiments on different scales of models and pre-training datasets. Experiments results show that ALIP achieves state-of-the-art performance on multiple downstream tasks including zero-shot image-text retrieval and linear probe. To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/ALIP.
count=1
* Class Prior-Free Positive-Unlabeled Learning with Taylor Variational Loss for Hyperspectral Remote Sensing Imagery
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Class_Prior-Free_Positive-Unlabeled_Learning_with_Taylor_Variational_Loss_for_Hyperspectral_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Class_Prior-Free_Positive-Unlabeled_Learning_with_Taylor_Variational_Loss_for_Hyperspectral_ICCV_2023_paper.pdf)]
    * Title: Class Prior-Free Positive-Unlabeled Learning with Taylor Variational Loss for Hyperspectral Remote Sensing Imagery
    * Year: `2023`
    * Authors: Hengwei Zhao, Xinyu Wang, Jingtao Li, Yanfei Zhong
    * Abstract: Positive-unlabeled learning (PU learning) in hyperspectral remote sensing imagery (HSI) is aimed at learning a binary classifier from positive and unlabeled data, which has broad prospects in various earth vision applications. However, when PU learning meets limited labeled HSI, the unlabeled data may dominate the optimization process, which makes the neural networks overfit the unlabeled data. In this paper, a Taylor variational loss is proposed for HSI PU learning, which reduces the weight of the gradient of the unlabeled data by Taylor series expansion to enable the network to find a balance between overfitting and underfitting. In addition, the self-calibrated optimization strategy is designed to stabilize the training process. Experiments on 7 benchmark datasets (21 tasks in total) validate the effectiveness of the proposed method. Code is at: https://github.com/Hengwei-Zhao96/T-HOneCls.
count=1
* Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-Trained Vision-Language Models
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Regularized_Mask_Tuning_Uncovering_Hidden_Knowledge_in_Pre-Trained_Vision-Language_Models_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Regularized_Mask_Tuning_Uncovering_Hidden_Knowledge_in_Pre-Trained_Vision-Language_Models_ICCV_2023_paper.pdf)]
    * Title: Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-Trained Vision-Language Models
    * Year: `2023`
    * Authors: Kecheng Zheng, Wei Wu, Ruili Feng, Kai Zhu, Jiawei Liu, Deli Zhao, Zheng-Jun Zha, Wei Chen, Yujun Shen
    * Abstract: Prompt tuning and adapter tuning have shown great potential in transferring pre-trained vision-language models (VLMs) to various downstream tasks. In this work, we design a new type of tuning method, termed as regularized mask tuning, which masks the network parameters through a learnable selection. Inspired by neural pathways, we argue that the knowledge required by a downstream task already exists in the pre-trained weights but just gets concealed in the upstream pre-training stage. To bring the useful knowledge back into light, we first identify a set of parameters that are important to a given downstream task, then attach a binary mask to each parameter, and finally optimize these masks on the downstream data with the parameters frozen. When updating the mask, we introduce a novel gradient dropout strategy to regularize the parameter selection, in order to prevent the model from forgetting old knowledge and overfitting the downstream data. Experimental results on 11 datasets demonstrate the consistent superiority of our method over previous alternatives. It is noteworthy that we manage to deliver 18.73% performance improvement compared to the zero-shot CLIP via masking an average of only 2.56% parameters. Furthermore, our method is synergistic with most existing parameter-efficient tuning methods and can boost the performance on top of them. Code will be made publicly available.
count=1
* Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.pdf)]
    * Title: Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement
    * Year: `2023`
    * Authors: Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou, Dong Wang, Bin Zhao, Peng Gao
    * Abstract: The popularity of Contrastive Language-Image Pre-training (CLIP) has propelled its application to diverse downstream vision tasks. To improve its capacity on downstream tasks, few-shot learning has become a widely-adopted technique. However, existing methods either exhibit limited performance or suffer from excessive learnable parameters. In this paper, we propose APE, an Adaptive Prior rEfinement method for CLIP's pre-trained knowledge, which achieves superior accuracy with high computational efficiency. Via a prior refinement module, we analyze the inter-class disparity in the downstream data and decouple the domain-specific knowledge from the CLIP-extracted cache model. On top of that, we introduce two model variants, a training-free APE and a training-required APE-T. We explore the trilateral affinities between the test image, prior cache model, and textual representations, and only enable a lightweight category-residual module to be trained. For the average accuracy over 11 benchmarks, both APE and APE-T attain state-of-the-art and respectively outperform the second-best by +1.59% and +1.99% under 16 shots with x30 less learnable parameters.
count=1
* Prompt-aligned Gradient for Prompt Tuning
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Prompt-aligned_Gradient_for_Prompt_Tuning_ICCV_2023_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Prompt-aligned_Gradient_for_Prompt_Tuning_ICCV_2023_paper.pdf)]
    * Title: Prompt-aligned Gradient for Prompt Tuning
    * Year: `2023`
    * Authors: Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, Hanwang Zhang
    * Abstract: Thanks to the large pre-trained vision-language models (VLMs) like CLIP, we can craft a zero-shot classifier by discrete prompt design, e.g., the confidence score of an image being "[CLASS]" can be obtained by using the VLM provided similarity between the image and the prompt sentence "a photo of a [CLASS]". Furthermore, prompting shows great potential for fast adaptation of VLMs to downstream tasks if we fine-tune the soft prompts with few samples. However, we find a common failure that improper fine-tuning or learning with extremely few-shot samples may even under-perform the zero-shot prediction. Existing methods still address this problem by using traditional anti-overfitting techniques such as early stopping and data augmentation, which lack a principled solution specific to prompting. In this paper, we present Prompt-aligned Gradient, dubbed ProGrad to prevent prompt tuning from forgetting the general knowledge learned from VLMs. In particular, ProGrad only updates the prompt whose gradient is aligned (or non-conflicting) to the general knowledge, which is represented as the optimization direction offered by the predefined prompt predictions. Extensive experiments under the few-shot learning, domain generalization, base-to-new generalization and cross-dataset transfer settings demonstrate the stronger few-shot generalization ability of ProGrad over state-of-the-art prompt tuning methods. Codes and theoretical proof are in Appendix.
count=1
* Bias Loss for Mobile Neural Networks
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Abrahamyan_Bias_Loss_for_Mobile_Neural_Networks_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Abrahamyan_Bias_Loss_for_Mobile_Neural_Networks_ICCV_2021_paper.pdf)]
    * Title: Bias Loss for Mobile Neural Networks
    * Year: `2021`
    * Authors: Lusine Abrahamyan, Valentin Ziatchin, Yiming Chen, Nikos Deligiannis
    * Abstract: Compact convolutional neural networks (CNNs) have witnessed exceptional improvements in performance in recent years. However, they still fail to provide the same predictive power as CNNs with a large number of parameters. The diverse and even abundant features captured by the layers is an important characteristic of these successful CNNs. However, differences in this characteristic between large CNNs and their compact counterparts have rarely been investigated. In compact CNNs, due to the limited number of parameters, abundant features are unlikely to be obtained, and feature diversity becomes an essential characteristic. Diverse features present in the activation maps derived from a data point during model inference may indicate the presence of a set of unique descriptors necessary to distinguish between objects of different classes. In contrast, data points with low feature diversity may not provide a sufficient amount of unique descriptors to make a valid prediction; we refer to them as random predictions. Random predictions can negatively impact the optimization process and harm the final performance. This paper proposes addressing the problem raised by random predictions by reshaping the standard cross-entropy to make it biased toward data points with a limited number of unique descriptive features. Our novel Bias Loss focuses the training on a set of valuable data points and prevents the vast number of samples with poor learning features from misleading the optimization process. Furthermore, to show the importance of diversity, we present a family of SkipblockNet models whose architectures are brought to boost the number of unique descriptors in the last layers. Experiments conducted on benchmark datasets demonstrate the superiority of the proposed loss function over the cross-entropy loss. Moreover, our SkipblockNet-M can achieve 1% higher classification accuracy than MobileNetV3 Large with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. The code is available on the link - https://github.com/lusinlu/biasloss_skipblocknet.
count=1
* DRIVE: Deep Reinforced Accident Anticipation With Visual Explanation
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Bao_DRIVE_Deep_Reinforced_Accident_Anticipation_With_Visual_Explanation_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Bao_DRIVE_Deep_Reinforced_Accident_Anticipation_With_Visual_Explanation_ICCV_2021_paper.pdf)]
    * Title: DRIVE: Deep Reinforced Accident Anticipation With Visual Explanation
    * Year: `2021`
    * Authors: Wentao Bao, Qi Yu, Yu Kong
    * Abstract: Traffic accident anticipation aims to accurately and promptly predict the occurrence of a future accident from dashcam videos, which is vital for a safety-guaranteed self-driving system. To encourage an early and accurate decision, existing approaches typically focus on capturing the cues of spatial and temporal context before a future accident occurs. However, their decision-making lacks visual explanation and ignores the dynamic interaction with the environment. In this paper, we propose Deep ReInforced accident anticipation with Visual Explanation, named DRIVE. The method simulates both the bottom-up and top-down visual attention mechanism in a dashcam observation environment so that the decision from the proposed stochastic multi-task agent can be visually explained by attentive regions. Moreover, the proposed dense anticipation reward and sparse fixation reward are effective in training the DRIVE model with our improved reinforcement learning algorithm. Experimental results show that the DRIVE model achieves state-of-the-art performance on multiple real-world traffic accident datasets. Code and pre-trained models are available at https://www.rit.edu/actionlab/drive.
count=1
* Temporally-Coherent Surface Reconstruction via Metric-Consistent Atlases
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Bednarik_Temporally-Coherent_Surface_Reconstruction_via_Metric-Consistent_Atlases_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Bednarik_Temporally-Coherent_Surface_Reconstruction_via_Metric-Consistent_Atlases_ICCV_2021_paper.pdf)]
    * Title: Temporally-Coherent Surface Reconstruction via Metric-Consistent Atlases
    * Year: `2021`
    * Authors: Jan Bednarik, Vladimir G. Kim, Siddhartha Chaudhuri, Shaifali Parashar, Mathieu Salzmann, Pascal Fua, Noam Aigerman
    * Abstract: We propose a method for the unsupervised reconstruction of a temporally-coherent sequence of surfaces from a sequence of time-evolving point clouds, yielding dense, semantically meaningful correspondences between all keyframes. We represent the reconstructed surface as an atlas, using a neural network. Using canonical correspondences defined via the atlas, we encourage the reconstruction to be as isometric as possible across frames, leading to semantically-meaningful reconstruction. Through experiments and comparisons, we empirically show that our method achieves results that exceed that state of the art in the accuracy of correspondences and accuracy of surface reconstruction.
count=1
* On the Limits of Pseudo Ground Truth in Visual Camera Re-Localisation
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Brachmann_On_the_Limits_of_Pseudo_Ground_Truth_in_Visual_Camera_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Brachmann_On_the_Limits_of_Pseudo_Ground_Truth_in_Visual_Camera_ICCV_2021_paper.pdf)]
    * Title: On the Limits of Pseudo Ground Truth in Visual Camera Re-Localisation
    * Year: `2021`
    * Authors: Eric Brachmann, Martin Humenberger, Carsten Rother, Torsten Sattler
    * Abstract: Benchmark datasets that measure camera pose accuracy have driven progress in visual re-localisation research. To obtain poses for thousands of images, it is common to use a reference algorithm to generate pseudo ground truth. Popular choices include Structure-from-Motion (SfM) and Simultaneous-Localisation-and-Mapping (SLAM) using additional sensors like depth cameras if available. Re-localisation benchmarks thus measure how well each method replicates the results of the reference algorithm. This begs the question whether the choice of the reference algorithm favours a certain family of re-localisation methods. This paper analyzes two widely used re-localisation datasets and shows that evaluation outcomes indeed vary with the choice of the reference algorithm. We thus question common beliefs in the re-localisation literature, namely that learning-based scene coordinate regression outperforms classical feature-based methods, and that RGB-D- based methods outperform RGB-based methods. We argue that any claims on ranking re-localisation methods should take the type of the reference algorithm, and the similarity of the methods to the reference algorithm, into account.
count=1
* Adaptive Task-Relational Context (ATRC)
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Bruggemann_Exploring_Relational_Context_for_Multi-Task_Dense_Prediction_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Bruggemann_Exploring_Relational_Context_for_Multi-Task_Dense_Prediction_ICCV_2021_paper.pdf)]
    * Title: Exploring Relational Context for Multi-Task Dense Prediction
    * Year: `2021`
    * Authors: David Brüggemann, Menelaos Kanakis, Anton Obukhov, Stamatios Georgoulis, Luc Van Gool
    * Abstract: The timeline of computer vision research is marked with advances in learning and utilizing efficient contextual representations. Most of them, however, are targeted at improving model performance on a single downstream task. We consider a multi-task environment for dense prediction tasks, represented by a common backbone and independent task-specific heads. Our goal is to find the most efficient way to refine each task prediction by capturing cross-task contexts dependent on tasks' relations. We explore various attention-based contexts, such as global and local, in the multi-task setting and analyze their behavior when applied to refine each task independently. Empirical findings confirm that different source-target task pairs benefit from different context types. To automate the selection process, we propose an Adaptive Task-Relational Context (ATRC) module, which samples the pool of all available contexts for each task pair using neural architecture search and outputs the optimal configuration for deployment. Our method achieves state-of-the-art performance on two important multi-task benchmarks, namely NYUD-v2 and PASCAL-Context. The proposed ATRC has a low computational toll and can be used as a drop-in refinement module for any supervised multi-task architecture.
count=1
* SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_SGPA_Structure-Guided_Prior_Adaptation_for_Category-Level_6D_Object_Pose_Estimation_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_SGPA_Structure-Guided_Prior_Adaptation_for_Category-Level_6D_Object_Pose_Estimation_ICCV_2021_paper.pdf)]
    * Title: SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation
    * Year: `2021`
    * Authors: Kai Chen, Qi Dou
    * Abstract: Category-level 6D object pose estimation aims to predict the position and orientation for unseen objects, which plays a pillar role in many scenarios such as robotics and augmented reality. The significant intra-class variation is the bottleneck challenge in this task yet remains unsolved so far. In this paper, we take advantage of category prior to overcome this problem by innovating a structure-guided prior adaptation scheme to accurately estimate 6D pose for individual objects. Different from existing prior-based methods, given one object and its corresponding category prior, we propose to leverage their structure similarity to dynamically adapt the prior to the observed object. The prior adaptation intrinsically associates the adopted prior with different objects, from which we can accurately reconstruct the 3D canonical model of the specific object for pose estimation. To further enhance the structure characteristic of objects, we extract low-rank structure points from the dense object point cloud, therefore more efficiently incorporating sparse structural information during prior adaptation. Extensive experiments on CAMERA25 and REAL275 benchmarks demonstrate significant performance improvement. Project homepage: https://www.cse.cuhk.edu.hk/ kaichen/projects/sgpa/sgpa.html.
count=1
* Adaptive Confidence Thresholding for Monocular Depth Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Choi_Adaptive_Confidence_Thresholding_for_Monocular_Depth_Estimation_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Choi_Adaptive_Confidence_Thresholding_for_Monocular_Depth_Estimation_ICCV_2021_paper.pdf)]
    * Title: Adaptive Confidence Thresholding for Monocular Depth Estimation
    * Year: `2021`
    * Authors: Hyesong Choi, Hunsang Lee, Sunkyung Kim, Sunok Kim, Seungryong Kim, Kwanghoon Sohn, Dongbo Min
    * Abstract: Self-supervised monocular depth estimation has become an appealing solution to the lack of ground truth labels, but its reconstruction loss often produces over-smoothed results across object boundaries and is incapable of handling occlusion explicitly. In this paper, we propose a new approach to leverage pseudo ground truth depth maps of stereo images generated from self-supervised stereo matching methods. The confidence map of the pseudo ground truth depth map is estimated to mitigate performance degeneration by inaccurate pseudo depth maps. To cope with the prediction error of the confidence map itself, we also leverage the threshold network that learns the threshold dynamically conditioned on the pseudo depth maps. The pseudo depth labels filtered out by the thresholded confidence map are used to supervise the monocular depth network. Furthermore, we propose the probabilistic framework that refines the monocular depth map with the help of its uncertainty map through the pixel-adaptive convolution (PAC) layer. Experimental results demonstrate superior performance to state-of-the-art monocular depth estimation methods. Lastly, we exhibit that the proposed threshold learning can also be used to improve the performance of existing confidence estimation approaches.
count=1
* MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Dang_MSR-GCN_Multi-Scale_Residual_Graph_Convolution_Networks_for_Human_Motion_Prediction_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Dang_MSR-GCN_Multi-Scale_Residual_Graph_Convolution_Networks_for_Human_Motion_Prediction_ICCV_2021_paper.pdf)]
    * Title: MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction
    * Year: `2021`
    * Authors: Lingwei Dang, Yongwei Nie, Chengjiang Long, Qing Zhang, Guiqing Li
    * Abstract: Human motion prediction is a challenging task due to the stochasticity and aperiodicity of future poses. Recently, graph convolutional network has been proven to be very effective to learn dynamic relations among pose joints, which is helpful for pose prediction. On the other hand, one can abstract a human pose recursively to obtain a set of poses at multiple scales. With the increase of the abstraction level, the motion of the pose becomes more stable, which benefits pose prediction too. In this paper, we propose a novel Multi-Scale Residual Graph Convolution Network (MSR-GCN) for human pose prediction task in the manner of end-to-end. The GCNs are used to extract features from fine to coarse scale and then from coarse to fine scale. The extracted features at each scale are then combined and decoded to obtain the residuals between the input and target poses. Intermediate supervisions are imposed on all the predicted poses, which enforces the network to learn more representative features. Our proposed approach is evaluated on two standard benchmark datasets, i.e., the Human3.6M dataset and the CMU Mocap dataset. Experimental results demonstrate that our method outperforms the state-of-the-art approaches. Code and pre-trained models are available at https://github.com/Droliven/MSRGCN.
count=1
* Contrastive Coding for Active Learning Under Class Distribution Mismatch
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Du_Contrastive_Coding_for_Active_Learning_Under_Class_Distribution_Mismatch_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Du_Contrastive_Coding_for_Active_Learning_Under_Class_Distribution_Mismatch_ICCV_2021_paper.pdf)]
    * Title: Contrastive Coding for Active Learning Under Class Distribution Mismatch
    * Year: `2021`
    * Authors: Pan Du, Suyun Zhao, Hui Chen, Shuwen Chai, Hong Chen, Cuiping Li
    * Abstract: Active learning (AL) is successful based on the assumption that labeled and unlabeled data are obtained from the same class distribution. However, its performance deteriorates under class distribution mismatch, wherein the unlabeled data contain many samples out of the class distribution of labeled data. To effectively handle the problems under class distribution mismatch, we propose a contrastive coding based AL framework named CCAL. Unlike the existing AL methods that focus on selecting the most informative samples for annotating, CCAL extracts both semantic and distinctive features by contrastive learning and combines them in a query strategy to choose the most informative unlabeled samples with matched categories. Theoretically, we prove that the AL error of CCAL has a tight upper bound. Experimentally, we evaluate its performance on CIFAR10, CIFAR100, and an artificial cross-dataset that consists of five datasets; consequently, CCAL achieves state-of-the-art performance by a large margin with remarkably lower annotation cost. To the best of our knowledge, CCAL is the first work related to AL for class distribution mismatch.
count=1
* With a Little Help From My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Dwibedi_With_a_Little_Help_From_My_Friends_Nearest-Neighbor_Contrastive_Learning_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Dwibedi_With_a_Little_Help_From_My_Friends_Nearest-Neighbor_Contrastive_Learning_ICCV_2021_paper.pdf)]
    * Title: With a Little Help From My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations
    * Year: `2021`
    * Authors: Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman
    * Abstract: Self-supervised learning algorithms based on instance discrimination train encoders to be invariant to pre-defined transformations of the same instance. While most methods treat different views of the same image as positives for a contrastive loss, we are interested in using positives from other instances in the dataset. Our method, Nearest-Neighbor Contrastive Learning of visual Representations (NNCLR), samples the nearest neighbors from the dataset in the latent space, and treats them as positives. This provides more semantic variations than pre-defined transformations. We find that using the nearest-neighbor as positive in contrastive losses improves performance significantly on ImageNet classification, from 71.7% to 75.6%, outperforming previous state-of-the-art methods. On semi-supervised learning benchmarks we improve performance significantly when only 1% ImageNet labels are available, from 53.8% to 56.5%. On transfer learning benchmarks our method outperforms state-of-the-art methods (including supervised learning with ImageNet) on 8 out of 12 downstream datasets. Furthermore, we demonstrate empirically that our method is less reliant on complex data augmentations. We see a relative reduction of only 2.1% ImageNet Top-1 accuracy when we train using only random crops.
count=1
* Stochastic Partial Swap: Enhanced Model Generalization and Interpretability for Fine-Grained Recognition
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Huang_Stochastic_Partial_Swap_Enhanced_Model_Generalization_and_Interpretability_for_Fine-Grained_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Stochastic_Partial_Swap_Enhanced_Model_Generalization_and_Interpretability_for_Fine-Grained_ICCV_2021_paper.pdf)]
    * Title: Stochastic Partial Swap: Enhanced Model Generalization and Interpretability for Fine-Grained Recognition
    * Year: `2021`
    * Authors: Shaoli Huang, Xinchao Wang, Dacheng Tao
    * Abstract: Learning mid-level representation for fine-grained recognition is easily dominated by a limited number of highly discriminative patterns, degrading its robustness and generalization capability. To this end, we propose a novel Stochastic Partial Swap (SPS) scheme to address this issue. Our method performs element-wise swapping for partial features between samples to inject noise during training. It equips a regularization effect similar to Dropout, which promotes more neurons to represent the concepts. Furthermore, it also exhibits other advantages: 1) suppressing over-activation to some part patterns to improve feature representativeness, and 2) enriching pattern combination and simulating noisy cases to enhance classifier generalization. We verify the effectiveness of our approach through comprehensive experiments across four network backbones and three fine-grained datasets. Moreover, we demonstrate its ability to complement high-level representations, allowing a simple model to achieve performance comparable to the top-performing technologies in fine-grained recognition, indoor scene recognition, and material recognition while improving model interpretability.
count=1
* Egocentric Pose Estimation From Human Vision Span
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Jiang_Egocentric_Pose_Estimation_From_Human_Vision_Span_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_Egocentric_Pose_Estimation_From_Human_Vision_Span_ICCV_2021_paper.pdf)]
    * Title: Egocentric Pose Estimation From Human Vision Span
    * Year: `2021`
    * Authors: Hao Jiang, Vamsi Krishna Ithapu
    * Abstract: Estimating camera wearer's body pose from an egocentric view (egopose) is a vital task in augmented and virtual reality. Existing approaches either use a narrow field of view front facing camera that barely captures the wearer, or an extended head-mounted top-down camera for maximal wearer visibility. In this paper, we tackle the egopose estimation from a more natural human vision span, where camera wearer can be seen in the peripheral view and depending on the head pose the wearer may become invisible or has a limited partial view. This is a realistic visual field for user-centric wearable devices like glasses which have front facing wide angle cameras. Existing solutions are not appropriate for this setting, and so, we propose a novel deep learning system taking advantage of both the dynamic features from camera SLAM and the body shape imagery. We compute 3D head pose, 3D body pose, the figure/ground separation, all at the same time while explicitly enforcing a certain geometric consistency across pose attributes. We further show that this system can be trained robustly with lots of existing mocap data so we do not have to collect and annotate large new datasets. Lastly, our system estimates egopose in real time and on the fly while maintaining high accuracy.
count=1
* DepthInSpace: Exploitation and Fusion of Multiple Video Frames for Structured-Light Depth Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Johari_DepthInSpace_Exploitation_and_Fusion_of_Multiple_Video_Frames_for_Structured-Light_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Johari_DepthInSpace_Exploitation_and_Fusion_of_Multiple_Video_Frames_for_Structured-Light_ICCV_2021_paper.pdf)]
    * Title: DepthInSpace: Exploitation and Fusion of Multiple Video Frames for Structured-Light Depth Estimation
    * Year: `2021`
    * Authors: Mohammad Mahdi Johari, Camilla Carta, François Fleuret
    * Abstract: We present DepthInSpace, a self-supervised deep-learning method for depth estimation using a structured-light camera. The design of this method is motivated by the commercial use case of embedded depth sensors in nowadays smartphones. We first propose to use estimated optical flow from ambient information of multiple video frames as a complementary guide for training a single-frame depth estimation network, helping to preserve edges and reduce over-smoothing issues. Utilizing optical flow, we also propose to fuse the data of multiple video frames to get a more accurate depth map. In particular, fused depth maps are more robust in occluded areas and incur less in flying pixels artifacts. We finally demonstrate that these more precise fused depth maps can be used as self-supervision for fine-tuning a single-frame depth estimation network to improve its performance. Our models' effectiveness is evaluated and compared with state-of-the-art models on both synthetic and our newly introduced real datasets. The implementation code, training procedure, and both synthetic and captured real datasets are available at https://www.idiap.ch/paper/depthinspace.
count=1
* Deep Virtual Markers for Articulated 3D Shapes
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Kim_Deep_Virtual_Markers_for_Articulated_3D_Shapes_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Deep_Virtual_Markers_for_Articulated_3D_Shapes_ICCV_2021_paper.pdf)]
    * Title: Deep Virtual Markers for Articulated 3D Shapes
    * Year: `2021`
    * Authors: Hyomin Kim, Jungeon Kim, Jaewon Kam, Jaesik Park, Seungyong Lee
    * Abstract: We propose deep virtual markers, a framework for estimating dense and accurate positional information for various types of 3D data. We design a concept and construct a framework that maps 3D points of 3D articulated models, like humans, into virtual marker labels. To realize the framework, we adopt a sparse convolutional neural network and classify 3D points of an articulated model into virtual marker labels. We propose to use soft labels for the classifier to learn rich and dense interclass relationships based on geodesic distance. To measure the localization accuracy of the virtual markers, we test FAUST challenge, and our result outperforms the state-of-the-art. We also observe outstanding performance on the generalizability test, unseen data evaluation, and different 3D data types (meshes and depth maps). We show additional applications using the estimated virtual markers, such as non-rigid registration, texture transfer, and realtime dense marker prediction from depth maps.
count=1
* Learning With Memory-Based Virtual Classes for Deep Metric Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Ko_Learning_With_Memory-Based_Virtual_Classes_for_Deep_Metric_Learning_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Ko_Learning_With_Memory-Based_Virtual_Classes_for_Deep_Metric_Learning_ICCV_2021_paper.pdf)]
    * Title: Learning With Memory-Based Virtual Classes for Deep Metric Learning
    * Year: `2021`
    * Authors: Byungsoo Ko, Geonmo Gu, Han-Gyu Kim
    * Abstract: The core of deep metric learning (DML) involves learning visual similarities in high-dimensional embedding space. One of the main challenges is to generalize from seen classes of training data to unseen classes of test data. Recent works have focused on exploiting past embeddings to increase the number of instances for the seen classes. Such methods achieve performance improvement via augmentation, while the strong focus on seen classes still remains. This can be undesirable for DML, where training and test data exhibit entirely different classes. In this work, we present a novel training strategy for DML called MemVir. Unlike previous works, MemVir memorizes both embedding features and class weights to utilize them as additional virtual classes. The exploitation of virtual classes not only utilizes augmented information for training but also alleviates a strong focus on seen classes for better generalization. Moreover, we embed the idea of curriculum learning by slowly adding virtual classes for a gradual increase in learning difficulty, which improves the learning stability as well as the final performance. MemVir can be easily applied to many existing loss functions without any modification. Extensive experimental results on famous benchmarks demonstrate the superiority of MemVir over state-of-the-art competitors. Code of MemVir is publicly available.
count=1
* Mean Shift for Self-Supervised Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Koohpayegani_Mean_Shift_for_Self-Supervised_Learning_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Koohpayegani_Mean_Shift_for_Self-Supervised_Learning_ICCV_2021_paper.pdf)]
    * Title: Mean Shift for Self-Supervised Learning
    * Year: `2021`
    * Authors: Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash
    * Abstract: Most recent self-supervised learning (SSL) algorithms learn features by contrasting between instances of images or by clustering the images and then contrasting between the image clusters. We introduce a simple mean-shift algorithm that learns representations by grouping images together without contrasting between them or adopting much of prior on the structure or number of the clusters. We simply "shift" the embedding of each image to be close to the "mean" of the neighbors of its augmentation. Since the closest neighbor is always another augmentation of the same image, our model will be identical to BYOL when using only one nearest neighbor instead of 5 used in our experiments. Our model achieves 72.4% on ImageNet linear evaluation with ResNet50 at 200 epochs outperforming BYOL. Also, our method outperforms the SOTA by a large margin when using weak augmentations only, facilitating the adoption of SSL for other modalities. Our code is available here: https://github.com/UMBCvision/MSF
count=1
* Improve Unsupervised Pretraining for Few-Label Transfer
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Li_Improve_Unsupervised_Pretraining_for_Few-Label_Transfer_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Improve_Unsupervised_Pretraining_for_Few-Label_Transfer_ICCV_2021_paper.pdf)]
    * Title: Improve Unsupervised Pretraining for Few-Label Transfer
    * Year: `2021`
    * Authors: Suichan Li, Dongdong Chen, Yinpeng Chen, Lu Yuan, Lei Zhang, Qi Chu, Bin Liu, Nenghai Yu
    * Abstract: Unsupervised pretraining has achieved great success and many recently works have shown unsupervised pretraining can achieve comparable or even slightly better transfer performance than supervised pretraining on downstream target datasets. But in this paper, we find this conclusion may not hold when the target dataset has very few labeled samples for finetuning, ie, few-label transfer. We analyze the possible reason from the clustering perspective: 1) The clustering quality of target samples is of great importance to few-label transfer; 2) Though contrastive learning is essentially to learn how to cluster, its clustering quality is still inferior to supervised pretraining due to lack of label supervision. Based on the analysis, we interestingly discover that only involving some unlabeled target domain into the unsupervised pretraining can improve the clustering quality, subsequently reducing the transfer performance gap with supervised pretraining. This finding also motivates us to propose a new progressive few-label transfer algorithm for real applications, which aims to maximize the transfer performance under a limited annotation budget. To support our analysis and proposed method, we conduct extensive experiments on nine different target datasets. Experimental results show our proposed method can significantly boost the few-label transfer performance of unsupervised pretraining.
count=1
* ISD: Self-Supervised Learning by Iterative Similarity Distillation
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Tejankar_ISD_Self-Supervised_Learning_by_Iterative_Similarity_Distillation_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Tejankar_ISD_Self-Supervised_Learning_by_Iterative_Similarity_Distillation_ICCV_2021_paper.pdf)]
    * Title: ISD: Self-Supervised Learning by Iterative Similarity Distillation
    * Year: `2021`
    * Authors: Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Vipin Pillai, Paolo Favaro, Hamed Pirsiavash
    * Abstract: Recently, contrastive learning has achieved great results in self-supervised learning, where the main idea is to pull two augmentations of an image (positive pairs) closer compared to other random images (negative pairs). We argue that not all negative images are equally negative. Hence, we introduce a self-supervised learning algorithm where we use a soft similarity for the negative images rather than a binary distinction between positive and negative pairs. We iteratively distill a slowly evolving teacher model to the student model by capturing the similarity of a query image to some random images and transferring that knowledge to the student. Specifically, our method should handle unbalanced and unlabeled data better than existing contrastive learning methods, because the randomly chosen negative set might include many samples that are semantically similar to the query image. In this case, our method labels them as highly similar while standard contrastive methods label them as negatives. Our method achieves comparable results to the state-of-the-art models. Our code is available here: https://github.com/UMBCvision/ISD
count=1
* Divide and Contrast: Self-Supervised Learning From Uncurated Data
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Tian_Divide_and_Contrast_Self-Supervised_Learning_From_Uncurated_Data_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Tian_Divide_and_Contrast_Self-Supervised_Learning_From_Uncurated_Data_ICCV_2021_paper.pdf)]
    * Title: Divide and Contrast: Self-Supervised Learning From Uncurated Data
    * Year: `2021`
    * Authors: Yonglong Tian, Olivier J. Hénaff, Aäron van den Oord
    * Abstract: Self-supervised learning holds promise in leveraging large amounts of unlabeled data, however much of its progress has thus far been limited to highly curated pre-training data such as ImageNet. We explore the effects of contrastive learning from larger, less-curated image datasets such as YFCC, and find there is indeed a large difference in the resulting representation quality. We hypothesize that this curation gap is due to a shift in the distribution of image classes---which is more diverse and heavy-tailed---resulting in less relevant negative samples to learn from. We test this hypothesis with a new approach, Divide and Contrast (DnC), which alternates between contrastive learning and clustering-based hard negative mining. When pretrained on less curated datasets, DnC greatly improves the performance of self-supervised learning on downstream tasks, while remaining competitive with the current state-of-the-art on curated datasets.
count=1
* Grafit: Learning Fine-Grained Image Representations With Coarse Labels
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Touvron_Grafit_Learning_Fine-Grained_Image_Representations_With_Coarse_Labels_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Touvron_Grafit_Learning_Fine-Grained_Image_Representations_With_Coarse_Labels_ICCV_2021_paper.pdf)]
    * Title: Grafit: Learning Fine-Grained Image Representations With Coarse Labels
    * Year: `2021`
    * Authors: Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord, Hervé Jégou
    * Abstract: This paper tackles the problem of learning a finer representation than the one provided by training labels. This enables fine-grained category retrieval of images in a collection annotated with coarse labels only. Our network is learned with a nearest-neighbor classifier objective, and an instance loss inspired by self-supervised learning. By jointly leveraging the coarse labels and the underlying fine-grained latent space, it significantly improves the accuracy of category-level retrieval methods. Our strategy outperforms all competing methods for retrieving or classifying images at a finer granularity than that available at train time. It also improves the accuracy for transfer learning tasks to fine-grained datasets.
count=1
* Road Anomaly Detection by Partial Image Reconstruction With Segmentation Coupling
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Vojir_Road_Anomaly_Detection_by_Partial_Image_Reconstruction_With_Segmentation_Coupling_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Vojir_Road_Anomaly_Detection_by_Partial_Image_Reconstruction_With_Segmentation_Coupling_ICCV_2021_paper.pdf)]
    * Title: Road Anomaly Detection by Partial Image Reconstruction With Segmentation Coupling
    * Year: `2021`
    * Authors: Tomas Vojir, Tomáš Šipka, Rahaf Aljundi, Nikolay Chumerin, Daniel Olmeda Reino, Jiri Matas
    * Abstract: We present a novel approach to the detection of unknown objects in the context of autonomous driving. The problem is formulated as anomaly detection, since we assume that the unknown stuff or object appearance cannot be learned. To that end, we propose a reconstruction module that can be used with many existing semantic segmentation networks, and that is trained to recognize and reconstruct road (drivable) surface from a small bottleneck. We postulate that poor reconstruction of the road surface is due to areas that are outside of the training distribution, which is a strong indicator of an anomaly. The road structural similarity error is coupled with the semantic segmentation to incorporate information from known classes and produce final per-pixel anomaly scores. The proposed JSR-Net was evaluated on four datasets, Lost-and-found, Road Anomaly, Road Obstacles, and FishyScapes, achieving state-of-art performance on all, reducing the false positives significantly, while typically having the highest average precision for wide range of operation points.
count=1
* Continual Learning for Image-Based Camera Localization
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Continual_Learning_for_Image-Based_Camera_Localization_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Continual_Learning_for_Image-Based_Camera_Localization_ICCV_2021_paper.pdf)]
    * Title: Continual Learning for Image-Based Camera Localization
    * Year: `2021`
    * Authors: Shuzhe Wang, Zakaria Laskar, Iaroslav Melekhov, Xiaotian Li, Juho Kannala
    * Abstract: For several emerging technologies such as augmented reality, autonomous driving and robotics, visual localization is a critical component. Directly regressing camera pose/3D scene coordinates from the input image using deep neural networks has shown great potential. However, such methods assume a stationary data distribution with all scenes simultaneously available during training. In this paper, we approach the problem of visual localization in a continual learning setup -- whereby the model is trained on scenes in an incremental manner. Our results show that similar to the classification domain, non-stationary data induces catastrophic forgetting in deep networks for visual localization. To address this issue, a strong baseline based on storing and replaying images from a fixed buffer is proposed. Furthermore, we propose a new sampling method based on coverage score (Buff-CS) that adapts the existing sampling strategies in the buffering process to the problem of visual localization. Results demonstrate consistent improvements over standard buffering methods on two challenging datasets -- 7Scenes, 12Scenes, and also 19Scenes by combining the former scenes.
count=1
* Generalizable Mixed-Precision Quantization via Attribution Rank Preservation
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Generalizable_Mixed-Precision_Quantization_via_Attribution_Rank_Preservation_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Generalizable_Mixed-Precision_Quantization_via_Attribution_Rank_Preservation_ICCV_2021_paper.pdf)]
    * Title: Generalizable Mixed-Precision Quantization via Attribution Rank Preservation
    * Year: `2021`
    * Authors: Ziwei Wang, Han Xiao, Jiwen Lu, Jie Zhou
    * Abstract: In this paper, we propose a generalizable mixed-precision quantization (GMPQ) method for efficient inference. Conventional methods require the consistency of datasets for bitwidth search and model deployment to guarantee the policy optimality, leading to heavy search cost on challenging largescale datasets in realistic applications. On the contrary, our GMPQ searches the mixed-quantization policy that can be generalized to largescale datasets with only a small amount of data, so that the search cost is significantly reduced without performance degradation. Specifically, we observe that locating network attribution correctly is general ability for accurate visual analysis across different data distribution. Therefore, despite of pursuing higher model accuracy and complexity, we preserve attribution rank consistency between the quantized models and their full-precision counterparts via efficient capacity-aware attribution imitation for generalizable mixed-precision quantization strategy search. Extensive experiments show that our method obtains competitive accuracy-complexity trade-off compared with the state-of-the-art mixed-precision networks in significantly reduced search cost. The code is available at https://github.com/ZiweiWangTHU/GMPQ.git.
count=1
* P2-Net: Joint Description and Detection of Local Features for Pixel and Point Matching
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_P2-Net_Joint_Description_and_Detection_of_Local_Features_for_Pixel_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_P2-Net_Joint_Description_and_Detection_of_Local_Features_for_Pixel_ICCV_2021_paper.pdf)]
    * Title: P2-Net: Joint Description and Detection of Local Features for Pixel and Point Matching
    * Year: `2021`
    * Authors: Bing Wang, Changhao Chen, Zhaopeng Cui, Jie Qin, Chris Xiaoxuan Lu, Zhengdi Yu, Peijun Zhao, Zhen Dong, Fan Zhu, Niki Trigoni, Andrew Markham
    * Abstract: Accurately describing and detecting 2D and 3D keypoints is crucial to establishing correspondences across images and point clouds. Despite a plethora of learning-based 2D or 3D local feature descriptors and detectors having been proposed, the derivation of a shared descriptor and joint keypoint detector that directly matches pixels and points remains under-explored by the community. This work takes the initiative to establish fine-grained correspondences between 2D images and 3D point clouds. In order to directly match pixels and points, a dual fully convolutional framework is presented that maps 2D and 3D inputs into a shared latent representation space to simultaneously describe and detect keypoints. Furthermore, an ultra-wide reception mechanism and a novel loss function are designed to mitigate the intrinsic information variations between pixel and point local regions. Extensive experimental results demonstrate that our framework shows competitive performance in fine-grained matching between images and point clouds and achieves state-of-the-art results for the task of indoor visual localization. Our source code will be available at [no-name-for-blind-review].
count=1
* Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic Segmentation
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Xu_Dynamic_Divide-and-Conquer_Adversarial_Training_for_Robust_Semantic_Segmentation_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Dynamic_Divide-and-Conquer_Adversarial_Training_for_Robust_Semantic_Segmentation_ICCV_2021_paper.pdf)]
    * Title: Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic Segmentation
    * Year: `2021`
    * Authors: Xiaogang Xu, Hengshuang Zhao, Jiaya Jia
    * Abstract: Adversarial training is promising for improving robustness of deep neural networks towards adversarial perturbations, especially on the classification task. The effect of this type of training on semantic segmentation, contrarily, just commences. We make the initial attempt to explore the defense strategy on semantic segmentation by formulating a general adversarial training procedure that can perform decently on both adversarial and clean samples. We propose a dynamic divide-and-conquer adversarial training (DDC-AT) strategy to enhance the defense effect, by setting additional branches in the target model during training, and dealing with pixels with diverse properties towards adversarial perturbation. Our dynamical division mechanism divides pixels into multiple branches automatically. Note all these additional branches can be abandoned during inference and thus leave no extra parameter and computation cost. Extensive experiments with various segmentation models are conducted on PASCAL VOC 2012 and Cityscapes datasets, in which DDC-AT yields satisfying performance under both white- and black-box attack. The code is available at https://github.com/dvlab-research/Robust-Semantic-Segmentation.
count=1
* NASOA: Towards Faster Task-Oriented Online Fine-Tuning With a Zoo of Models
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Xu_NASOA_Towards_Faster_Task-Oriented_Online_Fine-Tuning_With_a_Zoo_of_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_NASOA_Towards_Faster_Task-Oriented_Online_Fine-Tuning_With_a_Zoo_of_ICCV_2021_paper.pdf)]
    * Title: NASOA: Towards Faster Task-Oriented Online Fine-Tuning With a Zoo of Models
    * Year: `2021`
    * Authors: Hang Xu, Ning Kang, Gengwei Zhang, Chuanlong Xie, Xiaodan Liang, Zhenguo Li
    * Abstract: Fine-tuning from pre-trained ImageNet models has been a simple, effective, and popular approach for various computer vision tasks. The common practice of fine-tuning is to adopt a default hyperparameter setting with a fixed pre-trained model, while both of them are not optimized for specific tasks and time constraints. Moreover, in cloud computing or GPU clusters where the tasks arrive sequentially in a stream, faster online fine-tuning is a more desired and realistic strategy for saving money, energy consumption, and CO2 emission. In this paper, we propose a joint Neural Architecture Search and Online Adaption framework named NASOA towards a faster task-oriented fine-tuning upon the request of users. Specifically, NASOA first adopts an offline NAS to identify a group of training-efficient networks to form a pretrained model zoo. We propose a novel joint block and macro level search space to enable a flexible and efficient search. Then, by estimating fine-tuning performance via an adaptive model by accumulating experience from the past tasks, an online schedule generator is proposed to pick up the most suitable model and generate a personalized training regime with respect to each desired task in a one-shot fashion. The resulting model zoo is more training efficient than SOTA NAS models, e.g. 6x faster than RegNetY-16GF, and 1.7x faster than EfficientNetB3. Experiments on multiple datasets also show that NASOA achieves much better fine-tuning results, i.e. improving around 2.1% accuracy than the best performance in RegNet series under various time constraints and tasks; 40x faster compared to the BOHB method.
count=1
* T-AutoML: Automated Machine Learning for Lesion Segmentation Using Transformers in 3D Medical Imaging
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Yang_T-AutoML_Automated_Machine_Learning_for_Lesion_Segmentation_Using_Transformers_in_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_T-AutoML_Automated_Machine_Learning_for_Lesion_Segmentation_Using_Transformers_in_ICCV_2021_paper.pdf)]
    * Title: T-AutoML: Automated Machine Learning for Lesion Segmentation Using Transformers in 3D Medical Imaging
    * Year: `2021`
    * Authors: Dong Yang, Andriy Myronenko, Xiaosong Wang, Ziyue Xu, Holger R. Roth, Daguang Xu
    * Abstract: Lesion segmentation in medical imaging has been an important topic in clinical research. Researchers have proposed various detection and segmentation algorithms to address this task. Recently, deep learning-based approaches have significantly improved the performance over conventional methods. However, most state-of-the-art deep learning methods require the manual design of multiple network components and training strategies. In this paper, we propose a new automated machine learning algorithm, T-AutoML, which not only searches for the best neural architecture, but also finds the best combination of hyper-parameters and data augmentation strategies simultaneously. The proposed method utilizes the modern transformer model, which is introduced to adapt to the dynamic length of the search space embedding and can significantly improve the ability of the search. We validate T-AutoML on several large-scale public lesion segmentation data-sets and achieve state-of-the-art performance.
count=1
* Deep Relational Metric Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content/ICCV2021/html/Zheng_Deep_Relational_Metric_Learning_ICCV_2021_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content/ICCV2021/papers/Zheng_Deep_Relational_Metric_Learning_ICCV_2021_paper.pdf)]
    * Title: Deep Relational Metric Learning
    * Year: `2021`
    * Authors: Wenzhao Zheng, Borui Zhang, Jiwen Lu, Jie Zhou
    * Abstract: This paper presents a deep relational metric learning (DRML) framework for image clustering and retrieval. Most existing deep metric learning methods learn an embedding space with a general objective of increasing interclass distances and decreasing intraclass distances. However, the conventional losses of metric learning usually suppress intraclass variations which might be helpful to identify samples of unseen classes. To address this problem, we propose to adaptively learn an ensemble of features that characterizes an image from different aspects to model both interclass and intraclass distributions. We further employ a relational module to capture the correlations among each feature in the ensemble and construct a graph to represent an image. We then perform relational inference on the graph to integrate the ensemble and obtain a relation-aware embedding to measure the similarities. Extensive experiments on the widely-used CUB-200-2011, Cars196, and Stanford Online Products datasets demonstrate that our framework improves existing deep metric learning methods and achieves very competitive results.
count=1
* Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Brachmann_Neural-Guided_RANSAC_Learning_Where_to_Sample_Model_Hypotheses_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Brachmann_Neural-Guided_RANSAC_Learning_Where_to_Sample_Model_Hypotheses_ICCV_2019_paper.pdf)]
    * Title: Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses
    * Year: `2019`
    * Authors: Eric Brachmann,  Carsten Rother
    * Abstract: We present Neural-Guided RANSAC (NG-RANSAC), an extension to the classic RANSAC algorithm from robust optimization. NG-RANSAC uses prior information to improve model hypothesis search, increasing the chance of finding outlier-free minimal sets. Previous works use heuristic side-information like hand-crafted descriptor distance to guide hypothesis search. In contrast, we learn hypothesis search in a principled fashion that lets us optimize an arbitrary task loss during training, leading to large improvements on classic computer vision tasks. We present two further extensions to NG-RANSAC. Firstly, using the inlier count itself as training signal allows us to train neural guidance in a self-supervised fashion. Secondly, we combine neural guidance with differentiable RANSAC to build neural networks which focus on certain parts of the input data and make the output predictions as good as possible. We evaluate NG-RANSAC on a wide array of computer vision tasks, namely estimation of epipolar geometry, horizon line estimation and camera re-localization. We achieve superior or competitive results compared to state-of-the-art robust estimators, including very recent, learned ones.
count=1
* Learning Joint 2D-3D Representations for Depth Completion
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Learning_Joint_2D-3D_Representations_for_Depth_Completion_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Learning_Joint_2D-3D_Representations_for_Depth_Completion_ICCV_2019_paper.pdf)]
    * Title: Learning Joint 2D-3D Representations for Depth Completion
    * Year: `2019`
    * Authors: Yun Chen,  Bin Yang,  Ming Liang,  Raquel Urtasun
    * Abstract: In this paper, we tackle the problem of depth completion from RGBD data. Towards this goal, we design a simple yet effective neural network block that learns to extract joint 2D and 3D features. Specifically, the block consists of two domain-specific sub-networks that apply 2D convolution on image pixels and continuous convolution on 3D points, with their output features fused in image space. We build the depth completion network simply by stacking the proposed block, which has the advantage of learning hierarchical representations that are fully fused between 2D and 3D spaces at multiple levels. We demonstrate the effectiveness of our approach on the challenging KITTI depth completion benchmark and show that our approach outperforms the state-of-the-art.
count=1
* Learning Spatial Awareness to Improve Crowd Counting
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Learning_Spatial_Awareness_to_Improve_Crowd_Counting_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Learning_Spatial_Awareness_to_Improve_Crowd_Counting_ICCV_2019_paper.pdf)]
    * Title: Learning Spatial Awareness to Improve Crowd Counting
    * Year: `2019`
    * Authors: Zhi-Qi Cheng,  Jun-Xiu Li,  Qi Dai,  Xiao Wu,  Alexander G. Hauptmann
    * Abstract: The aim of crowd counting is to estimate the number of people in images by leveraging the annotation of center positions for pedestrians' heads. Promising progresses have been made with the prevalence of deep Convolutional Neural Networks. Existing methods widely employ the Euclidean distance (i.e., L_2 loss) to optimize the model, which, however, has two main drawbacks: (1) the loss has difficulty in learning the spatial awareness (i.e., the position of head) since it struggles to retain the high-frequency variation in the density map, and (2) the loss is highly sensitive to various noises in crowd counting, such as the zero-mean noise, head size changes, and occlusions. Although the Maximum Excess over SubArrays (MESA) loss has been previously proposed by [??] to address the above issues by finding the rectangular subregion whose predicted density map has the maximum difference from the ground truth, it cannot be solved by gradient descent, thus can hardly be integrated into the deep learning framework. In this paper, we present a novel architecture called SPatial Awareness Network (SPANet) to incorporate spatial context for crowd counting. The Maximum Excess over Pixels (MEP) loss is proposed to achieve this by finding the pixel-level subregion with high discrepancy to the ground truth. To this end, we devise a weakly supervised learning scheme to generate such region with a multi-branch architecture. The proposed framework can be integrated into existing deep crowd counting methods and is end-to-end trainable. Extensive experiments on four challenging benchmarks show that our method can significantly improve the performance of baselines. More remarkably, our approach outperforms the state-of-the-art methods on all benchmark datasets.
count=1
* Boundary-Aware Feature Propagation for Scene Segmentation
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Ding_Boundary-Aware_Feature_Propagation_for_Scene_Segmentation_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_Boundary-Aware_Feature_Propagation_for_Scene_Segmentation_ICCV_2019_paper.pdf)]
    * Title: Boundary-Aware Feature Propagation for Scene Segmentation
    * Year: `2019`
    * Authors: Henghui Ding,  Xudong Jiang,  Ai Qun Liu,  Nadia Magnenat Thalmann,  Gang Wang
    * Abstract: In this work, we address the challenging issue of scene segmentation. To increase the feature similarity of the same object while keeping the feature discrimination of different objects, we explore to propagate information throughout the image under the control of objects' boundaries. To this end, we first propose to learn the boundary as an additional semantic class to enable the network to be aware of the boundary layout. Then, we propose unidirectional acyclic graphs (UAGs) to model the function of undirected cyclic graphs (UCGs), which structurize the image via building graphic pixel-by-pixel connections, in an efficient and effective way. Furthermore, we propose a boundary-aware feature propagation (BFP) module to harvest and propagate the local features within their regions isolated by the learned boundaries in the UAG-structured image. The proposed BFP is capable of splitting the feature propagation into a set of semantic groups via building strong connections among the same segment region but weak connections between different segment regions. Without bells and whistles, our approach achieves new state-of-the-art segmentation performance on three challenging semantic segmentation datasets, i.e., PASCAL-Context, CamVid, and Cityscapes.
count=1
* Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Fan_Understanding_Human_Gaze_Communication_by_Spatio-Temporal_Graph_Reasoning_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Fan_Understanding_Human_Gaze_Communication_by_Spatio-Temporal_Graph_Reasoning_ICCV_2019_paper.pdf)]
    * Title: Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning
    * Year: `2019`
    * Authors: Lifeng Fan,  Wenguan Wang,  Siyuan Huang,  Xinyu Tang,  Song-Chun Zhu
    * Abstract: This paper addresses a new problem of understanding human gaze communication in social videos from both atomic-level and event-level, which is significant for studying human social interactions. To tackle this novel and challenging problem, we contribute a large-scale video dataset, VACATION, which covers diverse daily social scenes and gaze communication behaviors with complete annotations of objects and human faces, human attention, and communication structures and labels in both atomic-level and event-level. Together with VACATION, we propose a spatio-temporal graph neural network to explicitly represent the diverse gaze interactions in the social scenes and to infer atomic-level gaze communication by message passing. We further propose an event network with encoder-decoder structure to predict the event-level gaze communication. Our experiments demonstrate that the proposed model improves various baselines significantly in predicting the atomic-level and event-level gaze communications.
count=1
* Photo-Realistic Monocular Gaze Redirection Using Generative Adversarial Networks
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/He_Photo-Realistic_Monocular_Gaze_Redirection_Using_Generative_Adversarial_Networks_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Photo-Realistic_Monocular_Gaze_Redirection_Using_Generative_Adversarial_Networks_ICCV_2019_paper.pdf)]
    * Title: Photo-Realistic Monocular Gaze Redirection Using Generative Adversarial Networks
    * Year: `2019`
    * Authors: Zhe He,  Adrian Spurr,  Xucong Zhang,  Otmar Hilliges
    * Abstract: Gaze redirection is the task of changing the gaze to a desired direction for a given monocular eye patch image. Many applications such as videoconferencing, films, games, and generation of training data for gaze estimation require redirecting the gaze, without distorting the appearance of the area surrounding the eye and while producing photo-realistic images. Existing methods lack the ability to generate perceptually plausible images. In this work, we present a novel method to alleviate this problem by leveraging generative adversarial training to synthesize an eye image conditioned on a target gaze direction. Our method ensures perceptual similarity and consistency of synthesized images to the real images. Furthermore, a gaze estimation loss is used to control the gaze direction accurately. To attain high-quality images, we incorporate perceptual and cycle consistency losses into our architecture. In extensive evaluations we show that the proposed method outperforms state-of-the-art approaches in terms of both image quality and redirection precision. Finally, we show that generated images can bring significant improvement for the gaze estimation task if used to augment real training data.
count=1
* Learnable Triangulation of Human Pose
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Iskakov_Learnable_Triangulation_of_Human_Pose_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Iskakov_Learnable_Triangulation_of_Human_Pose_ICCV_2019_paper.pdf)]
    * Title: Learnable Triangulation of Human Pose
    * Year: `2019`
    * Authors: Karim Iskakov,  Egor Burkov,  Victor Lempitsky,  Yury Malkov
    * Abstract: We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The first (baseline) solution is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images. The second, more complex, solution is based on volumetric aggregation of 2D feature maps from the 2D backbone followed by refinement via 3D convolutions that produce final 3D joint heatmaps. Crucially, both of the approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multi-view state of the art on the Human3.6M dataset.
count=1
* Gaze360: Physically Unconstrained Gaze Estimation in the Wild
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Kellnhofer_Gaze360_Physically_Unconstrained_Gaze_Estimation_in_the_Wild_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kellnhofer_Gaze360_Physically_Unconstrained_Gaze_Estimation_in_the_Wild_ICCV_2019_paper.pdf)]
    * Title: Gaze360: Physically Unconstrained Gaze Estimation in the Wild
    * Year: `2019`
    * Authors: Petr Kellnhofer,  Adria Recasens,  Simon Stent,  Wojciech Matusik,  Antonio Torralba
    * Abstract: Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale remote gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting. Our dataset and models will be made available at http://gaze360.csail.mit.edu.
count=1
* Bayes-Factor-VAE: Hierarchical Bayesian Deep Auto-Encoder Models for Factor Disentanglement
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Bayes-Factor-VAE_Hierarchical_Bayesian_Deep_Auto-Encoder_Models_for_Factor_Disentanglement_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Bayes-Factor-VAE_Hierarchical_Bayesian_Deep_Auto-Encoder_Models_for_Factor_Disentanglement_ICCV_2019_paper.pdf)]
    * Title: Bayes-Factor-VAE: Hierarchical Bayesian Deep Auto-Encoder Models for Factor Disentanglement
    * Year: `2019`
    * Authors: Minyoung Kim,  Yuting Wang,  Pritish Sahu,  Vladimir Pavlovic
    * Abstract: We propose a family of novel hierarchical Bayesian deep auto-encoder models capable of identifying disentangled factors of variability in data. While many recent attempts at factor disentanglement have focused on sophisticated learning objectives within the VAE framework, their choice of a standard normal as the latent factor prior is both suboptimal and detrimental to performance. Our key observation is that the disentangled latent variables responsible for major sources of variability, the relevant factors, can be more appropriately modeled using long-tail distributions. The typical Gaussian priors are, on the other hand, better suited for modeling of nuisance factors. Motivated by this, we extend the VAE to a hierarchical Bayesian model by introducing hyper-priors on the variances of Gaussian latent priors, mimicking an infinite mixture, while maintaining tractable learning and inference of the traditional VAEs. This analysis signifies the importance of partitioning and treating in a different manner the latent dimensions corresponding to relevant factors and nuisances. Our proposed models, dubbed Bayes-Factor-VAEs, are shown to outperform existing methods both quantitatively and qualitatively in terms of latent disentanglement across several challenging benchmark tasks.
count=1
* Instance-Level Future Motion Estimation in a Single Image Based on Ordinal Regression
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Instance-Level_Future_Motion_Estimation_in_a_Single_Image_Based_on_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Instance-Level_Future_Motion_Estimation_in_a_Single_Image_Based_on_ICCV_2019_paper.pdf)]
    * Title: Instance-Level Future Motion Estimation in a Single Image Based on Ordinal Regression
    * Year: `2019`
    * Authors: Kyung-Rae Kim,  Whan Choi,  Yeong Jun Koh,  Seong-Gyun Jeong,  Chang-Su Kim
    * Abstract: A novel algorithm to estimate instance-level future motion in a single image is proposed in this paper. We first represent the future motion of an instance with its direction, speed, and action classes. Then, we develop a deep neural network that exploits different levels of semantic information to perform the future motion estimation. For effective future motion classification, we adopt ordinal regression. Especially, we develop the cyclic ordinal regression scheme using binary classifiers. Experiments demonstrate that the proposed algorithm provides reliable performance and thus can be used effectively for vision applications, including single and multi object tracking. Furthermore, we release the future motion (FM) dataset, collected from diverse sources and annotated manually, as a benchmark for single-image future motion estimation.
count=1
* Deep Head Pose Estimation Using Synthetic Images and Partial Adversarial Domain Adaption for Continuous Label Spaces
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Kuhnke_Deep_Head_Pose_Estimation_Using_Synthetic_Images_and_Partial_Adversarial_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kuhnke_Deep_Head_Pose_Estimation_Using_Synthetic_Images_and_Partial_Adversarial_ICCV_2019_paper.pdf)]
    * Title: Deep Head Pose Estimation Using Synthetic Images and Partial Adversarial Domain Adaption for Continuous Label Spaces
    * Year: `2019`
    * Authors: Felix Kuhnke,  Jorn Ostermann
    * Abstract: Head pose estimation aims at predicting an accurate pose from an image. Current approaches rely on supervised deep learning, which typically requires large amounts of labeled data. Manual or sensor-based annotations of head poses are prone to errors. A solution is to generate synthetic training data by rendering 3D face models. However, the differences (domain gap) between rendered (source-domain) and real-world (target-domain) images can cause low performance. Advances in visual domain adaptation allow reducing the influence of domain differences using adversarial neural networks, which match the feature spaces between domains by enforcing domain-invariant features. While previous work on visual domain adaptation generally assumes discrete and shared label spaces, these assumptions are both invalid for pose estimation tasks. We are the first to present domain adaptation for head pose estimation with a focus on partially shared and continuous label spaces. More precisely, we adapt the predominant weighting approaches to continuous label spaces by applying a weighted resampling of the source domain during training. To evaluate our approach, we revise and extend existing datasets resulting in a new benchmark for visual domain adaption. Our experiments show that our method improves the accuracy of head pose estimation for real-world images despite using only labels from synthetic images.
count=1
* LAP-Net: Level-Aware Progressive Network for Image Dehazing
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_LAP-Net_Level-Aware_Progressive_Network_for_Image_Dehazing_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_LAP-Net_Level-Aware_Progressive_Network_for_Image_Dehazing_ICCV_2019_paper.pdf)]
    * Title: LAP-Net: Level-Aware Progressive Network for Image Dehazing
    * Year: `2019`
    * Authors: Yunan Li,  Qiguang Miao,  Wanli Ouyang,  Zhenxin Ma,  Huijuan Fang,  Chao Dong,  Yining Quan
    * Abstract: In this paper, we propose a level-aware progressive network (LAP-Net) for single image dehazing. Unlike previous multi-stage algorithms that generally learn in a coarse-to-fine fashion, each stage of LAP-Net learns different levels of haze with different supervision. Then the network can progressively learn the gradually aggravating haze. With this design, each stage can focus on a region with specific haze level and restore clear details. To effectively fuse the results of varying haze levels at different stages, we develop an adaptive integration strategy to yield the final dehazed image. This strategy is achieved by a hierarchical integration scheme, which is in cooperation with the memory network and the domain knowledge of dehazing to highlight the best-restored regions of each stage. Extensive experiments on both real-world images and two dehazing benchmarks validate the effectiveness of our proposed method.
count=1
* GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_GridDehazeNet_Attention-Based_Multi-Scale_Network_for_Image_Dehazing_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_GridDehazeNet_Attention-Based_Multi-Scale_Network_for_Image_Dehazing_ICCV_2019_paper.pdf)]
    * Title: GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing
    * Year: `2019`
    * Authors: Xiaohong Liu,  Yongrui Ma,  Zhihao Shi,  Jun Chen
    * Abstract: We propose an end-to-end trainable Convolutional Neural Network (CNN), named GridDehazeNet, for single image dehazing. The GridDehazeNet consists of three modules: pre-processing, backbone, and post-processing. The trainable pre-processing module can generate learned inputs with better diversity and more pertinent features as compared to those derived inputs produced by hand-selected pre-processing methods. The backbone module implements a novel attention-based multi-scale estimation on a grid network, which can effectively alleviate the bottleneck issue often encountered in the conventional multi-scale approach. The post-processing module helps to reduce the artifacts in the final output. Experimental results indicate that the GridDehazeNet outperforms the state-of-the-arts on both synthetic and real-world images. The proposed hazing method does not rely on the atmosphere scattering model, and we provide an explanation as to why it is not necessarily beneficial to take advantage of the dimension reduction offered by the atmosphere scattering model for image dehazing, even if only the dehazing results on synthetic images are concerned.
count=1
* Bayesian Loss for Crowd Count Estimation With Point Supervision
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Bayesian_Loss_for_Crowd_Count_Estimation_With_Point_Supervision_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Bayesian_Loss_for_Crowd_Count_Estimation_With_Point_Supervision_ICCV_2019_paper.pdf)]
    * Title: Bayesian Loss for Crowd Count Estimation With Point Supervision
    * Year: `2019`
    * Authors: Zhiheng Ma,  Xing Wei,  Xiaopeng Hong,  Yihong Gong
    * Abstract: In crowd counting datasets, each person is annotated by a point, which is usually the center of the head. And the task is to estimate the total count in a crowd scene. Most of the state-of-the-art methods are based on density map estimation, which convert the sparse point annotations into a "ground truth" density map through a Gaussian kernel, and then use it as the learning target to train a density map estimator. However, such a "ground-truth" density map is imperfect due to occlusions, perspective effects, variations in object shapes, etc. On the contrary, we propose Bayesian loss, a novel loss function which constructs a density contribution probability model from the point annotations. Instead of constraining the value at every pixel in the density map, the proposed training loss adopts a more reliable supervision on the count expectation at each annotated point. Without bells and whistles, the loss function makes substantial improvements over the baseline loss on all tested datasets. Moreover, our proposed loss function equipped with a standard backbone network, without using any external detectors or multi-scale architectures, plays favourably against the state of the arts. Our method outperforms previous best approaches by a large margin on the latest and largest UCF-QNRF dataset.
count=1
* Drive&Act: A Multi-Modal Dataset for Fine-Grained Driver Behavior Recognition in Autonomous Vehicles
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Martin_DriveAct_A_Multi-Modal_Dataset_for_Fine-Grained_Driver_Behavior_Recognition_in_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Martin_DriveAct_A_Multi-Modal_Dataset_for_Fine-Grained_Driver_Behavior_Recognition_in_ICCV_2019_paper.pdf)]
    * Title: Drive&Act: A Multi-Modal Dataset for Fine-Grained Driver Behavior Recognition in Autonomous Vehicles
    * Year: `2019`
    * Authors: Manuel Martin,  Alina Roitberg,  Monica Haurilet,  Matthias Horne,  Simon Reiss,  Michael Voit,  Rainer Stiefelhagen
    * Abstract: We introduce the novel domain-specific Drive&Act benchmark for fine-grained categorization of driver behavior. Our dataset features twelve hours and over 9.6 million frames of people engaged in distractive activities during both, manual and automated driving. We capture color, infrared, depth and 3D body pose information from six views and densely label the videos with a hierarchical annotation scheme, resulting in 83 categories. The key challenges of our dataset are: (1) recognition of fine-grained behavior inside the vehicle cabin; (2) multi-modal activity recognition, focusing on diverse data streams; and (3) a cross view recognition benchmark, where a model handles data from an unfamiliar domain, as sensor type and placement in the cabin can change between vehicles. Finally, we provide challenging benchmarks by adopting prominent methods for video- and body pose-based action recognition.
count=1
* Incremental Class Discovery for Semantic Segmentation With RGBD Sensing
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Nakajima_Incremental_Class_Discovery_for_Semantic_Segmentation_With_RGBD_Sensing_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Nakajima_Incremental_Class_Discovery_for_Semantic_Segmentation_With_RGBD_Sensing_ICCV_2019_paper.pdf)]
    * Title: Incremental Class Discovery for Semantic Segmentation With RGBD Sensing
    * Year: `2019`
    * Authors: Yoshikatsu Nakajima,  Byeongkeun Kang,  Hideo Saito,  Kris Kitani
    * Abstract: This work addresses the task of open world semantic segmentation using RGBD sensing to discover new semantic classes over time. Although there are many types of objects in the real-word, current semantic segmentation methods make a closed world assumption and are trained only to segment a limited number of object classes. Towards a more open world approach, we propose a novel method that incrementally learns new classes for image segmentation. The proposed system first segments each RGBD frame using both color and geometric information, and then aggregates that information to build a single segmented dense 3D map of the environment. The segmented 3D map representation is a key component of our approach as it is used to discover new object classes by identifying coherent regions in the 3D map that have no semantic label. The use of coherent region in the 3D map as a primitive element, rather than traditional elements such as surfels or voxels, also significantly reduces the computational complexity and memory use of our method. It thus leads to semi-real-time performance at 10.7 Hz when incrementally updating the dense 3D map at every frame. Through experiments on the NYUDv2 dataset, we demonstrate that the proposed method is able to correctly cluster objects of both known and unseen classes. We also show the quantitative comparison with the state-of-the-art supervised methods, the processing time of each step, and the influences of each component.
count=1
* Few-Shot Adaptive Gaze Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Park_Few-Shot_Adaptive_Gaze_Estimation_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Few-Shot_Adaptive_Gaze_Estimation_ICCV_2019_paper.pdf)]
    * Title: Few-Shot Adaptive Gaze Estimation
    * Year: `2019`
    * Authors: Seonwook Park,  Shalini De Mello,  Pavlo Molchanov,  Umar Iqbal,  Otmar Hilliges,  Jan Kautz
    * Abstract: Inter-personal anatomical differences limit the accuracy of person-independent gaze estimation networks. Yet there is a need to lower gaze errors further to enable applications requiring higher quality. Further gains can be achieved by personalizing gaze networks, ideally with few calibration samples. However, over-parameterized neural networks are not amenable to learning from few examples as they can quickly over-fit. We embrace these challenges and propose a novel framework for Few-shot Adaptive GaZE Estimation (Faze) for learning person-specific gaze networks with very few (<= 9) calibration samples. Faze learns a rotation-aware latent representation of gaze via a disentangling encoder-decoder architecture along with a highly adaptable gaze estimator trained using meta-learning. It is capable of adapting to any new person to yield significant performance gains with as few as 3 samples, yielding state-of-the-art performance of 3.18-deg on GazeCapture, a 19% improvement over prior art. We open-source our code at https://github.com/NVlabs/few_shot_gaze
count=1
* Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Park_Pix2Pose_Pixel-Wise_Coordinate_Regression_of_Objects_for_6D_Pose_Estimation_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Pix2Pose_Pixel-Wise_Coordinate_Regression_of_Objects_for_6D_Pose_Estimation_ICCV_2019_paper.pdf)]
    * Title: Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation
    * Year: `2019`
    * Authors: Kiru Park,  Timothy Patten,  Markus Vincze
    * Abstract: Estimating the 6D pose of objects using only RGB images remains challenging because of problems such as occlusion and symmetries. It is also difficult to construct 3D models with precise texture without expert knowledge or specialized scanning devices. To address these problems, we propose a novel pose estimation method, Pix2Pose, that predicts the 3D coordinates of each object pixel without textured models. An auto-encoder architecture is designed to estimate the 3D coordinates and expected errors per pixel. These pixel-wise predictions are then used in multiple stages to form 2D-3D correspondences to directly compute poses with the PnP algorithm with RANSAC iterations. Our method is robust to occlusion by leveraging recent achievements in generative adversarial training to precisely recover occluded parts. Furthermore, a novel loss function, the transformer loss, is proposed to handle symmetric objects by guiding predictions to the closest symmetric pose. Evaluations on three different benchmark datasets containing symmetric and occluded objects show our method outperforms the state of the art using only RGB images.
count=1
* Distillation-Based Training for Multi-Exit Architectures
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Phuong_Distillation-Based_Training_for_Multi-Exit_Architectures_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Phuong_Distillation-Based_Training_for_Multi-Exit_Architectures_ICCV_2019_paper.pdf)]
    * Title: Distillation-Based Training for Multi-Exit Architectures
    * Year: `2019`
    * Authors: Mary Phuong,  Christoph H. Lampert
    * Abstract: Multi-exit architectures, in which a stack of processing layers is interleaved with early output layers, allow the processing of a test example to stop early and thus save computation time and/or energy. In this work, we propose a new training procedure for multi-exit architectures based on the principle of knowledge distillation. The method encourages early exits to mimic later, more accurate exits, by matching their probability outputs. Experiments on CIFAR100 and ImageNet show that distillation-based training significantly improves the accuracy of early exits while maintaining state-of-the-art accuracy for late ones. The method is particularly beneficial when training data is limited and also allows a straight-forward extension to semi-supervised learning, i.e. make use also of unlabeled data at training time. Moreover, it takes only a few lines to implement and imposes almost no computational overhead at training time, and none at all at test time.
count=1
* Multi-Level Bottom-Top and Top-Bottom Feature Fusion for Crowd Counting
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Sindagi_Multi-Level_Bottom-Top_and_Top-Bottom_Feature_Fusion_for_Crowd_Counting_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Sindagi_Multi-Level_Bottom-Top_and_Top-Bottom_Feature_Fusion_for_Crowd_Counting_ICCV_2019_paper.pdf)]
    * Title: Multi-Level Bottom-Top and Top-Bottom Feature Fusion for Crowd Counting
    * Year: `2019`
    * Authors: Vishwanath A. Sindagi,  Vishal M. Patel
    * Abstract: Crowd counting presents enormous challenges in the form of large variation in scales within images and across the dataset. These issues are further exacerbated in highly congested scenes. Approaches based on straightforward fusion of multi-scale features from a deep network seem to be obvious solutions to this problem. However, these fusion approaches do not yield significant improvements in the case of crowd counting in congested scenes. This is usually due to their limited abilities in effectively combining the multi-scale features for problems like crowd counting. To overcome this, we focus on how to efficiently leverage information present in different layers of the network. Specifically, we present a network that involves: (i) a multi-level bottom-top and top-bottom fusion (MBTTBF) method to combine information from shallower to deeper layers and vice versa at multiple levels, (ii) scale complementary feature extraction blocks (SCFB) involving cross-scale residual functions to explicitly enable flow of complementary features from adjacent conv layers along the fusion paths. Furthermore, in order to increase the effectiveness of the multi-scale fusion, we employ a principled way of generating scale-aware ground-truth density maps for training. Experiments conducted on three datasets that contain highly congested scenes (ShanghaiTech, UCF_CC_50, and UCF-QNRF) demonstrate that the proposed method is able to outperform several recent methods in all the datasets
count=1
* Pushing the Frontiers of Unconstrained Crowd Counting: New Dataset and Benchmark Method
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Sindagi_Pushing_the_Frontiers_of_Unconstrained_Crowd_Counting_New_Dataset_and_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Sindagi_Pushing_the_Frontiers_of_Unconstrained_Crowd_Counting_New_Dataset_and_ICCV_2019_paper.pdf)]
    * Title: Pushing the Frontiers of Unconstrained Crowd Counting: New Dataset and Benchmark Method
    * Year: `2019`
    * Authors: Vishwanath A. Sindagi,  Rajeev Yasarla,  Vishal M. Patel
    * Abstract: In this work, we propose a novel crowd counting network that progressively generates crowd density maps via residual error estimation. The proposed method uses VGG16 as the backbone network and employs density map generated by the final layer as a coarse prediction to refine and generate finer density maps in a progressive fashion using residual learning. Additionally, the residual learning is guided by an uncertainty-based confidence weighting mechanism that permits the flow of only high-confidence residuals in the refinement path. The proposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is evaluated on recent complex datasets, and it achieves significant improvements in errors. Furthermore, we introduce a new large scale unconstrained crowd counting dataset (JHU-CROWD) that is 2.8 larger than the most recent crowd counting datasets in terms of the number of images. It contains 4,250 images with 1.11 million annotations. In comparison to existing datasets, the proposed dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations in addition to many distractor images, making it a very challenging dataset. Additionally, the dataset consists of rich annotations at both image-level and head-level. Several recent methods are evaluated and compared on this dataset.
count=1
* RIO: 3D Object Instance Re-Localization in Changing Indoor Environments
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Wald_RIO_3D_Object_Instance_Re-Localization_in_Changing_Indoor_Environments_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Wald_RIO_3D_Object_Instance_Re-Localization_in_Changing_Indoor_Environments_ICCV_2019_paper.pdf)]
    * Title: RIO: 3D Object Instance Re-Localization in Changing Indoor Environments
    * Year: `2019`
    * Authors: Johanna Wald,  Armen Avetisyan,  Nassir Navab,  Federico Tombari,  Matthias Niessner
    * Abstract: In this work, we introduce the task of 3D object instance re-localization (RIO): given one or multiple objects in an RGB-D scan, we want to estimate their corresponding 6DoF poses in another 3D scan of the same environment taken at a later point in time. We consider RIO a particularly important task in 3D vision since it enables a wide range of practical applications, including AI-assistants or robots that are asked to find a specific object in a 3D scene. To address this problem, we first introduce 3RScan, a novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans. Automatically finding 6DoF object poses leads to a particular challenging feature matching task due to varying partial observations and changes in the surrounding context. To this end, we introduce a new data-driven approach that efficiently finds matching features using a fully-convolutional 3D correspondence network operating on multiple spatial scales. Combined with a 6DoF pose optimization, our method outperforms state-of-the-art baselines on our newly-established benchmark, achieving an accuracy of 30.58%.
count=1
* Cost-Aware Fine-Grained Recognition for IoTs Based on Sequential Fixations
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Cost-Aware_Fine-Grained_Recognition_for_IoTs_Based_on_Sequential_Fixations_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Cost-Aware_Fine-Grained_Recognition_for_IoTs_Based_on_Sequential_Fixations_ICCV_2019_paper.pdf)]
    * Title: Cost-Aware Fine-Grained Recognition for IoTs Based on Sequential Fixations
    * Year: `2019`
    * Authors: Hanxiao Wang,  Venkatesh Saligrama,  Stan Sclaroff,  Vitaly Ablavsky
    * Abstract: We consider the problem of fine-grained classification on an edge camera device that has limited power. The edge device must sparingly interact with the cloud to minimize communication bits to conserve power, and the cloud upon receiving the edge inputs returns a classification label. To deal with fine-grained classification, we adopt the perspective of sequential fixation with a foveated field-of-view to model cloud-edge interactions. We propose a novel deep reinforcement learning-based foveation model, DRIFT, that sequentially generates and recognizes mixed-acuity images. Training of DRIFT requires only image-level category labels and encourages fixations to contain task-relevant information, while maintaining data efficiency. Specifically, we train a foveation actor network with a novel Deep Deterministic Policy Gradient by Conditioned Critic and Coaching(DDPGC3) algorithm. In addition, we propose to shape the reward to provide informative feedback after each fixation to better guide RL training. We demonstrate the effectiveness of DRIFT on this task by evaluating on five fine-grained classification benchmark datasets, and show that the proposed approach achieves state-of-the-art performance with over 3X reduction in transmitted pixels.
count=1
* Recurrent U-Net for Resource-Constrained Segmentation
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Recurrent_U-Net_for_Resource-Constrained_Segmentation_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Recurrent_U-Net_for_Resource-Constrained_Segmentation_ICCV_2019_paper.pdf)]
    * Title: Recurrent U-Net for Resource-Constrained Segmentation
    * Year: `2019`
    * Authors: Wei Wang,  Kaicheng Yu,  Joachim Hugonot,  Pascal Fua,  Mathieu Salzmann
    * Abstract: State-of-the-art segmentation methods rely on very deep networks that are not always easy to train without very large training datasets and tend to be relatively slow to run on standard GPUs. In this paper, we introduce a novel recurrent U-Net architecture that preserves the compactness of the original U-Net, while substantially increasing its performance to the point where it outperforms the state of the art on several benchmarks. We will demonstrate its effectiveness for several tasks, including hand segmentation, retina vessel segmentation, and road segmentation. We also introduce a large-scale dataset for hand segmentation.
count=1
* Incremental Learning Using Conditional Adversarial Networks
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Xiang_Incremental_Learning_Using_Conditional_Adversarial_Networks_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiang_Incremental_Learning_Using_Conditional_Adversarial_Networks_ICCV_2019_paper.pdf)]
    * Title: Incremental Learning Using Conditional Adversarial Networks
    * Year: `2019`
    * Authors: Ye Xiang,  Ying Fu,  Pan Ji,  Hua Huang
    * Abstract: Incremental learning using Deep Neural Networks (DNNs) suffers from catastrophic forgetting. Existing methods mitigate it by either storing old image examples or only updating a few fully connected layers of DNNs, which, however, requires large memory footprints or hurts the plasticity of models. In this paper, we propose a new incremental learning strategy based on conditional adversarial networks. Our new strategy allows us to use memory-efficient statistical information to store old knowledge, and fine-tune both convolutional layers and fully connected layers to consolidate new knowledge. Specifically, we propose a model consisting of three parts, i.e., a base sub-net, a generator, and a discriminator. The base sub-net works as a feature extractor which can be pre-trained on large scale datasets and shared across multiple image recognition tasks. The generator conditioned on labeled embeddings aims to construct pseudo-examples with the same distribution as the old data. The discriminator combines real-examples from new data and pseudo-examples generated from the old data distribution to learn representation for both old and new classes. Through adversarial training of the discriminator and generator, we accomplish the multiple continuous incremental learning. Comparison with the state-of-the-arts on public CIFAR-100 and CUB-200 datasets shows that our method achieves the best accuracies on both old and new classes while requiring relatively less memory storage.
count=1
* STD: Sparse-to-Dense 3D Object Detector for Point Cloud
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_STD_Sparse-to-Dense_3D_Object_Detector_for_Point_Cloud_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_STD_Sparse-to-Dense_3D_Object_Detector_for_Point_Cloud_ICCV_2019_paper.pdf)]
    * Title: STD: Sparse-to-Dense 3D Object Detector for Point Cloud
    * Year: `2019`
    * Authors: Zetong Yang,  Yanan Sun,  Shu Liu,  Xiaoyong Shen,  Jiaya Jia
    * Abstract: We propose a two-stage 3D object detection framework, named sparse-to-dense 3D Object Detector (STD). The first stage is a bottom-up proposal generation network that uses raw point clouds as input to generate accurate proposals by seeding each point with a new spherical anchor. It achieves a higher recall with less computation compared with prior works. Then, PointsPool is applied for proposal feature generation by transforming interior point features from sparse expression to compact representation, which saves even more computation. In box prediction, which is the second stage, we implement a parallel intersection-over-union (IoU) branch to increase awareness of localization accuracy, resulting in further improved performance. We conduct experiments on KITTI dataset, and evaluate our method on 3D object and Bird's Eye View (BEV) detection. Our method outperforms other methods by a large margin, especially on the hard set, with 10+ FPS inference speed.
count=1
* Unsupervised Out-of-Distribution Detection by Maximum Classifier Discrepancy
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Unsupervised_Out-of-Distribution_Detection_by_Maximum_Classifier_Discrepancy_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Unsupervised_Out-of-Distribution_Detection_by_Maximum_Classifier_Discrepancy_ICCV_2019_paper.pdf)]
    * Title: Unsupervised Out-of-Distribution Detection by Maximum Classifier Discrepancy
    * Year: `2019`
    * Authors: Qing Yu,  Kiyoharu Aizawa
    * Abstract: Since deep learning models have been implemented in many commercial applications, it is important to detect out-of-distribution (OOD) inputs correctly to maintain the performance of the models, ensure the quality of the collected data, and prevent the applications from being used for other-than-intended purposes. In this work, we propose a two-head deep convolutional neural network (CNN) and maximize the discrepancy between the two classifiers to detect OOD inputs. We train a two-head CNN consisting of one common feature extractor and two classifiers which have different decision boundaries but can classify in-distribution (ID) samples correctly. Unlike previous methods, we also utilize unlabeled data for unsupervised training and we use these unlabeled data to maximize the discrepancy between the decision boundaries of two classifiers to push OOD samples outside the manifold of the in-distribution (ID) samples, which enables us to detect OOD samples that are far from the support of the ID samples. Overall, our approach significantly outperforms other state-of-the-art methods on several OOD detection benchmarks and two cases of real-world simulation.
count=1
* DPOD: 6D Pose Object Detector and Refiner
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_DPOD_6D_Pose_Object_Detector_and_Refiner_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zakharov_DPOD_6D_Pose_Object_Detector_and_Refiner_ICCV_2019_paper.pdf)]
    * Title: DPOD: 6D Pose Object Detector and Refiner
    * Year: `2019`
    * Authors: Sergey Zakharov,  Ivan Shugurov,  Slobodan Ilic
    * Abstract: In this paper we present a novel deep learning method for 3D object detection and 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose Object Detector), estimates dense multi-class 2D-3D correspondence maps between an input image and available 3D models. Given the correspondences, a 6DoF pose is computed via PnP and RANSAC. An additional RGB pose refinement of the initial pose estimates is performed using a custom deep learning-based refinement scheme. Our results and comparison to a vast number of related works demonstrate that a large number of correspondences is beneficial for obtaining high-quality 6D poses both before and after refinement. Unlike other methods that mainly use real data for training and do not train on synthetic renderings, we perform evaluation on both synthetic and real training data demonstrating superior results before and after refinement when compared to all recent detectors. While being precise, the presented approach is still real-time capable.
count=1
* Deep Multiple-Attribute-Perceived Network for Real-World Texture Recognition
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Deep_Multiple-Attribute-Perceived_Network_for_Real-World_Texture_Recognition_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_Deep_Multiple-Attribute-Perceived_Network_for_Real-World_Texture_Recognition_ICCV_2019_paper.pdf)]
    * Title: Deep Multiple-Attribute-Perceived Network for Real-World Texture Recognition
    * Year: `2019`
    * Authors: Wei Zhai,  Yang Cao,  Jing Zhang,  Zheng-Jun Zha
    * Abstract: Texture recognition is a challenging visual task as multiple perceptual attributes may be perceived from the same texture image when combined with different spatial context. Some recent works building upon Convolutional Neural Network (CNN) incorporate feature encoding with orderless aggregating to provide invariance to spatial layouts. However, these existing methods ignore visual texture attributes, which are important cues for describing the real-world texture images, resulting in incomplete description and inaccurate recognition. To address this problem, we propose a novel deep Multiple-Attribute-Perceived Network (MAP-Net) by progressively learning visual texture attributes in a mutually reinforced manner. Specifically, a multi-branch network architecture is devised, in which cascaded global contexts are learned by introducing similarity constraint at each branch, and leveraged as guidance of spatial feature encoding at next branch through an attribute transfer scheme. To enhance the modeling capability of spatial transformation, a deformable pooling strategy is introduced to augment the spatial sampling with adaptive offsets to the global context, leading to perceive new visual attributes. An attribute fusion module is then introduced to jointly utilize the perceived visual attributes and the abstracted semantic concepts at each branch. Experimental results on the five most challenging texture recognition datasets have demonstrated the superiority of the proposed model against the state-of-the-arts.
count=1
* Cascaded Context Pyramid for Full-Resolution 3D Semantic Scene Completion
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Cascaded_Context_Pyramid_for_Full-Resolution_3D_Semantic_Scene_Completion_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Cascaded_Context_Pyramid_for_Full-Resolution_3D_Semantic_Scene_Completion_ICCV_2019_paper.pdf)]
    * Title: Cascaded Context Pyramid for Full-Resolution 3D Semantic Scene Completion
    * Year: `2019`
    * Authors: Pingping Zhang,  Wei Liu,  Yinjie Lei,  Huchuan Lu,  Xiaoyun Yang
    * Abstract: Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric occupancy and semantic category of a 3D scene. It helps intelligent devices to understand and interact with the surrounding scenes. Due to the high-memory requirement, current methods only produce low-resolution completion predictions, and generally lose the object details. Furthermore, they also ignore the multi-scale spatial contexts, which play a vital role for the 3D inference. To address these issues, in this work we propose a novel deep learning framework, named Cascaded Context Pyramid Network (CCPNet), to jointly infer the occupancy and semantic labels of a volumetric 3D scene from a single depth image. The proposed CCPNet improves the labeling coherence with a cascaded context pyramid. Meanwhile, based on the low-level features, it progressively restores the fine-structures of objects with Guided Residual Refinement (GRR) modules. Our proposed framework has three outstanding advantages: (1) it explicitly models the 3D spatial context for performance improvement; (2) full-resolution 3D volumes are produced with structure-preserving details; (3) light-weight models with low-memory requirements are captured with a good extensibility. Extensive experiments demonstrate that in spite of taking a single-view depth map, our proposed framework can generate high-quality SSC results, and outperforms state-of-the-art approaches on both the synthetic SUNCG and real NYU datasets.
count=1
* Weakly Aligned Cross-Modal Learning for Multispectral Pedestrian Detection
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Weakly_Aligned_Cross-Modal_Learning_for_Multispectral_Pedestrian_Detection_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Weakly_Aligned_Cross-Modal_Learning_for_Multispectral_Pedestrian_Detection_ICCV_2019_paper.pdf)]
    * Title: Weakly Aligned Cross-Modal Learning for Multispectral Pedestrian Detection
    * Year: `2019`
    * Authors: Lu Zhang,  Xiangyu Zhu,  Xiangyu Chen,  Xu Yang,  Zhen Lei,  Zhiyong Liu
    * Abstract: Multispectral pedestrian detection has shown great advantages under poor illumination conditions, since the thermal modality provides complementary information for the color image. However, real multispectral data suffers from the position shift problem, i.e. the color-thermal image pairs are not strictly aligned, making one object has different positions in different modalities. In deep learning based methods, this problem makes it difficult to fuse the feature maps from both modalities and puzzles the CNN training. In this paper, we propose a novel Aligned Region CNN (AR-CNN) to handle the weakly aligned multispectral data in an end-to-end way. Firstly, we design a Region Feature Alignment (RFA) module to capture the position shift and adaptively align the region features of the two modalities. Secondly, we present a new multimodal fusion method, which performs feature re-weighting to select more reliable features and suppress the useless ones. Besides, we propose a novel RoI jitter strategy to improve the robustness to unexpected shift patterns of different devices and system settings. Finally, since our method depends on a new kind of labelling: bounding boxes that match each modality, we manually relabel the KAIST dataset by locating bounding boxes in both modalities and building their relationships, providing a new KAIST-Paired Annotation. Extensive experimental validations on existing datasets are performed, demonstrating the effectiveness and robustness of the proposed method. Code and data are available at https://github.com/luzhang16/AR-CNN.
count=1
* Uncertainty Modeling of Contextual-Connections Between Tracklets for Unconstrained Video-Based Face Recognition
    [[abs-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_Uncertainty_Modeling_of_Contextual-Connections_Between_Tracklets_for_Unconstrained_Video-Based_Face_ICCV_2019_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_Uncertainty_Modeling_of_Contextual-Connections_Between_Tracklets_for_Unconstrained_Video-Based_Face_ICCV_2019_paper.pdf)]
    * Title: Uncertainty Modeling of Contextual-Connections Between Tracklets for Unconstrained Video-Based Face Recognition
    * Year: `2019`
    * Authors: Jingxiao Zheng,  Ruichi Yu,  Jun-Cheng Chen,  Boyu Lu,  Carlos D. Castillo,  Rama Chellappa
    * Abstract: Unconstrained video-based face recognition is a challenging problem due to significant within-video variations caused by pose, occlusion and blur. To tackle this problem, an effective idea is to propagate the identity from high-quality faces to low-quality ones through contextual connections, which are constructed based on context such as body appearance. However, previous methods have often propagated erroneous information due to lack of uncertainty modeling of the noisy contextual connections. In this paper, we propose the Uncertainty-Gated Graph (UGG), which conducts graph-based identity propagation between tracklets, which are represented by nodes in a graph. UGG explicitly models the uncertainty of the contextual connections by adaptively updating the weights of the edge gates according to the identity distributions of the nodes during inference. UGG is a generic graphical model that can be applied at only inference time or with end-to-end training. We demonstrate the effectiveness of UGG with state-of-the-art results in the recently released challenging Cast Search in Movies and IARPA Janus Surveillance Video Benchmark dataset.
count=1
* Look, Listen and Learn
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Arandjelovic_Look_Listen_and_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Arandjelovic_Look_Listen_and_ICCV_2017_paper.pdf)]
    * Title: Look, Listen and Learn
    * Year: `2017`
    * Authors: Relja Arandjelovic, Andrew Zisserman
    * Abstract: We consider the question: what can be learnt by looking at and listening to a large number of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel "Audio-Visual Correspondence" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good visual and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks.
count=1
* Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Changpinyo_Predicting_Visual_Exemplars_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Changpinyo_Predicting_Visual_Exemplars_ICCV_2017_paper.pdf)]
    * Title: Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning
    * Year: `2017`
    * Authors: Soravit Changpinyo, Wei-Lun Chao, Fei Sha
    * Abstract: Leveraging class semantic descriptions and examples of known objects, zero-shot learning makes it possible to train a recognition model for an object class whose examples are not available. In this paper, we propose a novel zero-shot learning model that takes advantage of clustering structures in the semantic embedding space. The key idea is to impose the structural constraint that semantic representations must be predictive of the locations of their corresponding visual exemplars. To this end, this reduces to training multiple kernel-based regressors from semantic representation-exemplar pairs from labeled data of the seen object categories. Despite its simplicity, our approach significantly outperforms existing zero-shot learning methods in three out of four benchmark datasets, including the ImageNet dataset with more than 20,000 unseen categories.
count=1
* Learning Hand Articulations by Hallucinating Heat Distribution
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Choi_Learning_Hand_Articulations_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Choi_Learning_Hand_Articulations_ICCV_2017_paper.pdf)]
    * Title: Learning Hand Articulations by Hallucinating Heat Distribution
    * Year: `2017`
    * Authors: Chiho Choi, Sangpil Kim, Karthik Ramani
    * Abstract: We propose a robust hand pose estimation method by learning hand articulations from depth features and auxiliary modality features. As an additional modality to depth data, we present a function of geometric properties on the surface of the hand described by heat diffusion. The proposed heat distribution descriptor is robust to identify the keypoints on the surface as it incorporates both the local geometry of the hand and global structural representation at multiple time scales. Along this line, we train our heat distribution network to learn the geometrically descriptive representations from the proposed descriptors with the fingertip position labels. Then the hallucination network is guided to mimic the intermediate responses of the heat distribution modality from a paired depth image. We use the resulting geometrically informed responses together with the discriminative depth features estimated from the depth network to regularize the angle parameters in the refinement network. To this end, we conduct extensive evaluations to validate that the proposed framework is powerful as it achieves state-of-the-art performance.
count=1
* Robust Hand Pose Estimation During the Interaction With an Unknown Object
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Choi_Robust_Hand_Pose_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Choi_Robust_Hand_Pose_ICCV_2017_paper.pdf)]
    * Title: Robust Hand Pose Estimation During the Interaction With an Unknown Object
    * Year: `2017`
    * Authors: Chiho Choi, Sang Ho Yoon, Chin-Ning Chen, Karthik Ramani
    * Abstract: This paper proposes a robust solution for accurate 3D hand pose estimation in the presence of an external object interacting with hands. Our main insight is that the shape of an object causes a configuration of the hand in the form of a hand grasp. Along this line, we simultaneously train deep neural networks using paired depth images. The object-oriented network learns functional grasps from an object perspective, whereas the hand-oriented network explores the details of hand configurations from a hand perspective. The two networks share intermediate observations produced from different perspectives to create a more informed representation. Our system then collaboratively classifies the grasp types and orientation of the hand and further constrains a pose space using these estimates. Finally, we collectively refine the unknown pose parameters to reconstruct the final hand pose. To this end, we conduct extensive evaluations to validate the efficacy of the proposed collaborative learning approach by comparing it with self-generated baselines and the state-of-the-art method.
count=1
* Long Short-Term Memory Kalman Filters: Recurrent Neural Estimators for Pose Regularization
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Coskun_Long_Short-Term_Memory_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Coskun_Long_Short-Term_Memory_ICCV_2017_paper.pdf)]
    * Title: Long Short-Term Memory Kalman Filters: Recurrent Neural Estimators for Pose Regularization
    * Year: `2017`
    * Authors: Huseyin Coskun, Felix Achilles, Robert DiPietro, Nassir Navab, Federico Tombari
    * Abstract: One-shot pose estimation for tasks such as body joint localization, camera pose estimation, and object tracking are generally noisy, and temporal filters have been extensively used for regularization. One of the most widely-used methods is the Kalman filter, which is both extremely simple and general. However, Kalman filters require a motion model and measurement model to be specified a priori, which burdens the modeler and simultaneously demands that we use explicit models that are often only crude approximations of reality. For example, in the pose-estimation tasks mentioned above, it is common to use motion models that assume constant velocity or constant acceleration, and we believe that these simplified representations are severely inhibitive. In this work, we propose to instead learn rich, dynamic representations of the motion and noise models. In particular, we propose learning these models from data using long short-term memory, which allows representations that depend on all previous observations and all previous states. We evaluate our method using three of the most popular pose estimation tasks in computer vision, and in all cases we obtain state-of-the-art performance.
count=1
* Attributes2Classname: A Discriminative Model for Attribute-Based Unsupervised Zero-Shot Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Demirel_Attributes2Classname_A_Discriminative_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Demirel_Attributes2Classname_A_Discriminative_ICCV_2017_paper.pdf)]
    * Title: Attributes2Classname: A Discriminative Model for Attribute-Based Unsupervised Zero-Shot Learning
    * Year: `2017`
    * Authors: Berkan Demirel, Ramazan Gokberk Cinbis, Nazli Ikizler-Cinbis
    * Abstract: We propose a novel approach for unsupervised zero-shot learning (ZSL) of classes based on their names. Most existing unsupervised ZSL methods aim to learn a model for directly comparing image features and class names. However, this proves to be a difficult task due to dominance of non-visual semantics in underlying vector-space embeddings of class names. To address this issue, we discriminatively learn a word representation such that the similarities between class and combination of attribute names fall in line with the visual similarity. Contrary to the traditional zero-shot learning approaches that are built upon attribute presence, our approach bypasses the laborious attribute-class relation annotations for unseen classes. In addition, our proposed approach renders text-only training possible, hence, the training can be augmented without the need to collect additional image data. The experimental results show that our method yields state-of-the-art results for unsupervised ZSL in three benchmark datasets.
count=1
* Class Rectification Hard Mining for Imbalanced Deep Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Dong_Class_Rectification_Hard_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Dong_Class_Rectification_Hard_ICCV_2017_paper.pdf)]
    * Title: Class Rectification Hard Mining for Imbalanced Deep Learning
    * Year: `2017`
    * Authors: Qi Dong, Shaogang Gong, Xiatian Zhu
    * Abstract: Recognising detailed facial or clothing attributes in images of people is a challenging task for computer vision, especially when the training data are both in very large scale and extremely imbalanced among different attribute classes. To address this problem, we formulate a novel scheme for batch incremental hard sample mining of minority attribute classes from imbalanced large scale training data. We develop an end-to-end deep learning framework capable of avoiding the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes. This is made possible by introducing a Class Rectification Loss (CRL) regularising algorithm. We demonstrate the advantages and scalability of CRL over existing state-of-the-art attribute recognition and imbalanced data learning models on two large scale imbalanced benchmark datasets, the CelebA facial attribute dataset and the X-Domain clothing attribute dataset.
count=1
* RMPE: Regional Multi-Person Pose Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Fang_RMPE_Regional_Multi-Person_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Fang_RMPE_Regional_Multi-Person_ICCV_2017_paper.pdf)]
    * Title: RMPE: Regional Multi-Person Pose Estimation
    * Year: `2017`
    * Authors: Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, Cewu Lu
    * Abstract: Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve 76.7 mAP on the MPII (multi person) dataset. Our model and source codes are made publicly available.
count=1
* Beyond Planar Symmetry: Modeling Human Perception of Reflection and Rotation Symmetries in the Wild
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Funk_Beyond_Planar_Symmetry_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Funk_Beyond_Planar_Symmetry_ICCV_2017_paper.pdf)]
    * Title: Beyond Planar Symmetry: Modeling Human Perception of Reflection and Rotation Symmetries in the Wild
    * Year: `2017`
    * Authors: Christopher Funk, Yanxi Liu
    * Abstract: Humans take advantage of real world symmetries for various tasks, yet capturing their superb symmetry perception mechanism with a computational model remains elusive. Motivated by a new study demonstrating the extremely high inter-person accuracy of human perceived symmetries in the wild, we have constructed the first deep-learning neural network for reflection and rotation symmetry detection (Sym-NET), trained on photos from MS-COCO (Microsoft-Common Object in COntext) dataset with nearly 11K consistent symmetry-labels from more than 400 human observers. We employ novel methods to convert discrete human labels into symmetry heatmaps, capture symmetry densely in an image and quantitatively evaluate Sym-NET against multiple existing computer vision algorithms. On CVPR 2013 symmetry competition testsets and unseen MS-COCO photos, Sym-NET significantly outperforms all other competitors. Beyond mathematically well-defined symmetries on a plane, Sym-NET demonstrates abilities to identify viewpoint-varied 3D symmetries, partially occluded symmetrical objects, and symmetries at a semantic level.
count=1
* Semantic Video CNNs Through Representation Warping
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Gadde_Semantic_Video_CNNs_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Gadde_Semantic_Video_CNNs_ICCV_2017_paper.pdf)]
    * Title: Semantic Video CNNs Through Representation Warping
    * Year: `2017`
    * Authors: Raghudeep Gadde, Varun Jampani, Peter V. Gehler
    * Abstract: In this work, we propose a technique to convert CNN models for semantic segmentation of static images into CNNs for video data. We describe a warping method that can be used to augment existing architectures with very little extra computational cost. This module is called NetWarp and we demonstrate its use for a range of network architectures. The main design principle is to use optical flow of adjacent frames for warping internal network representations across time. A key insight of this work is that fast optical flow methods can be combined with many different CNN architectures for improved performance and end-to-end training. Experiments validate that the proposed approach incurs only little extra computational cost, while improving performance, when video streams are available. We achieve new state-of-the-art results on the CamVid and Cityscapes benchmark datasets and show consistent improvements over different baseline networks. Our code and models are available at http://segmentation.is.tue.mpg.de
count=1
* Learning Discriminative Latent Attributes for Zero-Shot Classification
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Jiang_Learning_Discriminative_Latent_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Jiang_Learning_Discriminative_Latent_ICCV_2017_paper.pdf)]
    * Title: Learning Discriminative Latent Attributes for Zero-Shot Classification
    * Year: `2017`
    * Authors: Huajie Jiang, Ruiping Wang, Shiguang Shan, Yi Yang, Xilin Chen
    * Abstract: Zero-shot learning (ZSL) aims to transfer knowledge from observed classes to the unseen classes, based on the assumption that both the seen and unseen classes share a common semantic space, among which attributes enjoy a great popularity. However, few works study whether the human-designed semantic attributes are discriminative enough to recognize different classes. Moreover, attributes are often correlated with each other, which makes it less desirable to learn each attribute independently. In this paper, we propose to learn a latent attribute space, which is not only discriminative but also semantic-preserving, to perform the ZSL task. Specifically, a dictionary learning framework is exploited to connect the latent attribute space with attribute space and similarity space. Extensive experiments on four benchmark datasets show the effectiveness of the proposed approach.
count=1
* End-To-End Learning of Geometry and Context for Deep Stereo Regression
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Kendall_End-To-End_Learning_of_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Kendall_End-To-End_Learning_of_ICCV_2017_paper.pdf)]
    * Title: End-To-End Learning of Geometry and Context for Deep Stereo Regression
    * Year: `2017`
    * Authors: Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, Adam Bry
    * Abstract: We propose a novel deep learning architecture for regressing disparity from a rectified pair of stereo images. We leverage knowledge of the problem's geometry to form a cost volume using deep feature representations. We learn to incorporate contextual information using 3-D convolutions over this volume. Disparity values are regressed from the cost volume using a proposed differentiable soft argmin operation, which allows us to train our method end-to-end to sub-pixel accuracy without any additional post-processing or regularization. We evaluate our method on the Scene Flow and KITTI datasets and on KITTI we set a new state-of-the-art benchmark, while being significantly faster than competing approaches.
count=1
* From Point Clouds to Mesh Using Regression
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Ladicky_From_Point_Clouds_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Ladicky_From_Point_Clouds_ICCV_2017_paper.pdf)]
    * Title: From Point Clouds to Mesh Using Regression
    * Year: `2017`
    * Authors: Lubor Ladicky, Olivier Saurer, SoHyeon Jeong, Fabio Maninchedda, Marc Pollefeys
    * Abstract: Surface reconstruction from a point cloud is a standard subproblem in many algorithms for dense 3D reconstruction from RGB images or depth maps. Methods, performing only local operations in the vicinity of individual points, are very fast, but reconstructed models typically contain lots of visually unpleasant holes. On the other hand, regularized volumetric approaches, formulated as a global optimization, are typically too slow for real-time interactive applications. We propose to use a regression forest based method, which predicts the projection of a grid point to the surface, depending on the spatial configuration of point density in the grid point neighborhood. We designed a suitable feature vector and efficient oct-tree based GPU evaluation, capable of predicting surface of high resolution 3D models in milliseconds. Our method learns and predicts surfaces from an observed point cloud sparser than the evaluation grid, and therefore effectively acts as a regularizer.
count=1
* Introspective Neural Networks for Generative Modeling
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Lazarow_Introspective_Neural_Networks_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Lazarow_Introspective_Neural_Networks_ICCV_2017_paper.pdf)]
    * Title: Introspective Neural Networks for Generative Modeling
    * Year: `2017`
    * Authors: Justin Lazarow, Long Jin, Zhuowen Tu
    * Abstract: We study unsupervised learning by developing a generative model built from progressively learned deep convolutional neural networks. The resulting generator is additionally a discriminator, capable of "introspection" in a sense --- being able to self-evaluate the difference between its generated samples and the given training data. Through repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. Specifically, our model learns a sequence of CNN classifiers using a synthesis-by-classification algorithm. In the experiments, we observe encouraging results on a number of applications including texture modeling, artistic style transferring, face modeling, and unsupervised feature learning.
count=1
* Primary Video Object Segmentation via Complementary CNNs and Neighborhood Reversible Flow
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Li_Primary_Video_Object_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Li_Primary_Video_Object_ICCV_2017_paper.pdf)]
    * Title: Primary Video Object Segmentation via Complementary CNNs and Neighborhood Reversible Flow
    * Year: `2017`
    * Authors: Jia Li, Anlin Zheng, Xiaowu Chen, Bin Zhou
    * Abstract: This paper proposes a novel approach for segmenting primary video objects by using Complementary Convolutional Neural Networks (CCNN) and neighborhood reversible flow. The proposed approach first pre-trains CCNN on massive images with manually annotated salient objects in an end-to-end manner, and the trained CCNN has two separate branches that simultaneously handle two complementary tasks, i.e., foregroundness and backgroundness estimation. By applying CCNN on each video frame, the spatial foregroundness and backgroundness maps can be initialized, which are then propagated between various frames so as to segment primary video objects and suppress distractors. To enforce efficient temporal propagation, we divide each frame into superpixels and construct neighborhood reversible flow that reflects the most reliable temporal correspondences between superpixels in far-away frames. Within such flow, the initialized foregroundness and backgroundness can be efficiently and accurately propagated along the temporal axis so that primary video objects gradually pop-out and distractors are well suppressed. Extensive experimental results on three video datasets show that the proposed approach achieves impressive performance in comparisons with 18 state-of-the-art models.
count=1
* Material Editing Using a Physically Based Rendering Network
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Liu_Material_Editing_Using_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Material_Editing_Using_ICCV_2017_paper.pdf)]
    * Title: Material Editing Using a Physically Based Rendering Network
    * Year: `2017`
    * Authors: Guilin Liu, Duygu Ceylan, Ersin Yumer, Jimei Yang, Jyh-Ming Lien
    * Abstract: The ability to edit materials of objects in images is desirable by many content creators. However, this is an extremely challenging task as it requires to disentangle intrinsic physical properties of an image. We propose an end-to-end network architecture that replicates the forward image formation process to accomplish this task. Specifically, given a single image, the network first predicts intrinsic properties, i.e. shape, illumination, and material, which are then provided to a rendering layer. This layer performs in-network image synthesis, thereby enabling the network to understand the physics behind the image formation process. The proposed rendering layer is fully differentiable, supports both diffuse and specular materials, and thus can be applicable in a variety of problem settings. We demonstrate a rich set of visually plausible material editing examples and provide an extensive comparative study.
count=1
* DeepRoadMapper: Extracting Road Topology From Aerial Images
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Mattyus_DeepRoadMapper_Extracting_Road_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Mattyus_DeepRoadMapper_Extracting_Road_ICCV_2017_paper.pdf)]
    * Title: DeepRoadMapper: Extracting Road Topology From Aerial Images
    * Year: `2017`
    * Authors: Gellert Mattyus, Wenjie Luo, Raquel Urtasun
    * Abstract: Creating road maps is essential to the success of many applications such as autonomous driving and city planning. Most approaches in industry focus on leveraging expensive sensors mounted on top of a fleet of cars. This results in very accurate estimates when using techniques that involve a user in the loop. However, these solutions are very expensive and have small coverage. In contrast, in this paper we propose an approach that directly estimates road topology from aerial images. This provides us with an affordable solution which has large coverage. Towards this goal, we take advantage of the latest developments in deep learning to have an initial segmentation of the aerial images. We then propose an algorithm that reasons about missing connections in the extracted road topology as a shortest path problem which can be solved efficiently. We demonstrate the effectiveness of our approach in the challenging TorontoCity dataset and show very significant improvements over the state-of-the-art.
count=1
* Real-Time Hand Tracking Under Occlusion From an Egocentric RGB-D Sensor
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Mueller_Real-Time_Hand_Tracking_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Mueller_Real-Time_Hand_Tracking_ICCV_2017_paper.pdf)]
    * Title: Real-Time Hand Tracking Under Occlusion From an Egocentric RGB-D Sensor
    * Year: `2017`
    * Authors: Franziska Mueller, Dushyant Mehta, Oleksandr Sotnychenko, Srinath Sridhar, Dan Casas, Christian Theobalt
    * Abstract: We present an approach for real-time, robust, and accurate hand pose estimation from moving egocentric RGB-D cameras in cluttered real environments. Existing methods typically fail for hand-object interactions in cluttered scenes imaged from egocentric viewpoints, common for virtual or augmented reality applications. Our approach uses two subsequently applied Convolutional Neural Networks (CNNs) to localize the hand and regress 3D joint locations. Hand localization is achieved by using a CNN to estimate the 2D position of the hand center in the input, even in the presence of clutter and occlusions. The localized hand position, together with the corresponding input depth value, is used to generate a normalized cropped image that is fed into a second CNN to regress relative 3D hand joint locations in real-time. For added accuracy, robustness, and temporal stability, we refine the pose estimates using a kinematic pose tracking energy. To train the CNNs, we introduce a new photorealistic dataset that uses a merged reality approach to capture and synthesize large amounts of annotated data of natural hand interaction in cluttered scenes. Through quantitative and qualitative evaluation, we show that our method is robust to self-occlusion and occlusions by objects, specifically in moving egocentric perspectives.
count=1
* Face Sketch Matching via Coupled Deep Transform Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Nagpal_Face_Sketch_Matching_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Nagpal_Face_Sketch_Matching_ICCV_2017_paper.pdf)]
    * Title: Face Sketch Matching via Coupled Deep Transform Learning
    * Year: `2017`
    * Authors: Shruti Nagpal, Maneet Singh, Richa Singh, Mayank Vatsa, Afzel Noore, Angshul Majumdar
    * Abstract: Face sketch to digital image matching is an important challenge of face recognition that involves matching across different domains. Current research efforts have primarily focused on extracting domain invariant representations or learning a mapping from one domain to the other. In this research, we propose a novel transform learning based approach termed as DeepTransformer, which learns a transformation and mapping function between the features of two domains. The proposed formulation is independent of the input information and can be applied with any existing learned or hand-crafted feature. Since the mapping function is directional in nature, we propose two variants of DeepTransformer: (i) semi-coupled and (ii) symmetrically-coupled deep transform learning. This research also uses a novel IIIT-D Composite Sketch with Age (CSA) variations database which contains sketch images of 150 subjects along with age-separated digital photos. The performance of the proposed models is evaluated on a novel application of sketch-to-sketch matching, along with sketch-to-digital photo matching. Experimental results demonstrate the robustness of the proposed models in comparison to existing state-of-the-art sketch matching algorithms and a commercial face recognition system.
count=1
* Personalized Cinemagraphs Using Semantic Understanding and Collaborative Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Oh_Personalized_Cinemagraphs_Using_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Oh_Personalized_Cinemagraphs_Using_ICCV_2017_paper.pdf)]
    * Title: Personalized Cinemagraphs Using Semantic Understanding and Collaborative Learning
    * Year: `2017`
    * Authors: Tae-Hyun Oh, Kyungdon Joo, Neel Joshi, Baoyuan Wang, In So Kweon, Sing Bing Kang
    * Abstract: Cinemagraphs are a compelling way to convey dynamic aspects of a scene. In these media, dynamic and still elements are juxtaposed to create an artistic and narrative experience. Creating a high-quality, aesthetically pleasing cinemagraph requires isolating objects in a semantically meaningful way and then selecting good start times and looping periods for those objects to minimize visual artifacts (such a tearing). To achieve this, we present a new technique that uses object recognition and semantic segmentation as part of an optimization method to automatically create cinemagraphs from videos that are both visually appealing and semantically meaningful. Given a scene with multiple objects, there are many cinemagraphs one could create. Our method evaluates these multiple candidates and presents the best one, as determined by a model trained to predict human preferences in a collaborative way. We demonstrate the effectiveness of our approach with multiple results and a user study.
count=1
* BIER - Boosting Independent Embeddings Robustly
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Opitz_BIER_-_Boosting_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Opitz_BIER_-_Boosting_ICCV_2017_paper.pdf)]
    * Title: BIER - Boosting Independent Embeddings Robustly
    * Year: `2017`
    * Authors: Michael Opitz, Georg Waltner, Horst Possegger, Horst Bischof
    * Abstract: Learning similarity functions between image pairs with deep neural networks yields highly correlated activations of large embeddings. In this work, we show how to improve the robustness of embeddings by exploiting independence in ensembles. We divide the last embedding layer of a deep network into an embedding ensemble and formulate training this ensemble as an online gradient boosting problem. Each learner receives a reweighted training sample from the previous learners. This leverages large embedding sizes more effectively by significantly reducing correlation of the embedding and consequently increases retrieval accuracy of the embedding. Our method does not introduce any additional parameters and works with any differentiable loss function. We evaluate our metric learning method on image retrieval tasks and show that it improves over state-of-the-art methods on the CUB-200-2011, Cars-196, Stanford Online Products, In-Shop Clothes Retrieval and VehicleID datasets by a significant margin.
count=1
* Learning Action Recognition Model From Depth and Skeleton Videos
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Rahmani_Learning_Action_Recognition_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Rahmani_Learning_Action_Recognition_ICCV_2017_paper.pdf)]
    * Title: Learning Action Recognition Model From Depth and Skeleton Videos
    * Year: `2017`
    * Authors: Hossein Rahmani, Mohammed Bennamoun
    * Abstract: Depth sensors open up possibilities of dealing with the human action recognition problem by providing 3D human skeleton data and depth images of the scene. Analysis of human actions based on 3D skeleton data has become popular recently, due to its robustness and view-invariant representation. However, the skeleton alone is insufficient to distinguish actions which involve human-object interactions. In this paper, we propose a deep model which efficiently models human-object interactions and intra-class variations under viewpoint changes. First, a human body-part model is introduced to transfer the depth appearances of body-parts to a shared view-invariant space. Second, an end-to-end learning framework is proposed which is able to effectively combine the view-invariant body-part representation from skeletal and depth images, and learn the relations between the human body-parts and the environmental objects, the interactions between different human body-parts, and the temporal structure of human actions. We have evaluated the performance of our proposed model against 15 existing techniques on two large benchmark human action recognition datasets including NTU RGB+D and UWA3DII. The Experimental results show that our technique provides a significant improvement over state-of-the-art methods.
count=1
* DeepSetNet: Predicting Sets With Deep Neural Networks
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Rezatofighi_DeepSetNet_Predicting_Sets_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Rezatofighi_DeepSetNet_Predicting_Sets_ICCV_2017_paper.pdf)]
    * Title: DeepSetNet: Predicting Sets With Deep Neural Networks
    * Year: `2017`
    * Authors: S. Hamid Rezatofighi, Vijay Kumar B G, Anton Milan, Ehsan Abbasnejad, Anthony Dick, Ian Reid
    * Abstract: This paper addresses the task of set prediction using deep learning. This is important because the output of many computer vision tasks, including image tagging and object detection, are naturally expressed as sets of entities rather than vectors. As opposed to a vector, the size of a set is not fixed in advance, and it is invariant to the ordering of entities within it. We define a likelihood for a set distribution and learn its parameters using a deep neural network. We also derive a loss for predicting a discrete distribution corresponding to set cardinality. Set prediction is demonstrated on the problem of multi-class image classification. Moreover, we show that the proposed cardinality loss can also trivially be applied to the tasks of object counting and pedestrian detection. Our approach outperforms existing methods in all three cases on standard datasets.
count=1
* Playing for Benchmarks
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Richter_Playing_for_Benchmarks_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Richter_Playing_for_Benchmarks_ICCV_2017_paper.pdf)]
    * Title: Playing for Benchmarks
    * Year: `2017`
    * Authors: Stephan R. Richter, Zeeshan Hayder, Vladlen Koltun
    * Abstract: We present a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. To create the benchmark, we have developed a new approach to collecting ground-truth data from simulated worlds without access to their source code or content. We conduct statistical analyses that show that the composition of the scenes in the benchmark closely matches the composition of corresponding physical environments. The realism of the collected data is further validated via perceptual experiments. We analyze the performance of state-of-the-art methods for multiple tasks, providing reference baselines and highlighting challenges for future research.
count=1
* Embedding 3D Geometric Features for Rigid Object Part Segmentation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Song_Embedding_3D_Geometric_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Song_Embedding_3D_Geometric_ICCV_2017_paper.pdf)]
    * Title: Embedding 3D Geometric Features for Rigid Object Part Segmentation
    * Year: `2017`
    * Authors: Yafei Song, Xiaowu Chen, Jia Li, Qinping Zhao
    * Abstract: Object part segmentation is a challenging and fundamental problem in computer vision. Its difficulties may be caused by the varying viewpoints, poses, and topological structures, which can be attributed to an essential reason, i.e., a specific object is a 3D model rather than a 2D figure. Therefore, we conjecture that not only 2D appearance features but also 3D geometric features could be helpful. With this in mind, we propose a 2-stream FCN. One stream, named AppNet, is to extract 2D appearance features from the input image. The other stream, named GeoNet, is to extract 3D geometric features. However, the problem is that the input is just an image. To this end, we design a 2D-convolution based CNN structure to extract 3D geometric features from 3D volume, which is named VolNet. Then a teacher-student strategy is adopted and VolNet teaches GeoNet how to extract 3D geometric features from an image. To perform this teaching process, we synthesize training data using 3D models. Each training sample consists of an image and its corresponding volume. A perspective voxelization algorithm is further proposed to align them. Experimental results verify our conjecture and the effectiveness of both the proposed 2-stream CNN and VolNet.
count=1
* Unsupervised Action Discovery and Localization in Videos
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Soomro_Unsupervised_Action_Discovery_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Soomro_Unsupervised_Action_Discovery_ICCV_2017_paper.pdf)]
    * Title: Unsupervised Action Discovery and Localization in Videos
    * Year: `2017`
    * Authors: Khurram Soomro, Mubarak Shah
    * Abstract: This paper is the first to address the problem of unsupervised action localization in videos. Given unlabeled data without bounding box annotations, we propose a novel approach that: 1) Discovers action class labels and 2) Spatio-temporally localizes actions in videos. It begins by computing local video features to apply spectral clustering on a set of unlabeled training videos. For each cluster of videos, an undirected graph is constructed to extract a dominant set, which are known for high internal homogeneity and inhomogeneity between vertices outside it. Next, a discriminative clustering approach is applied, by training a classifier for each cluster, to iteratively select videos from the non dominant set and obtain complete video action classes. Once classes are discovered, training videos within each cluster are selected to perform automatic spatio-temporal annotations, by first oversegmenting videos in each discovered class into supervoxels and constructing a directed graph to apply a variant of knapsack problem with temporal constraints. Knapsack optimization jointly collects a subset of supervoxels, by enforcing the annotated action to be spatio-temporally connected and its volume to be the size of an actor. These annotations are used to train SVM action classifiers. During testing, actions are localized using a similar Knapsack approach, where supervoxels are grouped together and SVM, learned using videos from discovered action classes, is used to recognize these actions. We evaluate our approach on UCF Sports, Sub-JHMDB, JHMDB, THUMOS13 and UCF101 datasets. Our experiments suggest that despite using no action class labels and no bounding box annotations, we are able to get competitive results to the state-of-the-art supervised methods.
count=1
* Compositional Human Pose Regression
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Sun_Compositional_Human_Pose_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Sun_Compositional_Human_Pose_ICCV_2017_paper.pdf)]
    * Title: Compositional Human Pose Regression
    * Year: `2017`
    * Authors: Xiao Sun, Jiaxiang Shang, Shuang Liang, Yichen Wei
    * Abstract: Regression based methods are not performing as well as detection based methods for human pose estimation. A central problem is that the structural information in the pose is not well exploited in the previous regression methods. In this work, we propose a structure-aware regression approach. It adopts a reparameterized pose representation using bones instead of joints. It exploits the joint connection structure to define a compositional loss function that encodes the long range interactions in the pose. It is simple, effective, and general for both 2D and 3D pose estimation in a unified setting. Comprehensive evaluation validates the effectiveness of our approach. It significantly advances the state-of-the-art on Human3.6M and is competitive with state-of-the-art results on MPII.
count=1
* Robust Object Tracking Based on Temporal and Spatial Deep Networks
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Teng_Robust_Object_Tracking_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Teng_Robust_Object_Tracking_ICCV_2017_paper.pdf)]
    * Title: Robust Object Tracking Based on Temporal and Spatial Deep Networks
    * Year: `2017`
    * Authors: Zhu Teng, Junliang Xing, Qiang Wang, Congyan Lang, Songhe Feng, Yi Jin
    * Abstract: Recently deep neural networks have been widely employed to deal with the visual tracking problem. In this work, we present a new deep architecture which incorporates the temporal and spatial information to boost the tracking performance. Our deep architecture contains three networks, a Feature Net, a Temporal Net, and a Spatial Net. The Feature Net extracts general feature representations of the target. With these feature representations, the Temporal Net encodes the trajectory of the target and directly learns temporal correspondences to estimate the object state from a global perspective. Based on the learning results of the Temporal Net, the Spatial Net further refines the object tracking state using local spatial object information. Extensive experiments on four of the largest tracking benchmarks, including VOT2014, VOT2016, OTB50, and OTB100, demonstrate competing performance of the proposed tracker over a number of state-of-the-art algorithms.
count=1
* WeText: Scene Text Detection Under Weak Supervision
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Tian_WeText_Scene_Text_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Tian_WeText_Scene_Text_ICCV_2017_paper.pdf)]
    * Title: WeText: Scene Text Detection Under Weak Supervision
    * Year: `2017`
    * Authors: Shangxuan Tian, Shijian Lu, Chongshou Li
    * Abstract: The requiring of large amounts of annotated training data has become a common constraint on various deep learning systems. In this paper, we propose a weakly supervised scene text detection method (WeText) that trains robust and accurate scene text detection models by learning from unannotated or weakly annotated data. With a "light" supervised model trained on a small fully annotated dataset, we explore semi-supervised and weakly supervised learning on a large unannotated dataset and a large weakly annotated dataset, respectively. For the unsupervised learning, the light supervised model is applied to the unannotated dataset to search for more character training samples, which are further combined with the small annotated dataset to retrain a superior character detection model. For the weakly supervised learning, the character searching is guided by high-level annotations of words/text lines that are widely available and also much easier to prepare. In addition, we design an unified scene character detector by adapting regression based deep networks, which greatly relieves the error accumulation issue that widely exists in most traditional approaches. Extensive experiments across different unannotated and weakly annotated datasets show that the scene text detection performance can be clearly boosted under both scenarios, where the weakly supervised learning can achieve the state-of-the-art performance by using only 229 fully annotated scene text images.
count=1
* Image Super-Resolution Using Dense Skip Connections
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Tong_Image_Super-Resolution_Using_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Tong_Image_Super-Resolution_Using_ICCV_2017_paper.pdf)]
    * Title: Image Super-Resolution Using Dense Skip Connections
    * Year: `2017`
    * Authors: Tong Tong, Gen Li, Xiejie Liu, Qinquan Gao
    * Abstract: Recent studies have shown that the performance of single-image super-resolution methods can be significantly boosted by using deep convolutional neural networks. In this study, we present a novel single-image super-resolution method by introducing dense skip connections in a very deep network. In the proposed network, the feature maps of each layer are propagated into all subsequent layers, providing an effective way to combine the low-level features and high-level features to boost the reconstruction performance. In addition, the dense skip connections in the network enable short paths to be built directly from the output to each layer, alleviating the vanishing-gradient problem of very deep networks. Moreover, deconvolution layers are integrated into the network to learn the upsampling filters and to speedup the reconstruction process. Further, the proposed method substantially reduces the number of parameters, enhancing the computational efficiency. We evaluate the proposed method using images from four benchmark datasets and set a new state of the art.
count=1
* Unsupervised Adaptation for Deep Stereo
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Tonioni_Unsupervised_Adaptation_for_ICCV_2017_paper.pdf)]
    * Title: Unsupervised Adaptation for Deep Stereo
    * Year: `2017`
    * Authors: Alessio Tonioni, Matteo Poggi, Stefano Mattoccia, Luigi Di Stefano
    * Abstract: Recent ground-breaking works have shown that deep neural networks can be trained end-to-end to regress dense disparity maps directly from image pairs. Computer generated imagery is deployed to gather the large data corpus required to train such networks, an additional fine-tuning allowing to adapt the model to work well also on real and possibly diverse environments. Yet, besides a few public datasets such as Kitti, the ground-truth needed to adapt the network to a new scenario is hardly available in practice. In this paper we propose a novel unsupervised adaptation approach that enables to fine-tune a deep learning stereo model without any ground-truth information. We rely on off-the-shelf stereo algorithms together with state-of-the-art confidence measures, the latter able to ascertain upon correctness of the measurements yielded by former. Thus, we train the network based on a novel loss-function that penalizes predictions disagreeing with the highly confident disparities provided by the algorithm and enforces a smoothness constraint. Experiments on popular datasets (KITTI 2012, KITTI 2015 and Middlebury 2014) and other challenging test images demonstrate the effectiveness of our proposal.
count=1
* Deep Growing Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Wang_Deep_Growing_Learning_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Deep_Growing_Learning_ICCV_2017_paper.pdf)]
    * Title: Deep Growing Learning
    * Year: `2017`
    * Authors: Guangcong Wang, Xiaohua Xie, Jianhuang Lai, Jiaxuan Zhuo
    * Abstract: Semi-supervised learning (SSL) is an import paradigm to make full use of a large amount of unlabeled data in machine learning. A bottleneck of SSL is the overfitting problem when training over the limited labeled data, especially on a complex model like a deep neural network. To get around this bottleneck, we propose a bio-inspired SSL framework on deep neural network, namely Deep Growing Learning (DGL). Specifically, we formulate the SSL as an EM-like process, where the deep network alternately iterates between automatically growing convolutional layers and selecting reliable pseudo-labeled data for training. The DGL guarantees that a shallow neural network is trained with labeled data, while a deeper neural network is trained with growing amount of reliable pseudo-labeled data, so as to alleviate the overfitting problem. Experiments on different visual recognition tasks have verified the effectiveness of DGL.
count=1
* Common Action Discovery and Localization in Unconstrained Videos
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Yang_Common_Action_Discovery_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Yang_Common_Action_Discovery_ICCV_2017_paper.pdf)]
    * Title: Common Action Discovery and Localization in Unconstrained Videos
    * Year: `2017`
    * Authors: Jiong Yang, Junsong Yuan
    * Abstract: Similar to common object discovery in images or videos, it is of great interests to discover and locate common actions in videos, which can benefit many video analytics applications such as video summarization, search, and understanding. In this work, we tackle the problem of common action discovery and localization in unconstrained videos, where we do not assume to know the types, numbers or locations of the common actions in the videos. Furthermore, each video can contain zero, one or several common action instances. To perform automatic discovery and localization in such challenging scenarios, we first generate action proposals using human prior. By building an affinity graph among all action proposals, we formulate the common action discovery as a subgraph density maximization problem to select the proposals containing common actions. To avoid enumerating in the exponentially large solution space, we propose an efficient polynomial time optimization algorithm. It solves the problem up to a user specified error bound with respect to the global optimal solution. The experimental results on several datasets show that even without any prior knowledge of common actions, our method can robustly locate the common actions in a collection of videos.
count=1
* Unsupervised Learning of Stereo Matching
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2017/html/Zhou_Unsupervised_Learning_of_ICCV_2017_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhou_Unsupervised_Learning_of_ICCV_2017_paper.pdf)]
    * Title: Unsupervised Learning of Stereo Matching
    * Year: `2017`
    * Authors: Chao Zhou, Hong Zhang, Xiaoyong Shen, Jiaya Jia
    * Abstract: In recent years, convolutional neural networks have shown its strong power for stereo matching cost learning. Current approaches learn the parameters of their models from public datasets with ground truth disparity. However, due to the limitations of these datasets and the difficulty of collecting new stereo data, current methods fail in real-life cases. In this work, we present a framework for learning stereo matching cost without human supervision. Our method updates the network parameter in a iterative manner. It starts with randomly initialized network. Correct matchings are carefully picked and used as training data in each round. In the end, the networks converges to a stable state, which performs comparably with supervised methods on various benchmarks.
count=1
* Robust Optimization for Deep Regression
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Belagiannis_Robust_Optimization_for_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Belagiannis_Robust_Optimization_for_ICCV_2015_paper.pdf)]
    * Title: Robust Optimization for Deep Regression
    * Year: `2015`
    * Authors: Vasileios Belagiannis, Christian Rupprecht, Gustavo Carneiro, Nassir Navab
    * Abstract: Convolutional Neural Networks (ConvNets) have successfully contributed to improve the accuracy of regression-based methods for computer vision tasks such as human pose estimation, landmark localization, and object detection. The network optimization has been usually performed with L2 loss and without considering the impact of outliers on the training process, where an outlier in this context is defined by a sample estimation that lies at an abnormal distance from the other training sample estimations in the objective space. In this work, we propose a regression model with ConvNets that achieves robustness to such outliers by minimizing Tukey's biweight function, an M-estimator robust to outliers, as the loss function for the ConvNet. In addition to the robust loss, we introduce a coarse-to-fine model, which processes input images of progressively higher resolutions for improving the accuracy of the regressed values. In our experiments, we demonstrate faster convergence and better generalization of our robust loss function for the tasks of human pose estimation and age estimation from face images. We also show that the combination of the robust loss function with the coarse-to-fine model produces comparable or better results than current state-of-the-art approaches in four publicly available human pose estimation datasets.
count=1
* High-for-Low and Low-for-High: Efficient Boundary Detection From Deep Object Features and its Applications to High-Level Vision
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Bertasius_High-for-Low_and_Low-for-High_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Bertasius_High-for-Low_and_Low-for-High_ICCV_2015_paper.pdf)]
    * Title: High-for-Low and Low-for-High: Efficient Boundary Detection From Deep Object Features and its Applications to High-Level Vision
    * Year: `2015`
    * Authors: Gedas Bertasius, Jianbo Shi, Lorenzo Torresani
    * Abstract: Most of the current boundary detection systems rely exclusively on low-level features, such as color and texture. However, perception studies suggest that humans employ object-level reasoning when judging if a particular pixel is a boundary. Inspired by this observation, in this work we show how to predict boundaries by exploiting object-level features from a pretrained object-classification network. Our method can be viewed as a "High-for-Low" approach where high-level object features inform the low-level boundary detection process. Our model achieves state-of-the-art performance on an established boundary detection benchmark and it is efficient to run. Additionally, we show that due to the semantic nature of our boundaries we can use them to aid a number of high-level vision tasks. We demonstrate that using our boundaries we improve the performance of state-of-the-art methods on the problems of semantic boundary labeling, semantic segmentation and object proposal generation. We can view this process as a "Low-for-High'" scheme, where low-level boundaries aid high-level vision tasks. Thus, our contributions include a boundary detection system that is accurate, efficient, generalizes well to multiple datasets, and is also shown to improve existing state-of-the-art high-level vision methods on three distinct tasks.
count=1
* Learning Complexity-Aware Cascades for Deep Pedestrian Detection
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Cai_Learning_Complexity-Aware_Cascades_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Cai_Learning_Complexity-Aware_Cascades_ICCV_2015_paper.pdf)]
    * Title: Learning Complexity-Aware Cascades for Deep Pedestrian Detection
    * Year: `2015`
    * Authors: Zhaowei Cai, Mohammad Saberian, Nuno Vasconcelos
    * Abstract: The design of complexity-aware cascaded detectors, combining features of very different complexities, is considered. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network (CNN). This is demonstrated through the design of a pedestrian detector with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds.
count=1
* Query Adaptive Similarity Measure for RGB-D Object Recognition
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.pdf)]
    * Title: Query Adaptive Similarity Measure for RGB-D Object Recognition
    * Year: `2015`
    * Authors: Yanhua Cheng, Rui Cai, Chi Zhang, Zhiwei Li, Xin Zhao, Kaiqi Huang, Yong Rui
    * Abstract: This paper studies the problem of improving the top-1 accuracy of RGB-D object recognition. Despite of the impressive top-5 accuracies achieved by existing methods, their top-1 accuracies are not very satisfactory. The reasons are in two-fold: (1) existing similarity measures are sensitive to object pose and scale changes, as well as intra-class variations; and (2) effectively fusing RGB and depth cues is still an open problem. To address these problems, this paper first proposes a new similarity measure based on dense matching, through which objects in comparison are warped and aligned, to better tolerate variations. Towards RGB and depth fusion, we argue that a constant and golden weight doesn't exist. The two modalities have varying contributions when comparing objects from different categories. To capture such a dynamic characteristic, a group of matchers equipped with various fusion weights is constructed, to explore the responses of dense matching under different fusion configurations. All the response scores are finally merged following a learning-to-combination way, which provides quite good generalization ability in practice. The proposed approach win the best results on several public benchmarks, e.g., achieves 92.7% top-1 test accuracy on the Washington RGB-D object dataset, with a 5.1% improvement over the state-of-the-art.
count=1
* Learning Large-Scale Automatic Image Colorization
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Deshpande_Learning_Large-Scale_Automatic_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Deshpande_Learning_Large-Scale_Automatic_ICCV_2015_paper.pdf)]
    * Title: Learning Large-Scale Automatic Image Colorization
    * Year: `2015`
    * Authors: Aditya Deshpande, Jason Rock, David Forsyth
    * Abstract: We describe an automated method for image colorization that learns to colorize from examples. Our method exploits a LEARCH framework to train a quadratic objective function in the chromaticity maps, comparable to a Gaussian random field. The coefficients of the objective function are conditioned on image features, using a random forest. The objective function admits correlations on long spatial scales, and can control spatial error in the colorization of the image. Images are then colorized by minimizing this objective function. We demonstrate that our method strongly outperforms a natural baseline on large-scale experiments with images of real scenes using a demanding loss function. We demonstrate that learning a model that is conditioned on scene produces improved results. We show how to incorporate a desired color histogram into the objective function, and that doing so can lead to further improvements in results.
count=1
* Single Image 3D Without a Single 3D Image
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Fouhey_Single_Image_3D_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Fouhey_Single_Image_3D_ICCV_2015_paper.pdf)]
    * Title: Single Image 3D Without a Single 3D Image
    * Year: `2015`
    * Authors: David F. Fouhey, Wajahat Hussain, Abhinav Gupta, Martial Hebert
    * Abstract: Do we really need 3D labels in order to learn how to predict 3D? In this paper, we show that one can learn a mapping from appearance to 3D properties without ever seeing a single explicit 3D label. Rather than use explicit supervision, we use the regularity of indoor scenes to learn the mapping in a completely unsupervised manner. We demonstrate this on both a standard 3D scene understanding dataset as well as Internet images for which 3D is unavailable, precluding supervised learning. Despite never seeing a 3D label, our method produces competitive results.
count=1
* Beyond Tree Structure Models: A New Occlusion Aware Graphical Model for Human Pose Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Fu_Beyond_Tree_Structure_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Fu_Beyond_Tree_Structure_ICCV_2015_paper.pdf)]
    * Title: Beyond Tree Structure Models: A New Occlusion Aware Graphical Model for Human Pose Estimation
    * Year: `2015`
    * Authors: Lianrui Fu, Junge Zhang, Kaiqi Huang
    * Abstract: Occlusion is a main challenge for human pose estimation, which is largely ignored in popular tree structure models. The tree structure model is simple and convenient for exact inference, but short in modeling the occlusion coherence especially in the case of self-occlusion. We propose an occlusion aware graphical model which is able to model both self-occlusion and occlusion by the other objects simultaneously. The proposed model structure can encode the interactions between human body parts and objects, and hence enable it to learn occlusion coherence from data discriminatively. We evaluate our model on several public benchmarks for human pose estimation including challenging subsets featuring significant occlusion. The experimental results show that our method obtains comparable accuracy with the state-of-the-arts, and achieves promising performance in 2D human pose estimation with occlusion.
count=1
* DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Ghodrati_DeepProposal_Hunting_Objects_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Ghodrati_DeepProposal_Hunting_Objects_ICCV_2015_paper.pdf)]
    * Title: DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers
    * Year: `2015`
    * Authors: Amir Ghodrati, Ali Diba, Marco Pedersoli, Tinne Tuytelaars, Luc Van Gool
    * Abstract: In this paper we evaluate the quality of the activation layers of a convolutional neural network (CNN) for the generation of object proposals. We generate hypotheses in a sliding-window fashion over different activation layers and show that the final convolutional layers can find the object of interest with high recall but poor localization due to the coarseness of the feature maps. Instead, the first layers of the network can better localize the object of interest but with a reduced recall. Based on this observation we design a method for proposing object locations that is based on CNN features and that combines the best of both worlds. We build an inverse cascade that, going from the final to the initial convolutional layers of the CNN, selects the most promising object locations and refines their boxes in a coarse-to-fine manner. The method is efficient, because i) it uses the same features extracted for detection, ii) it aggregates features using integral images, and iii) it avoids a dense evaluation of the proposals due to the inverse coarse-to-fine cascade. The method is also accurate; it outperforms most of the previously proposed object proposals approaches and when plugged into a CNN-based detector produces state-of-the-art detection performance.
count=1
* Contextual Action Recognition With R*CNN
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Gkioxari_Contextual_Action_Recognition_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Gkioxari_Contextual_Action_Recognition_ICCV_2015_paper.pdf)]
    * Title: Contextual Action Recognition With R*CNN
    * Year: `2015`
    * Authors: Georgia Gkioxari, Ross Girshick, Jitendra Malik
    * Abstract: There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R*CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R*CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R*CNN is not limited to action recognition. In particular, R*CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset.
count=1
* Learning Ensembles of Potential Functions for Structured Prediction With Latent Variables
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Hajimirsadeghi_Learning_Ensembles_of_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Hajimirsadeghi_Learning_Ensembles_of_ICCV_2015_paper.pdf)]
    * Title: Learning Ensembles of Potential Functions for Structured Prediction With Latent Variables
    * Year: `2015`
    * Authors: Hossein Hajimirsadeghi, Greg Mori
    * Abstract: Many visual recognition tasks involve modeling variables which are structurally related. Hidden conditional random fields (HCRFs) are a powerful class of models for encoding structure in weakly supervised training examples. This paper presents HCRF-Boost, a novel and general framework for learning HCRFs in functional space. An algorithm is proposed to learn the potential functions of an HCRF as a combination of abstract nonlinear feature functions, expressed by regression models. Consequently, the resulting latent structured model is not restricted to traditional log-linear potential functions or any explicit parameterization. Further, functional optimization helps to avoid direct interactions with the possibly large parameter space of nonlinear models and improves efficiency. As a result, a complex and flexible ensemble method is achieved for structured prediction which can be successfully used in a variety of applications. We validate the effectiveness of this method on tasks such as group activity recognition, human action recognition, and multi-instance learning of video events.
count=1
* Objects2action: Classifying and Localizing Actions Without Any Video Example
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Jain_Objects2action_Classifying_and_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Jain_Objects2action_Classifying_and_ICCV_2015_paper.pdf)]
    * Title: Objects2action: Classifying and Localizing Actions Without Any Video Example
    * Year: `2015`
    * Authors: Mihir Jain, Jan C. van Gemert, Thomas Mensink, Cees G. M. Snoek
    * Abstract: The goal of this paper is to recognize actions in video without the need for examples. Different from traditional zero-shot approaches we do not demand the design and specification of attribute classifiers and class-to-attribute mappings to allow for transfer from seen classes to unseen classes. Our key contribution is objects2action, a semantic word embedding that is spanned by a skip-gram model of thousands of object categories. Action labels are assigned to an object encoding of unseen video based on a convex combination of action and object affinities. Our semantic embedding has three main characteristics to accommodate for the specifics of actions. First, we propose a mechanism to exploit multiple-word descriptions of actions and objects. Second, we incorporate the automated selection of the most responsive objects per action. And finally, we demonstrate how to extend our zero-shot approach to the spatio-temporal localization of actions in video. Experiments on four action datasets demonstrate the potential of our approach.
count=1
* Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Keuper_Efficient_Decomposition_of_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Keuper_Efficient_Decomposition_of_ICCV_2015_paper.pdf)]
    * Title: Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts
    * Year: `2015`
    * Authors: Margret Keuper, Evgeny Levinkov, Nicolas Bonneel, Guillaume Lavoue, Thomas Brox, Bjorn Andres
    * Abstract: Formulations of the Image Decomposition Problem as a Multicut Problem (MP) w.r.t. a superpixel graph have received considerable attention. In contrast, instances of the MP w.r.t. a pixel grid graph have received little attention, firstly, because the MP is NP-hard and instances w.r.t. a pixel grid graph are hard to solve in practice, and, secondly, due to the lack of long-range terms in the objective function of the MP. We propose a generalization of the MP with long-range terms (LMP). We design and implement two efficient algorithms (primal feasible heuristics) for the MP and LMP which allow us to study instances of both problems w.r.t. the pixel grid graphs of the images in the BSDS-500 benchmark. The decompositions we obtain do not differ significantly from the state of the art, suggesting that the LMP is a competitive formulation of the Image Decomposition Problem. To demonstrate the generality of the LMP, we apply it also to the Mesh Decomposition Problem posed by the Princeton benchmark, obtaining state-of-the-art decompositions.
count=1
* Category-Blind Human Action Recognition: A Practical Recognition System
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Li_Category-Blind_Human_Action_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Li_Category-Blind_Human_Action_ICCV_2015_paper.pdf)]
    * Title: Category-Blind Human Action Recognition: A Practical Recognition System
    * Year: `2015`
    * Authors: Wenbo Li, Longyin Wen, Mooi Choo Chuah, Siwei Lyu
    * Abstract: Existing human action recognition systems for 3D sequences obtained from the depth camera are designed to cope with only one action category, either single-person action or two-person interaction, and are difficult to be extended to scenarios where both action categories co-exist. In this paper, we propose the category-blind human recognition method (CHARM) which can recognize a human action without making assumptions of the action category. In our CHARM approach, we represent a human action (either a single-person action or a two-person interaction) class using a co-occurrence of motion primitives. Subsequently, we classify an action instance based on matching its motion primitive co-occurrence patterns to each class representation. The matching task is formulated as maximum clique problems. We conduct extensive evaluations of CHARM using three datasets for single-person actions, two-person interactions, and their mixtures. Experimental results show that CHARM performs favorably when compared with several state-of-the-art single-person action and two-person interaction based methods without making explicit assumptions of action category.
count=1
* Semi-Supervised Zero-Shot Classification With Label Representation Learning
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Li_Semi-Supervised_Zero-Shot_Classification_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Li_Semi-Supervised_Zero-Shot_Classification_ICCV_2015_paper.pdf)]
    * Title: Semi-Supervised Zero-Shot Classification With Label Representation Learning
    * Year: `2015`
    * Authors: Xin Li, Yuhong Guo, Dale Schuurmans
    * Abstract: Given the challenge of gathering labeled training data, zero-shot classification, which transfers information from observed classes to recognize unseen classes, has become increasingly popular in the computer vision community. Most existing zero-shot learning methods require a user to first provide a set of semantic visual attributes for each class as side information before applying a two-step prediction procedure that introduces an intermediate attribute prediction problem. In this paper, we propose a novel zero-shot classification approach that automatically learns label embeddings from the input data in a semi-supervised large-margin learning framework. The proposed framework jointly considers multi-class classification over all classes (observed and unseen) and tackles the target prediction problem directly without introducing intermediate prediction problems. It also has the capacity to incorporate semantic label information from different sources when available. To evaluate the proposed approach, we conduct experiments on standard zero-shot data sets. The empirical results show the proposed approach outperforms existing state-of-the-art zero-shot learning methods.
count=1
* Learning Informative Edge Maps for Indoor Scene Layout Prediction
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Mallya_Learning_Informative_Edge_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Mallya_Learning_Informative_Edge_ICCV_2015_paper.pdf)]
    * Title: Learning Informative Edge Maps for Indoor Scene Layout Prediction
    * Year: `2015`
    * Authors: Arun Mallya, Svetlana Lazebnik
    * Abstract: In this paper, we introduce new edge-based features for the task of recovering the 3D layout of an indoor scene from a single image. Indoor scenes have certain edges that are very informative about the spatial layout of the room, namely, the edges formed by the pairwise intersections of room faces (two walls, wall and ceiling, wall and floor). In contrast with previous approaches that rely on area-based features like geometric context and orientation maps, our method attempts to directly detect these informative edges. We learn to predict 'informative edge' probability maps using two recent methods that exploit local and global context, respectively: structured edge detection forests, and a fully convolutional network for pixelwise labeling. We show that the fully convolutional network is quite successful at predicting the informative edges even when they lack contrast or are occluded, and that the accuracy can be further improved by training the network to jointly predict the edges and the geometric context. Using features derived from the 'informative edge' maps, we learn a maximum margin structured classifier that achieves state-of-the-art performance on layout prediction.
count=1
* Enhancing Road Maps by Parsing Aerial Images Around the World
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Mattyus_Enhancing_Road_Maps_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Mattyus_Enhancing_Road_Maps_ICCV_2015_paper.pdf)]
    * Title: Enhancing Road Maps by Parsing Aerial Images Around the World
    * Year: `2015`
    * Authors: Gellert Mattyus, Shenlong Wang, Sanja Fidler, Raquel Urtasun
    * Abstract: In recent years, contextual models that exploit maps have been shown to be very effective for many recognition and localization tasks. In this paper we propose to exploit aerial images in order to enhance freely available world maps. Towards this goal, we make use of OpenStreetMap and formulate the problem as the one of inference in a Markov random field parameterized in terms of the location of the road-segment centerlines as well as their width. This parameterization enables very efficient inference and returns only topologically correct roads. In particular, we can segment all OSM roads in the whole world in a single day using a small cluster of 10 computers. Importantly, our approach generalizes very well; it can be trained using only 1.5 km2 aerial imagery and produce very accurate results in any location across the globe. We demonstrate the effectiveness of our approach outperforming the state-of-the-art in two new benchmarks that we collect. We then show how our enhanced maps are beneficial for semantic segmentation of ground images.
count=1
* BubbLeNet: Foveated Imaging for Visual Discovery
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Matzen_BubbLeNet_Foveated_Imaging_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Matzen_BubbLeNet_Foveated_Imaging_ICCV_2015_paper.pdf)]
    * Title: BubbLeNet: Foveated Imaging for Visual Discovery
    * Year: `2015`
    * Authors: Kevin Matzen, Noah Snavely
    * Abstract: We propose a new method for turning an Internet-scale corpus of categorized images into a small set of human-interpretable discriminative visual elements using powerful tools based on deep learning. A key challenge with deep learning methods is generating human-interpretable models. To address this, we propose a new technique that uses bubble images -- images where most of the content has been obscured -- to identify spatially localized, discriminative content in each image. By modifying the model training procedure to use both the source imagery and these bubble images, we can arrive at final models which retain much of the original classification performance, but are much more amenable to identifying interpretable visual elements. We apply our algorithm to a wide variety of datasets, including two new Internet-scale datasets of people and places, and show applications to visual mining and discovery. Our method is simple, scalable, and produces visual elements that are highly representative compared to prior work.
count=1
* Robust Model-Based 3D Head Pose Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Meyer_Robust_Model-Based_3D_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Meyer_Robust_Model-Based_3D_ICCV_2015_paper.pdf)]
    * Title: Robust Model-Based 3D Head Pose Estimation
    * Year: `2015`
    * Authors: Gregory P. Meyer, Shalini Gupta, Iuri Frosio, Dikpal Reddy, Jan Kautz
    * Abstract: We introduce a method for accurate three dimensional head pose estimation using a commodity depth camera. We perform pose estimation by registering a morphable face model to the measured depth data, using a combination of particle swarm optimization (PSO) and the iterative closest point (ICP) algorithm, which minimizes a cost function that includes a 3D registration and a 2D overlap term. The pose is estimated on the fly without requiring an explicit initialization or training phase. Our method handles large pose angles and partial occlusions by dynamically adapting to the reliable visible parts of the face. It is robust and generalizes to different depth sensors without modification. On the Biwi Kinect dataset, we achieve best-in-class performance, with average angular errors of 2.1, 2.1 and 2.4 degrees for yaw, pitch, and roll, respectively, and an average translational error of 5.9 mm, while running at 6 fps on a graphics processing unit.
count=1
* Im2Calories: Towards an Automated Mobile Vision Food Diary
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Meyers_Im2Calories_Towards_an_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Meyers_Im2Calories_Towards_an_ICCV_2015_paper.pdf)]
    * Title: Im2Calories: Towards an Automated Mobile Vision Food Diary
    * Year: `2015`
    * Authors: Austin Meyers, Nick Johnston, Vivek Rathod, Anoop Korattikara, Alex Gorban, Nathan Silberman, Sergio Guadarrama, George Papandreou, Jonathan Huang, Kevin P. Murphy
    * Abstract: We present a system which can recognize the contents of your meal from a single image, and then predict its nutritional contents, such as calories. The simplest version assumes that the user is eating at a restaurant for which we know the menu. In this case, we can collect images offline to train a multi-label classifier. At run time, we apply the classifier (running on your phone) to predict which foods are present in your meal, and we lookup the corresponding nutritional facts. We apply this method to a new dataset of images from 23 different restaurants, using a CNN-based classifier, significantly outperforming previous work. The more challenging setting works outside of restaurants. In this case, we need to estimate the size of the foods, as well as their labels. This requires solving segmentation and depth / volume estimation from a single image. We present CNN-based approaches to these problems, with promising preliminary results.
count=1
* Building Dynamic Cloud Maps From the Ground Up
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Murdock_Building_Dynamic_Cloud_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Murdock_Building_Dynamic_Cloud_ICCV_2015_paper.pdf)]
    * Title: Building Dynamic Cloud Maps From the Ground Up
    * Year: `2015`
    * Authors: Calvin Murdock, Nathan Jacobs, Robert Pless
    * Abstract: Satellite imagery of cloud cover is extremely important for understanding and predicting weather. We demonstrate how this imagery can be constructed "from the ground up" without requiring expensive geo-stationary satellites. This is accomplished through a novel approach to approximate continental-scale cloud maps using only ground-level imagery from publicly-available webcams. We collected a year's worth of satellite data and simultaneously-captured, geo-located outdoor webcam images from 4388 sparsely distributed cameras across the continental USA. The satellite data is used to train a dynamic model of cloud motion alongside 4388 regression models (one for each camera) to relate ground-level webcam data to the satellite data at the camera's location. This novel application of large-scale computer vision to meteorology and remote sensing is enabled by a smoothed, hierarchically-regularized dynamic texture model whose system dynamics are driven to remain consistent with measurements from the geo-located webcams. We show that our hierarchical model is better able to incorporate sparse webcam measurements resulting in more accurate cloud maps in comparison to a standard dynamic textures implementation. Finally, we demonstrate that our model can be successfully applied to other natural image sequences from the DynTex database, suggesting a broader applicability of our method.
count=1
* Training a Feedback Loop for Hand Pose Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Oberweger_Training_a_Feedback_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Oberweger_Training_a_Feedback_ICCV_2015_paper.pdf)]
    * Title: Training a Feedback Loop for Hand Pose Estimation
    * Year: `2015`
    * Authors: Markus Oberweger, Paul Wohlhart, Vincent Lepetit
    * Abstract: We propose an entirely data-driven approach to estimating the 3D pose of a hand given a depth image. We show that we can correct the mistakes made by a Convolutional Neural Network trained to predict an estimate of the 3D pose by using a feedback loop. The components of this feedback loop are also Deep Networks, optimized using training data. They remove the need for fitting a 3D model to the input data, which requires both a carefully designed fitting function and algorithm. We show that our approach outperforms state-of-the-art methods, and is efficient as our implementation runs at over 400 fps on a single GPU.
count=1
* Action Localization in Videos Through Context Walk
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Soomro_Action_Localization_in_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Soomro_Action_Localization_in_ICCV_2015_paper.pdf)]
    * Title: Action Localization in Videos Through Context Walk
    * Year: `2015`
    * Authors: Khurram Soomro, Haroon Idrees, Mubarak Shah
    * Abstract: This paper presents an efficient approach for localizing actions by learning contextual relations, in the form of relative locations between different video regions. We begin by over-segmenting the videos into supervoxels, which have the ability to preserve action boundaries and also reduce the complexity of the problem. Context relations are learned during training which capture displacements from all the supervoxels in a video to those belonging to foreground actions. Then, given a testing video, we select a supervoxel randomly and use the context information acquired during training to estimate the probability of each supervoxel belonging to the foreground action. The walk proceeds to a new supervoxel and the process is repeated for a few steps. This ``context walk'' generates a conditional distribution of an action over all the supervoxels. A Conditional Random Field is then used to find action proposals in the video, whose confidences are obtained using SVMs. We validated the proposed approach on several datasets and show that context in the form of relative displacements between supervoxels can be extremely useful for action localization. This also results in significantly fewer evaluations of the classifier, in sharp contrast to the alternate sliding window approaches.
count=1
* Listening With Your Eyes: Towards a Practical Visual Speech Recognition System Using Deep Boltzmann Machines
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Sui_Listening_With_Your_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Sui_Listening_With_Your_ICCV_2015_paper.pdf)]
    * Title: Listening With Your Eyes: Towards a Practical Visual Speech Recognition System Using Deep Boltzmann Machines
    * Year: `2015`
    * Authors: Chao Sui, Mohammed Bennamoun, Roberto Togneri
    * Abstract: This paper presents a novel feature learning method for visual speech recognition using Deep Boltzmann Machines (DBM). Unlike all existing visual feature extraction techniques which solely extracts features from video sequences, our method is able to explore both acoustic information and visual information to learn a better visual feature representation in the training stage. During the test stage, instead of using both audio and visual signals, only the videos are used for generating the missing audio feature, and both the given visual and given audio features are used to obtain a joint representation. We carried out our experiments on a large scale audio-visual data corpus, and experimental results show that our proposed techniques outperforms the performance of the hadncrafted features and features learned by other commonly used deep learning techniques.
count=1
* Automatic Concept Discovery From Parallel Text and Visual Corpora
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Sun_Automatic_Concept_Discovery_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Sun_Automatic_Concept_Discovery_ICCV_2015_paper.pdf)]
    * Title: Automatic Concept Discovery From Parallel Text and Visual Corpora
    * Year: `2015`
    * Authors: Chen Sun, Chuang Gan, Ram Nevatia
    * Abstract: Humans connect language and vision to perceive the world. How to build a similar connection for computers? One possible way is via visual concepts, which are text terms that relate to visually discriminative entities. We propose an automatic visual concept discovery algorithm using parallel text and visual corpora; it filters text terms based on the visual discriminative power of the associated images, and groups them into concepts using visual and semantic similarities. We illustrate the applications of the discovered concepts using bidirectional image and sentence retrieval task and image tagging task, and show that the discovered concepts not only outperform several large sets of manually selected concepts significantly, but also achieves the state-of-the-art performance in the retrieval task.
count=1
* Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Tang_Opening_the_Black_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Tang_Opening_the_Black_ICCV_2015_paper.pdf)]
    * Title: Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose
    * Year: `2015`
    * Authors: Danhang Tang, Jonathan Taylor, Pushmeet Kohli, Cem Keskin, Tae-Kyun Kim, Jamie Shotton
    * Abstract: We address the problem of hand pose estimation, formulated as an inverse problem. Typical approaches optimize an energy function over pose parameters using a `black box' image generation procedure. This procedure knows little about either the relationships between the parameters or the form of the energy function. In this paper, we show that we can significantly improving upon black box optimization by exploiting high-level knowledge of the structure of the parameters and using a local surrogate energy function. Our new framework, hierarchical sampling optimization, consists of a sequence of predictors organized into a kinematic hierarchy. Each predictor is conditioned on its ancestors, and generates a set of samples over a subset of the pose parameters. The highly-efficient surrogate energy is used to select among samples. Having evaluated the full hierarchy, the partial pose samples are concatenated to generate a full-pose hypothesis. Several hypotheses are generated using the same procedure, and finally the original full energy function selects the best result. Experimental evaluation on three publically available datasets show that our method is particularly impressive in low-compute scenarios where it significantly outperforms all other state-of-the-art methods.
count=1
* Text Flow: A Unified Text Detection System in Natural Scene Images
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Tian_Text_Flow_A_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Tian_Text_Flow_A_ICCV_2015_paper.pdf)]
    * Title: Text Flow: A Unified Text Detection System in Natural Scene Images
    * Year: `2015`
    * Authors: Shangxuan Tian, Yifeng Pan, Chang Huang, Shijian Lu, Kai Yu, Chew Lim Tan
    * Abstract: The prevalent scene text detection approach follows four sequential steps comprising character candidate detection, false character candidate removal, text line extraction, and text line verification. However, errors occur and accumulate throughout each of these sequential steps which often lead to low detection performance. To address these issues, we propose a unified scene text detection system, namely Text Flow, by utilizing the minimum cost (min-cost) flow network model. With character candidates detected by cascade boosting, the min-cost flow network model integrates the last three sequential steps into a single process which solves the error accumulation problem at both character level and text line level effectively. The proposed technique has been tested on three public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset and a multilingual dataset and it outperforms the state-of-the-art methods on all three datasets with much higher recall and F-score. The good performance on the multilingual dataset shows that the proposed technique can be used for the detection of texts in different languages.
count=1
* Learning Visual Clothing Style With Heterogeneous Dyadic Co-Occurrences
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Veit_Learning_Visual_Clothing_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Veit_Learning_Visual_Clothing_ICCV_2015_paper.pdf)]
    * Title: Learning Visual Clothing Style With Heterogeneous Dyadic Co-Occurrences
    * Year: `2015`
    * Authors: Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, Serge Belongie
    * Abstract: With the rapid proliferation of smart mobile devices, users now take millions of photos every day. These include large numbers of clothing and accessory images. We would like to answer questions like `What outfit goes well with this pair of shoes?' To answer these types of questions, one has to go beyond learning visual similarity and learn a visual notion of compatibility across categories. In this paper, we propose a novel learning framework to help answer these types of questions. The main idea of this framework is to learn a feature transformation from images of items into a latent space that expresses compatibility. For the feature transformation, we use a Siamese Convolutional Neural Network (CNN) architecture, where training examples are pairs of items that are either compatible or incompatible. We model compatibility based on co-occurrence in large-scale user behavior data; in particular co-purchase data from Amazon.com. To learn cross-category fit, we introduce a strategic method to sample training data, where pairs of items are heterogeneous dyads, i.e., the two elements of a pair belong to different high-level categories. While this approach is applicable to a wide variety of settings, we focus on the representative problem of learning compatible clothing style. Our results indicate that the proposed framework is capable of learning semantic information about visual style and is able to generate outfits of clothes, with items from different categories, that go well together.
count=1
* Adaptively Unified Semi-Supervised Dictionary Learning With Active Points
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Wang_Adaptively_Unified_Semi-Supervised_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Adaptively_Unified_Semi-Supervised_ICCV_2015_paper.pdf)]
    * Title: Adaptively Unified Semi-Supervised Dictionary Learning With Active Points
    * Year: `2015`
    * Authors: Xiaobo Wang, Xiaojie Guo, Stan Z. Li
    * Abstract: Semi-supervised dictionary learning aims to construct a dictionary by utilizing both labeled and unlabeled data. To enhance the discriminative capability of the learned dictionary, numerous discriminative terms have been proposed by evaluating either the prediction loss or the class separation criterion on the coding vectors of labeled data, but with rare consideration of the power of the coding vectors corresponding to unlabeled data. In this paper, we present a novel semi-supervised dictionary learning method, which uses the informative coding vectors of both labeled and unlabeled data, and adaptively emphasizes the high confidence coding vectors of unlabeled data to enhance the dictionary discriminative capability simultaneously. By doing so, we integrate the discrimination of dictionary, the induction of classifier to new testing data and the transduction of labels to unlabeled data into a unified framework. To solve the proposed problem, an effective iterative algorithm is designed. Experimental results on a series of benchmark databases show that our method outperforms other state-of-the-art dictionary learning methods in most cases.
count=1
* MMSS: Multi-Modal Sharable and Specific Feature Learning for RGB-D Object Recognition
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Wang_MMSS_Multi-Modal_Sharable_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Wang_MMSS_Multi-Modal_Sharable_ICCV_2015_paper.pdf)]
    * Title: MMSS: Multi-Modal Sharable and Specific Feature Learning for RGB-D Object Recognition
    * Year: `2015`
    * Authors: Anran Wang, Jianfei Cai, Jiwen Lu, Tat-Jen Cham
    * Abstract: Most of the feature-learning methods for RGB-D object recognition either learn features from color and depth modalities separately, or simply treat RGB-D as undifferentiated four-channel data, which cannot adequately exploit the relationship between different modalities. Motivated by the intuition that different modalities should contain not only some modal-specific patterns but also some shared common patterns, we propose a multi-modal feature learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, and then connect them with our carefully designed multi-modal layers, which fuse color and depth information by enforcing a common part to be shared by features of different modalities. In this way, we obtain features reflecting shared properties as well as modal-specific properties in different modalities. The information of the multi-modal learning frameworks is back-propagated to the early CNN layers. Experimental results show that our proposed multi-modal feature learning method outperforms state-of-the-art approaches on two widely used RGB-D object benchmark datasets.
count=1
* Understanding and Diagnosing Visual Tracking Systems
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Wang_Understanding_and_Diagnosing_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Wang_Understanding_and_Diagnosing_ICCV_2015_paper.pdf)]
    * Title: Understanding and Diagnosing Visual Tracking Systems
    * Year: `2015`
    * Authors: Naiyan Wang, Jianping Shi, Dit-Yan Yeung, Jiaya Jia
    * Abstract: Several benchmark datasets for visual tracking research have been created in recent years. Despite their usefulness, whether they are sufficient for understanding and diagnosing the strengths and weaknesses of different trackers remains questionable. To address this issue, we propose a framework by breaking a tracker down into five constituent parts, namely, motion model, feature extractor, observation model, model updater, and ensemble post-processor. We then conduct ablative experiments on each component to study how it affects the overall result. Surprisingly, our findings are discrepant with some common beliefs in the visual tracking research community. We find that the feature extractor plays the most important role in a tracker. On the other hand, although the observation model is the focus of many studies, we find that it often brings no significant improvement. Moreover, the motion model and model updater contain many details that could affect the result. Also, the ensemble post-processor can improve the result substantially when the constituent trackers have high diversity. Based on our findings, we put together some very elementary building blocks to give a basic tracker which is competitive in performance to the state-of-the-art trackers. We believe our framework can provide a solid baseline when conducting controlled experiments for visual tracking research.
count=1
* Learning Concept Embeddings With Combined Human-Machine Expertise
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Wilber_Learning_Concept_Embeddings_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Wilber_Learning_Concept_Embeddings_ICCV_2015_paper.pdf)]
    * Title: Learning Concept Embeddings With Combined Human-Machine Expertise
    * Year: `2015`
    * Authors: Michael Wilber, Iljung S. Kwak, David Kriegman, Serge Belongie
    * Abstract: This paper presents our work on "SNaCK," a low-dimensional concept embedding algorithm that combines human expertise with automatic machine similarity kernels. Both parts are complimentary: human insight can capture relationships that are not apparent from the object's visual similarity and the machine can help relieve the human from having to exhaustively specify many constraints. We show that our SNaCK embeddings are useful in several tasks: distinguishing prime and nonprime numbers on MNIST, discovering labeling mistakes in the Caltech UCSD Birds (CUB) dataset with the help of deep-learned features, creating training datasets for bird classifiers, capturing subjective human taste on a new dataset of 10,000 foods, and qualitatively exploring an unstructured set of pictographic characters. Comparisons with the state-of-the-art in these tasks show that SNaCK produces better concept embeddings that require less human supervision than the leading methods.
count=1
* Robust Facial Landmark Detection Under Significant Head Poses and Occlusion
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Wu_Robust_Facial_Landmark_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Wu_Robust_Facial_Landmark_ICCV_2015_paper.pdf)]
    * Title: Robust Facial Landmark Detection Under Significant Head Poses and Occlusion
    * Year: `2015`
    * Authors: Yue Wu, Qiang Ji
    * Abstract: There have been tremendous improvements for facial landmark detection on general "in-the-wild" images. However, it is still challenging to detect the facial landmarks on images with severe occlusion and images with large head poses (e.g. profile face). In fact, the existing algorithms usually can only handle one of them. In this work, we propose a unified robust cascade regression framework that can handle both images with severe occlusion and images with large head poses. Specifically, the method iteratively predicts the landmark occlusions and the landmark locations. For occlusion estimation, instead of directly predicting the binary occlusion vectors, we introduce a supervised regression method that gradually updates the landmark visibility probabilities in each iteration to achieve robustness. In addition, we explicitly add occlusion pattern as a constraint to improve the performance of occlusion prediction. For landmark detection, we combine the landmark visibility probabilities, the local appearances, and the local shapes to iteratively update their positions. The experimental results show that the proposed method is significantly better than state-of-the-art works on images with severe occlusion and images with large head poses. It is also comparable to other methods on general "in-the-wild" images.
count=1
* Conditional Convolutional Neural Network for Modality-Aware Face Recognition
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Xiong_Conditional_Convolutional_Neural_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Xiong_Conditional_Convolutional_Neural_ICCV_2015_paper.pdf)]
    * Title: Conditional Convolutional Neural Network for Modality-Aware Face Recognition
    * Year: `2015`
    * Authors: Chao Xiong, Xiaowei Zhao, Danhang Tang, Karlekar Jayashree, Shuicheng Yan, Tae-Kyun Kim
    * Abstract: Faces in the wild are usually captured with various poses, illuminations and occlusions, and thus inherently multimodally distributed in many tasks. We propose a conditional Convolutional Neural Network, named as c-CNN, to handle multimodal face recognition. Different from traditional CNN that adopts fixed convolution kernels, samples in c-CNN are processed with dynamically activated sets of kernels. In particular, convolution kernels within each layer are only sparsely activated when a sample is passed through the network. For a given sample, the activations of convolution kernels in a certain layer are conditioned on its present intermediate representation and the activation status in the lower layers. The activated kernels across layers define the sample-specific adaptive routes that reveal the distribution of underlying modalities. Consequently, the proposed framework does not rely on any prior knowledge of modalities in contrast with most existing methods. To substantiate the generic framework, we introduce a special case of c-CNN via incorporating the conditional routing of the decision tree, which is evaluated with two problems of multimodality - multi-view face identification and occluded face verification. Extensive experiments demonstrate consistent improvements over the counterparts unaware of modalities.
count=1
* Zero-Shot Learning via Semantic Similarity Embedding
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Zhang_Zero-Shot_Learning_via_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Zhang_Zero-Shot_Learning_via_ICCV_2015_paper.pdf)]
    * Title: Zero-Shot Learning via Semantic Similarity Embedding
    * Year: `2015`
    * Authors: Ziming Zhang, Venkatesh Saligrama
    * Abstract: In this paper we consider a version of the zero-shot learning problem where seen class source and target domain data are provided. The goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information (e.g. attributes) for unseen classes. Our method is based on viewing each source or target data as a mixture of seen class proportions and we postulate that the mixture patterns have to be similar if the two instances belong to the same unseen class. This perspective leads us to learning source/target embedding functions that map an arbitrary source/target domain data into a same semantic space where similarity can be readily measured. We develop a max-margin framework to learn these similarity functions and jointly optimize parameters by means of cross validation. Our test results are compelling, leading to significant improvement in terms of accuracy on most benchmark datasets for zero-shot recognition.
count=1
* Conditional Random Fields as Recurrent Neural Networks
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2015/html/Zheng_Conditional_Random_Fields_ICCV_2015_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf)]
    * Title: Conditional Random Fields as Recurrent Neural Networks
    * Year: `2015`
    * Authors: Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr
    * Abstract: Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.
count=1
* Constructing Adaptive Complex Cells for Robust Visual Tracking
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Chen_Constructing_Adaptive_Complex_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Chen_Constructing_Adaptive_Complex_2013_ICCV_paper.pdf)]
    * Title: Constructing Adaptive Complex Cells for Robust Visual Tracking
    * Year: `2013`
    * Authors: Dapeng Chen, Zejian Yuan, Yang Wu, Geng Zhang, Nanning Zheng
    * Abstract: Representation is a fundamental problem in object tracking. Conventional methods track the target by describing its local or global appearance. In this paper we present that, besides the two paradigms, the composition of local region histograms can also provide diverse and important object cues. We use cells to extract local appearance, and construct complex cells to integrate the information from cells. With different spatial arrangements of cells, complex cells can explore various contextual information at multiple scales, which is important to improve the tracking performance. We also develop a novel template-matching algorithm for object tracking, where the template is composed of temporal varying cells and has two layers to capture the target and background appearance respectively. An adaptive weight is associated with each complex cell to cope with occlusion as well as appearance variation. A fusion weight is associated with each complex cell type to preserve the global distinctiveness. Our algorithm is evaluated on 25 challenging sequences, and the results not only confirm the contribution of each component in our tracking system, but also outperform other competing trackers.
count=1
* Learning a Dictionary of Shape Epitomes with Applications to Image Labeling
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Chen_Learning_a_Dictionary_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Chen_Learning_a_Dictionary_2013_ICCV_paper.pdf)]
    * Title: Learning a Dictionary of Shape Epitomes with Applications to Image Labeling
    * Year: `2013`
    * Authors: Liang-Chieh Chen, George Papandreou, Alan L. Yuille
    * Abstract: The first main contribution of this paper is a novel method for representing images based on a dictionary of shape epitomes. These shape epitomes represent the local edge structure of the image and include hidden variables to encode shift and rotations. They are learnt in an unsupervised manner from groundtruth edges. This dictionary is compact but is also able to capture the typical shapes of edges in natural images. In this paper, we illustrate the shape epitomes by applying them to the image labeling task. In other work, described in the supplementary material, we apply them to edge detection and image modeling. We apply shape epitomes to image labeling by using Conditional Random Field (CRF) Models. They are alternatives to the superpixel or pixel representations used in most CRFs. In our approach, the shape of an image patch is encoded by a shape epitome from the dictionary. Unlike the superpixel representation, our method avoids making early decisions which cannot be reversed. Our resulting hierarchical CRFs efficiently capture both local and global class co-occurrence properties. We demonstrate its quantitative and qualitative properties of our approach with image labeling experiments on two standard datasets: MSRC-21 and Stanford Background.
count=1
* Example-Based Facade Texture Synthesis
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Dai_Example-Based_Facade_Texture_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Dai_Example-Based_Facade_Texture_2013_ICCV_paper.pdf)]
    * Title: Example-Based Facade Texture Synthesis
    * Year: `2013`
    * Authors: Dengxin Dai, Hayko Riemenschneider, Gerhard Schmitt, Luc Van Gool
    * Abstract: There is an increased interest in the efficient creation of city models, be it virtual or as-built. We present a method for synthesizing complex, photo-realistic facade images, from a single example. After parsing the example image into its semantic components, a tiling for it is generated. Novel tilings can then be created, yielding facade textures with different dimensions or with occluded parts inpainted. A genetic algorithm guides the novel facades as well as inpainted parts to be consistent with the example, both in terms of their overall structure and their detailed textures. Promising results for multiple standard datasets in particular for the different building styles they contain demonstrate the potential of the method.
count=1
* Supervised Binary Hash Code Learning with Jensen Shannon Divergence
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Fan_Supervised_Binary_Hash_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Fan_Supervised_Binary_Hash_2013_ICCV_paper.pdf)]
    * Title: Supervised Binary Hash Code Learning with Jensen Shannon Divergence
    * Year: `2013`
    * Authors: Lixin Fan
    * Abstract: This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.
count=1
* Data-Driven 3D Primitives for Single Image Understanding
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Fouhey_Data-Driven_3D_Primitives_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Fouhey_Data-Driven_3D_Primitives_2013_ICCV_paper.pdf)]
    * Title: Data-Driven 3D Primitives for Single Image Understanding
    * Year: `2013`
    * Authors: David F. Fouhey, Abhinav Gupta, Martial Hebert
    * Abstract: What primitives should we use to infer the rich 3D world behind an image? We argue that these primitives should be both visually discriminative and geometrically informative and we present a technique for discovering such primitives. We demonstrate the utility of our primitives by using them to infer 3D surface normals given a single image. Our technique substantially outperforms the state-of-the-art and shows improved cross-dataset performance.
count=1
* Hidden Factor Analysis for Age Invariant Face Recognition
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Gong_Hidden_Factor_Analysis_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Gong_Hidden_Factor_Analysis_2013_ICCV_paper.pdf)]
    * Title: Hidden Factor Analysis for Age Invariant Face Recognition
    * Year: `2013`
    * Authors: Dihong Gong, Zhifeng Li, Dahua Lin, Jianzhuang Liu, Xiaoou Tang
    * Abstract: Age invariant face recognition has received increasing attention due to its great potential in real world applications. In spite of the great progress in face recognition techniques, reliably recognizing faces across ages remains a difficult task. The facial appearance of a person changes substantially over time, resulting in significant intra-class variations. Hence, the key to tackle this problem is to separate the variation caused by aging from the person-specific features that are stable. Specifically, we propose a new method, called Hidden Factor Analysis (HFA). This method captures the intuition above through a probabilistic model with two latent factors: an identity factor that is age-invariant and an age factor affected by the aging process. Then, the observed appearance can be modeled as a combination of the components generated based on these factors. We also develop a learning algorithm that jointly estimates the latent factors and the model parameters using an EM procedure. Extensive experiments on two well-known public domain face aging datasets: MORPH (the largest public face aging database) and FGNET, clearly show that the proposed method achieves notable improvement over state-of-the-art algorithms.
count=1
* Tracking via Robust Multi-task Multi-view Joint Sparse Representation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Hong_Tracking_via_Robust_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Hong_Tracking_via_Robust_2013_ICCV_paper.pdf)]
    * Title: Tracking via Robust Multi-task Multi-view Joint Sparse Representation
    * Year: `2013`
    * Authors: Zhibin Hong, Xue Mei, Danil Prokhorov, Dacheng Tao
    * Abstract: Combining multiple observation views has proven beneficial for tracking. In this paper, we cast tracking as a novel multi-task multi-view sparse learning problem and exploit the cues from multiple views including various types of visual features, such as intensity, color, and edge, where each feature observation can be sparsely represented by a linear combination of atoms from an adaptive feature dictionary. The proposed method is integrated in a particle filter framework where every view in each particle is regarded as an individual task. We jointly consider the underlying relationship between tasks across different views and different particles, and tackle it in a unified robust multi-task formulation. In addition, to capture the frequently emerging outlier tasks, we decompose the representation matrix to two collaborative components which enable a more robust and accurate approximation. We show that the proposed formulation can be efficiently solved using the Accelerated Proximal Gradient method with a small number of closed-form updates. The presented tracker is implemented using four types of features and is tested on numerous benchmark video sequences. Both the qualitative and quantitative results demonstrate the superior performance of the proposed approach compared to several stateof-the-art trackers.
count=1
* Recognising Human-Object Interaction via Exemplar Based Modelling
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Hu_Recognising_Human-Object_Interaction_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Hu_Recognising_Human-Object_Interaction_2013_ICCV_paper.pdf)]
    * Title: Recognising Human-Object Interaction via Exemplar Based Modelling
    * Year: `2013`
    * Authors: Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, Shaogang Gong, Tao Xiang
    * Abstract: Human action can be recognised from a single still image by modelling Human-object interaction (HOI), which infers the mutual spatial structure information between human and object as well as their appearance. Existing approaches rely heavily on accurate detection of human and object, and estimation of human pose. They are thus sensitive to large variations of human poses, occlusion and unsatisfactory detection of small size objects. To overcome this limitation, a novel exemplar based approach is proposed in this work. Our approach learns a set of spatial pose-object interaction exemplars, which are density functions describing how a person is interacting with a manipulated object for different activities spatially in a probabilistic way. A representation based on our HOI exemplar thus has great potential for being robust to the errors in human/object detection and pose estimation. A new framework consists of a proposed exemplar based HOI descriptor and an activity specific matching model that learns the parameters is formulated for robust human activity recognition. Experiments on two benchmark activity datasets demonstrate that the proposed approach obtains state-ofthe-art performance.
count=1
* Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Leordeanu_Locally_Affine_Sparse-to-Dense_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Leordeanu_Locally_Affine_Sparse-to-Dense_2013_ICCV_paper.pdf)]
    * Title: Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation
    * Year: `2013`
    * Authors: Marius Leordeanu, Andrei Zanfir, Cristian Sminchisescu
    * Abstract: Estimating a dense correspondence field between successive video frames, under large displacement, is important in many visual learning and recognition tasks. We propose a novel sparse-to-dense matching method for motion field estimation and occlusion detection. As an alternative to the current coarse-to-fine approaches from the optical flow literature, we start from the higher level of sparse matching with rich appearance and geometric constraints collected over extended neighborhoods, using an occlusion aware, locally affine model. Then, we move towards the simpler, but denser classic flow field model, with an interpolation procedure that offers a natural transition between the sparse and the dense correspondence fields. We experimentally demonstrate that our appearance features and our complex geometric constraints permit the correct motion estimation even in difficult cases of large displacements and significant appearance changes. We also propose a novel classification method for occlusion detection that works in conjunction with the sparse-to-dense matching model. We validate our approach on the newly released Sintel dataset and obtain state-of-the-art results.
count=1
* A Deep Sum-Product Architecture for Robust Facial Attributes Analysis
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Luo_A_Deep_Sum-Product_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Luo_A_Deep_Sum-Product_2013_ICCV_paper.pdf)]
    * Title: A Deep Sum-Product Architecture for Robust Facial Attributes Analysis
    * Year: `2013`
    * Authors: Ping Luo, Xiaogang Wang, Xiaoou Tang
    * Abstract: Recent works have shown that facial attributes are useful in a number of applications such as face recognition and retrieval. However, estimating attributes in images with large variations remains a big challenge. This challenge is addressed in this paper. Unlike existing methods that assume the independence of attributes during their estimation, our approach captures the interdependencies of local regions for each attribute, as well as the high-order correlations between different attributes, which makes it more robust to occlusions and misdetection of face regions. First, we have modeled region interdependencies with a discriminative decision tree, where each node consists of a detector and a classifier trained on a local region. The detector allows us to locate the region, while the classifier determines the presence or absence of an attribute. Second, correlations of attributes and attribute predictors are modeled by organizing all of the decision trees into a large sum-product network (SPN), which is learned by the EM algorithm and yields the most probable explanation (MPE) of the facial attributes in terms of the region's localization and classification. Experimental results on a large data set with 22, 400 images show the effectiveness of the proposed approach.
count=1
* Proportion Priors for Image Sequence Segmentation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Nieuwenhuis_Proportion_Priors_for_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Nieuwenhuis_Proportion_Priors_for_2013_ICCV_paper.pdf)]
    * Title: Proportion Priors for Image Sequence Segmentation
    * Year: `2013`
    * Authors: Claudia Nieuwenhuis, Evgeny Strekalovskiy, Daniel Cremers
    * Abstract: We propose a convex multilabel framework for image sequence segmentation which allows to impose proportion priors on object parts in order to preserve their size ratios across multiple images. The key idea is that for strongly deformable objects such as a gymnast the size ratio of respective regions (head versus torso, legs versus full body, etc.) is typically preserved. We propose different ways to impose such priors in a Bayesian framework for image segmentation. We show that near-optimal solutions can be computed using convex relaxation techniques. Extensive qualitative and quantitative evaluations demonstrate that the proportion priors allow for highly accurate segmentations, avoiding seeping-out of regions and preserving semantically relevant small-scale structures such as hands or feet. They naturally apply to multiple object instances such as players in sports scenes, and they can relate different objects instead of object parts, e.g. organs in medical imaging. The algorithm is efficient and easily parallelized leading to proportion-consistent segmentations at runtimes around one second.
count=1
* Joint Deep Learning for Pedestrian Detection
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Ouyang_Joint_Deep_Learning_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Ouyang_Joint_Deep_Learning_2013_ICCV_paper.pdf)]
    * Title: Joint Deep Learning for Pedestrian Detection
    * Year: `2013`
    * Authors: Wanli Ouyang, Xiaogang Wang
    * Abstract: Feature extraction, deformation handling, occlusion handling, and classi?cation are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture 1 . By establishing automatic, mutual interaction among components, the deep model achieves a 9% reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset.
count=1
* Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/San_Biagio_Heterogeneous_Auto-similarities_of_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/San_Biagio_Heterogeneous_Auto-similarities_of_2013_ICCV_paper.pdf)]
    * Title: Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification
    * Year: `2013`
    * Authors: Marco San Biagio, Marco Crocco, Marco Cristani, Samuele Martelli, Vittorio Murino
    * Abstract: Capturing the essential characteristics of visual objects by considering how their features are inter-related is a recent philosophy of object classification. In this paper, we embed this principle in a novel image descriptor, dubbed Heterogeneous Auto-Similarities of Characteristics (HASC). HASC is applied to heterogeneous dense features maps, encoding linear relations by covariances and nonlinear associations through information-theoretic measures such as mutual information and entropy. In this way, highly complex structural information can be expressed in a compact, scale invariant and robust manner. The effectiveness of HASC is tested on many diverse detection and classification scenarios, considering objects, textures and pedestrians, on widely known benchmarks (Caltech-101, Brodatz, Daimler Multi-Cue). In all the cases, the results obtained with standard classifiers demonstrate the superiority of HASC with respect to the most adopted local feature descriptors nowadays, such as SIFT, HOG, LBP and feature covariances. In addition, HASC sets the state-of-the-art on the Brodatz texture dataset and the Daimler Multi-Cue pedestrian dataset, without exploiting ad-hoc sophisticated classifiers.
count=1
* Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Straehle_Weakly_Supervised_Learning_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Straehle_Weakly_Supervised_Learning_2013_ICCV_paper.pdf)]
    * Title: Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria
    * Year: `2013`
    * Authors: Christoph Straehle, Ullrich Koethe, Fred A. Hamprecht
    * Abstract: We propose a scheme that allows to partition an image into a previously unknown number of segments, using only minimal supervision in terms of a few must-link and cannotlink annotations. We make no use of regional data terms, learning instead what constitutes a likely boundary between segments. Since boundaries are only implicitly specified through cannot-link constraints, this is a hard and nonconvex latent variable problem. We address this problem in a greedy fashion using a randomized decision tree on features associated with interpixel edges. We use a structured purity criterion during tree construction and also show how a backtracking strategy can be used to prevent the greedy search from ending up in poor local optima. The proposed strategy is compared with prior art on natural images.
count=1
* Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Tian_Hierarchical_Data-Driven_Descent_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Tian_Hierarchical_Data-Driven_Descent_2013_ICCV_paper.pdf)]
    * Title: Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation
    * Year: `2013`
    * Authors: Yuandong Tian, Srinivasa G. Narasimhan
    * Abstract: Real-world surfaces such as clothing, water and human body deform in complex ways. The image distortions observed are high-dimensional and non-linear, making it hard to estimate these deformations accurately. The recent datadriven descent approach [17] applies Nearest Neighbor estimators iteratively on a particular distribution of training samples to obtain a globally optimal and dense deformation field between a template and a distorted image. In this work, we develop a hierarchical structure for the Nearest Neighbor estimators, each of which can have only a local image support. We demonstrate in both theory and practice that this algorithm has several advantages over the nonhierarchical version: it guarantees global optimality with significantly fewer training samples, is several orders faster, provides a metric to decide whether a given image is "hard" (or "easy") requiring more (or less) samples, and can handle more complex scenes that include both global motion and local deformation. The proposed algorithm successfully tracks a broad range of non-rigid scenes including water, clothing, and medical images, and compares favorably against several other deformation estimation and tracking approaches that do not provide optimality guarantees.
count=1
* Image Co-segmentation via Consistent Functional Maps
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Wang_Image_Co-segmentation_via_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Wang_Image_Co-segmentation_via_2013_ICCV_paper.pdf)]
    * Title: Image Co-segmentation via Consistent Functional Maps
    * Year: `2013`
    * Authors: Fan Wang, Qixing Huang, Leonidas J. Guibas
    * Abstract: Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. This formulation allows us to extract segmentations even with no training data, but can also exploit such data when available. The collective effect of the joint processing using functional maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets.
count=1
* Pose-Configurable Generic Tracking of Elongated Objects
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Wesierski_Pose-Configurable_Generic_Tracking_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Wesierski_Pose-Configurable_Generic_Tracking_2013_ICCV_paper.pdf)]
    * Title: Pose-Configurable Generic Tracking of Elongated Objects
    * Year: `2013`
    * Authors: Daniel Wesierski, Patrick Horain
    * Abstract: Elongated objects have various shapes and can shift, rotate, change scale, and be rigid or deform by flexing, articulating, and vibrating, with examples as varied as a glass bottle, a robotic arm, a surgical suture, a finger pair, a tram, and a guitar string. This generally makes tracking of poses of elongated objects very challenging. We describe a unified, configurable framework for tracking the pose of elongated objects, which move in the image plane and extend over the image region. Our method strives for simplicity, versatility, and efficiency. The object is decomposed into a chained assembly of segments of multiple parts that are arranged under a hierarchy of tailored spatio-temporal constraints. In this hierarchy, segments can rescale independently while their elasticity is controlled with global orientations and local distances. While the trend in tracking is to design complex, structure-free algorithms that update object appearance online, we show that our tracker, with the novel but remarkably simple, structured organization of parts with constant appearance, reaches or improves state-of-the-art performance. Most importantly, our model can be easily configured to track exact pose of arbitrary, elongated objects in the image plane. The tracker can run up to 100 fps on a desktop PC, yet the computation time scales linearly with the number of object parts. To our knowledge, this is the first approach to generic tracking of elongated objects.
count=1
* Hierarchical Part Matching for Fine-Grained Visual Categorization
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Xie_Hierarchical_Part_Matching_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Xie_Hierarchical_Part_Matching_2013_ICCV_paper.pdf)]
    * Title: Hierarchical Part Matching for Fine-Grained Visual Categorization
    * Year: `2013`
    * Authors: Lingxi Xie, Qi Tian, Richang Hong, Shuicheng Yan, Bo Zhang
    * Abstract: As a special topic in computer vision, fine-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classification tasks in which objects have large inter-class variation, the visual concepts in the fine-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difficult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model. In this paper, we propose a powerful flowchart named Hierarchical Part Matching (HPM) to cope with finegrained classification tasks. We extend the Bag-of-Features (BoF) model by introducing several novel modules to integrate into image representation, including foreground inference and segmentation, Hierarchical Structure Learning (HSL), and Geometric Phrase Pooling (GPP). We verify in experiments that our algorithm achieves the state-ofthe-art classification accuracy in the Caltech-UCSD-Birds200-2011 dataset by making full use of the ground-truth part annotations.
count=1
* Cascaded Shape Space Pruning for Robust Facial Landmark Detection
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Zhao_Cascaded_Shape_Space_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Zhao_Cascaded_Shape_Space_2013_ICCV_paper.pdf)]
    * Title: Cascaded Shape Space Pruning for Robust Facial Landmark Detection
    * Year: `2013`
    * Authors: Xiaowei Zhao, Shiguang Shan, Xiujuan Chai, Xilin Chen
    * Abstract: In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.
count=1
* Exemplar-Based Graph Matching for Robust Facial Landmark Localization
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Zhou_Exemplar-Based_Graph_Matching_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Zhou_Exemplar-Based_Graph_Matching_2013_ICCV_paper.pdf)]
    * Title: Exemplar-Based Graph Matching for Robust Facial Landmark Localization
    * Year: `2013`
    * Authors: Feng Zhou, Jonathan Brandt, Zhe Lin
    * Abstract: Localizing facial landmarks is a fundamental step in facial image analysis. However, the problem is still challenging due to the large variability in pose and appearance, and the existence of occlusions in real-world face images. In this paper, we present exemplar-based graph matching (EGM), a robust framework for facial landmark localization. Compared to conventional algorithms, EGM has three advantages: (1) an affine-invariant shape constraint is learned online from similar exemplars to better adapt to the test face; (2) the optimal landmark configuration can be directly obtained by solving a graph matching problem with the learned shape constraint; (3) the graph matching problem can be optimized efficiently by linear programming. To our best knowledge, this is the first attempt to apply a graph matching technique for facial landmark localization. Experiments on several challenging datasets demonstrate the advantages of EGM over state-of-the-art methods.
count=1
* Video Synopsis by Heterogeneous Multi-source Correlation
    [[abs-ICCV](https://openaccess.thecvf.com/content_iccv_2013/html/Zhu_Video_Synopsis_by_2013_ICCV_paper.html)]
    [[pdf-ICCV](https://openaccess.thecvf.com/content_iccv_2013/papers/Zhu_Video_Synopsis_by_2013_ICCV_paper.pdf)]
    * Title: Video Synopsis by Heterogeneous Multi-source Correlation
    * Year: `2013`
    * Authors: Xiatian Zhu, Chen Change Loy, Shaogang Gong
    * Abstract: Generating coherent synopsis for surveillance video stream remains a formidable challenge due to the ambiguity and uncertainty inherent to visual observations. In contrast to existing video synopsis approaches that rely on visual cues alone, we propose a novel multi-source synopsis framework capable of correlating visual data and independent non-visual auxiliary information to better describe and summarise subtle physical events in complex scenes. Specifically, our unsupervised framework is capable of seamlessly uncovering latent correlations among heterogeneous types of data sources, despite the non-trivial heteroscedasticity and dimensionality discrepancy problems. Additionally, the proposed model is robust to partial or missing non-visual information. We demonstrate the effectiveness of our framework on two crowded public surveillance datasets.
count=1
* Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/0001ca33ba34ce0351e4612b744b3936-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/0001ca33ba34ce0351e4612b744b3936-Paper-Conference.pdf)]
    * Title: Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder
    * Year: `2023`
    * Authors: Michael Bereket, Theofanis Karaletsos
    * Abstract: Generative models of observations under interventions have been a vibrant topic of interest across machine learning and the sciences in recent years. For example, in drug discovery, there is a need to model the effects of diverse interventions on cells in order to characterize unknown biological mechanisms of action. We propose the Sparse Additive Mechanism Shift Variational Autoencoder, SAMS-VAE, to combine compositionality, disentanglement, and interpretability for perturbation models. SAMS-VAE models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention effects. Crucially, SAMS-VAE sparsifies these global latent variables for individual perturbations to identify disentangled, perturbation-specific latent subspaces that are flexibly composable. We evaluate SAMS-VAE both quantitatively and qualitatively on a range of tasks using two popular single cell sequencing datasets.In order to measure perturbation-specific model-properties, we also introduce a framework for evaluation of perturbation models based on average treatment effects with links to posterior predictive checks. SAMS-VAE outperforms comparable models in terms of generalization across in-distribution and out-of-distribution tasks, including a combinatorial reasoning task under resource paucity, and yields interpretable latent structures which correlate strongly to known biological mechanisms. Our results suggest SAMS-VAE is an interesting addition to the modeling toolkit for machine learning-driven scientific discovery.
count=1
* Hyperbolic VAE via Latent Gaussian Distributions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/01ecd39ca49ddecc5729ca996304781b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/01ecd39ca49ddecc5729ca996304781b-Paper-Conference.pdf)]
    * Title: Hyperbolic VAE via Latent Gaussian Distributions
    * Year: `2023`
    * Authors: Seunghyuk Cho, Juyong Lee, Dongwoo Kim
    * Abstract: We propose a Gaussian manifold variational auto-encoder (GM-VAE) whose latent space consists of a set of Gaussian distributions. It is known that the set of the univariate Gaussian distributions with the Fisher information metric form a hyperbolic space, which we call a Gaussian manifold. To learn the VAE endowed with the Gaussian manifolds, we propose a pseudo-Gaussian manifold normal distribution based on the Kullback-Leibler divergence, a local approximation of the squared Fisher-Rao distance, to define a density over the latent space. We demonstrate the efficacy of GM-VAE on two different tasks: density estimation of image datasets and state representation learning for model-based reinforcement learning. GM-VAE outperforms the other variants of hyperbolic- and Euclidean-VAEs on density estimation tasks and shows competitive performance in model-based reinforcement learning. We observe that our model provides strong numerical stability, addressing a common limitation reported in previous hyperbolic-VAEs. The implementation is available at https://github.com/ml-postech/GM-VAE.
count=1
* Bounce: Reliable High-Dimensional Bayesian Optimization for Combinatorial and Mixed Spaces
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/05d2175de7ee637588d1b5ced8b15b32-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/05d2175de7ee637588d1b5ced8b15b32-Paper-Conference.pdf)]
    * Title: Bounce: Reliable High-Dimensional Bayesian Optimization for Combinatorial and Mixed Spaces
    * Year: `2023`
    * Authors: Leonard Papenmeier, Luigi Nardi, Matthias Poloczek
    * Abstract: Impactful applications such as materials discovery, hardware design, neural architecture search, or portfolio optimization require optimizing high-dimensional black-box functions with mixed and combinatorial input spaces.While Bayesian optimization has recently made significant progress in solving such problems, an in-depth analysis reveals that the current state-of-the-art methods are not reliable. Their performances degrade substantially when the unknown optima of the function do not have a certain structure. To fill the need for a reliable algorithm for combinatorial and mixed spaces, this paper proposes Bounce that relies on a novel map of various variable types into nested embeddings of increasing dimensionality.Comprehensive experiments show that Bounce reliably achieves and often even improves upon state-of-the-art performance on a variety of high-dimensional problems.
count=1
* A Path to Simpler Models Starts With Noise
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/0a49935d2b3d3342ca08d6db0adcfa34-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/0a49935d2b3d3342ca08d6db0adcfa34-Paper-Conference.pdf)]
    * Title: A Path to Simpler Models Starts With Noise
    * Year: `2023`
    * Authors: Lesia Semenova, Harry Chen, Ronald Parr, Cynthia Rudin
    * Abstract: The Rashomon set is the set of models that perform approximately equally well on a given dataset, and the Rashomon ratio is the fraction of all models in a given hypothesis space that are in the Rashomon set. Rashomon ratios are often large for tabular datasets in criminal justice, healthcare, lending, education, and in other areas, which has practical implications about whether simpler models can attain the same level of accuracy as more complex models. An open question is why Rashomon ratios often tend to be large. In this work, we propose and study a mechanism of the data generation process, coupled with choices usually made by the analyst during the learning process, that determines the size of the Rashomon ratio. Specifically, we demonstrate that noisier datasets lead to larger Rashomon ratios through the way that practitioners train models. Additionally, we introduce a measure called pattern diversity, which captures the average difference in predictions between distinct classification patterns in the Rashomon set, and motivate why it tends to increase with label noise. Our results explain a key aspect of why simpler models often tend to perform as well as black box models on complex, noisier datasets.
count=1
* Characterizing the Impacts of Semi-supervised Learning for Weak Supervision
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/0d6270381e018b3d83eb9be7d0b06036-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/0d6270381e018b3d83eb9be7d0b06036-Paper-Conference.pdf)]
    * Title: Characterizing the Impacts of Semi-supervised Learning for Weak Supervision
    * Year: `2023`
    * Authors: Jeffrey Li, Jieyu Zhang, Ludwig Schmidt, Alexander J. Ratner
    * Abstract: Labeling training data is a critical and expensive step in producing high accuracy ML models, whether training from scratch or fine-tuning. To make labeling more efficient, two major approaches are programmatic weak supervision (WS) and semi-supervised learning (SSL). More recent works have either explicitly or implicitly used techniques at their intersection, but in various complex and ad hoc ways. In this work, we define a simple, modular design space to study the use of SSL techniques for WS more systematically. Surprisingly, we find that fairly simple methods from our design space match the performance of more complex state-of-the-art methods, averaging a 3 p.p. increase in accuracy/F1-score across 8 standard WS benchmarks. Further, we provide practical guidance on when different components are worth their added complexity and training costs. Contrary to current understanding, we find using SSL is not necessary to obtain the best performance on most WS benchmarks but is more effective when: (1) end models are smaller, and (2) WS provides labels for only a small portion of training examples.
count=1
* On the Exploration of Local Significant Differences For Two-Sample Test
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/10fc83943b4540a9524af6fc67a23fef-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/10fc83943b4540a9524af6fc67a23fef-Paper-Conference.pdf)]
    * Title: On the Exploration of Local Significant Differences For Two-Sample Test
    * Year: `2023`
    * Authors: Zhijian Zhou, Jie Ni, Jia-He Yao, Wei Gao
    * Abstract: Recent years have witnessed increasing attentions on two-sample test with diverse real applications, while this work takes one more step on the exploration of local significant differences for two-sample test. We propose the ME$_\text{MaBiD}$, an effective test for two-sample testing, and the basic idea is to exploit local information by multiple Mahalanobis kernels and introduce bi-directional hypothesis for testing. On the exploration of local significant differences, we first partition the embedding space into several rectangle regions via a new splitting criterion, which is relevant to test power and data correlation. We then explore local significant differences based on our bi-directional masked $p$-value together with the ME$_\text{MaBiD}$ test. Theoretically, we present the asymptotic distribution and lower bounds of test power for our ME$_\text{MaBiD}$ test, and control the familywise error rate on the exploration of local significant differences. We finally conduct extensive experiments to validate the effectiveness of our proposed methods on two-sample test and the exploration of local significant differences.
count=1
* Neural Frailty Machine: Beyond proportional hazard assumption in neural survival regressions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/11a7f429d75f9f8c6e9c630aeb6524b5-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/11a7f429d75f9f8c6e9c630aeb6524b5-Paper-Conference.pdf)]
    * Title: Neural Frailty Machine: Beyond proportional hazard assumption in neural survival regressions
    * Year: `2023`
    * Authors: Ruofan Wu, Jiawei Qiao, Mingzhe Wu, Wen Yu, Ming Zheng, Tengfei LIU, Tianyi Zhang, Weiqiang Wang
    * Abstract: We present neural frailty machine (NFM), a powerful and flexible neural modeling framework for survival regressions. The NFM framework utilizes the classical idea of multiplicative frailty in survival analysis as a principled way of extending the proportional hazard assumption, at the same time being able to leverage the strong approximation power of neural architectures for handling nonlinear covariate dependence. Two concrete models are derived under the framework that extends neural proportional hazard models and nonparametric hazard regression models. Both models allow efficient training under the likelihood objective. Theoretically, for both proposed models, we establish statistical guarantees of neural function approximation with respect to nonparametric components via characterizing their rate of convergence. Empirically, we provide synthetic experiments that verify our theoretical statements. We also conduct experimental evaluations over $6$ benchmark datasets of different scales, showing that the proposed NFM models achieve predictive performance comparable to or sometimes surpassing state-of-the-art survival models. Our code is publicly availabel at https://github.com/Rorschach1989/nfm
count=1
* Efficient Activation Function Optimization through Surrogate Modeling
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/154d63285d3ed7826e7f026c0b350d69-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/154d63285d3ed7826e7f026c0b350d69-Paper-Conference.pdf)]
    * Title: Efficient Activation Function Optimization through Surrogate Modeling
    * Year: `2023`
    * Authors: Garrett Bingham, Risto Miikkulainen
    * Abstract: Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activation functions in several real-world tasks, with a surprising finding: a sigmoidal design that outperformed all other activation functions was discovered, challenging the status quo of always using rectifier nonlinearities in deep learning. Each of these steps is a contribution in its own right; together they serve as a practical and theoretical foundation for further research on activation function optimization.
count=1
* De novo Drug Design using Reinforcement Learning with Multiple GPT Agents
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/1737656c4dc65027939e47e4587ce95e-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/1737656c4dc65027939e47e4587ce95e-Paper-Conference.pdf)]
    * Title: De novo Drug Design using Reinforcement Learning with Multiple GPT Agents
    * Year: `2023`
    * Authors: Xiuyuan Hu, Guoqing Liu, Yang Zhao, Hao Zhang
    * Abstract: De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT.
count=1
* Necessary and Sufficient Conditions for Optimal Decision Trees using Dynamic Programming
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/1d5fce9627e15c84db572a66e029b1fc-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/1d5fce9627e15c84db572a66e029b1fc-Paper-Conference.pdf)]
    * Title: Necessary and Sufficient Conditions for Optimal Decision Trees using Dynamic Programming
    * Year: `2023`
    * Authors: Jacobus van der Linden, Mathijs de Weerdt, Emir Demirović
    * Abstract: Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue.Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees.We explore this relationship in detail and show the necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints.Experiments on five application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.
count=1
* Polyhedron Attention Module: Learning Adaptive-order Interactions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/1d83ad88759cef8192451543e5d59bf6-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/1d83ad88759cef8192451543e5d59bf6-Paper-Conference.pdf)]
    * Title: Polyhedron Attention Module: Learning Adaptive-order Interactions
    * Year: `2023`
    * Authors: Tan Zhu, Fei Dou, Xinyu Wang, Jin Lu, Jinbo Bi
    * Abstract: Learning feature interactions can be the key for multivariate predictive modeling. ReLU-activated neural networks create piecewise linear prediction models, and other nonlinear activation functions lead to models with only high-order feature interactions. Recent methods incorporate candidate polynomial terms of fixed orders into deep learning, which is subject to the issue of combinatorial explosion, or learn the orders that are difficult to adapt to different regions of the feature space. We propose a Polyhedron Attention Module (PAM) to create piecewise polynomial models where the input space is split into polyhedrons which define the different pieces and on each piece the hyperplanes that define the polyhedron boundary multiply to form the interactive terms, resulting in interactions of adaptive order to each piece. PAM is interpretable to identify important interactions in predicting a target. Theoretic analysis shows that PAM has stronger expression capability than ReLU-activated networks. Extensive experimental results demonstrate the superior classification performance of PAM on massive datasets of the click-through rate prediction and PAM can learn meaningful interaction effects in a medical problem.
count=1
* Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/1e58b1bf9f218fcd19e4539e982752a5-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/1e58b1bf9f218fcd19e4539e982752a5-Paper-Conference.pdf)]
    * Title: Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training
    * Year: `2023`
    * Authors: Rie Johnson, Tong Zhang
    * Abstract: As deep neural networks are highly expressive, it is important to find solutions with small generalization gap (the difference between the performance on the training data and unseen data). Focusing on the stochastic nature of training, we first present a theoretical analysis in which the bound of generalization gap depends on what we call inconsistency and instability of model outputs, which can be estimated on unlabeled data. Our empirical study based on this analysis shows that instability and inconsistency are strongly predictive of generalization gap in various settings. In particular, our finding indicates that inconsistency is a more reliable indicator of generalization gap than the sharpness of the loss landscape. Furthermore, we show that algorithmic reduction of inconsistency leads to superior performance. The results also provide a theoretical basis for existing methods such as co-distillation and ensemble.
count=1
* SHAP-IQ: Unified Approximation of any-order Shapley Interactions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/264f2e10479c9370972847e96107db7f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/264f2e10479c9370972847e96107db7f-Paper-Conference.pdf)]
    * Title: SHAP-IQ: Unified Approximation of any-order Shapley Interactions
    * Year: `2023`
    * Authors: Fabian Fumagalli, Maximilian Muschalik, Patrick Kolpaczki, Eyke Hüllermeier, Barbara Hammer
    * Abstract: Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature attributions for any black box model. Shapley interaction indices extend the SV to define any-order feature interactions. Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms. Moreover, each definition requires a specific approximation technique. Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e. interaction indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates. For the special case of SV, our approach reveals a novel representation of the SV and corresponds to Unbiased KernelSHAP with a greatly simplified calculation. We illustrate the computational efficiency and effectiveness by explaining language, image classification and high-dimensional synthetic models.
count=1
* Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/29962c2c9daf1fbd92530a7c958dfc2b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/29962c2c9daf1fbd92530a7c958dfc2b-Paper-Conference.pdf)]
    * Title: Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition
    * Year: `2023`
    * Authors: Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai Zheng, Mu Li, Alexander J. Smola, Xu Sun
    * Abstract: This work proposes POMP, a prompt pre-training method for vision-language models. Being memory and computation efficient, POMP enables the learned prompt to condense semantic information for a rich set of visual concepts with over twenty-thousand classes. Once pre-trained, the prompt with a strong transferable ability can be directly plugged into a variety of visual recognition tasks including image classification, semantic segmentation, and object detection, to boost recognition performances in a zero-shot manner. Empirical evaluation shows that POMP achieves state-of-the-art performances on 21 datasets, e.g., 67.0% average accuracy on 10 classification datasets (+3.1% compared to CoOp) and 84.4 hIoU on open-vocabulary Pascal VOC segmentation (+6.9 compared to ZSSeg).
count=1
* GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/2b25c39788e5cf11d3541de433ebf4c0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/2b25c39788e5cf11d3541de433ebf4c0-Paper-Conference.pdf)]
    * Title: GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph
    * Year: `2023`
    * Authors: Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang
    * Abstract: Adapter-style efficient transfer learning (ETL) has shown excellent performance in the tuning of vision-language models (VLMs) under the low-data regime, where only a few additional parameters are introduced to excavate the task-specific knowledge based on the general and powerful representation of VLMs. However, most adapter-style works face two limitations: (i) modeling task-specific knowledge with a single modality only; and (ii) overlooking the exploitation of the inter-class relationships in downstream tasks, thereby leading to sub-optimal solutions. To mitigate that, we propose an effective adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual adapter by explicitly modeling the dual-modality structure knowledge (i.e., the correlation of different semantics/classes in textual and visual modalities) with a dual knowledge graph. In particular, the dual knowledge graph is established with two sub-graphs, i.e., a textual knowledge sub-graph, and a visual knowledge sub-graph, where the nodes and edges represent the semantics/classes and their correlations in two modalities, respectively. This enables the textual feature of each prompt to leverage the task-specific structure knowledge from both textual and visual modalities, yielding a more effective classifier for downstream tasks. Extensive experimental results on 11 benchmark datasets reveal that our GraphAdapter significantly outperforms the previous adapter-based methods.
count=1
* Model Spider: Learning to Rank Pre-Trained Models Efficiently
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/2c71b14637802ed08eaa3cf50342b2b9-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/2c71b14637802ed08eaa3cf50342b2b9-Paper-Conference.pdf)]
    * Title: Model Spider: Learning to Rank Pre-Trained Models Efficiently
    * Year: `2023`
    * Authors: Yi-Kai Zhang, Ting-Ji Huang, Yao-Xiang Ding, De-Chuan Zhan, Han-Jia Ye
    * Abstract: Figuring out which Pre-Trained Model (PTM) from a model zoo fits the target task is essential to take advantage of plentiful model resources. With the availability of numerous heterogeneous PTMs from diverse fields, efficiently selecting the most suitable one is challenging due to the time-consuming costs of carrying out forward or backward passes over all PTMs. In this paper, we propose Model Spider, which tokenizes both PTMs and tasks by summarizing their characteristics into vectors to enable efficient PTM selection. By leveraging the approximated performance of PTMs on a separate set of training tasks, Model Spider learns to construct representation and measure the fitness score between a model-task pair via their representation. The ability to rank relevant PTMs higher than others generalizes to new tasks. With the top-ranked PTM candidates, we further learn to enrich task repr. with their PTM-specific semantics to re-rank the PTMs for better selection. Model Spider balances efficiency and selection ability, making PTM selection like a spider preying on a web. Model Spider exhibits promising performance across diverse model zoos, including visual models and Large Language Models (LLMs). Code is available at https://github.com/zhangyikaii/Model-Spider.
count=1
* Two-Stage Predict+Optimize for MILPs with Unknown Parameters in Constraints
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/2e14be0332c04c76742710e417cedb2a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/2e14be0332c04c76742710e417cedb2a-Paper-Conference.pdf)]
    * Title: Two-Stage Predict+Optimize for MILPs with Unknown Parameters in Constraints
    * Year: `2023`
    * Authors: Xinyi Hu, Jasper Lee, Jimmy Lee
    * Abstract: Consider the setting of constrained optimization, with some parameters unknown at solving time and requiring prediction from relevant features. Predict+Optimize is a recent framework for end-to-end training supervised learning models for such predictions, incorporating information about the optimization problem in the training process in order to yield better predictions in terms of the quality of the predicted solution under the true parameters. Almost all prior works have focused on the special case where the unknowns appear only in the optimization objective and not the constraints. Hu et al. proposed the first adaptation of Predict+Optimize to handle unknowns appearing in constraints, but the framework has somewhat ad-hoc elements, and they provided a training algorithm only for covering and packing linear programs. In this work, we give a new simpler and more powerful framework called Two-Stage Predict+Optimize, which we believe should be the canonical framework for the Predict+Optimize setting. We also give a training algorithm usable for all mixed integer linear programs, vastly generalizing the applicability of the framework. Experimental results demonstrate the superior prediction performance of our training framework over all classical and state-of-the-art methods.
count=1
* Fast Model DeBias with Machine Unlearning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/2ecc80084c96cc25b11b0ab995c25f47-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/2ecc80084c96cc25b11b0ab995c25f47-Paper-Conference.pdf)]
    * Title: Fast Model DeBias with Machine Unlearning
    * Year: `2023`
    * Authors: Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, YANG FENG, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu
    * Abstract: Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing method (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual concept and quantifies the influence of data samples with influence functions. Moreover, we design a machine unlearning-based strategy to efficiently and effectively remove the bias in a trained model with a small counterfactual dataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasets demonstrate that our method achieves superior or competing classification accuracies compared with state-of-the-art retraining-based methods while attaining significantly fewer biases and requiring much less debiasing cost. Notably, our method requires only a small external dataset and updating a minimal amount of model parameters, without the requirement of access to training data that may be too large or unavailable in practice.
count=1
* Agnostically Learning Single-Index Models using Omnipredictors
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/2f46ef5725a8eca24f7f24a17955ad1a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/2f46ef5725a8eca24f7f24a17955ad1a-Paper-Conference.pdf)]
    * Title: Agnostically Learning Single-Index Models using Omnipredictors
    * Year: `2023`
    * Authors: Aravind Gollakota, Parikshit Gopalan, Adam Klivans, Konstantinos Stavropoulos
    * Abstract: We give the first result for agnostically learning Single-Index Models (SIMs) with arbitrary monotone and Lipschitz activations. All prior work either held only in the realizable setting or required the activation to be known. Moreover, we only require the marginal to have bounded second moments, whereas all prior work required stronger distributional assumptions (such as anticoncentration or boundedness). Our algorithm is based on recent work by Gopalan et al. [2023] on Omniprediction using predictors satisfying calibrated multiaccuracy. Our analysis is simple and relies on the relationship between Bregman divergences (or matching losses) and $\ell_p$ distances. We also provide new guarantees for standard algorithms like GLMtron and logistic regression in the agnostic setting.
count=1
* Efficient Low-rank Backpropagation for Vision Transformer Adaptation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/2f75a57e9c71e8369da0150ea769d5a2-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/2f75a57e9c71e8369da0150ea769d5a2-Paper-Conference.pdf)]
    * Title: Efficient Low-rank Backpropagation for Vision Transformer Adaptation
    * Year: `2023`
    * Authors: Yuedong Yang, Hung-Yueh Chiang, Guihong Li, Diana Marculescu, Radu Marculescu
    * Abstract: The increasing scale of vision transformers (ViT) has made the efficient fine-tuning of these large models for specific needs a significant challenge in various applications. This issue originates from the computationally demanding matrix multiplications required during the backpropagation process through linear layers in ViT.In this paper, we tackle this problem by proposing a new Low-rank BackPropagation via Walsh-Hadamard Transformation (LBP-WHT) method. Intuitively, LBP-WHT projects the gradient into a low-rank space and carries out backpropagation. This approach substantially reduces the computation needed for adapting ViT, as matrix multiplication in the low-rank space is far less resource-intensive. We conduct extensive experiments with different models (ViT, hybrid convolution-ViT model) on multiple datasets to demonstrate the effectiveness of our method. For instance, when adapting an EfficientFormer-L1 model on CIFAR100, our LBP-WHT achieves 10.4\% higher accuracy than the state-of-the-art baseline, while requiring 9 MFLOPs less computation.As the first work to accelerate ViT adaptation with low-rank backpropagation, our LBP-WHT method is complementary to many prior efforts and can be combined with them for better performance.
count=1
* Easy Learning from Label Proportions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/3085fd61063840fdb2e6eafac58589f8-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/3085fd61063840fdb2e6eafac58589f8-Paper-Conference.pdf)]
    * Title: Easy Learning from Label Proportions
    * Year: `2023`
    * Authors: Róbert Busa-Fekete, Heejin Choi, Travis Dick, Claudio Gentile, Andres Munoz Medina
    * Abstract: We consider the problem of Learning from Label Proportions (LLP), a weakly supervised classification setup where instances are grouped into i.i.d. “bags”, and only the frequency of class labels at each bag is available. Albeit, the objective of the learner is to achieve low task loss at an individual instance level. Here we propose EASYLLP, a flexible and simple-to-implement debiasing approach based on aggregate labels, which operates on arbitrary loss functions. Our technique allows us to accurately estimate the expected loss of an arbitrary model at an individual level. We elucidate the differences between our method and standard methods based on label proportion matching, in terms of applicability and optimality conditions. We showcase the flexibility of our approach compared to alternatives by applying our method to popular learning frameworks, like Empirical Risk Minimization (ERM) and Stochastic Gradient Descent (SGD) with provable guarantees on instance level performance. Finally, we validate our theoretical results on multiple datasets, empirically illustrating the conditions under which our algorithm is expected to perform better or worse than previous LLP approaches
count=1
* Future-Dependent Value-Based Off-Policy Evaluation in POMDPs
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/3380e8116452e0efbf36f35d95e88c94-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/3380e8116452e0efbf36f35d95e88c94-Paper-Conference.pdf)]
    * Title: Future-Dependent Value-Based Off-Policy Evaluation in POMDPs
    * Year: `2023`
    * Authors: Masatoshi Uehara, Haruka Kiyohara, Andrew Bennett, Victor Chernozhukov, Nan Jiang, Nathan Kallus, Chengchun Shi, Wen Sun
    * Abstract: We study off-policy evaluation (OPE) for partially observable MDPs (POMDPs) with general function approximation. Existing methods such as sequential importance sampling estimators and fitted-Q evaluation suffer from the curse of horizon in POMDPs. To circumvent this problem, we develop a novel model-free OPE method by introducing future-dependent value functions that take future proxies as inputs. Future-dependent value functions play similar roles as classical value functions in fully-observable MDPs. We derive a new off-policy Bellman equation for future-dependent value functions as conditional moment equations that use history proxies as instrumental variables. We further propose a minimax learning method to learn future-dependent value functions using the new Bellman equation. We obtain the PAC result, which implies our OPE estimator is close to the true policy value as long as futures and histories contain sufficient information about latent states, and the Bellman completeness. Our code is available at https://github.com/aiueola/neurips2023-future-dependent-ope
count=1
* Augmented Memory Replay-based Continual Learning Approaches for Network Intrusion Detection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/3755a02b1035fbadd5f93a022170e46f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/3755a02b1035fbadd5f93a022170e46f-Paper-Conference.pdf)]
    * Title: Augmented Memory Replay-based Continual Learning Approaches for Network Intrusion Detection
    * Year: `2023`
    * Authors: suresh kumar amalapuram, Sumohana Channappayya, Bheemarjuna Reddy Tamma
    * Abstract: Intrusion detection is a form of anomalous activity detection in communication network traffic. Continual learning (CL) approaches to the intrusion detection task accumulate old knowledge while adapting to the latest threat knowledge. Previous works have shown the effectiveness of memory replay-based CL approaches for this task. In this work, we present two novel contributions to improve the performance of CL-based network intrusion detection in the context of class imbalance and scalability. First, we extend class balancing reservoir sampling (CBRS), a memory-based CL method, to address the problems of severe class imbalance for large datasets. Second, we propose a novel approach titled perturbation assistance for parameter approximation (PAPA) based on the Gaussian mixture model to reduce the number of \textit{virtual stochastic gradient descent (SGD) parameter} computations needed to discover maximally interfering samples for CL. We demonstrate that the proposed approaches perform remarkably better than the baselines on standard intrusion detection benchmarks created over shorter periods (KDDCUP'99, NSL-KDD, CICIDS-2017/2018, UNSW-NB15, and CTU-13) and a longer period with distribution shift (AnoShift). We also validated proposed approaches on standard continual learning benchmarks (SVHN, CIFAR-10/100, and CLEAR-10/100) and anomaly detection benchmarks (SMAP, SMD, and MSL). Further, the proposed PAPA approach significantly lowers the number of virtual SGD update operations, thus resulting in training time savings in the range of 12 to 40\% compared to the maximally interfered samples retrieval algorithm.
count=1
* Diffusion-TTA: Test-time Adaptation of Discriminative Models via Generative Feedback
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/38e511a690709603d4cc3a1c52b4a9fd-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/38e511a690709603d4cc3a1c52b4a9fd-Paper-Conference.pdf)]
    * Title: Diffusion-TTA: Test-time Adaptation of Discriminative Models via Generative Feedback
    * Year: `2023`
    * Authors: Mihir Prabhudesai, Tsung-Wei Ke, Alex Li, Deepak Pathak, Katerina Fragkiadaki
    * Abstract: The advancements in generative modeling, particularly the advent of diffusion models, have sparked a fundamental question: how can these models be effectively used for discriminative tasks? In this work, we find that generative models can be great test-time adapters for discriminative models. Our method, Diffusion-TTA, adapts pre-trained discriminative models such as image classifiers, segmenters and depth predictors, to each unlabelled example in the test set using generative feedback from a diffusion model. We achieve this by modulating the conditioning of the diffusion model using the output of the discriminative model. We then maximize the image likelihood objective by backpropagating the gradients to discriminative model’s parameters. We show Diffusion-TTA significantly enhances the accuracy of various large-scale pre-trained discriminative models, such as, ImageNet classifiers, CLIP models, image pixel labellers and image depth predictors. Diffusion-TTA outperforms existing test-time adaptation methods, including TTT-MAE and TENT, and particularly shines in online adaptation setups, where the discriminative model is continually adapted to each example in the test set. We provide access to code, results, and visualizations on our website: diffusion-tta.github.io/
count=1
* DFRD: Data-Free Robustness Distillation for Heterogeneous Federated Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/39ca8893ea38905a9d2ffe786e85af0f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/39ca8893ea38905a9d2ffe786e85af0f-Paper-Conference.pdf)]
    * Title: DFRD: Data-Free Robustness Distillation for Heterogeneous Federated Learning
    * Year: `2023`
    * Authors: kangyang Luo, Shuai Wang, Yexuan Fu, Xiang Li, Yunshi Lan, Ming Gao
    * Abstract: Federated Learning (FL) is a privacy-constrained decentralized machine learning paradigm in which clients enable collaborative training without compromising private data. However, how to learn a robust global model in the data-heterogeneous and model-heterogeneous FL scenarios is challenging. To address it, we resort to data-free knowledge distillation to propose a new FL method (namely DFRD).DFRD equips a conditional generator on the server to approximate the training space of the local models uploaded by clients, and systematically investigates its training in terms of fidelity, transferability and diversity. To overcome the catastrophic forgetting of the global model caused by the distribution shifts of the generator across communication rounds, we maintain an exponential moving average copy of the generator on the server. Additionally, we propose dynamic weighting and label sampling to accurately extract knowledge from local models. Finally, our extensive experiments on various image classification tasks illustrate that DFRD achieves significant performance gains compared to SOTA baselines.
count=1
* Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/3c6696d70d364337cf98dcb7c652a770-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/3c6696d70d364337cf98dcb7c652a770-Paper-Conference.pdf)]
    * Title: Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models
    * Year: `2023`
    * Authors: Julien Siems, Konstantin Ditschuneit, Winfried Ripken, Alma Lindborg, Maximilian Schambach, Johannes Otterbach, Martin Genzel
    * Abstract: Generalized Additive Models (GAMs) have recently experienced a resurgence in popularity due to their interpretability, which arises from expressing the target value as a sum of non-linear transformations of the features. Despite the current enthusiasm for GAMs, their susceptibility to concurvity — i.e., (possibly non-linear) dependencies between the features — has hitherto been largely overlooked. Here, we demonstrate how concurvity can severly impair the interpretability of GAMs and propose a remedy: a conceptually simple, yet effective regularizer which penalizes pairwise correlations of the non-linearly transformed feature variables. This procedure is applicable to any differentiable additive model, such as Neural Additive Models or NeuralProphet, and enhances interpretability by eliminating ambiguities due to self-canceling feature contributions. We validate the effectiveness of our regularizer in experiments on synthetic as well as real-world datasets for time-series and tabular data. Our experiments show that concurvity in GAMs can be reduced without significantly compromising prediction quality, improving interpretability and reducing variance in the feature importances.
count=1
* Fair Adaptive Experiments
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/3d007df4ae13adf9001f8969555b11bd-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/3d007df4ae13adf9001f8969555b11bd-Paper-Conference.pdf)]
    * Title: Fair Adaptive Experiments
    * Year: `2023`
    * Authors: Waverly Wei, Xinwei Ma, Jingshen Wang
    * Abstract: Randomized experiments have been the gold standard for assessing the effectiveness of a treatment, policy, or intervention, spanning various fields, including social sciences, biomedical studies, and e-commerce. The classical complete randomization approach assigns treatments based on a pre-specified probability and may lead to inefficient use of data. Adaptive experiments improve upon complete randomization by sequentially learning and updating treatment assignment probabilities using accrued evidence during the experiment. Hence, they can help achieve efficient data use and higher estimation efficiency. However, their application can also raise fairness and equity concerns, as assignment probabilities may vary drastically across groups of participants. Furthermore, when treatment is expected to be extremely beneficial to certain groups of participants, it is more appropriate to expose many of these participants to favorable treatment. In response to these challenges, we propose a fair adaptive experiment strategy that simultaneously enhances data use efficiency, achieves an ``envy-free'' treatment assignment guarantee, and improves the overall welfare of participants. An important feature of our proposed strategy is that we do not impose parametric modeling assumptions on the outcome variables, making it more versatile and applicable to a wider array of applications. Through our theoretical investigation, we characterize the convergence rate of the estimated treatment effects and the associated standard deviations at the group level and further prove that our adaptive treatment assignment algorithm, despite not having a closed-form expression, approaches the optimal allocation rule asymptotically. Our proof strategy takes into account the fact that the allocation decisions in our design depend on sequentially accumulated data, which poses a significant challenge in characterizing the properties and conducting statistical inference of our method. We further provide simulation evidence and two synthetic data studies to showcase the performance of our fair adaptive experiment strategy.
count=1
* Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/3d17b7f7d52c83ab6e97e2dc0bda2e71-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/3d17b7f7d52c83ab6e97e2dc0bda2e71-Paper-Conference.pdf)]
    * Title: Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions
    * Year: `2023`
    * Authors: Abhineet Agarwal, Anish Agarwal, Suhas Vijaykumar
    * Abstract: We consider a setting where there are $N$ heterogeneous units and $p$ interventions. Our goal is to learn unit-specific potential outcomes for any combination of these $p$ interventions, i.e., $N \times 2^p$ causal parameters. Choosing a combination of interventions is a problem that naturally arises in a variety of applications such as factorial design experiments and recommendation engines (e.g., showing a set of movies that maximizes engagement for a given user). Running $N \times 2^p$ experiments to estimate the various parameters is likely expensive and/or infeasible as $N$ and $p$ grow. Further, with observational data there is likely confounding, i.e., whether or not a unit is seen under a combination is correlated with its potential outcome under that combination. We study this problem under a novel model that imposes latent structure across both units and combinations of interventions. Specifically, we assume latent similarity in potential outcomes across units (i.e., the matrix of potential outcomes is approximately rank $r$) and regularity in how combinations of interventions interact (i.e., the coefficients in the Fourier expansion of the potential outcomes is approximately $s$ sparse). We establish identification for all $N \times 2^p$ parameters despite unobserved confounding. We propose an estimation procedure, Synthetic Combinations, and establish finite-sample consistency under precise conditions on the observation pattern. We show that Synthetic Combinations is able to consistently estimate unit-specific potential outcomes given a total of $\text{poly}(r) \times \left( N + s^2p\right)$ observations. In comparison, previous methods that do not exploit structure across both units and combinations have poorer sample complexity scaling as $\min(N \times s^2p, \ \ r \times (N + 2^p))$.
count=1
* PUe: Biased Positive-Unlabeled Learning Enhancement by Causal Inference
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/3efb4bdc6bfe13e1ff95b4407c37961d-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/3efb4bdc6bfe13e1ff95b4407c37961d-Paper-Conference.pdf)]
    * Title: PUe: Biased Positive-Unlabeled Learning Enhancement by Causal Inference
    * Year: `2023`
    * Authors: Xutao Wang, Hanting Chen, Tianyu Guo, Yunhe Wang
    * Abstract: Positive-Unlabeled (PU) learning aims to achieve high-accuracy binary classification with limited labeled positive examples and numerous unlabeled ones. Existing cost-sensitive-based methods often rely on strong assumptions that examples with an observed positive label were selected entirely at random. In fact, the uneven distribution of labels is prevalent in real-world PU problems, indicating that most actual positive and unlabeled data are subject to selection bias. In this paper, we propose a PU learning enhancement (PUe) algorithm based on causal inference theory, which employs normalized propensity scores and normalized inverse probability weighting (NIPW) techniques to reconstruct the loss function, thus obtaining a consistent, unbiased estimate of the classifier and enhancing the model's performance. Moreover, we investigate and propose a method for estimating propensity scores in deep learning using regularization techniques when the labeling mechanism is unknown. Our experiments on three benchmark datasets demonstrate the proposed PUe algorithm significantly improves the accuracy of classifiers on non-uniform label distribution datasets compared to advanced cost-sensitive PU methods. Codes are available at https://github.com/huawei-noah/Noah-research/tree/master/PUe and https://gitee.com/mindspore/models/tree/master/research/cv/PUe.
count=1
* Causal Effect Regularization: Automated Detection and Removal of Spurious Correlations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/42770daf4a3384b712ea9c36e9279998-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/42770daf4a3384b712ea9c36e9279998-Paper-Conference.pdf)]
    * Title: Causal Effect Regularization: Automated Detection and Removal of Spurious Correlations
    * Year: `2023`
    * Authors: Abhinav Kumar, Amit Deshpande, Amit Sharma
    * Abstract: In many classification datasets, the task labels are spuriously correlated with some input attributes. Classifiers trained on such datasets often rely on these attributes for prediction, especially when the spurious correlation is high, and thus fail togeneralize whenever there is a shift in the attributes’ correlation at deployment. If we assume that the spurious attributes are known a priori, several methods have been proposed to learn a classifier that is invariant to the specified attributes. However, in real-world data, information about spurious attributes is typically unavailable. Therefore, we propose a method that automatically identifies spurious attributes by estimating their causal effect on the label and then uses a regularization objective to mitigate the classifier’s reliance on them. Although causal effect of an attribute on the label is not always identified, we present two commonly occurring data-generating processes where the effect can be identified. Compared to recent work for identifying spurious attributes, we find that our method, AutoACER, ismore accurate in removing the attribute from the learned model, especially when spurious correlation is high. Specifically, across synthetic, semi-synthetic, and real-world datasets, AutoACER shows significant improvement in a metric used to quantify the dependence of a classifier on spurious attributes ($\Delta$Prob), while obtaining better or similar accuracy. Empirically we find that AutoACER mitigatesthe reliance on spurious attributes even under noisy estimation of causal effects or when the causal effect is not identified. To explain the empirical robustness of our method, we create a simple linear classification task with two sets of attributes: causal and spurious. Under this setting, we prove that AutoACER only requires the ranking of estimated causal effects to be correct across attributes to select thecorrect classifier.
count=1
* Conformal PID Control for Time Series Prediction
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/47f2fad8c1111d07f83c91be7870f8db-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/47f2fad8c1111d07f83c91be7870f8db-Paper-Conference.pdf)]
    * Title: Conformal PID Control for Time Series Prediction
    * Year: `2023`
    * Authors: Anastasios Angelopoulos, Emmanuel Candes, Ryan J. Tibshirani
    * Abstract: We study the problem of uncertainty quantification for time series prediction, with the goal of providing easy-to-use algorithms with formal guarantees. The algorithms we present build upon ideas from conformal prediction and control theory, are able to prospectively model conformal scores in an online setting, and adapt to the presence of systematic errors due to seasonality, trends, and general distribution shifts. Our theory both simplifies and strengthens existing analyses in online conformal prediction. Experiments on 4-week-ahead forecasting of statewide COVID-19 death counts in the U.S. show an improvement in coverage over the ensemble forecaster used inofficial CDC communications. We also run experiments on predicting electricity demand, market returns, and temperature using autoregressive, Theta, Prophet, and Transformer models. We provide an extendable codebase for testing our methods and for the integration of new algorithms, data sets, and forecasting rules at this link.
count=1
* Beyond Invariance: Test-Time Label-Shift Adaptation for Addressing "Spurious" Correlations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/4b0c1645f3d6a1730931e65ecbf91ac3-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/4b0c1645f3d6a1730931e65ecbf91ac3-Paper-Conference.pdf)]
    * Title: Beyond Invariance: Test-Time Label-Shift Adaptation for Addressing "Spurious" Correlations
    * Year: `2023`
    * Authors: Qingyao Sun, Kevin P. Murphy, Sayna Ebrahimi, Alexander D'Amour
    * Abstract: Changes in the data distribution at test time can have deleterious effects on the performance of predictive models $p(y|x)$.We consider situations where there are additional meta-data labels (such as group labels), denoted by $z$, that can account for such changes in the distribution.In particular, we assume that the prior distribution $p(y,z)$, which models the dependence between the class label $y$ and the "nuisance" factors $z$, may change across domains, either due to a change in the correlation between these terms, or a change in one of their marginals.However, we assume that the generative model for features $p(x|y,z)$ is invariant across domains.We note that this corresponds to an expanded version of the widely used "label shift" assumption, where the labels now also include the nuisance factors $z$. Based on this observation, we propose a test-time label shift correction that adapts to changes in the joint distribution $p(y, z)$ using EM applied to unlabeled samples from the target domain distribution, $p_t(x)$.Importantly, we are able to avoid fitting a generative model $p(x|y,z)$, and merely need to reweight the outputs of a discriminative model $p_s(y,z|x)$ trained on the source distribution.We evaluate our method, which we call "Test-Time Label-Shift Adaptation" (TTLSA), on several standard image and text datasets, as well as the CheXpert chest X-ray dataset, and show that it improves performance over methods that target invariance to changes in the distribution, as well as baseline empirical risk minimization methods.Code for reproducing experiments is available at https://github.com/nalzok/test-time-label-shift.
count=1
* Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/4bc6e94f2308c888fb69626138a2633e-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/4bc6e94f2308c888fb69626138a2633e-Paper-Conference.pdf)]
    * Title: Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations
    * Year: `2023`
    * Authors: Thomas Yerxa, Yilun Kuang, Eero Simoncelli, SueYeon Chung
    * Abstract: The efficient coding hypothesis proposes that the response properties of sensory systems are adapted to the statistics of their inputs such that they capture maximal information about the environment, subject to biological constraints. While elegant, information theoretic properties are notoriously difficult to measure in practical settings or to employ as objective functions in optimization. This difficulty has necessitated that computational models designed to test the hypothesis employ several different information metrics ranging from approximations and lower bounds to proxy measures like reconstruction error. Recent theoretical advances have characterized a novel and ecologically relevant efficiency metric, the ``manifold capacity,” which is the number of object categories that may be represented in a linearly separable fashion. However, calculating manifold capacity is a computationally intensive iterative procedure that until now has precluded its use as an objective. Here we outline the simplifying assumptions that allow manifold capacity to be optimized directly, yielding Maximum Manifold Capacity Representations (MMCR). The resulting method is closely related to and inspired by advances in the field of self supervised learning (SSL), and we demonstrate that MMCRs are competitive with state of the art results on standard SSL benchmarks. Empirical analyses reveal differences between MMCRs and representations learned by other SSL frameworks, and suggest a mechanism by which manifold compression gives rise to class separability. Finally we evaluate a set of SSL methods on a suite of neural predicitivity benchmarks, and find MMCRs are higly competitive as models of the ventral stream.
count=1
* Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/4c8f197b24e9b05d22028c2de16a45d2-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/4c8f197b24e9b05d22028c2de16a45d2-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events
    * Year: `2023`
    * Authors: Matthew McDermott, Bret Nestor, Peniel Argaw, Isaac S Kohane
    * Abstract: Generative, pre-trained transformers (GPTs, a type of "Foundation Models") have reshaped natural language processing (NLP) through their versatility in diverse downstream tasks. However, their potential extends far beyond NLP. This paper provides a software utility to help realize this potential, extending the applicability of GPTs to continuous-time sequences of complex events with internal dependencies, such as medical record datasets. Despite their potential, the adoption of foundation models in these domains has been hampered by the lack of suitable tools for model construction and evaluation. To bridge this gap, we introduce Event Stream GPT (ESGPT), an open-source library designed to streamline the end-to-end process for building GPTs for continuous-time event sequences. ESGPT allows users to (1) build flexible, foundation-model scale input datasets by specifying only a minimal configuration file, (2) leverage a Hugging Face compatible modeling API for GPTs over this modality that incorporates intra-event causal dependency structures and autoregressive generation capabilities, and (3) evaluate models via standardized processes that can assess few and even zero-shot performance of pre-trained models on user-specified fine-tuning tasks.
count=1
* Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/50a057e9fe79ffa3f4120fb6fb88071a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/50a057e9fe79ffa3f4120fb6fb88071a-Paper-Conference.pdf)]
    * Title: Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP
    * Year: `2023`
    * Authors: Qi Qian, Yuanhong Xu, Juhua Hu
    * Abstract: Vision-language pre-training methods, e.g., CLIP, demonstrate an impressive zero-shot performance on visual categorizations with the class proxy from the text embedding of the class name. However, the modality gap between the text and vision space can result in a sub-optimal performance. We theoretically show that the gap cannot be reduced sufficiently by minimizing the contrastive loss in CLIP and the optimal proxy for vision tasks may reside only in the vision space. Therefore, given unlabeled target vision data, we propose to learn the vision proxy directly with the help from the text proxy for zero-shot transfer. Moreover, according to our theoretical analysis, strategies are developed to further refine the pseudo label obtained by the text proxy to facilitate the intra-modal proxy learning (InMaP) for vision. Experiments on extensive downstream tasks confirm the effectiveness and efficiency of our proposal. Concretely, InMaP can obtain the vision proxy within one minute on a single GPU while improving the zero-shot accuracy from $77.02\%$ to $80.21\%$ on ImageNet with ViT-L/14@336 pre-trained by CLIP.
count=1
* Trade-off Between Efficiency and Consistency for Removal-based Explanations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/51484744337f4bf5fea0e4dd92ddab0b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/51484744337f4bf5fea0e4dd92ddab0b-Paper-Conference.pdf)]
    * Title: Trade-off Between Efficiency and Consistency for Removal-based Explanations
    * Year: `2023`
    * Authors: Yifan Zhang, Haowei He, Zhiquan Tan, Yang Yuan
    * Abstract: In the current landscape of explanation methodologies, most predominant approaches, such as SHAP and LIME, employ removal-based techniques to evaluate the impact of individual features by simulating various scenarios with specific features omitted. Nonetheless, these methods primarily emphasize efficiency in the original context, often resulting in general inconsistencies. In this paper, we demonstrate that such inconsistency is an inherent aspect of these approaches by establishing the Impossible Trinity Theorem, which posits that interpretability, efficiency, and consistency cannot hold simultaneously. Recognizing that the attainment of an ideal explanation remains elusive, we propose the utilization of interpretation error as a metric to gauge inefficiencies and inconsistencies. To this end, we present two novel algorithms founded on the standard polynomial basis, aimed at minimizing interpretation error. Our empirical findings indicate that the proposed methods achieve a substantial reduction in interpretation error, up to 31.8 times lower when compared to alternative techniques.
count=1
* DataComp: In search of the next generation of multimodal datasets
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/56332d41d55ad7ad8024aac625881be7-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/56332d41d55ad7ad8024aac625881be7-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: DataComp: In search of the next generation of multimodal datasets
    * Year: `2023`
    * Authors: Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei W. Koh, Olga Saukh, Alexander J. Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt
    * Abstract: Multimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DataComp workflow leads to better training sets. Our best baseline, DataComp-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release \datanet and all accompanying code at www.datacomp.ai.
count=1
* Comparing Apples to Oranges: Learning Similarity Functions for Data Produced by Different Distributions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/59056767478c7df64e6250eadfeb0a04-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/59056767478c7df64e6250eadfeb0a04-Paper-Conference.pdf)]
    * Title: Comparing Apples to Oranges: Learning Similarity Functions for Data Produced by Different Distributions
    * Year: `2023`
    * Authors: Leonidas Tsepenekas, Ivan Brugere, Freddy Lecue, Daniele Magazzeni
    * Abstract: Similarity functions measure how comparable pairs of elements are, and play a key role in a wide variety of applications, e.g., notions of Individual Fairness abiding by the seminal paradigm of Dwork et al., as well as Clustering problems. However, access to an accurate similarity function should not always be considered guaranteed, and this point was even raised by Dwork et al. For instance, it is reasonable to assume that when the elements to be compared are produced by different distributions, or in other words belong to different ``demographic'' groups, knowledge of their true similarity might be very difficult to obtain. In this work, we present an efficient sampling framework that learns these across-groups similarity functions, using only a limited amount of experts' feedback. We show analytical results with rigorous theoretical bounds, and empirically validate our algorithms via a large suite of experiments.
count=1
* StressID: a Multimodal Dataset for Stress Identification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/5f09bfe6730e9627a9f800d01a8ad5cd-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/5f09bfe6730e9627a9f800d01a8ad5cd-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: StressID: a Multimodal Dataset for Stress Identification
    * Year: `2023`
    * Authors: Hava Chaptoukaev, Valeriya Strizhkova, Michele Panariello, Bianca Dalpaos, Aglind Reka, Valeria Manera, Susanne Thümmler, Esma ISMAILOVA, Nicholas W., francois bremond, Massimiliano Todisco, Maria A Zuluaga, Laura M. Ferrari
    * Abstract: StressID is a new dataset specifically designed for stress identification fromunimodal and multimodal data. It contains videos of facial expressions, audiorecordings, and physiological signals. The video and audio recordings are acquiredusing an RGB camera with an integrated microphone. The physiological datais composed of electrocardiography (ECG), electrodermal activity (EDA), andrespiration signals that are recorded and monitored using a wearable device. Thisexperimental setup ensures a synchronized and high-quality multimodal data col-lection. Different stress-inducing stimuli, such as emotional video clips, cognitivetasks including mathematical or comprehension exercises, and public speakingscenarios, are designed to trigger a diverse range of emotional responses. Thefinal dataset consists of recordings from 65 participants who performed 11 tasks,as well as their ratings of perceived relaxation, stress, arousal, and valence levels.StressID is one of the largest datasets for stress identification that features threedifferent sources of data and varied classes of stimuli, representing more than39 hours of annotated data in total. StressID offers baseline models for stressclassification including a cleaning, feature extraction, and classification phase foreach modality. Additionally, we provide multimodal predictive models combiningvideo, audio, and physiological inputs. The data and the code for the baselines areavailable at https://project.inria.fr/stressid/.
count=1
* Vocabulary-free Image Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/619cbddb92b8c6fecaf2b86463153be9-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/619cbddb92b8c6fecaf2b86463153be9-Paper-Conference.pdf)]
    * Title: Vocabulary-free Image Classification
    * Year: `2023`
    * Authors: Alessandro Conti, Enrico Fini, Massimiliano Mancini, Paolo Rota, Yiming Wang, Elisa Ricci
    * Abstract: Recent advances in large vision-language models have revolutionized the image classification paradigm. Despite showing impressive zero-shot capabilities, a pre-defined set of categories, a.k.a. the vocabulary, is assumed at test time for composing the textual prompts. However, such assumption can be impractical when the semantic context is unknown and evolving. We thus formalize a novel task, termed as Vocabulary-free Image Classification (VIC), where we aim to assign to an input image a class that resides in an unconstrained language-induced semantic space, without the prerequisite of a known vocabulary. VIC is a challenging task as the semantic space is extremely large, containing millions of concepts, with hard-to-discriminate fine-grained categories. In this work, we first empirically verify that representing this semantic space by means of an external vision-language database is the most effective way to obtain semantically relevant content for classifying the image. We then propose Category Search from External Databases (CaSED), a method that exploits a pre-trained vision-language model and an external vision-language database to address VIC in a training-free manner. CaSED first extracts a set of candidate categories from captions retrieved from the database based on their semantic similarity to the image, and then assigns to the image the best matching candidate category according to the same vision-language model. Experiments on benchmark datasets validate that CaSED outperforms other complex vision-language frameworks, while being efficient with much fewer parameters, paving the way for future research in this direction.
count=1
* Implicit Contrastive Representation Learning with Guided Stop-gradient
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/6274172f7d981a8d58bbfd52342a9d1f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/6274172f7d981a8d58bbfd52342a9d1f-Paper-Conference.pdf)]
    * Title: Implicit Contrastive Representation Learning with Guided Stop-gradient
    * Year: `2023`
    * Authors: Byeongchan Lee, Sehyun Lee
    * Abstract: In self-supervised representation learning, Siamese networks are a natural architecture for learning transformation-invariance by bringing representations of positive pairs closer together. But it is prone to collapse into a degenerate solution. To address the issue, in contrastive learning, a contrastive loss is used to prevent collapse by moving representations of negative pairs away from each other. But it is known that algorithms with negative sampling are not robust to a reduction in the number of negative samples. So, on the other hand, there are algorithms that do not use negative pairs. Many positive-only algorithms adopt asymmetric network architecture consisting of source and target encoders as a key factor in coping with collapse. By exploiting the asymmetric architecture, we introduce a methodology to implicitly incorporate the idea of contrastive learning. As its implementation, we present a novel method guided stop-gradient. We apply our method to benchmark algorithms SimSiam and BYOL and show that our method stabilizes training and boosts performance. We also show that the algorithms with our method work well with small batch sizes and do not collapse even when there is no predictor. The code is available in the supplementary material.
count=1
* Foundation Model is Efficient Multimodal Multitask Model Selector
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/687b7b2bdcc2ced577c0a989b44e7078-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/687b7b2bdcc2ced577c0a989b44e7078-Paper-Conference.pdf)]
    * Title: Foundation Model is Efficient Multimodal Multitask Model Selector
    * Year: `2023`
    * Authors: Fanqing Meng, Wenqi Shao, zhanglin peng, Chonghe Jiang, Kaipeng Zhang, Yu Qiao, Ping Luo
    * Abstract: This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering.A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models’ transferability, they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model’s transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee. Extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models, making it the first model selection method in the multi-task scenario. For instance, compared with the state- of-the-art method LogME enhanced by our label embeddings, EMMS achieves 9.0%, 26.3%, 20.1%, 54.8%, 12.2% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, while bringing 5.13×, 6.29×, 3.59×, 6.19×, and 5.66× speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector.
count=1
* PIXIU: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/6a386d703b50f1cf1f61ab02a15967bb-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/6a386d703b50f1cf1f61ab02a15967bb-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: PIXIU: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance
    * Year: `2023`
    * Authors: Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, Jimin Huang
    * Abstract: Although large language models (LLMs) have shown great performance in natural language processing (NLP) in the financial domain, there are no publicly available financially tailored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 128K data samples to support the fine-tuning, and an evaluation benchmark with 8 tasks and 15 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs, we propose a standardized benchmark that covers a set of critical financial tasks, including six financial NLP tasks and two financial prediction tasks. With this benchmark, we conduct a detailed analysis of FinMA and several existing LLMs, uncovering their strengths and weaknesses in handling critical financial tasks. The model, datasets, benchmark, and experimental results are open-sourced to facilitate future research in financial AI.
count=1
* Tanimoto Random Features for Scalable Molecular Machine Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/6a69d44b3386e50c06f7107ef4f29302-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/6a69d44b3386e50c06f7107ef4f29302-Paper-Conference.pdf)]
    * Title: Tanimoto Random Features for Scalable Molecular Machine Learning
    * Year: `2023`
    * Authors: Austin Tripp, Sergio Bacallado, Sukriti Singh, José Miguel Hernández-Lobato
    * Abstract: The Tanimoto coefficient is commonly used to measure the similarity between molecules represented as discrete fingerprints,either as a distance metric or a positive definite kernel. While many kernel methods can be accelerated using random feature approximations, at present there is a lack of such approximations for the Tanimoto kernel. In this paper we propose two kinds of novel random features to allow this kernel to scale to large datasets, and in the process discover a novel extension of the kernel to real-valued vectors. We theoretically characterize these random features, and provide error bounds on the spectral norm of the Gram matrix. Experimentally, we show that these random features are effective at approximating the Tanimoto coefficient of real-world datasetsand are useful for molecular property prediction and optimization tasks. Future updates to this work will be available at http://arxiv.org/abs/2306.14809.
count=1
* An Information Theory Perspective on Variance-Invariance-Covariance Regularization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/6b1d4c03391b0aa6ddde0b807a78c950-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/6b1d4c03391b0aa6ddde0b807a78c950-Paper-Conference.pdf)]
    * Title: An Information Theory Perspective on Variance-Invariance-Covariance Regularization
    * Year: `2023`
    * Authors: Ravid Shwartz-Ziv, Randall Balestriero, Kenji Kawaguchi, Tim G. J. Rudner, Yann LeCun
    * Abstract: Variance-Invariance-Covariance Regularization (VICReg) is a self-supervised learning (SSL) method that has shown promising results on a variety of tasks. However, the fundamental mechanisms underlying VICReg remain unexplored. In this paper, we present an information-theoretic perspective on the VICReg objective. We begin by deriving information-theoretic quantities for deterministic networks as an alternative to unrealistic stochastic network assumptions. We then relate the optimization of the VICReg objective to mutual information optimization, highlighting underlying assumptions and facilitating a constructive comparison with other SSL algorithms and derive a generalization bound for VICReg, revealing its inherent advantages for downstream tasks. Building on these results, we introduce a family of SSL methods derived from information-theoretic principles that outperform existing SSL techniques.
count=1
* Improving CLIP Training with Language Rewrites
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/6fa4d985e7c434002fb6289ab9b2d654-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/6fa4d985e7c434002fb6289ab9b2d654-Paper-Conference.pdf)]
    * Title: Improving CLIP Training with Language Rewrites
    * Year: `2023`
    * Authors: Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, Yonglong Tian
    * Abstract: Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly selects either the original texts or the rewritten versions as text augmentations for each image. Extensive experiments on CC3M, CC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with language rewrites significantly improves the transfer performance without computation or memory overhead during training. Specifically for ImageNet zero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on LAION-400M.
count=1
* A Framework for Fast and Stable Representations of Multiparameter Persistent Homology Decompositions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/702b67152ec4435795f681865b67999c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/702b67152ec4435795f681865b67999c-Paper-Conference.pdf)]
    * Title: A Framework for Fast and Stable Representations of Multiparameter Persistent Homology Decompositions
    * Year: `2023`
    * Authors: David Loiseaux, Mathieu Carrière, Andrew Blumberg
    * Abstract: Topological data analysis (TDA) is an area of data science that focuses on using invariants from algebraic topology to provide multiscale shape descriptors for geometric data sets such as point clouds. One of the most important such descriptors is persistent homology, which encodes the change in shape as a filtration parameter changes; a typical parameter is the feature scale. For many data sets, it is useful to simultaneously vary multiple filtration parameters, for example feature scale and density. While the theoretical properties of single parameter persistent homology are well understood, less is known about the multiparameter case. A central question is the problem of representing multiparameter persistent homology by elements of a vector space for integration with standard machine learning algorithms. Existing approaches to this problem either ignore most of the multiparameter information to reduce to the one-parameter case or are heuristic and potentially unstable in the face of noise. In this article, we introduce a new general representation framework that leverages recent results on decompositions of multiparameter persistent homology. This framework is rich in information, fast to compute, and encompasses previous approaches. Moreover, we establish theoretical stability guarantees under this framework as well as efficient algorithms for practical computation, making this framework an applicable and versatile tool for analyzing geometric and point cloud data. We validate our stability results and algorithms with numerical experiments that demonstrate statistical convergence, prediction accuracy, and fast running times on several real data sets.
count=1
* Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/749252feedd44f7f10d47ec1d674a2f8-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/749252feedd44f7f10d47ec1d674a2f8-Paper-Conference.pdf)]
    * Title: Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning
    * Year: `2023`
    * Authors: Yihua Zhang, Yimeng Zhang, Aochuan Chen, Jinghan Jia, Jiancheng Liu, Gaowen Liu, Mingyi Hong, Shiyu Chang, Sijia Liu
    * Abstract: Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, i.e., how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings respectively, by revisiting the DP problem through the lens of source-target domain mapping. Furthermore, we demonstrate the effectiveness of our approach on numerous transfer learning tasks. We show that source data classes can be pruned by up to $40\%\sim 80\%$ without sacrificing the downstream performance, resulting in a significant $2\sim 5\times$ speed-up during the pretraining stage. Besides, our proposal exhibits broad applicability and can improve other computationally intensive transfer learning techniques, such as adversarial pretraining.
count=1
* Isometric Quotient Variational Auto-Encoders for Structure-Preserving Representation Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/7af8e3dfefe6e3141144197b8fa44f79-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/7af8e3dfefe6e3141144197b8fa44f79-Paper-Conference.pdf)]
    * Title: Isometric Quotient Variational Auto-Encoders for Structure-Preserving Representation Learning
    * Year: `2023`
    * Authors: In Huh, changwook jeong, Jae Myung Choe, YOUNGGU KIM, Daesin Kim
    * Abstract: We study structure-preserving low-dimensional representation of a data manifold embedded in a high-dimensional observation space based on variational auto-encoders (VAEs). We approach this by decomposing the data manifold $\mathcal{M}$ as $\mathcal{M} = \mathcal{M} / G \times G$, where $G$ and $\mathcal{M} / G$ are a group of symmetry transformations and a quotient space of $\mathcal{M}$ up to $G$, respectively. From this perspective, we define the structure-preserving representation of such a manifold as a latent space $\mathcal{Z}$ which is isometrically isomorphic (i.e., distance-preserving) to the quotient space $\mathcal{M} / G$ rather $\mathcal{M}$ (i.e., symmetry-preserving). To this end, we propose a novel auto-encoding framework, named isometric quotient VAEs (IQVAEs), that can extract the quotient space from observations and learn the Riemannian isometry of the extracted quotient in an unsupervised manner. Empirical proof-of-concept experiments reveal that the proposed method can find a meaningful representation of the learned data and outperform other competitors for downstream tasks.
count=1
* Sharp Bounds for Generalized Causal Sensitivity Analysis
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/7f8b8bc8ebac661c442c4dafd5d98c08-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/7f8b8bc8ebac661c442c4dafd5d98c08-Paper-Conference.pdf)]
    * Title: Sharp Bounds for Generalized Causal Sensitivity Analysis
    * Year: `2023`
    * Authors: Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel
    * Abstract: Causal inference from observational data is crucial for many disciplines such as medicine and economics. However, sharp bounds for causal effects under relaxations of the unconfoundedness assumption (causal sensitivity analysis) are subject to ongoing research. So far, works with sharp bounds are restricted to fairly simple settings (e.g., a single binary treatment). In this paper, we propose a unified framework for causal sensitivity analysis under unobserved confounding in various settings. For this, we propose a flexible generalization of the marginal sensitivity model (MSM) and then derive sharp bounds for a large class of causal effects. This includes (conditional) average treatment effects, effects for mediation analysis and path analysis, and distributional effects. Furthermore, our sensitivity model is applicable to discrete, continuous, and time-varying treatments. It allows us to interpret the partial identification problem under unobserved confounding as a distribution shift in the latent confounders while evaluating the causal effect of interest. In the special case of a single binary treatment, our bounds for (conditional) average treatment effects coincide with recent optimality results for causal sensitivity analysis. Finally, we propose a scalable algorithm to estimate our sharp bounds from observational data.
count=1
* Towards the Difficulty for a Deep Neural Network to Learn Concepts of Different Complexities
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/8143b8c73073a9a23b9c18e400066471-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/8143b8c73073a9a23b9c18e400066471-Paper-Conference.pdf)]
    * Title: Towards the Difficulty for a Deep Neural Network to Learn Concepts of Different Complexities
    * Year: `2023`
    * Authors: Dongrui Liu, Huiqi Deng, Xu Cheng, Qihan Ren, Kangrui Wang, Quanshi Zhang
    * Abstract: This paper theoretically explains the intuition that simple concepts are more likely to be learned by deep neural networks (DNNs) than complex concepts. In fact, recent studies have observed [24, 15] and proved [26] the emergence of interactive concepts in a DNN, i.e., it is proven that a DNN usually only encodes a small number of interactive concepts, and can be considered to use their interaction effects to compute inference scores. Each interactive concept is encoded by the DNN to represent the collaboration between a set of input variables. Therefore, in this study, we aim to theoretically explain that interactive concepts involving more input variables (i.e., more complex concepts) are more difficult to learn. Our finding clarifies the exact conceptual complexity that boosts the learning difficulty.
count=1
* DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/82d7d58cba24731c0ca952dff1de46ae-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/82d7d58cba24731c0ca952dff1de46ae-Paper-Conference.pdf)]
    * Title: DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization
    * Year: `2023`
    * Authors: Hua Wang, Sheng Gao, Huanyu Zhang, Weijie Su, Milan Shen
    * Abstract: Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset.Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best-performing hyperparameter.In contrast, in non-private settings, practitioners commonly utilize "adaptive" hyperparameter optimization methods such as Gaussian Process-based optimization, which select the next candidate based on information gathered from previous outputs.This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering framework for "adaptive" private hyperparameter optimization, aiming to bridge the gap between private and non-private hyperparameter optimization. To accomplish this, we provide a comprehensive differential privacy analysis of our framework. Furthermore, we empirically demonstrate the effectiveness of DP-HyPO on a diverse set of real-world datasets.
count=1
* Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/8514a5203b87cba5e440bd62ab18f2b4-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/8514a5203b87cba5e440bd62ab18f2b4-Paper-Conference.pdf)]
    * Title: Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone
    * Year: `2023`
    * Authors: Zeyinzi Jiang, Chaojie Mao, Ziyuan Huang, Ao Ma, Yiliang Lv, Yujun Shen, Deli Zhao, Jingren Zhou
    * Abstract: Parameter-efficient tuning has become a trend in transferring large-scale foundation models to downstream applications. Existing methods typically embed some light-weight tuners into the backbone, where both the design and the learning of the tuners are highly dependent on the base model. This work offers a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners from the backbone. With both theoretical and empirical evidence, we show that popular tuning approaches have their equivalent counterparts under our unbinding formulation, and hence can be integrated into our framework effortlessly. Thanks to the structural disentanglement, we manage to free the design of tuners from the network architecture, facilitating flexible combination of various tuning strategies. We further propose a memory-efficient variant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners) is effectively detached from the main branch, such that the gradients are back-propagated only to the tuners but not to the backbone. Such a detachment also allows one-time backbone forward for multi-task inference. Extensive experiments on both discriminative and generative tasks demonstrate the superiority of our method over existing alternatives from the perspectives of efficacy and efficiency. Project page: https://res-tuning.github.io/.
count=1
* FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/87cf37e2085655bad7bad0a014e0edad-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/87cf37e2085655bad7bad0a014e0edad-Paper-Conference.pdf)]
    * Title: FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning
    * Year: `2023`
    * Authors: Kun Song, Huimin Ma, Bochao Zou, Huishuai Zhang, Weiran Huang
    * Abstract: Due to the limited availability of data, existing few-shot learning methods trained from scratch fail to achieve satisfactory performance. In contrast, large-scale pre-trained models such as CLIP demonstrate remarkable few-shot and zero-shot capabilities. To enhance the performance of pre-trained models for downstream tasks, fine-tuning the model on downstream data is frequently necessary. However, fine-tuning the pre-trained model leads to a decrease in its generalizability in the presence of distribution shift, while the limited number of samples in few-shot learning makes the model highly susceptible to overfitting. Consequently, existing methods for fine-tuning few-shot learning primarily focus on fine-tuning the model's classification head or introducing additional structure. In this paper, we introduce a fine-tuning approach termed Feature Discrimination Alignment (FD-Align). Our method aims to bolster the model's generalizability by preserving the consistency of spurious features across the fine-tuning process. Extensive experimental results validate the efficacy of our approach for both ID and OOD tasks. Once fine-tuned, the model can seamlessly integrate with existing methods, leading to performance improvements. Our code can be found in https://github.com/skingorz/FD-Align.
count=1
* Hierarchical VAEs provide a normative account of motion processing in the primate brain
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/909d6b6a7c6ac13ea51de4c4cace35db-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/909d6b6a7c6ac13ea51de4c4cace35db-Paper-Conference.pdf)]
    * Title: Hierarchical VAEs provide a normative account of motion processing in the primate brain
    * Year: `2023`
    * Authors: Hadi Vafaii, Jacob Yates, Daniel Butts
    * Abstract: The relationship between perception and inference, as postulated by Helmholtz in the 19th century, is paralleled in modern machine learning by generative models like Variational Autoencoders (VAEs) and their hierarchical variants. Here, we evaluate the role of hierarchical inference and its alignment with brain function in the domain of motion perception. We first introduce a novel synthetic data framework, Retinal Optic Flow Learning (ROFL), which enables control over motion statistics and their causes. We then present a new hierarchical VAE and test it against alternative models on two downstream tasks: (i) predicting ground truth causes of retinal optic flow (e.g., self-motion); and (ii) predicting the responses of neurons in the motion processing pathway of primates. We manipulate the model architectures (hierarchical versus non-hierarchical), loss functions, and the causal structure of the motion stimuli. We find that hierarchical latent structure in the model leads to several improvements. First, it improves the linear decodability of ground truth variables and does so in a sparse and disentangled manner. Second, our hierarchical VAE outperforms previous state-of-the-art models in predicting neuronal responses and exhibits sparse latent-to-neuron relationships. These results depend on the causal structure of the world, indicating that alignment between brains and artificial neural networks depends not only on architecture but also on matching ecologically relevant stimulus statistics. Taken together, our results suggest that hierarchical Bayesian inference underlines the brain's understanding of the world, and hierarchical VAEs can effectively model this understanding.
count=1
* Image Captioners Are Scalable Vision Learners Too
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/92369a01fbe8046a093746389b2c413e-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/92369a01fbe8046a093746389b2c413e-Paper-Conference.pdf)]
    * Title: Image Captioners Are Scalable Vision Learners Too
    * Year: `2023`
    * Authors: Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, Lucas Beyer
    * Abstract: Contrastive pretraining on image-text pairs from the web is one of the most popular large-scale pretraining strategies for vision backbones, especially in the context of large multimodal models. At the same time, image captioning on this type of data is commonly considered an inferior pretraining strategy. In this paper, we perform a fair comparison of these two pretraining strategies, carefully matching training data, compute, and model capacity. Using a standard encoder-decoder transformer, we find that captioning alone is surprisingly effective: on classification tasks, captioning produces vision encoders competitive with contrastively pretrained encoders, while surpassing them on vision & language tasks. We further analyze the effect of the model architecture and scale, as well as the pretraining data on the representation quality, and find that captioning exhibits the same or better scaling behavior along these axes. Overall our results show that plain image captioning is a more powerful pretraining strategy than was previously believed. Code is available at https://github.com/google-research/big_vision.
count=1
* Scaling Laws for Hyperparameter Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/945c781d7194ea81026148838af95af7-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/945c781d7194ea81026148838af95af7-Paper-Conference.pdf)]
    * Title: Scaling Laws for Hyperparameter Optimization
    * Year: `2023`
    * Authors: Arlind Kadra, Maciej Janowski, Martin Wistuba, Josif Grabocka
    * Abstract: Hyperparameter optimization is an important subfield of machine learning that focuses on tuning the hyperparameters of a chosen algorithm to achieve peak performance. Recently, there has been a stream of methods that tackle the issue of hyperparameter optimization, however, most of the methods do not exploit the dominant power law nature of learning curves for Bayesian optimization. In this work, we propose Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern. Our method dynamically decides which configurations to pause and train incrementally by making use of gray-box evaluations. We compare our method against 7 state-of-the-art competitors on 3 benchmarks related to tabular, image, and NLP datasets covering 59 diverse tasks. Our method achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors.
count=1
* Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/9708c7d3a0fef3710f33ba05a74e10b3-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/9708c7d3a0fef3710f33ba05a74e10b3-Paper-Conference.pdf)]
    * Title: Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination
    * Year: `2023`
    * Authors: Yuchen BAI, Jean-Baptiste Durand, Grégoire Vincent, Florence Forbes
    * Abstract: Lidar (Light Detection and Ranging) has become an essential part of the remote sensing toolbox used for biosphere monitoring. In particular, Lidar provides the opportunity to map forest leaf area with unprecedented accuracy, while leaf area has remained an important source of uncertainty affecting models of gas exchanges between the vegetation and the atmosphere. Unmanned Aerial Vehicles (UAV) are easy to mobilize and therefore allow frequent revisits to track the response of vegetation to climate change. However, miniature sensors embarked on UAVs usually provide point clouds of limited density, which are further affected by a strong decrease in density from top to bottom of the canopy due to progressively stronger occlusion. In such a context, discriminating leaf points from wood points presents a significant challenge due in particular to strong class imbalance and spatially irregular sampling intensity. Here we introduce a neural network model based on the Pointnet ++ architecture which makes use of point geometry only (excluding any spectral information). To cope with local data sparsity, we propose an innovative sampling scheme which strives to preserve local important geometric information. We also propose a loss function adapted to the severe class imbalance. We show that our model outperforms state-of-the-art alternatives on UAV point clouds. We discuss future possible improvements, particularly regarding much denser point clouds acquired from below the canopy.
count=1
* StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/971f1e59cd956cc094da4e2f78c6ea7c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/971f1e59cd956cc094da4e2f78c6ea7c-Paper-Conference.pdf)]
    * Title: StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners
    * Year: `2023`
    * Authors: Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, Dilip Krishnan
    * Abstract: We investigate the potential of learning visual representations using synthetic images generated by text-to-image models. This is a natural question in the light of the excellent performance of such models in generating high-quality images. We consider specifically the Stable Diffusion, one of the leading open source text-to-image models. We show that (1) when the generative model is properly configured, training self-supervised methods on synthetic images can match or beat the real image counterpart;(2) by treating the multiple images generated from the same text prompt as positives for each other, we develop a multi-positive contrastive learning method, which we call StableRep. With solely synthetic images, the representations learned by StableRep surpass the performance of representations learned by SimCLR and CLIP using the same set of text prompts and corresponding real images, on large scale datasets. When we further add language supervision, \name~trained with 20M synthetic images (10M captions) achieves better accuracy than CLIP trained with 50M real images (50M captions).
count=1
* Learning Curves for Noisy Heterogeneous Feature-Subsampled Ridge Ensembles
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/9c940ba3be5bc9020ec74279d6e37c8a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/9c940ba3be5bc9020ec74279d6e37c8a-Paper-Conference.pdf)]
    * Title: Learning Curves for Noisy Heterogeneous Feature-Subsampled Ridge Ensembles
    * Year: `2023`
    * Authors: Ben Ruben, Cengiz Pehlevan
    * Abstract: Feature bagging is a well-established ensembling method which aims to reduceprediction variance by combining predictions of many estimators trained on subsetsor projections of features. Here, we develop a theory of feature-bagging in noisyleast-squares ridge ensembles and simplify the resulting learning curves in the specialcase of equicorrelated data. Using analytical learning curves, we demonstratethat subsampling shifts the double-descent peak of a linear predictor. This leadsus to introduce heterogeneous feature ensembling, with estimators built on varyingnumbers of feature dimensions, as a computationally efficient method to mitigatedouble-descent. Then, we compare the performance of a feature-subsamplingensemble to a single linear predictor, describing a trade-off between noise amplificationdue to subsampling and noise reduction due to ensembling. Our qualitativeinsights carry over to linear classifiers applied to image classification tasks withrealistic datasets constructed using a state-of-the-art deep learning feature map.
count=1
* DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/9f09f316a3eaf59d9ced5ffaefe97e0f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/9f09f316a3eaf59d9ced5ffaefe97e0f-Paper-Conference.pdf)]
    * Title: DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data
    * Year: `2023`
    * Authors: Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, Phillip Isola
    * Abstract: Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks. Our project page: https://dreamsim-nights.github.io/
count=1
* Bilevel Coreset Selection in Continual Learning: A New Formulation and Algorithm
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/a0251e494a7e75d59e06d37e646f46b7-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/a0251e494a7e75d59e06d37e646f46b7-Paper-Conference.pdf)]
    * Title: Bilevel Coreset Selection in Continual Learning: A New Formulation and Algorithm
    * Year: `2023`
    * Authors: Jie Hao, Kaiyi Ji, Mingrui Liu
    * Abstract: Coreset is a small set that provides a data summary for a large dataset, such that training solely on the small set achieves competitive performance compared with a large dataset. In rehearsal-based continual learning, the coreset is typically used in the memory replay buffer to stand for representative samples in previous tasks, and the coreset selection procedure is typically formulated as a bilevel problem. However, the typical bilevel formulation for coreset selection explicitly performs optimization over discrete decision variables with greedy search, which is computationally expensive. Several works consider other formulations to address this issue, but they ignore the nested nature of bilevel optimization problems and may not solve the bilevel coreset selection problem accurately. To address these issues, we propose a new bilevel formulation, where the inner problem tries to find a model which minimizes the expected training error sampled from a given probability distribution, and the outer problem aims to learn the probability distribution with approximately $K$ (coreset size) nonzero entries such that learned model in the inner problem minimizes the training error over the whole data. To ensure the learned probability has approximately $K$ nonzero entries, we introduce a novel regularizer based on the smoothed top-$K$ loss in the upper problem. We design a new optimization algorithm that provably converges to the $\epsilon$-stationary point with $O(1/\epsilon^4)$ computational complexity. We conduct extensive experiments in various settings in continual learning, including balanced data, imbalanced data, and label noise, to show that our proposed formulation and new algorithm significantly outperform competitive baselines. From bilevel optimization point of view, our algorithm significantly improves the vanilla greedy coreset selection method in terms of running time on continual learning benchmark datasets. The code is available at https://github.com/MingruiLiu-ML-Lab/Bilevel-Coreset-Selection-via-Regularization.
count=1
* Tuning Multi-mode Token-level Prompt Alignment across Modalities
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/a547d86953a4e36aa8a1390e6f4708e2-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/a547d86953a4e36aa8a1390e6f4708e2-Paper-Conference.pdf)]
    * Title: Tuning Multi-mode Token-level Prompt Alignment across Modalities
    * Year: `2023`
    * Authors: Dongsheng Wang, Miaoge Li, Xinyang Liu, MingSheng Xu, Bo Chen, Hanwang Zhang
    * Abstract: Advancements in prompt tuning of vision-language models have underscored their potential in enhancing open-world visual concept comprehension. However, prior works only primarily focus on single-mode (only one prompt for each modality) and holistic level (image or sentence) semantic alignment, which fails to capture the sample diversity, leading to sub-optimal prompt discovery. To address the limitation, we propose a multi-mode token-level tuning framework that leverages the optimal transportation to learn and align a set of prompt tokens across modalities. Specifically, we rely on two essential factors: 1) multi-mode prompts discovery, which guarantees diverse semantic representations, and 2) token-level alignment, which helps explore fine-grained similarity. Consequently, the similarity can be calculated as a hierarchical transportation problem between the modality-specific sets. Extensive experiments on popular image recognition benchmarks show the superior generalization and few-shot abilities of our approach. The qualitative analysis demonstrates that the learned prompt tokens have the ability to capture diverse visual concepts.
count=1
* Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/a81dc87f7b3b7ab8489d5bb48c4a8d92-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/a81dc87f7b3b7ab8489d5bb48c4a8d92-Paper-Conference.pdf)]
    * Title: Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods
    * Year: `2023`
    * Authors: Shang Liu, Zhongze Cai, Xiaocheng Li
    * Abstract: In this paper, we consider the uncertainty quantification problem for regression models. Specifically, we consider an individual calibration objective for characterizing the quantiles of the prediction model. While such an objective is well-motivated from downstream tasks such as newsvendor cost, the existing methods have been largely heuristic and lack of statistical guarantee in terms of individual calibration. We show via simple examples that the existing methods focusing on population-level calibration guarantees such as average calibration or sharpness can lead to harmful and unexpected results. We propose simple nonparametric calibration methods that are agnostic of the underlying prediction model and enjoy both computational efficiency and statistical consistency. Our approach enables a better understanding of the possibility of individual calibration, and we establish matching upper and lower bounds for the calibration error of our proposed methods. Technically, our analysis combines the nonparametric analysis with a covering number argument for parametric analysis, which advances the existing theoretical analyses in the literature of nonparametric density estimation and quantile bandit problems. Importantly, the nonparametric perspective sheds new theoretical insights into regression calibration in terms of the curse of dimensionality and reconciles the existing results on the impossibility of individual calibration. To our knowledge, we make the first effort to reach both individual calibration and finite-sample guarantee with minimal assumptions in terms of conformal prediction. Numerical experiments show the advantage of such a simple approach under various metrics, and also under covariates shift. We hope our work provides a simple benchmark and a starting point of theoretical ground for future research on regression calibration.
count=1
* Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/aa6287ca31ae1474ea802342d0c8ba63-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/aa6287ca31ae1474ea802342d0c8ba63-Paper-Conference.pdf)]
    * Title: Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand
    * Year: `2023`
    * Authors: Junfeng Guo, Yiming Li, Lixu Wang, Shu-Tao Xia, Heng Huang, Cong Liu, Bo Li
    * Abstract: The prosperity of deep neural networks (DNNs) is largely benefited from open-source datasets, based on which users can evaluate and improve their methods. In this paper, we revisit backdoor-based dataset ownership verification (DOV), which is currently the only feasible approach to protect the copyright of open-source datasets. We reveal that these methods are fundamentally harmful given that they could introduce malicious misclassification behaviors to watermarked DNNs by the adversaries. In this paper, we design DOV from another perspective by making watermarked models (trained on the protected dataset) correctly classify some `hard' samples that will be misclassified by the benign model. Our method is inspired by the generalization property of DNNs, where we find a \emph{hardly-generalized domain} for the original dataset (as its \emph{domain watermark}). It can be easily learned with the protected dataset containing modified samples. Specifically, we formulate the domain generation as a bi-level optimization and propose to optimize a set of visually-indistinguishable clean-label modified data with similar effects to domain-watermarked samples from the hardly-generalized domain to ensure watermark stealthiness. We also design a hypothesis-test-guided ownership verification via our domain watermark and provide the theoretical analyses of our method. Extensive experiments on three benchmark datasets are conducted, which verify the effectiveness of our method and its resistance to potential adaptive methods.
count=1
* A benchmark of categorical encoders for binary classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/ac01e21bb14609416760f790dd8966ae-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/ac01e21bb14609416760f790dd8966ae-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: A benchmark of categorical encoders for binary classification
    * Year: `2023`
    * Authors: Federico Matteucci, Vadim Arzamasov, Klemens Böhm
    * Abstract: Categorical encoders transform categorical features into numerical representations that are indispensable for a wide range of machine learning models.Existing encoder benchmark studies lack generalizability because of their limited choice of (1) encoders, (2) experimental factors, and (3) datasets. Additionally, inconsistencies arise from the adoption of varying aggregation strategies.This paper is the most comprehensive benchmark of categorical encoders to date, including an extensive evaluation of 32 configurations of encoders from diverse families, with 36 combinations of experimental factors, and on 50 datasets.The study shows the profound influence of dataset selection, experimental factors, and aggregation strategies on the benchmark's conclusions~---~aspects disregarded in previous encoder benchmarks.Our code is available at \url{https://github.com/DrCohomology/EncoderBenchmarking}.
count=1
* Importance-aware Co-teaching for Offline Model-based Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/ae8b0b5838ba510daff1198474e7b984-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/ae8b0b5838ba510daff1198474e7b984-Paper-Conference.pdf)]
    * Title: Importance-aware Co-teaching for Offline Model-based Optimization
    * Year: `2023`
    * Authors: Ye Yuan, Can (Sam) Chen, Zixuan Liu, Willie Neiswanger, Xue (Steve) Liu
    * Abstract: Offline model-based optimization aims to find a design that maximizes a property of interest using only an offline dataset, with applications in robot, protein, and molecule design, among others. A prevalent approach is gradient ascent, where a proxy model is trained on the offline dataset and then used to optimize the design. This method suffers from an out-of-distribution issue, where the proxy is not accurate for unseen designs. To mitigate this issue, we explore using a pseudo-labeler to generate valuable data for fine-tuning the proxy. Specifically, we propose $\textit{\textbf{I}mportance-aware \textbf{C}o-\textbf{T}eaching for Offline Model-based Optimization}~(\textbf{ICT})$. This method maintains three symmetric proxies with their mean ensemble as the final proxy, and comprises two steps. The first step is $\textit{pseudo-label-driven co-teaching}$. In this step, one proxy is iteratively selected as the pseudo-labeler for designs near the current optimization point, generating pseudo-labeled data. Subsequently, a co-teaching process identifies small-loss samples as valuable data and exchanges them between the other two proxies for fine-tuning, promoting knowledge transfer. This procedure is repeated three times, with a different proxy chosen as the pseudo-labeler each time, ultimately enhancing the ensemble performance.To further improve accuracy of pseudo-labels, we perform a secondary step of $\textit{meta-learning-based sample reweighting}$,which assigns importance weights to samples in the pseudo-labeled dataset and updates them via meta-learning. ICT achieves state-of-the-art results across multiple design-bench tasks, achieving the best mean rank $3.1$ and median rank $2$ among $15$ methods.Our source code can be accessed here.
count=1
* Exploring and Interacting with the Set of Good Sparse Generalized Additive Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/b1719f44953c2e0754a016ab267fe4e7-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/b1719f44953c2e0754a016ab267fe4e7-Paper-Conference.pdf)]
    * Title: Exploring and Interacting with the Set of Good Sparse Generalized Additive Models
    * Year: `2023`
    * Authors: Chudi Zhong, Zhi Chen, Jiachang Liu, Margo Seltzer, Cynthia Rudin
    * Abstract: In real applications, interaction between machine learning models and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present algorithms to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity, direct editing); and (3) investigating sudden changes in the shape functions. Experiments demonstrate the fidelity of the approximated Rashomon set and its effectiveness in solving practical challenges.
count=1
* Reverse Engineering Self-Supervised Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/b63ad8c24354b0e5bcb7aea16490beab-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/b63ad8c24354b0e5bcb7aea16490beab-Paper-Conference.pdf)]
    * Title: Reverse Engineering Self-Supervised Learning
    * Year: `2023`
    * Authors: Ido Ben-Shaul, Ravid Shwartz-Ziv, Tomer Galanti, Shai Dekel, Yann LeCun
    * Abstract: Understanding the learned representation and underlying mechanisms of Self-Supervised Learning (SSL) often poses a challenge. In this paper, we ‘reverse engineer’ SSL, conducting an in-depth empirical analysis of its learned internal representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing process within the SSL training: an inherent facilitation of semantic label-based clustering, which is surprisingly driven by the regularization component of the SSL objective. This clustering not only enhances downstream classification, but also compresses the information. We further illustrate that the alignment of the SSL-trained representation is more pronounced with semantic classes rather than random functions. Remarkably, the learned representations align with semantic classes across various hierarchical levels, with this alignment intensifying when going deeper into the network. This ‘reverse engineering’ approach provides valuable insights into the inner mechanism of SSL and their influences on the performance across different class sets.
count=1
* Feature Dropout: Revisiting the Role of Augmentations in Contrastive Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/c290d4373c495b2cad0625d6288260f0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/c290d4373c495b2cad0625d6288260f0-Paper-Conference.pdf)]
    * Title: Feature Dropout: Revisiting the Role of Augmentations in Contrastive Learning
    * Year: `2023`
    * Authors: Alex Tamkin, Margalit Glasgow, Xiluo He, Noah Goodman
    * Abstract: What role do augmentations play in contrastive learning? Recent work suggests that good augmentations are label-preserving with respect to a specific downstream task. We complicate this picture by showing that label-destroying augmentations can be useful in the foundation model setting, where the goal is to learn diverse, general-purpose representations for multiple downstream tasks. We perform contrastive learning experiments on a range of image and audio datasets with multiple downstream tasks (e.g. for digits superimposed on photographs, predicting the class of one vs. the other). We find that Viewmaker Networks, a recently proposed model for learning augmentations for contrastive learning, produce label-destroying augmentations that stochastically destroy features needed for different downstream tasks. These augmentations are interpretable (e.g. altering shapes, digits, or letters added to images) and surprisingly often result in better performance compared to expert-designed augmentations, despite not preserving label information. To support our empirical results, we theoretically analyze a simple contrastive learning setting with a linear model. In this setting, label-destroying augmentations are crucial for preventing one set of features from suppressing the learning of features useful for another downstream task. Our results highlight the need for analyzing the interaction between multiple downstream tasks when trying to explain the success of foundation models.
count=1
* Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/cbe1fd3136e0f049bb8bc104231ccb99-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/cbe1fd3136e0f049bb8bc104231ccb99-Paper-Conference.pdf)]
    * Title: Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models
    * Year: `2023`
    * Authors: Beier Zhu, Kaihua Tang, QIANRU SUN, Hanwang Zhang
    * Abstract: Foundation models like CLIP allow zero-shot transfer on various tasks without additional training data. Yet, the zero-shot performance is less competitive than a fully supervised one. Thus, to enhance the performance, fine-tuning and ensembling are also commonly adopted to better fit the downstream tasks. However, we argue that such prior work has overlooked the inherent biases in foundation models. Due to the highly imbalanced Web-scale training set, these foundation models are inevitably skewed toward frequent semantics, and thus the subsequent fine-tuning or ensembling is still biased. In this study, we systematically examine the biases in foundation models and demonstrate the efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that bias estimation in foundation models is challenging, as most pre-train data cannot be explicitly assessed like in traditional long-tailed classification tasks.To this end, GLA has an optimization-based bias estimation approach for debiasing foundation models. As our work resolves a fundamental flaw in the pre-training, the proposed GLA demonstrates significant improvements across a diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on long-tailed classification. Codes are in https://github.com/BeierZhu/GLA.
count=1
* SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/cdd0640218a27e9e2c0e52e324e25db0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/cdd0640218a27e9e2c0e52e324e25db0-Paper-Conference.pdf)]
    * Title: SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models
    * Year: `2023`
    * Authors: XIAOSONG MA, Jie ZHANG, Song Guo, Wenchao Xu
    * Abstract: Test-time adaptation (TTA) is a special and practical setting in unsupervised domain adaptation, which allows a pre-trained model in a source domain to adapt to unlabeled test data in another target domain. To avoid the computation-intensive backbone fine-tuning process, the zero-shot generalization potentials of the emerging pre-trained vision-language models (e.g., CLIP, CoOp) are leveraged to only tune the run-time prompt for unseen test domains. However, existing solutions have yet to fully exploit the representation capabilities of pre-trained models as they only focus on the entropy-based optimization and the performance is far below the supervised prompt adaptation methods, e.g., CoOp. In this paper, we propose SwapPrompt, a novel framework that can effectively leverage the self-supervised contrastive learning to facilitate the test-time prompt adaptation. SwapPrompt employs a dual prompts paradigm, i.e., an online prompt and a target prompt that averaged from the online prompt to retain historical information. In addition, SwapPrompt applies a swapped prediction mechanism, which takes advantage of the representation capabilities of pre-trained models to enhance the online prompt via contrastive learning. Specifically, we use the online prompt together with an augmented view of the input image to predict the class assignment generated by the target prompt together with an alternative augmented view of the same image. The proposed SwapPrompt can be easily deployed on vision-language models without additional requirement, and experimental results show that it achieves state-of-the-art test-time adaptation performance on ImageNet and nine other datasets. It is also shown that SwapPrompt can even achieve comparable performance with supervised prompt adaptation methods.
count=1
* Gacs-Korner Common Information Variational Autoencoder
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d04f08ccf582011f43af91ee1c1956d2-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d04f08ccf582011f43af91ee1c1956d2-Paper-Conference.pdf)]
    * Title: Gacs-Korner Common Information Variational Autoencoder
    * Year: `2023`
    * Authors: Michael Kleinman, Alessandro Achille, Stefano Soatto, Jonathan Kao
    * Abstract: We propose a notion of common information that allows one to quantify and separate the information that is shared between two random variables from the information that is unique to each. Our notion of common information is defined by an optimization problem over a family of functions and recovers the G\'acs-K\"orner common information as a special case. Importantly, our notion can be approximated empirically using samples from the underlying data distribution. We then provide a method to partition and quantify the common and unique information using a simple modification of a traditional variational auto-encoder. Empirically, we demonstrate that our formulation allows us to learn semantically meaningful common and unique factors of variation even on high-dimensional data such as images and videos. Moreover, on datasets where ground-truth latent factors are known, we show that we can accurately quantify the common information between the random variables.
count=1
* Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d07379f3acf3af51dfc8598862cadfa0-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d07379f3acf3af51dfc8598862cadfa0-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials
    * Year: `2023`
    * Authors: Shengchao Liu, weitao Du, Yanjing Li, Zhuoxinran Li, Zhiling Zheng, Chenru Duan, Zhi-Ming Ma, Omar Yaghi, Animashree Anandkumar, Christian Borgs, Jennifer Chayes, Hongyu Guo, Jian Tang
    * Abstract: Artificial intelligence for scientific discovery has recently generated significant interest within the machine learning and scientific communities, particularly in the domains of chemistry, biology, and material discovery. For these scientific problems, molecules serve as the fundamental building blocks, and machine learning has emerged as a highly effective and powerful tool for modeling their geometric structures. Nevertheless, due to the rapidly evolving process of the field and the knowledge gap between science ({\eg}, physics, chemistry, \& biology) and machine learning communities, a benchmarking study on geometrical representation for such data has not been conducted. To address such an issue, in this paper, we first provide a unified view of the current symmetry-informed geometric methods, classifying them into three main categories: invariance, equivariance with spherical frame basis, and equivariance with vector frame basis. Then we propose a platform, coined Geom3D, which enables benchmarking the effectiveness of geometric strategies. Geom3D contains 16 advanced symmetry-informed geometric representation models and 14 geometric pretraining methods over 52 diverse tasks, including small molecules, proteins, and crystalline materials. We hope that Geom3D can, on the one hand, eliminate barriers for machine learning researchers interested in exploring scientific problems; and, on the other hand, provide valuable guidance for researchers in computational chemistry, structural biology, and materials science, aiding in the informed selection of representation techniques for specific applications. The source code is available on \href{https://github.com/chao1224/Geom3D}{the GitHub repository}.
count=1
* Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d191ba4c8923ed8fd8935b7c98658b5f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d191ba4c8923ed8fd8935b7c98658b5f-Paper-Conference.pdf)]
    * Title: Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels
    * Year: `2023`
    * Authors: Jian Chen, Ruiyi Zhang, Tong Yu, Rohan Sharma, Zhiqiang Xu, Tong Sun, Changyou Chen
    * Abstract: Learning from noisy labels is an important and long-standing problem in machine learning for real applications. One of the main research lines focuses on learning a label corrector to purify potential noisy labels. However, these methods typically rely on strict assumptions and are limited to certain types of label noise. In this paper, we reformulate the label-noise problem from a generative-model perspective, i.e., labels are generated by gradually refining an initial random guess. This new perspective immediately enables existing powerful diffusion models to seamlessly learn the stochastic generative process. Once the generative uncertainty is modeled, we can perform classification inference using maximum likelihood estimation of labels. To mitigate the impact of noisy labels, we propose the Label-Retrieval-Augmented (LRA) diffusion model, which leverages neighbor consistency to effectively construct pseudo-clean labels for diffusion training. Our model is flexible and general, allowing easy incorporation of different types of conditional information, e.g., use of pre-trained models, to further boost model performance. Extensive experiments are conducted for evaluation. Our model achieves new state-of-the-art (SOTA) results on all the standard real-world benchmark datasets. Remarkably, by incorporating conditional information from the powerful CLIP model, our method can boost the current SOTA accuracy by 10-20 absolute points in many cases. Code is available: https://anonymous.4open.science/r/LRA-diffusion-5F2F
count=1
* EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d42db1f74df54cb992b3956eb7f15a6f-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d42db1f74df54cb992b3956eb7f15a6f-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models
    * Year: `2023`
    * Authors: Michael Wornow, Rahul Thapa, Ethan Steinberg, Jason Fries, Nigam Shah
    * Abstract: While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains de-identified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR. We provide an end-to-end pipeline for the community to validate and build upon its performance. Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaptation. Our model and dataset are available via a research data use agreement from here: https://stanfordaimi.azurewebsites.net/. Code to reproduce our results is available here: https://github.com/som-shahlab/ehrshot-benchmark.
count=1
* Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d5c0f9585592bad5251133813893a6c0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d5c0f9585592bad5251133813893a6c0-Paper-Conference.pdf)]
    * Title: Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends
    * Year: `2023`
    * Authors: Wang Xinrui, Wenhai Wan, Chuanxing Geng, Shao-Yuan Li, Songcan Chen
    * Abstract: Learning binary classifiers from positive and unlabeled data (PUL) is vital in many real-world applications, especially when verifying negative examples is difficult. Despite the impressive empirical performance of recent PUL methods, challenges like accumulated errors and increased estimation bias persist due to the absence of negative labels. In this paper, we unveil an intriguing yet long-overlooked observation in PUL: \textit{resampling the positive data in each training iteration to ensure a balanced distribution between positive and unlabeled examples results in strong early-stage performance. Furthermore, predictive trends for positive and negative classes display distinctly different patterns.} Specifically, the scores (output probability) of unlabeled negative examples consistently decrease, while those of unlabeled positive examples show largely chaotic trends. Instead of focusing on classification within individual time frames, we innovatively adopt a holistic approach, interpreting the scores of each example as a temporal point process (TPP). This reformulates the core problem of PUL as recognizing trends in these scores. We then propose a novel TPP-inspired measure for trend detection and prove its asymptotic unbiasedness in predicting changes. Notably, our method accomplishes PUL without requiring additional parameter tuning or prior assumptions, offering an alternative perspective for tackling this problem. Extensive experiments verify the superiority of our method, particularly in a highly imbalanced real-world setting, where it achieves improvements of up to $11.3\%$ in key metrics.
count=1
* Hardware Resilience Properties of Text-Guided Image Classifiers
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/d664de48bf5ad8e8e48c77e175eb0e80-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/d664de48bf5ad8e8e48c77e175eb0e80-Paper-Conference.pdf)]
    * Title: Hardware Resilience Properties of Text-Guided Image Classifiers
    * Year: `2023`
    * Authors: Syed Talal Wasim, Kabila Haile Soboka, Abdulrahman Mahmoud, Salman H. Khan, David Brooks, Gu-Yeon Wei
    * Abstract: This paper presents a novel method to enhance the reliability of image classification models during deployment in the face of transient hardware errors. By utilizing enriched text embeddings derived from GPT-3 with question prompts per class and CLIP pretrained text encoder, we investigate their impact as an initialization for the classification layer. Our approach achieves a remarkable $5.5\times$ average increase in hardware reliability (and up to $14\times$) across various architectures in the most critical layer, with minimal accuracy drop ($0.3\%$ on average) compared to baseline PyTorch models. Furthermore, our method seamlessly integrates with any image classification backbone, showcases results across various network architectures, decreases parameter and FLOPs overhead, and follows a consistent training recipe. This research offers a practical and efficient solution to bolster the robustness of image classification models against hardware failures, with potential implications for future studies in this domain. Our code and models are released at https://github.com/TalalWasim/TextGuidedResilience.
count=1
* xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/db68f1c25678f72561ab7c97ce15d912-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/db68f1c25678f72561ab7c97ce15d912-Paper-Conference.pdf)]
    * Title: xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data
    * Year: `2023`
    * Authors: Jing Gong, Minsheng Hao, Xingyi Cheng, Xin Zeng, Chiming Liu, Jianzhu Ma, Xuegong Zhang, Taifeng Wang, Le Song
    * Abstract: Advances in high-throughput sequencing technology have led to significant progress in measuring gene expressions at the single-cell level. The amount of publicly available single-cell RNA-seq (scRNA-seq) data is already surpassing 50M records for humans with each record measuring 20,000 genes. This highlights the need for unsupervised representation learning to fully ingest these data, yet classical transformer architectures are prohibitive to train on such data in terms of both computation and memory. To address this challenge, we propose a novel asymmetric encoder-decoder transformer for scRNA-seq data, called xTrimoGene$^\alpha$ (or xTrimoGene for short), which leverages the sparse characteristic of the data to scale up the pre-training. This scalable design of xTrimoGene reduces FLOPs by one to two orders of magnitude compared to classical transformers while maintaining high accuracy, enabling us to train the largest transformer models over the largest scRNA-seq dataset today. Our experiments also show that the performance of xTrimoGene improves as we scale up the model sizes, and it also leads to SOTA performance over various downstream tasks, such as cell type annotation, perturb-seq effect prediction, and drug combination prediction. xTrimoGene model is now available for use as a service via the following link: https://api.biomap.com/xTrimoGene/apply.
count=1
* ChatGPT-Powered Hierarchical Comparisons for Image Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/dc81297c791bb989deade65c6bd8c1d8-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/dc81297c791bb989deade65c6bd8c1d8-Paper-Conference.pdf)]
    * Title: ChatGPT-Powered Hierarchical Comparisons for Image Classification
    * Year: `2023`
    * Authors: Zhiyuan Ren, Yiyang Su, Xiaoming Liu
    * Abstract: The zero-shot open-vocabulary setting poses challenges for image classification.Fortunately, utilizing a vision-language model like CLIP, pre-trained on image-textpairs, allows for classifying images by comparing embeddings. Leveraging largelanguage models (LLMs) such as ChatGPT can further enhance CLIP’s accuracyby incorporating class-specific knowledge in descriptions. However, CLIP stillexhibits a bias towards certain classes and generates similar descriptions for similarclasses, disregarding their differences. To address this problem, we present anovel image classification framework via hierarchical comparisons. By recursivelycomparing and grouping classes with LLMs, we construct a class hierarchy. Withsuch a hierarchy, we can classify an image by descending from the top to the bottomof the hierarchy, comparing image and text embeddings at each level. Throughextensive experiments and analyses, we demonstrate that our proposed approach isintuitive, effective, and explainable. Code will be released upon publication.
count=1
* ADGym: Design Choices for Deep Anomaly Detection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/de670b9d118229d09d9a9bd9dec2598b-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/de670b9d118229d09d9a9bd9dec2598b-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: ADGym: Design Choices for Deep Anomaly Detection
    * Year: `2023`
    * Authors: Minqi Jiang, Chaochuan Hou, Ao Zheng, Songqiao Han, Hailiang Huang, Qingsong Wen, Xiyang Hu, Yue Zhao
    * Abstract: Deep learning (DL) techniques have recently found success in anomaly detection (AD) across various fields such as finance, medical services, and cloud computing. However, most of the current research tends to view deep AD algorithms as a whole, without dissecting the contributions of individual design choices like loss functions and network architectures. This view tends to diminish the value of preliminary steps like data preprocessing, as more attention is given to newly designed loss functions, network architectures, and learning paradigms. In this paper, we aim to bridge this gap by asking two key questions: (i) Which design choices in deep AD methods are crucial for detecting anomalies? (ii) How can we automatically select the optimal design choices for a given AD dataset, instead of relying on generic, pre-existing solutions? To address these questions, we introduce ADGym, a platform specifically crafted for comprehensive evaluation and automatic selection of AD design elements in deep methods. Our extensive experiments reveal that relying solely on existing leading methods is not sufficient. In contrast, models developed using ADGym significantly surpass current state-of-the-art techniques.
count=1
* The s-value: evaluating stability with respect to distributional shifts
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/e3fea99df80195b316cefa7aa6099cd5-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/e3fea99df80195b316cefa7aa6099cd5-Paper-Conference.pdf)]
    * Title: The s-value: evaluating stability with respect to distributional shifts
    * Year: `2023`
    * Authors: Suyash Gupta, Dominik Rothenhäusler
    * Abstract: Common statistical measures of uncertainty such as $p$-values and confidence intervals quantify the uncertainty due to sampling, that is, the uncertainty due to not observing the full population. However, sampling is not the only source of uncertainty. In practice, distributions change between locations and across time. This makes it difficult to gather knowledge that transfers across data sets. We propose a measure of instability that quantifies the distributional instability of a statistical parameter with respect to Kullback-Leibler divergence, that is, the sensitivity of the parameter under general distributional perturbations within a Kullback-Leibler divergence ball. In addition, we quantify the instability of parameters with respect to directional or variable-specific shifts. Measuring instability with respect to directional shifts can be used to detect under which kind of distribution shifts a statistical conclusion might be reversed. We discuss how such knowledge can inform data collection for transfer learning of statistical parameters under shifted distributions. We evaluate the performance of the proposed measure on real data and show that it can elucidate the distributional instability of a parameter with respect to certain shifts and can be used to improve estimation accuracy under shifted distributions.
count=1
* Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/e55c2f3fdde519014c879aa3554414c0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/e55c2f3fdde519014c879aa3554414c0-Paper-Conference.pdf)]
    * Title: Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis
    * Year: `2023`
    * Authors: Junfeng Fang, Wei Liu, Yuan Gao, Zemin Liu, An Zhang, Xiang Wang, Xiangnan He
    * Abstract: This work studies the evaluation of explaining graph neural networks (GNNs), which is crucial to the credibility of post-hoc explainability in practical usage. Conventional evaluation metrics, and even explanation methods -- which mainly follow the paradigm of feeding the explanatory subgraph and measuring output difference -- always suffer from the notorious out-of-distribution (OOD) issue. In this work, we endeavor to confront the issue by introducing a novel evaluation metric, termed OOD-resistant Adversarial Robustness (OAR). Specifically, we draw inspiration from the notion of adversarial robustness and evaluate post-hoc explanation subgraphs by calculating their robustness under attack. On top of that, an elaborate OOD reweighting block is inserted into the pipeline to confine the evaluation process to the original data distribution. For applications involving large datasets, we further devise a Simplified version of OAR (SimOAR), which achieves a significant improvement in computational efficiency at the cost of a small amount of performance. Extensive empirical studies validate the effectiveness of our OAR and SimOAR.
count=1
* BanditPAM++: Faster $k$-medoids Clustering
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/e885e5bc6e13b9dd8f80bc5482b1fa2f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/e885e5bc6e13b9dd8f80bc5482b1fa2f-Paper-Conference.pdf)]
    * Title: BanditPAM++: Faster $k$-medoids Clustering
    * Year: `2023`
    * Authors: Mo Tiwari, Ryan Kang, Donghyun Lee, Sebastian Thrun, Ilan Shomorony, Martin J. Zhang
    * Abstract: Clustering is a fundamental task in data science with wide-ranging applications. In $k$-medoids clustering, cluster centers must be actual datapoints and arbitrary distance metrics may be used; these features allow for greater interpretability of the cluster centers and the clustering of exotic objects in $k$-medoids clustering, respectively. $k$-medoids clustering has recently grown in popularity due to the discovery of more efficient $k$-medoids algorithms. In particular, recent research has proposed BanditPAM, a randomized $k$-medoids algorithm with state-of-the-art complexity and clustering accuracy. In this paper, we present BanditPAM++, which accelerates BanditPAM via two algorithmic improvements, and is $O(k)$ faster than BanditPAM in complexity and substantially faster than BanditPAM in wall-clock runtime. First, we demonstrate that BanditPAM has a special structure that allows the reuse of clustering information $\textit{within}$ each iteration. Second, we demonstrate that BanditPAM has additional structure that permits the reuse of information $\textit{across}$ different iterations. These observations inspire our proposed algorithm, BanditPAM++, which returns the same clustering solutions as BanditPAM but often several times faster. For example, on the CIFAR10 dataset, BanditPAM++ returns the same results as BanditPAM but runs over 10$\times$ faster. Finally, we provide a high-performance C++ implementation of BanditPAM++, callable from Python and R, that may be of interest to practitioners at https://github.com/motiwari/BanditPAM. Auxiliary code to reproduce all of our experiments via a one-line script is available at https://github.com/ThrunGroup/BanditPAM_plusplus_experiments.
count=1
* AutoGO: Automated Computation Graph Optimization for Neural Network Evolution
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/eb5d9195b201ec7ba66c8e20b396d349-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/eb5d9195b201ec7ba66c8e20b396d349-Paper-Conference.pdf)]
    * Title: AutoGO: Automated Computation Graph Optimization for Neural Network Evolution
    * Year: `2023`
    * Authors: Mohammad Salameh, Keith Mills, Negar Hassanpour, Fred Han, Shuting Zhang, Wei Lu, Shangling Jui, CHUNHUA ZHOU, Fengyu Sun, Di Niu
    * Abstract: Optimizing Deep Neural Networks (DNNs) to obtain high-quality models for efficient real-world deployment has posed multi-faceted challenges to machine learning engineers. Existing methods either search for neural architectures in heuristic design spaces or apply low-level adjustments to computation primitives to improve inference efficiency on hardware. We present Automated Graph Optimization (AutoGO), a framework to evolve neural networks in a low-level Computation Graph (CG) of primitive operations to improve both its performance and hardware friendliness. Through a tokenization scheme, AutoGO performs variable-sized segment mutations, making both primitive changes and larger-grained changes to CGs. We introduce our segmentation and mutation algorithms, efficient frequent segment mining technique, as well as a pretrained context-aware predictor to estimate the impact of segment replacements. Extensive experimental results show that AutoGO can automatically evolve several typical large convolutional networks to achieve significant task performance improvement and FLOPs reduction on a range of CV tasks, ranging from Classification, Semantic Segmentation, Human Pose Estimation, to Super Resolution, yet without introducing any newer primitive operations. We also demonstrate the lightweight deployment results of AutoGO-optimized super-resolution and denoising U-Nets on a cycle simulator for a Neural Processing Unit (NPU), achieving PSNR improvement and latency/power reduction simultaneously. Code available at https://github.com/Ascend-Research/AutoGO.
count=1
* TRIAGE: Characterizing and auditing training data for improved regression
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/ed687a5f52b651b19e7c18f702907b8b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/ed687a5f52b651b19e7c18f702907b8b-Paper-Conference.pdf)]
    * Title: TRIAGE: Characterizing and auditing training data for improved regression
    * Year: `2023`
    * Authors: Nabeel Seedat, Jonathan Crabbé, Zhaozhi Qian, Mihaela van der Schaar
    * Abstract: Data quality is crucial for robust machine learning algorithms, with the recent interest in data-centric AI emphasizing the importance of training data characterization. However, current data characterization methods are largely focused on classification settings, with regression settings largely understudied. To address this, we introduce TRIAGE, a novel data characterization framework tailored to regression tasks and compatible with a broad class of regressors. TRIAGE utilizes conformal predictive distributions to provide a model-agnostic scoring method, the TRIAGE score. We operationalize the score to analyze individual samples' training dynamics and characterize samples as under-, over-, or well-estimated by the model. We show that TRIAGE's characterization is consistent and highlight its utility to improve performance via data sculpting/filtering, in multiple regression settings. Additionally, beyond sample level, we show TRIAGE enables new approaches to dataset selection and feature acquisition. Overall, TRIAGE highlights the value unlocked by data characterization in real-world regression applications.
count=1
* The Tunnel Effect: Building Data Representations in Deep Neural Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/f249db9ab5975586f36df46f8958c008-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/f249db9ab5975586f36df46f8958c008-Paper-Conference.pdf)]
    * Title: The Tunnel Effect: Building Data Representations in Deep Neural Networks
    * Year: `2023`
    * Authors: Wojciech Masarczyk, Mateusz Ostaszewski, Ehsan Imani, Razvan Pascanu, Piotr Miłoś, Tomasz Trzcinski
    * Abstract: Deep neural networks are widely known for their remarkable effectiveness across various tasks, with the consensus that deeper networks implicitly learn more complex data representations. This paper shows that sufficiently deep networks trained for supervised image classification split into two distinct parts that contribute to the resulting data representations differently. The initial layers create linearly-separable representations, while the subsequent layers, which we refer to as \textit{the tunnel}, compress these representations and have a minimal impact on the overall performance. We explore the tunnel's behavior through comprehensive empirical studies, highlighting that it emerges early in the training process. Its depth depends on the relation between the network's capacity and task complexity. Furthermore, we show that the tunnel degrades out-of-distribution generalization and discuss its implications for continual learning.
count=1
* Exponentially Convergent Algorithms for Supervised Matrix Factorization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/f2c80b3c9cf8102d38c4b21af25d9740-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/f2c80b3c9cf8102d38c4b21af25d9740-Paper-Conference.pdf)]
    * Title: Exponentially Convergent Algorithms for Supervised Matrix Factorization
    * Year: `2023`
    * Authors: Joowon Lee, Hanbaek Lyu, Weixin Yao
    * Abstract: Supervised matrix factorization (SMF) is a classical machine learning method that simultaneously seeks feature extraction and classification tasks, which are not necessarily a priori aligned objectives. Our goal is to use SMF to learn low-rank latent factors that offer interpretable, data-reconstructive, and class-discriminative features, addressing challenges posed by high-dimensional data. Training SMF model involves solving a nonconvex and possibly constrained optimization with at least three blocks of parameters. Known algorithms are either heuristic or provide weak convergence guarantees for special cases. In this paper, we provide a novel framework that `lifts' SMF as a low-rank matrix estimation problem in a combined factor space and propose an efficient algorithm that provably converges exponentially fast to a global minimizer of the objective with arbitrary initialization under mild assumptions. Our framework applies to a wide range of SMF-type problems for multi-class classification with auxiliary features. To showcase an application, we demonstrate that our algorithm successfully identified well-known cancer-associated gene groups for various cancers.
count=1
* Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/fe8debfd5a36ada52e038c8b2078b2ce-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/fe8debfd5a36ada52e038c8b2078b2ce-Paper-Conference.pdf)]
    * Title: Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization
    * Year: `2023`
    * Authors: Jameel Abdul Samadh, Mohammad Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muhammad Muzammal Naseer, Fahad Shahbaz Khan, Salman H. Khan
    * Abstract: The promising zero-shot generalization of vision-language models such as CLIP has led to their adoption using prompt learning for numerous downstream tasks. Previous works have shown test-time prompt tuning using entropy minimization to adapt text prompts for unseen domains. While effective, this overlooks the key cause for performance degradation to unseen domains -- distribution shift. In this work, we explicitly handle this problem by aligning the out-of-distribution (OOD) test sample statistics to those of the source data using prompt tuning. We use a single test sample to adapt multi-modal prompts at test time by minimizing the feature distribution shift to bridge the gap in the test domain. Evaluating against the domain generalization benchmark, our method improves zero-shot top-1 accuracy beyond existing prompt-learning techniques, with a 3.08% improvement over the baseline MaPLe. In cross-dataset generalization with unseen categories across 10 datasets, our method improves consistently across all datasets compared to the existing state-of-the-art. Our source code and models are available at https://jameelhassan.github.io/promptalign
count=1
* Errors-in-variables Fr\'echet Regression with Low-rank Covariate Approximation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/hash/ff06c57ef80625386884906c2d2d2429-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2023/file/ff06c57ef80625386884906c2d2d2429-Paper-Conference.pdf)]
    * Title: Errors-in-variables Fr\'echet Regression with Low-rank Covariate Approximation
    * Year: `2023`
    * Authors: Dogyoon Song, Kyunghee Han
    * Abstract: Fr\'echet regression has emerged as a promising approach for regression analysis involving non-Euclidean response variables. However, its practical applicability has been hindered by its reliance on ideal scenarios with abundant and noiseless covariate data. In this paper, we present a novel estimation method that tackles these limitations by leveraging the low-rank structure inherent in the covariate matrix. Our proposed framework combines the concepts of global Fr\'echet regression and principal component regression, aiming to improve the efficiency and accuracy of the regression estimator. By incorporating the low-rank structure, our method enables more effective modeling and estimation, particularly in high-dimensional and errors-in-variables regression settings. We provide a theoretical analysis of the proposed estimator's large-sample properties, including a comprehensive rate analysis of bias, variance, and additional variations due to measurement errors. Furthermore, our numerical experiments provide empirical evidence that supports the theoretical findings, demonstrating the superior performance of our approach. Overall, this work introduces a promising framework for regression analysis of non-Euclidean variables, effectively addressing the challenges associated with limited and noisy covariate data, with potential applications in diverse fields.
count=1
* UniCLIP: Unified Framework for Contrastive Language-Image Pre-training
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/072fd0525592b43da661e254bbaadc27-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/072fd0525592b43da661e254bbaadc27-Paper-Conference.pdf)]
    * Title: UniCLIP: Unified Framework for Contrastive Language-Image Pre-training
    * Year: `2022`
    * Authors: Janghyeon Lee, Jongsuk Kim, Hyounguk Shon, Bumsoo Kim, Seung Hwan Kim, Honglak Lee, Junmo Kim
    * Abstract: Pre-training vision-language models with contrastive objectives has shown promising results that are both scalable to large uncurated datasets and transferable to many downstream applications. Some following works have targeted to improve data efficiency by adding self-supervision terms, but inter-domain (image-text) contrastive loss and intra-domain (image-image) contrastive loss are defined on individual spaces in those works, so many feasible combinations of supervision are overlooked. To overcome this issue, we propose UniCLIP, a Unified framework for Contrastive Language-Image Pre-training. UniCLIP integrates the contrastive loss of both inter-domain pairs and intra-domain pairs into a single universal space. The discrepancies that occur when integrating contrastive loss between different domains are resolved by the three key components of UniCLIP: (1) augmentation-aware feature embedding, (2) MP-NCE loss, and (3) domain dependent similarity measure. UniCLIP outperforms previous vision-language pre-training methods on various single- and multi-modality downstream tasks. In our experiments, we show that each component that comprises UniCLIP contributes well to the final performance.
count=1
* Adaptive Sampling for Discovery
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/07bc8125400bf4b140c332010756bd9b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/07bc8125400bf4b140c332010756bd9b-Paper-Conference.pdf)]
    * Title: Adaptive Sampling for Discovery
    * Year: `2022`
    * Authors: Ziping Xu, Eunjae Shim, Ambuj Tewari, Paul Zimmerman
    * Abstract: In this paper, we study a sequential decision-making problem, called Adaptive Sampling for Discovery (ASD). Starting with a large unlabeled dataset, algorithms for ASD adaptively label the points with the goal to maximize the sum of responses.This problem has wide applications to real-world discovery problems, for example drug discovery with the help of machine learning models. ASD algorithms face the well-known exploration-exploitation dilemma. The algorithm needs to choose points that yield information to improve model estimates but it also needs to exploit the model. We rigorously formulate the problem and propose a general information-directed sampling (IDS) algorithm. We provide theoretical guarantees for the performance of IDS in linear, graph and low-rank models. The benefits of IDS are shown in both simulation experiments and real-data experiments for discovering chemical reaction conditions.
count=1
* Functional Ensemble Distillation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/0b7f639ef28a9035a71f7e0c04c1d681-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/0b7f639ef28a9035a71f7e0c04c1d681-Paper-Conference.pdf)]
    * Title: Functional Ensemble Distillation
    * Year: `2022`
    * Authors: Coby Penso, Idan Achituve, Ethan Fetaya
    * Abstract: Bayesian models have many desirable properties, most notable is their ability to generalize from limited data and to properly estimate the uncertainty in their predictions. However, these benefits come at a steep computational cost as Bayesian inference, in most cases, is computationally intractable. One popular approach to alleviate this problem is using a Monte-Carlo estimation with an ensemble of models sampled from the posterior. However, this approach still comes at a significant computational cost, as one needs to store and run multiple models at test time. In this work, we investigate how to best distill an ensemble's predictions using an efficient model. First, we argue that current approaches are limited as they are constrained to classification and the Dirichlet distribution. Second, in many limited data settings, all ensemble members achieve nearly zero training loss, namely, they produce near-identical predictions on the training set which results in sub-optimal distilled models. To address both problems, we propose a novel and general distillation approach, named Functional Ensemble Distillation (FED), and we investigate how to best distill an ensemble in this setting. We find that learning the distilled model via a simple augmentation scheme in the form of mixup augmentation significantly boosts the performance. We evaluated our method on several tasks and showed that it achieves superior results in both accuracy and uncertainty estimation compared to current approaches.
count=1
* Use-Case-Grounded Simulations for Explanation Evaluation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/0b9536e186a77feff516893a5f393f7a-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/0b9536e186a77feff516893a5f393f7a-Paper-Conference.pdf)]
    * Title: Use-Case-Grounded Simulations for Explanation Evaluation
    * Year: `2022`
    * Authors: Valerie Chen, Nari Johnson, Nicholay Topin, Gregory Plumb, Ameet Talwalkar
    * Abstract: A growing body of research runs human subject evaluations to study whether providing users with explanations of machine learning models can help them with practical real-world use cases. However, running user studies is challenging and costly, and consequently each study typically only evaluates a limited number of different settings, e.g., studies often only evaluate a few arbitrarily selected model explanation methods. To address these challenges and aid user study design, we introduce Simulated Evaluations (SimEvals). SimEvals involve training algorithmic agents that take as input the information content (such as model explanations) that would be presented to the user, to predict answers to the use case of interest. The algorithmic agent's test set accuracy provides a measure of the predictiveness of the information content for the downstream use case. We run a comprehensive evaluation on three real-world use cases (forward simulation, model debugging, and counterfactual reasoning) to demonstrate that SimEvals can effectively identify which explanation methods will help humans for each use case. These results provide evidence that \simevals{} can be used to efficiently screen an important set of user study design decisions, e.g., selecting which explanations should be presented to the user, before running a potentially costly user study.
count=1
* On the Epistemic Limits of Personalized Prediction
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/0cfc9404f89400c5ed897035e0d3748c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/0cfc9404f89400c5ed897035e0d3748c-Paper-Conference.pdf)]
    * Title: On the Epistemic Limits of Personalized Prediction
    * Year: `2022`
    * Authors: Lucas Monteiro Paes, Carol Long, Berk Ustun, Flavio Calmon
    * Abstract: Machine learning models are often personalized by using group attributes that encode personal characteristics (e.g., sex, age group, HIV status). In such settings, individuals expect to receive more accurate predictions in return for disclosing group attributes to the personalized model. We study when we can tell that a personalized model upholds this principle for every group who provides personal data. We introduce a metric called the benefit of personalization (BoP) to measure the smallest gain in accuracy that any group expects to receive from a personalized model. We describe how the BoP can be used to carry out basic routines to audit a personalized model, including: (i) hypothesis tests to check that a personalized model improves performance for every group; (ii) estimation procedures to bound the minimum gain in personalization. We characterize the reliability of these routines in a finite-sample regime and present minimax bounds on both the probability of error for BoP hypothesis tests and the mean-squared error of BoP estimates. Our results show that we can only claim that personalization improves performance for each group who provides data when we explicitly limit the number of group attributes used by a personalized model. In particular, we show that it is impossible to reliably verify that a personalized classifier with $k \geq 19$ binary group attributes will benefit every group who provides personal data using a dataset of $n = 8\times10^9$ samples -- one for each person in the world.
count=1
* Deep Differentiable Logic Gate Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/0d3496dd0cec77a999c98d35003203ca-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/0d3496dd0cec77a999c98d35003203ca-Paper-Conference.pdf)]
    * Title: Deep Differentiable Logic Gate Networks
    * Year: `2022`
    * Authors: Felix Petersen, Christian Borgelt, Hilde Kuehne, Oliver Deussen
    * Abstract: Recently, research has increasingly focused on developing efficient neural network architectures. In this work, we explore logic gate networks for machine learning tasks by learning combinations of logic gates. These networks comprise logic gates such as "AND" and "XOR", which allow for very fast execution. The difficulty in learning logic gate networks is that they are conventionally non-differentiable and therefore do not allow training with gradient descent. Thus, to allow for effective training, we propose differentiable logic gate networks, an architecture that combines real-valued logics and a continuously parameterized relaxation of the network. The resulting discretized logic gate networks achieve fast inference speeds, e.g., beyond a million images of MNIST per second on a single CPU core.
count=1
* CascadeXML: Rethinking Transformers for End-to-end Multi-resolution Training in Extreme Multi-label Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/0e0157ce5ea15831072be4744cbd5334-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/0e0157ce5ea15831072be4744cbd5334-Paper-Conference.pdf)]
    * Title: CascadeXML: Rethinking Transformers for End-to-end Multi-resolution Training in Extreme Multi-label Classification
    * Year: `2022`
    * Authors: Siddhant Kharbanda, Atmadeep Banerjee, Erik Schultheis, Rohit Babbar
    * Abstract: Extreme Multi-label Text Classification (XMC) involves learning a classifier that can assign an input with a subset of most relevant labels from millions of label choices. Recent approaches, such as XR-Transformer and LightXML, leverage a transformer instance to achieve state-of-the-art performance. However, in this process, these approaches need to make various trade-offs between performance and computational requirements. A major shortcoming, as compared to the Bi-LSTM based AttentionXML, is that they fail to keep separate feature representations for each resolution in a label tree. We thus propose CascadeXML, an end-to-end multi-resolution learning pipeline, which can harness the multi-layered architecture of a transformer model for attending to different label resolutions with separate feature representations. CascadeXML significantly outperforms all existing approaches with non-trivial gains obtained on benchmark datasets consisting of up to three million labels. Code for CascadeXML will be made publicly available at https://github.com/xmc-aalto/cascadexml.
count=1
* Debiased Machine Learning without Sample-Splitting for Stable Estimators
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/1498a03a04f9bcd3a7d44058fc5dc639-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/1498a03a04f9bcd3a7d44058fc5dc639-Paper-Conference.pdf)]
    * Title: Debiased Machine Learning without Sample-Splitting for Stable Estimators
    * Year: `2022`
    * Authors: Qizhao Chen, Vasilis Syrgkanis, Morgane Austern
    * Abstract: Estimation and inference on causal parameters is typically reduced to a generalized method of moments problem, which involves auxiliary functions that correspond to solutions to a regression or classification problem. Recent line of work on debiased machine learning shows how one can use generic machine learning estimators for these auxiliary problems, while maintaining asymptotic normality and root-$n$ consistency of the target parameter of interest, while only requiring mean-squared-error guarantees from the auxiliary estimation algorithms. The literature typically requires that these auxiliary problems are fitted on a separate sample or in a cross-fitting manner. We show that when these auxiliary estimation algorithms satisfy natural leave-one-out stability properties, then sample splitting is not required. This allows for sample re-use, which can be beneficial in moderately sized sample regimes. For instance, we show that the stability properties that we propose are satisfied for ensemble bagged estimators, built via sub-sampling without replacement, a popular technique in machine learning practice.
count=1
* Your Out-of-Distribution Detection Method is Not Robust!
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/1f6591cc41be737e9ba4cc487ac8082d-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/1f6591cc41be737e9ba4cc487ac8082d-Paper-Conference.pdf)]
    * Title: Your Out-of-Distribution Detection Method is Not Robust!
    * Year: `2022`
    * Authors: Mohammad Azizmalayeri, Arshia Soltani Moakhar, Arman Zarei, Reihaneh Zohrabi, Mohammad Manzuri, Mohammad Hossein Rohban
    * Abstract: Out-of-distribution (OOD) detection has recently gained substantial attention due to the importance of identifying out-of-domain samples in reliability and safety. Although OOD detection methods have advanced by a great deal, they are still susceptible to adversarial examples, which is a violation of their purpose. To mitigate this issue, several defenses have recently been proposed. Nevertheless, these efforts remained ineffective, as their evaluations are based on either small perturbation sizes, or weak attacks. In this work, we re-examine these defenses against an end-to-end PGD attack on in/out data with larger perturbation sizes, e.g. up to commonly used $\epsilon=8/255$ for the CIFAR-10 dataset. Surprisingly, almost all of these defenses perform worse than a random detection under the adversarial setting. Next, we aim to provide a robust OOD detection method. In an ideal defense, the training should expose the model to almost all possible adversarial perturbations, which can be achieved through adversarial training. That is, such training perturbations should based on both in- and out-of-distribution samples. Therefore, unlike OOD detection in the standard setting, access to OOD, as well as in-distribution, samples sounds necessary in the adversarial training setup. These tips lead us to adopt generative OOD detection methods, such as OpenGAN, as a baseline. We subsequently propose the Adversarially Trained Discriminator (ATD), which utilizes a pre-trained robust model to extract robust features, and a generator model to create OOD samples. We noted that, for the sake of training stability, in the adversarial training of the discriminator, one should attack real in-distribution as well as real outliers, but not generated outliers. Using ATD with CIFAR-10 and CIFAR-100 as the in-distribution data, we could significantly outperform all previous methods in the robust AUROC while maintaining high standard AUROC and classification accuracy. The code repository is available at https://github.com/rohban-lab/ATD.
count=1
* Discovering and Overcoming Limitations of Noise-engineered Data-free Knowledge Distillation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/1f96b24df4b06f5d68389845a9a13ed9-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/1f96b24df4b06f5d68389845a9a13ed9-Paper-Conference.pdf)]
    * Title: Discovering and Overcoming Limitations of Noise-engineered Data-free Knowledge Distillation
    * Year: `2022`
    * Authors: Piyush Raikwar, Deepak Mishra
    * Abstract: Distillation in neural networks using only the samples randomly drawn from a Gaussian distribution is possibly the most straightforward solution one can think of for the complex problem of knowledge transfer from one network (teacher) to the other (student). If successfully done, it can eliminate the requirement of teacher's training data for knowledge distillation and avoid often arising privacy concerns in sensitive applications such as healthcare. There have been some recent attempts at Gaussian noise-based data-free knowledge distillation, however, none of them offer a consistent or reliable solution. We identify the shift in the distribution of hidden layer activation as the key limiting factor, which occurs when Gaussian noise is fed to the teacher network instead of the accustomed training data. We propose a simple solution to mitigate this shift and show that for vision tasks, such as classification, it is possible to achieve a performance close to the teacher by just using the samples randomly drawn from a Gaussian distribution. We validate our approach on CIFAR10, CIFAR100, SVHN, and Food101 datasets. We further show that in situations of sparsely available original data for distillation, the proposed Gaussian noise-based knowledge distillation method can outperform the distillation using the available data with a large margin. Our work lays the foundation for further research in the direction of noise-engineered knowledge distillation using random samples.
count=1
* Sample Constrained Treatment Effect Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/240ea1741b205ea295721d55184ac43b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/240ea1741b205ea295721d55184ac43b-Paper-Conference.pdf)]
    * Title: Sample Constrained Treatment Effect Estimation
    * Year: `2022`
    * Authors: Raghavendra Addanki, David Arbour, Tung Mai, Cameron Musco, Anup Rao
    * Abstract: Treatment effect estimation is a fundamental problem in causal inference. We focus on designing efficient randomized controlled trials, to accurately estimate the effect of some treatment on a population of $n$ individuals. In particular, we study \textit{sample-constrained treatment effect estimation}, where we must select a subset of $s \ll n$ individuals from the population to experiment on. This subset must be further partitioned into treatment and control groups. Algorithms for partitioning the entire population into treatment and control groups, or for choosing a single representative subset, have been well-studied. The key challenge in our setting is jointly choosing a representative subset and a partition for that set. We focus on both individual and average treatment effect estimation, under a linear effects model. We give provably efficient experimental designs and corresponding estimators, by identifying connections to discrepancy minimization and leverage-score-based sampling used in randomized numerical linear algebra. Our theoretical results obtain a smooth transition to known guarantees when $s$ equals the population size. We also empirically demonstrate the performance of our algorithms.
count=1
* RényiCL: Contrastive Representation Learning with Skew Rényi Divergence
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/2a7157c84dcf263f77b37d6c11d7d149-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/2a7157c84dcf263f77b37d6c11d7d149-Paper-Conference.pdf)]
    * Title: RényiCL: Contrastive Representation Learning with Skew Rényi Divergence
    * Year: `2022`
    * Authors: Kyungmin Lee, Jinwoo Shin
    * Abstract: Contrastive representation learning seeks to acquire useful representations by estimating the shared information between multiple views of data. Here, the choice of data augmentation is sensitive to the quality of learned representations: as harder the data augmentations are applied, the views share more task-relevant information, but also task-irrelevant one that can hinder the generalization capability of representation. Motivated by this, we present a new robust contrastive learning scheme, coined RényiCL, which can effectively manage harder augmentations by utilizing Rényi divergence. Our method is built upon the variational lower bound of a Rényi divergence, but a naive usage of a variational method exhibits unstable training due to the large variance. To tackle this challenge, we propose a novel contrastive objective that conducts variational estimation of a skew Renyi divergence and provides a theoretical guarantee on how variational estimation of skew divergence leads to stable training. We show that Rényi contrastive learning objectives perform innate hard negative sampling and easy positive sampling simultaneously so that it can selectively learn useful features and ignore nuisance features. Through experiments on ImageNet, we show that Rényi contrastive learning with stronger augmentations outperforms other self-supervised methods without extra regularization or computational overhead. Also, we validate our method on various domains such as graph and tabular datasets, showing empirical gain over original contrastive methods.
count=1
* Guaranteed Conservation of Momentum for Learning Particle-based Fluid Dynamics
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/2dd7f33ffbb59b4ff987be5442a13016-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/2dd7f33ffbb59b4ff987be5442a13016-Paper-Conference.pdf)]
    * Title: Guaranteed Conservation of Momentum for Learning Particle-based Fluid Dynamics
    * Year: `2022`
    * Authors: Lukas Prantl, Benjamin Ummenhofer, Vladlen Koltun, Nils Thuerey
    * Abstract: We present a novel method for guaranteeing linear momentum in learned physics simulations. Unlike existing methods, we enforce conservation of momentum with a hard constraint, which we realize via antisymmetrical continuous convolutional layers. We combine these strict constraints with a hierarchical network architecture, a carefully constructed resampling scheme, and a training approach for temporal coherence. In combination, the proposed method allows us to increase the physical accuracy of the learned simulator substantially. In addition, the induced physical bias leads to significantly better generalization performance and makes our method more reliable in unseen test cases. We evaluate our method on a range of different, challenging fluid scenarios. Among others, we demonstrate that our approach generalizes to new scenarios with up to one million particles. Our results show that the proposed algorithm can learn complex dynamics while outperforming existing approaches in generalization and training performance. An implementation of our approach is available at https://github.com/tum-pbs/DMCF.
count=1
* Out-of-Distribution Detection with An Adaptive Likelihood Ratio on Informative Hierarchical VAE
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/3066f60a91d652f4dc690637ac3a2f8c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/3066f60a91d652f4dc690637ac3a2f8c-Paper-Conference.pdf)]
    * Title: Out-of-Distribution Detection with An Adaptive Likelihood Ratio on Informative Hierarchical VAE
    * Year: `2022`
    * Authors: Yewen Li, Chaojie Wang, Xiaobo Xia, Tongliang Liu, xin miao, Bo An
    * Abstract: Unsupervised out-of-distribution (OOD) detection is essential for the reliability of machine learning. In the literature, existing work has shown that higher-level semantics captured by hierarchical VAEs can be used to detect OOD instances.However, we empirically show that, the inherent issue of hierarchical VAEs, i.e., `posterior collapse'', would seriously limit their capacity for OOD detection.Based on a thorough analysis forposterior collapse'', we propose a novel informative hierarchical VAE to alleviate this issue through enhancing the connections between the data sample and its multi-layer stochastic latent representations during training.Furthermore, we propose a novel score function for unsupervised OOD detection, referred to as Adaptive Likelihood Ratio. With this score function, one can selectively aggregate the semantic information on multiple hidden layers of hierarchical VAEs, leading to a strong separability between in-distribution and OOD samples. Experimental results demonstrate that our method can significantly outperform existing state-of-the-art unsupervised OOD detection approaches.
count=1
* Log-Linear-Time Gaussian Processes Using Binary Tree Kernels
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/359ddb9caccb4c54cc915dceeacf4892-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/359ddb9caccb4c54cc915dceeacf4892-Paper-Conference.pdf)]
    * Title: Log-Linear-Time Gaussian Processes Using Binary Tree Kernels
    * Year: `2022`
    * Authors: Michael K. Cohen, Samuel Daulton, Michael A Osborne
    * Abstract: Gaussian processes (GPs) produce good probabilistic models of functions, but most GP kernels require $O((n+m)n^2)$ time, where $n$ is the number of data points and $m$ the number of predictive locations. We present a new kernel that allows for Gaussian process regression in $O((n+m)\log(n+m))$ time. Our "binary tree" kernel places all data points on the leaves of a binary tree, with the kernel depending only on the depth of the deepest common ancestor. We can store the resulting kernel matrix in $O(n)$ space in $O(n \log n)$ time, as a sum of sparse rank-one matrices, and approximately invert the kernel matrix in $O(n)$ time. Sparse GP methods also offer linear run time, but they predict less well than higher dimensional kernels. On a classic suite of regression tasks, we compare our kernel against Mat\'ern, sparse, and sparse variational kernels. The binary tree GP assigns the highest likelihood to the test data on a plurality of datasets, usually achieves lower mean squared error than the sparse methods, and often ties or beats the Mat\'ern GP. On large datasets, the binary tree GP is fastest, and much faster than a Mat\'ern GP.
count=1
* Neural Basis Models for Interpretability
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/37da88965c016dca016514df0e420c72-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/37da88965c016dca016514df0e420c72-Paper-Conference.pdf)]
    * Title: Neural Basis Models for Interpretability
    * Year: `2022`
    * Authors: Filip Radenovic, Abhimanyu Dubey, Dhruv Mahajan
    * Abstract: Due to the widespread use of complex machine learning models in real-world applications, it is becoming critical to explain model predictions. However, these models are typically black-box deep neural networks, explained post-hoc via methods with known faithfulness limitations. Generalized Additive Models (GAMs) are an inherently interpretable class of models that address this limitation by learning a non-linear shape function for each feature separately, followed by a linear model on top. However, these models are typically difficult to train, require numerous parameters, and are difficult to scale. We propose an entirely new subfamily of GAMs that utilizes basis decomposition of shape functions. A small number of basis functions are shared among all features, and are learned jointly for a given task, thus making our model scale much better to large-scale data with high-dimensional features, especially when features are sparse. We propose an architecture denoted as the Neural Basis Model (NBM) which uses a single neural network to learn these bases. On a variety of tabular and image datasets, we demonstrate that for interpretable machine learning, NBMs are the state-of-the-art in accuracy, model size, and, throughput and can easily model all higher-order feature interactions. Source code is available at \href{https://github.com/facebookresearch/nbm-spam}{\ttfamily github.com/facebookresearch/nbm-spam}.
count=1
* Empirical Gateaux Derivatives for Causal Inference
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/3848fef259495bfd04d60cdc5c1b4db7-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/3848fef259495bfd04d60cdc5c1b4db7-Paper-Conference.pdf)]
    * Title: Empirical Gateaux Derivatives for Causal Inference
    * Year: `2022`
    * Authors: Michael Jordan, Yixin Wang, Angela Zhou
    * Abstract: We study a constructive procedure that approximates Gateaux derivatives for statistical functionals by finite-differencing, with attention to causal inference functionals. We focus on the case where probability distributions are not known a priori but need also to be estimated from data, leading to empirical Gateaux derivatives, and study relationships between empirical, numerical, and analytical Gateaux derivatives. Starting with a case study of counterfactual mean estimation, we verify the exact relationship between finite-differences and the analytical Gateaux derivative. We then derive requirements on the rates of numerical approximation in perturbation and smoothing that preserve statistical benefits. We study more complicated functionals such as dynamic treatment regimes and the linear-programming formulation for policy optimization infinite-horizon Markov decision processes. In the case of the latter, this approach can be used to approximate bias adjustments in the presence of arbitrary constraints, illustrating the usefulness of constructive approaches for Gateaux derivatives. We find that, omitting unfavorable dimension dependence of smoothing, although rate-double robustness permits for coarser rates of perturbation size than implied by generic approximation analysis of finite-differences for the case of the counterfactual mean, this is not the case for the infinite-horizon MDP policy value.
count=1
* “Why Not Other Classes?”: Towards Class-Contrastive Back-Propagation Explanations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/3b7a66b2d1258e892c89f485b8f896e0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/3b7a66b2d1258e892c89f485b8f896e0-Paper-Conference.pdf)]
    * Title: “Why Not Other Classes?”: Towards Class-Contrastive Back-Propagation Explanations
    * Year: `2022`
    * Authors: Yipei Wang, Xiaoqian Wang
    * Abstract: Numerous methods have been developed to explain the inner mechanism of deep neural network (DNN) based classifiers. Existing explanation methods are often limited to explaining predictions of a pre-specified class, which answers the question “why is the input classified into this class?” However, such explanations with respect to a single class are inherently insufficient because they do not capture features with class-discriminative power. That is, features that are important for predicting one class may also be important for other classes. To capture features with true class-discriminative power, we should instead ask “why is the input classified into this class, but not others?” To answer this question, we propose a weighted contrastive framework for explaining DNNs. Our framework can easily convert any existing back-propagation explanation methods to build class-contrastive explanations. We theoretically validate our weighted contrast explanation in general back-propagation explanations, and show that our framework enables class-contrastive explanations with significant improvements in both qualitative and quantitative experiments. Based on the results, we point out an important blind spot in the current explainable artificial intelligence (XAI) study, where explanations towards the predicted logits and the probabilities are obfuscated. We suggest that these two aspects should be distinguished explicitly any time explanation methods are applied.
count=1
* ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/3c4688b6a76f25f2311daa0d75a58f1a-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/3c4688b6a76f25f2311daa0d75a58f1a-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models
    * Year: `2022`
    * Authors: Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, Jianfeng Gao
    * Abstract: Learning visual representations from natural language supervision has recently shown great promise in a number of pioneering works. In general, these language-augmented visual models demonstrate strong transferability to a variety of datasets/tasks. However, it remains challenging to evaluate the transferablity of these foundation models due to the lack of easy-to-use toolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation of Language-augmented Visual Task-level Transfer), the first benchmark to compare and evaluate pre-trained language-augmented visual models. Several highlights include: (i) Datasets. As downstream evaluation suites, it consists of 20 image classification datasets and 35 object detection datasets, each of which is augmented with external knowledge. (ii) Toolkit. An automatic hyper-parameter tuning toolkit is developed to ensure the fairness in model adaption. To leverage the full power of language-augmented visual models, novel language-aware initialization methods are proposed to significantly improve the adaption performance. (iii) Metrics. A variety of evaluation metrics are used, including sample-efficiency (zero-shot and few-shot) and parameter-efficiency (linear probing and full model fine-tuning). We will publicly release ELEVATER.
count=1
* RainNet: A Large-Scale Imagery Dataset and Benchmark for Spatial Precipitation Downscaling
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/3fbf0c1ea0716c03dea93bb6be78dd6f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/3fbf0c1ea0716c03dea93bb6be78dd6f-Paper-Conference.pdf)]
    * Title: RainNet: A Large-Scale Imagery Dataset and Benchmark for Spatial Precipitation Downscaling
    * Year: `2022`
    * Authors: Xuanhong Chen, Kairui Feng, Naiyuan Liu, Bingbing Ni, Yifan Lu, Zhengyan Tong, Ziang Liu
    * Abstract: AI-for-science approaches have been applied to solve scientific problems (e.g., nuclear fusion, ecology, genomics, meteorology) and have achieved highly promising results. Spatial precipitation downscaling is one of the most important meteorological problem and urgently requires the participation of AI. However, the lack of a well-organized and annotated large-scale dataset hinders the training and verification of more effective and advancing deep-learning models for precipitation downscaling. To alleviate these obstacles, we present the first large-scale spatial precipitation downscaling dataset named RainNet, which contains more than 62,400 pairs of high-quality low/high-resolution precipitation maps for over 17 years, ready to help the evolution of deep learning models in precipitation downscaling. Specifically, the precipitation maps carefully collected in RainNet cover various meteorological phenomena (e.g., hurricane, squall), which is of great help to improve the model generalization ability. In addition, the map pairs in RainNet are organized in the form of image sequences (720 maps per month or 1 map/hour), showing complex physical properties, e.g., temporal misalignment, temporal sparse, and fluid properties. Furthermore, two deep-learning-oriented metrics are specifically introduced to evaluate or verify the comprehensive performance of the trained model (e.g., prediction maps reconstruction accuracy). To illustrate the applications of RainNet, 14 state-of-the-art models, including deep models and traditional approaches, are evaluated. To fully explore potential downscaling solutions, we propose an implicit physical estimation benchmark framework to learn the above characteristics. Extensive experiments demonstrate the value of RainNet in training and evaluating downscaling models. Our dataset is available at https://neuralchen.github.io/RainNet/.
count=1
* Dataset Distillation using Neural Feature Regression
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/3fe2a777282299ecb4f9e7ebb531f0ab-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/3fe2a777282299ecb4f9e7ebb531f0ab-Paper-Conference.pdf)]
    * Title: Dataset Distillation using Neural Feature Regression
    * Year: `2022`
    * Authors: Yongchao Zhou, Ehsan Nezhadarya, Jimmy Ba
    * Abstract: Dataset distillation aims to learn a small synthetic dataset that preserves most of the information from the original dataset. Dataset distillation can be formulated as a bi-level meta-learning problem where the outer loop optimizes the meta-dataset and the inner loop trains a model on the distilled data. Meta-gradient computation is one of the key challenges in this formulation, as differentiating through the inner loop learning procedure introduces significant computation and memory costs. In this paper, we address these challenges using neural Feature Regression with Pooling (FRePo), achieving the state-of-the-art performance with an order of magnitude less memory requirement and two orders of magnitude faster training than previous methods. The proposed algorithm is analogous to truncated backpropagation through time with a pool of models to alleviate various types of overfitting in dataset distillation. FRePo significantly outperforms the previous methods on CIFAR100, Tiny ImageNet, and ImageNet-1K. Furthermore, we show that high-quality distilled data can greatly improve various downstream applications, such as continual learning and membership inference defense. Please check out our webpage at https://sites.google.com/view/frepo.
count=1
* Momentum Adversarial Distillation: Handling Large Distribution Shifts in Data-Free Knowledge Distillation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/41128e5b3a7622da5b17588757599077-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/41128e5b3a7622da5b17588757599077-Paper-Conference.pdf)]
    * Title: Momentum Adversarial Distillation: Handling Large Distribution Shifts in Data-Free Knowledge Distillation
    * Year: `2022`
    * Authors: Kien Do, Thai Hung Le, Dung Nguyen, Dang Nguyen, HARIPRIYA HARIKUMAR, Truyen Tran, Santu Rana, Svetha Venkatesh
    * Abstract: Data-free Knowledge Distillation (DFKD) has attracted attention recently thanks to its appealing capability of transferring knowledge from a teacher network to a student network without using training data. The main idea is to use a generator to synthesize data for training the student. As the generator gets updated, the distribution of synthetic data will change. Such distribution shift could be large if the generator and the student are trained adversarially, causing the student to forget the knowledge it acquired at the previous steps. To alleviate this problem, we propose a simple yet effective method called Momentum Adversarial Distillation (MAD) which maintains an exponential moving average (EMA) copy of the generator and uses synthetic samples from both the generator and the EMA generator to train the student. Since the EMA generator can be considered as an ensemble of the generator's old versions and often undergoes a smaller change in updates compared to the generator, training on its synthetic samples can help the student recall the past knowledge and prevent the student from adapting too quickly to the new updates of the generator. Our experiments on six benchmark datasets including big datasets like ImageNet and Places365 demonstrate the superior performance of MAD over competing methods for handling the large distribution shift problem. Our method also compares favorably to existing DFKD methods and even achieves state-of-the-art results in some cases.
count=1
* End-to-end Symbolic Regression with Transformers
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/42eb37cdbefd7abae0835f4b67548c39-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/42eb37cdbefd7abae0835f4b67548c39-Paper-Conference.pdf)]
    * Title: End-to-end Symbolic Regression with Transformers
    * Year: `2022`
    * Authors: Pierre-alexandre Kamienny, Stéphane d'Ascoli, Guillaume Lample, Francois Charton
    * Abstract: Symbolic regression, the task of predicting the mathematical expression of a function from the observation of its values, is a difficult task which usually involves a two-step procedure: predicting the "skeleton" of the expression up to the choice of numerical constants, then fitting the constants by optimizing a non-convex loss function. The dominant approach is genetic programming, which evolves candidates by iterating this subroutine a large number of times. Neural networks have recently been tasked to predict the correct skeleton in a single try, but remain much less powerful.In this paper, we challenge this two-step procedure, and task a Transformer to directly predict the full mathematical expression, constants included. One can subsequently refine the predicted constants by feeding them to the non-convex optimizer as an informed initialization. We present ablations to show that this end-to-end approach yields better results, sometimes even without the refinement step. We evaluate our model on problems from the SRBench benchmark and show that our model approaches the performance of state-of-the-art genetic programming with several orders of magnitude faster inference.
count=1
* Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/43119db5d59f07cc08fca7ba6820179a-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/43119db5d59f07cc08fca7ba6820179a-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time
    * Year: `2022`
    * Authors: Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei W. Koh, Chelsea Finn
    * Abstract: Distribution shifts occur when the test distribution differs from the training distribution, and can considerably degrade performance of machine learning models deployed in the real world. While recent works have studied robustness to distribution shifts, distribution shifts arising from the passage of time have the additional structure of timestamp metadata. Real-world examples of such shifts are underexplored, and it is unclear whether existing models can leverage trends in past distribution shifts to reliably extrapolate into the future. To address this gap, we curate Wild-Time, a benchmark of 5 datasets that reflect temporal distribution shifts arising in a variety of real-world applications, including drug discovery, patient prognosis, and news classification. On these datasets, we systematically benchmark 13 approaches with various inductive biases. We evaluate methods in domain-generalization, continual learning, self-supervised learning, and ensemble learning, which leverage timestamps to extract the common structure of the distribution shifts. We extend several domain-generalization methods to the temporal distribution shift setting by treating windows of time as different domains. Finally, we propose two evaluation strategies to evaluate model performance under temporal distribution shifts---evaluation with a fixed time split (Eval-Fix) and evaluation with a data stream (Eval-Stream). Eval-Fix, our primary evaluation strategy, aims to provide a simple evaluation protocol for the broader machine learning community, while Eval-Stream serves as a complementary benchmark for continual learning approaches. Our experiments demonstrate that existing methods are limited in tackling temporal distribution shift: across all settings, we observe an average performance drop of 20% from in-distribution to out-of-distribution data.
count=1
* Improving Self-Supervised Learning by Characterizing Idealized Representations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/494f876fad056843f310ad647274dd99-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/494f876fad056843f310ad647274dd99-Paper-Conference.pdf)]
    * Title: Improving Self-Supervised Learning by Characterizing Idealized Representations
    * Year: `2022`
    * Authors: Yann Dubois, Stefano Ermon, Tatsunori B. Hashimoto, Percy S. Liang
    * Abstract: Despite the empirical successes of self-supervised learning (SSL) methods, it is unclear what characteristics of their representations lead to high downstream accuracies. In this work, we characterize properties that SSL representations should ideally satisfy. Specifically, we prove necessary and sufficient conditions such that for any task invariant to given data augmentations, probes (e.g., linear or MLP) trained on that representation attain perfect accuracy. These requirements lead to a unifying conceptual framework for improving existing SSL methods and deriving new ones. For contrastive learning, our framework prescribes simple but significant improvements to previous methods such as using asymmetric projection heads. For non-contrastive learning, we use our framework to derive a simple and novel objective. Our resulting SSL algorithms outperform baselines on standard benchmarks, including SwAV+multicrops on linear probing of ImageNet.
count=1
* Split-kl and PAC-Bayes-split-kl Inequalities for Ternary Random Variables
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/49ffa271264808cf500ea528ed8ec9b3-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/49ffa271264808cf500ea528ed8ec9b3-Paper-Conference.pdf)]
    * Title: Split-kl and PAC-Bayes-split-kl Inequalities for Ternary Random Variables
    * Year: `2022`
    * Authors: Yi-Shan Wu, Yevgeny Seldin
    * Abstract: We present a new concentration of measure inequality for sums of independent bounded random variables, which we name a split-kl inequality. The inequality combines the combinatorial power of the kl inequality with ability to exploit low variance. While for Bernoulli random variables the kl inequality is tighter than the Empirical Bernstein, for random variables taking values inside a bounded interval and having low variance the Empirical Bernstein inequality is tighter than the kl. The proposed split-kl inequality yields the best of both worlds. We discuss an application of the split-kl inequality to bounding excess losses. We also derive a PAC-Bayes-split-kl inequality and use a synthetic example and several UCI datasets to compare it with the PAC-Bayes-kl, PAC-Bayes Empirical Bernstein, PAC-Bayes Unexpected Bernstein, and PAC-Bayes Empirical Bennett inequalities.
count=1
* Joint Entropy Search For Maximally-Informed Bayesian Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/4b03821747e89ce803b2dac590f6a39b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/4b03821747e89ce803b2dac590f6a39b-Paper-Conference.pdf)]
    * Title: Joint Entropy Search For Maximally-Informed Bayesian Optimization
    * Year: `2022`
    * Authors: Carl Hvarfner, Frank Hutter, Luigi Nardi
    * Abstract: Information-theoretic Bayesian optimization techniques have become popular for optimizing expensive-to-evaluate black-box functions due to their non-myopic qualities. Entropy Search and Predictive Entropy Search both consider the entropy over the optimum in the input space, while the recent Max-value Entropy Search considers the entropy over the optimal value in the output space. We propose Joint Entropy Search (JES), a novel information-theoretic acquisition function that considers an entirely new quantity, namely the entropy over the joint optimal probability density over both input and output space. To incorporate this information, we consider the reduction in entropy from conditioning on fantasized optimal input/output pairs. The resulting approach primarily relies on standard GP machinery and removes complex approximations typically associated with information-theoretic methods. With minimal computational overhead, JES shows superior decision-making, and yields state-of-the-art performance for information-theoretic approaches across a wide suite of tasks. As a light-weight approach with superior results, JES provides a new go-to acquisition function for Bayesian optimization.
count=1
* GAL: Gradient Assisted Learning for Decentralized Multi-Organization Collaborations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/4d6938f94ab47d32128c239a4bfedae0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/4d6938f94ab47d32128c239a4bfedae0-Paper-Conference.pdf)]
    * Title: GAL: Gradient Assisted Learning for Decentralized Multi-Organization Collaborations
    * Year: `2022`
    * Authors: Enmao Diao, Jie Ding, Vahid Tarokh
    * Abstract: Collaborations among multiple organizations, such as financial institutions, medical centers, and retail markets in decentralized settings are crucial to providing improved service and performance. However, the underlying organizations may have little interest in sharing their local data, models, and objective functions. These requirements have created new challenges for multi-organization collaboration. In this work, we propose Gradient Assisted Learning (GAL), a new method for multiple organizations to assist each other in supervised learning tasks without sharing local data, models, and objective functions. In this framework, all participants collaboratively optimize the aggregate of local loss functions, and each participant autonomously builds its own model by iteratively fitting the gradients of the overarching objective function. We also provide asymptotic convergence analysis and practical case studies of GAL. Experimental studies demonstrate that GAL can achieve performance close to centralized learning when all data, models, and objective functions are fully disclosed.
count=1
* Decision Trees with Short Explainable Rules
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/500637d931d4feb99d5cce84af1f53ba-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/500637d931d4feb99d5cce84af1f53ba-Paper-Conference.pdf)]
    * Title: Decision Trees with Short Explainable Rules
    * Year: `2022`
    * Authors: Victor Feitosa Souza, Ferdinando Cicalese, Eduardo Laber, Marco Molinaro
    * Abstract: Decision trees are widely used in many settings where interpretable models are preferred or required. As confirmed by recent empirical studies, the interpretability/explanability of a decision tree critically depends on some of its structural parameters, like size and the average/maximum depth of its leaves. There is indeed a vast literature on the design and analysis of decision tree algorithms that aim at optimizing these parameters.This paper contributes to this important line of research: we propose as a novel criterion of measuring the interpretability of a decision tree, the sparsity of the set of attributes that are (on average) required to explain the classification of the examples. We give a tight characterization of the best possible guarantees achievable by a decision tree built to optimize both our newmeasure (which we call the {\em explanation size}) and the more classical measures of worst-case and average depth. In particular, we give an algorithm that guarantees $O(\ln n )$-approximation (hence optimal if $P \neq NP$) for the minimization of both the average/worst-case explanation size and the average/worst-case depth. In addition to our theoretical contributions, experiments with 20 real datasets show that our algorithm has accuracy competitive with CART while producing trees that allow for much simpler explanations.
count=1
* Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/55bfedfd31489e5ae83c9ce8eec7b0e1-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/55bfedfd31489e5ae83c9ce8eec7b0e1-Paper-Conference.pdf)]
    * Title: Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection
    * Year: `2022`
    * Authors: Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, Bo Li
    * Abstract: Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean-label settings. We also discuss how to use the proposed untargeted backdoor watermark for dataset ownership verification. Experiments on benchmark datasets verify the effectiveness of our methods and their resistance to existing backdoor defenses.
count=1
* Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/5bf2b802e24106064dc547ae9283bb0c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/5bf2b802e24106064dc547ae9283bb0c-Paper-Conference.pdf)]
    * Title: Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models
    * Year: `2022`
    * Authors: Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, Chaowei Xiao
    * Abstract: Pre-trained vision-language models (e.g., CLIP) have shown promising zero-shot generalization in many downstream tasks with properly designed text prompts. Instead of relying on hand-engineered prompts, recent works learn prompts using the training data from downstream tasks. While effective, training on domain-specific data reduces a model's generalization capability to unseen new domains. In this work, we propose test-time prompt tuning (TPT), a method that can learn adaptive prompts on the fly with a single test sample. TPT optimizes the prompt by minimizing the entropy with confidence selection so that the model has consistent predictions across different augmented views of each test sample. In evaluating generalization to natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP by 3.6\% on average, surpassing previous prompt tuning approaches that require additional task-specific training data. In evaluating cross-dataset generalization with unseen categories, TPTperforms on par with the state-of-the-art approaches that use additional training data.
count=1
* Retaining Knowledge for Learning with Dynamic Definition
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/5fcd540792da599adf1b932624e98f1f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/5fcd540792da599adf1b932624e98f1f-Paper-Conference.pdf)]
    * Title: Retaining Knowledge for Learning with Dynamic Definition
    * Year: `2022`
    * Authors: Zichang Liu, Benjamin Coleman, Tianyi Zhang, Anshumali Shrivastava
    * Abstract: Machine learning models are often deployed in settings where they must be constantly updated in response to the changes in class definitions while retaining high accuracy on previously learned definitions. A classical use case is fraud detection, where new fraud schemes come one after another. While such an update can be accomplished by re-training on the complete data, the process is inefficient and prevents real-time and on-device learning. On the other hand, efficient methods that incrementally learn from new data often result in the forgetting of previously-learned knowledge. We define this problem as Learning with Dynamic Definition (LDD) and demonstrate that popular models, such as the Vision Transformer and Roberta, exhibit substantial forgetting of past definitions. We present the first practical and provable solution to LDD. Our proposal is a hash-based sparsity model \textit{RIDDLE} that solves evolving definitions by associating samples only to relevant parameters. We prove that our model is a universal function approximator and theoretically bounds the knowledge lost during the update process. On practical tasks with evolving class definition in vision and natural language processing, \textit{RIDDLE} outperforms baselines by up to 30\% on the original dataset while providing competitive accuracy on the update dataset.
count=1
* K-LITE: Learning Transferable Visual Models with External Knowledge
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/63fef0802863f47775c3563e18cbba17-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/63fef0802863f47775c3563e18cbba17-Paper-Conference.pdf)]
    * Title: K-LITE: Learning Transferable Visual Models with External Knowledge
    * Year: `2022`
    * Authors: Sheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei Yang, Pengchuan Zhang, Zhe Gan, Lijuan Wang, Lu Yuan, Ce Liu, Kurt Keutzer, Trevor Darrell, Anna Rohrbach, Jianfeng Gao
    * Abstract: The new generation of state-of-the-art computer vision systems are trained from natural language supervision, ranging from simple object category names to descriptive captions. This form of supervision ensures high generality and usability of the learned visual models, based on the broad concept coverage achieved through large-scale data collection process. Alternatively, we argue that learning with external knowledge about images is a promising way which leverages a much more structured source of supervision and offers sample efficiency. In this paper, we propose K-LITE (Knowledge-augmented Language-Image Training and Evaluation), a simple strategy to leverage external knowledge for building transferable visual systems: In training, it enriches entities in natural language with WordNet and Wiktionary knowledge, leading to an efficient and scalable approach to learning image representations that uses knowledge about the visual concepts; In evaluation, the natural language is also augmented with external knowledge and then used to reference learned visual concepts (or describe new ones) to enable zero-shot and few-shot transfer of the pre-trained models. We study the performance of K-LITE on two important computer vision problems, image classification and object detection, benchmarking on 20 and 13 different existing datasets, respectively. The proposed knowledge-augmented models show significant improvement in transfer learning performance over existing methods. Our code is released at https://github.com/microsoft/klite.
count=1
* AutoML Two-Sample Test
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/66247b78cb1aa7259dcf856a18c9e294-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/66247b78cb1aa7259dcf856a18c9e294-Paper-Conference.pdf)]
    * Title: AutoML Two-Sample Test
    * Year: `2022`
    * Authors: Jonas M. Kübler, Vincent Stimper, Simon Buchholz, Krikamol Muandet, Bernhard Schölkopf
    * Abstract: Two-sample tests are important in statistics and machine learning, both as tools for scientific discovery as well as to detect distribution shifts.This led to the development of many sophisticated test procedures going beyond the standard supervised learning frameworks, whose usage can require specialized knowledge about two-sample testing. We use a simple test that takes the mean discrepancy of a witness function as the test statistic and prove that minimizing a squared loss leads to a witness with optimal testing power. This allows us to leverage recent advancements in AutoML. Without any user input about the problems at hand, and using the same method for all our experiments, our AutoML two-sample test achieves competitive performance on a diverse distribution shift benchmark as well as on challenging two-sample testing problems.
count=1
* Robust Learning against Relational Adversaries
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/6752ced903c3f0265108caa10933965f-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/6752ced903c3f0265108caa10933965f-Paper-Conference.pdf)]
    * Title: Robust Learning against Relational Adversaries
    * Year: `2022`
    * Authors: Yizhen Wang, Mohannad Alhanahnah, Xiaozhu Meng, Ke Wang, Mihai Christodorescu, Somesh Jha
    * Abstract: Test-time adversarial attacks have posed serious challenges to the robustness of machine-learning models, and in many settings the adversarial perturbation need not be bounded by small $\ell_p$-norms. Motivated by attacks in program analysis and security tasks, we investigate $\textit{relational adversaries}$, a broad class of attackers who create adversarial examples in a reflexive-transitive closure of a logical relation. We analyze the conditions for robustness against relational adversaries and investigate different levels of robustness-accuracy trade-off due to various patterns in a relation. Inspired by the insights, we propose $\textit{normalize-and-predict}$, a learning framework that leverages input normalization to achieve provable robustness. The framework solves the pain points of adversarial training against relational adversaries and can be combined with adversarial training for the benefits of both approaches. Guided by our theoretical findings, we apply our framework to source code authorship attribution and malware detection. Results of both tasks show our learning framework significantly improves the robustness of models against relational adversaries. In the process, it outperforms adversarial training, the most noteworthy defense mechanism, by a wide margin.
count=1
* AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/69e2f49ab0837b71b0e0cb7c555990f8-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/69e2f49ab0837b71b0e0cb7c555990f8-Paper-Conference.pdf)]
    * Title: AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition
    * Year: `2022`
    * Authors: Shoufa Chen, Chongjian GE, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, Ping Luo
    * Abstract: Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains.To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently.It possesses several benefits more appealing than prior arts.Firstly, AdaptFormer introduces lightweight modules that only add less than 2% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\% fully fine-tuned models on action recognition benchmarks.Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks.Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5% extra parameters, it achieves about 10% and 19% relative improvement compared to the fully fine-tuned models on Something-Something~v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.
count=1
* Bounding and Approximating Intersectional Fairness through Marginal Fairness
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/6ae7df1f40f5faeda474b36b61197822-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/6ae7df1f40f5faeda474b36b61197822-Paper-Conference.pdf)]
    * Title: Bounding and Approximating Intersectional Fairness through Marginal Fairness
    * Year: `2022`
    * Authors: Mathieu Molina, Patrick Loiseau
    * Abstract: Discrimination in machine learning often arises along multiple dimensions (a.k.a. protected attributes); it is then desirable to ensure \emph{intersectional fairness}---i.e., that no subgroup is discriminated against. It is known that ensuring \emph{marginal fairness} for every dimension independently is not sufficient in general. Due to the exponential number of subgroups, however, directly measuring intersectional fairness from data is impossible. In this paper, our primary goal is to understand in detail the relationship between marginal and intersectional fairness through statistical analysis. We first identify a set of sufficient conditions under which an exact relationship can be obtained. Then, we prove bounds (easily computable through marginal fairness and other meaningful statistical quantities) in high-probability on intersectional fairness in the general case. Beyond their descriptive value, we show that these theoretical bounds can be leveraged to derive a heuristic improving the approximation and bounds of intersectional fairness by choosing, in a relevant manner, protected attributes for which we describe intersectional subgroups. Finally, we test the performance of our approximations and bounds on real and synthetic data-sets.
count=1
* Exact Shape Correspondence via 2D graph convolution
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/72d9a23e3895b5670e650c2e742065c9-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/72d9a23e3895b5670e650c2e742065c9-Paper-Conference.pdf)]
    * Title: Exact Shape Correspondence via 2D graph convolution
    * Year: `2022`
    * Authors: Barakeel Fanseu Kamhoua, Lin Zhang, Yongqiang Chen, Han Yang, MA Kaili, Bo Han, Bo Li, James Cheng
    * Abstract: For exact 3D shape correspondence (matching or alignment), i.e., the task of matching each point on a shape to its exact corresponding point on the other shape (or to be more specific, matching at geodesic error 0), most existing methods do not perform well due to two main problems. First, on nearly-isometric shapes (i.e., low noise levels), most existing methods use the eigen-vectors (eigen-functions) of the Laplace Beltrami Operator (LBO) or other shape descriptors to update an initialized correspondence which is not exact, leading to an accumulation of update errors. Thus, though the final correspondence may generally be smooth, it is generally inexact. Second, on non-isometric shapes (noisy shapes), existing methods are generally not robust to noise as they usually assume near-isometry. In addition, existing methods that attempt to address the non-isometric shape problem (e.g., GRAMPA) are generally computationally expensive and do not generalise to nearly-isometric shapes. To address these two problems, we propose a 2D graph convolution-based framework called 2D-GEM. 2D-GEM is robust to noise on non-isometric shapes and with a few additional constraints, it also addresses the errors in the update on nearly-isometric shapes. We demonstrate the effectiveness of 2D-GEM by achieving a high accuracy of 90.5$\%$ at geodesic error 0 on the non-isometric benchmark SHREC16, i.e., TOPKIDS (while being much faster than GRAMPA), and on nearly-isometric benchmarks by achieving a high accuracy of 92.5$\%$ on TOSCA and 84.9$\%$ on SCAPE at geodesic error 0.
count=1
* Optimal Positive Generation via Latent Transformation for Contrastive Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/74a31a3b862eb7f01defbbed8e5f0c69-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/74a31a3b862eb7f01defbbed8e5f0c69-Paper-Conference.pdf)]
    * Title: Optimal Positive Generation via Latent Transformation for Contrastive Learning
    * Year: `2022`
    * Authors: Yinqi Li, Hong Chang, Bingpeng MA, Shiguang Shan, Xilin Chen
    * Abstract: Contrastive learning, which learns to contrast positive with negative pairs of samples, has been popular for self-supervised visual representation learning. Although great effort has been made to design proper positive pairs through data augmentation, few works attempt to generate optimal positives for each instance. Inspired by semantic consistency and computational advantage in latent space of pretrained generative models, this paper proposes to learn instance-specific latent transformations to generate Contrastive Optimal Positives (COP-Gen) for self-supervised contrastive learning. Specifically, we formulate COP-Gen as an instance-specific latent space navigator which minimizes the mutual information between the generated positive pair subject to the semantic consistency constraint. Theoretically, the learned latent transformation creates optimal positives for contrastive learning, which removes as much nuisance information as possible while preserving the semantics. Empirically, using generated positives by COP-Gen consistently outperforms other latent transformation methods and even real-image-based methods in self-supervised contrastive learning.
count=1
* AutoMS: Automatic Model Selection for Novelty Detection with Error Rate Control
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/7dced224614f3c50b1626473f48312bf-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/7dced224614f3c50b1626473f48312bf-Paper-Conference.pdf)]
    * Title: AutoMS: Automatic Model Selection for Novelty Detection with Error Rate Control
    * Year: `2022`
    * Authors: Yifan Zhang, Haiyan Jiang, Haojie Ren, Changliang Zou, Dejing Dou
    * Abstract: Given an unsupervised novelty detection task on a new dataset, how can we automatically select a ''best'' detection model while simultaneously controlling the error rate of the best model? For novelty detection analysis, numerous detectors have been proposed to detect outliers on a new unseen dataset based on a score function trained on available clean data. However, due to the absence of labeled data for model evaluation and comparison, there is a lack of systematic approaches that are able to select a ''best'' model/detector (i.e., the algorithm as well as its hyperparameters) and achieve certain error rate control simultaneously. In this paper, we introduce a unified data-driven procedure to address this issue. The key idea is to maximize the number of detected outliers while controlling the false discovery rate (FDR) with the help of Jackknife prediction. We establish non-asymptotic bounds for the false discovery proportions and show that the proposed procedure yields valid FDR control under some mild conditions. Numerical experiments on both synthetic and real data validate the theoretical results and demonstrate the effectiveness of our proposed AutoMS method. The code is available at https://github.com/ZhangYifan1996/AutoMS.
count=1
* Simultaneous Missing Value Imputation and Structure Learning with Groups
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/7e57131fdeb815764434b65162c88895-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/7e57131fdeb815764434b65162c88895-Paper-Conference.pdf)]
    * Title: Simultaneous Missing Value Imputation and Structure Learning with Groups
    * Year: `2022`
    * Authors: Pablo Morales-Alvarez, Wenbo Gong, Angus Lamb, Simon Woodhead, Simon Peyton Jones, Nick Pawlowski, Miltiadis Allamanis, Cheng Zhang
    * Abstract: Learning structures between groups of variables from data with missing values is an important task in the real world, yet difficult to solve. One typical scenario is discovering the structure among topics in the education domain to identify learning pathways. Here, the observations are student performances for questions under each topic which contain missing values. However, most existing methods focus on learning structures between a few individual variables from the complete data. In this work, we propose VISL, a novel scalable structure learning approach that can simultaneously infer structures between groups of variables under missing data and perform missing value imputations with deep learning. Particularly, we propose a generative model with a structured latent space and a graph neural network-based architecture, scaling to a large number of variables. Empirically, we conduct extensive experiments on synthetic, semi-synthetic, and real-world education data sets. We show improved performances on both imputation and structure learning accuracy compared to popular and recent approaches.
count=1
* CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/8078e76f913e31b8467e85b4c0f0d22b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/8078e76f913e31b8467e85b4c0f0d22b-Paper-Conference.pdf)]
    * Title: CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP
    * Year: `2022`
    * Authors: Andreas Fürst, Elisabeth Rumetshofer, Johannes Lehner, Viet T. Tran, Fei Tang, Hubert Ramsauer, David Kreil, Michael Kopp, Günter Klambauer, Angela Bitto, Sepp Hochreiter
    * Abstract: CLIP yielded impressive results on zero-shot transfer learning tasks and is considered as a foundation model like BERT or GPT3. CLIP vision models that have a rich representation are pre-trained using the InfoNCE objective and natural language supervision before they are fine-tuned on particular tasks. Though CLIP excels at zero-shot transfer learning, it suffers from an explaining away problem, that is, it focuses on one or few features, while neglecting other relevant features. This problem is caused by insufficiently extracting the covariance structure in the original multi-modal data. We suggest to use modern Hopfield networks to tackle the problem of explaining away. Their retrieved embeddings have an enriched covariance structure derived from co-occurrences of features in the stored embeddings. However, modern Hopfield networks increase the saturation effect of the InfoNCE objective which hampers learning. We propose to use the InfoLOOB objective to mitigate this saturation effect. We introduce the novel "Contrastive Leave One Out Boost" (CLOOB), which uses modern Hopfield networks for covariance enrichment together with the InfoLOOB objective. In experiments we compare CLOOB to CLIP after pre-training on the Conceptual Captions and the YFCC dataset with respect to their zero-shot transfer learning performance on other datasets. CLOOB consistently outperforms CLIP at zero-shot transfer learning across all considered architectures and datasets.
count=1
* Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/8644353f7d307baaf29bc1e56fe8e0ec-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/8644353f7d307baaf29bc1e56fe8e0ec-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization
    * Year: `2022`
    * Authors: Wenhao Gao, Tianfan Fu, Jimeng Sun, Connor Coley
    * Abstract: Molecular optimization is a fundamental goal in the chemical sciences and is of central interest to drug and material design. In recent years, significant progress has been made in solving challenging problems across various aspects of computational molecular optimizations, emphasizing high validity, diversity, and, most recently, synthesizability. Despite this progress, many papers report results on trivial or self-designed tasks, bringing additional challenges to directly assessing the performance of new methods. Moreover, the sample efficiency of the optimization---the number of molecules evaluated by the oracle---is rarely discussed, despite being an essential consideration for realistic discovery applications.To fill this gap, we have created an open-source benchmark for practical molecular optimization, PMO, to facilitate the transparent and reproducible evaluation of algorithmic advances in molecular optimization. This paper thoroughly investigates the performance of 25 molecular design algorithms on 23 single-objective (scalar) optimization tasks with a particular focus on sample efficiency. Our results show that most ``state-of-the-art'' methods fail to outperform their predecessors under a limited oracle budget allowing 10K queries and that no existing algorithm can efficiently solve certain molecular optimization problems in this setting. We analyze the influence of the optimization algorithm choices, molecular assembly strategies, and oracle landscapes on the optimization performance to inform future algorithm development and benchmarking. PMO provides a standardized experimental setup to comprehensively evaluate and compare new molecule optimization methods with existing ones. All code can be found at https://github.com/wenhao-gao/mol_opt.
count=1
* On-Device Training Under 256KB Memory
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/90c56c77c6df45fc8e556a096b7a2b2e-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/90c56c77c6df45fc8e556a096b7a2b2e-Paper-Conference.pdf)]
    * Title: On-Device Training Under 256KB Memory
    * Year: `2022`
    * Authors: Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, Song Han
    * Abstract: On-device training enables the model to adapt to new data collected from the sensors by fine-tuning a pre-trained model. Users can benefit from customized AI models without having to transfer the data to the cloud, protecting the privacy. However, the training memory consumption is prohibitive for IoT devices that have tiny memory resources. We propose an algorithm-system co-design framework to make on-device training possible with only 256KB of memory. On-device training faces two unique challenges: (1) the quantized graphs of neural networks are hard to optimize due to low bit-precision and the lack of normalization; (2) the limited hardware resource (memory and computation) does not allow full backpropagation. To cope with the optimization difficulty, we propose Quantization- Aware Scaling to calibrate the gradient scales and stabilize 8-bit quantized training. To reduce the memory footprint, we propose Sparse Update to skip the gradient computation of less important layers and sub-tensors. The algorithm innovation is implemented by a lightweight training system, Tiny Training Engine, which prunes the backward computation graph to support sparse updates and offload the runtime auto-differentiation to compile time. Our framework is the first practical solution for on-device transfer learning of visual recognition on tiny IoT devices (e.g., a microcontroller with only 256KB SRAM), using less than 1/1000 of the memory of PyTorch and TensorFlow while matching the accuracy. Our study enables IoT devices not only to perform inference but also to continuously adapt to new data for on-device lifelong learning. A video demo can be found here: https://youtu.be/XaDCO8YtmBw.
count=1
* Sample-Then-Optimize Batch Neural Thompson Sampling
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/940f8a526b8b36f110265f4b6059d81b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/940f8a526b8b36f110265f4b6059d81b-Paper-Conference.pdf)]
    * Title: Sample-Then-Optimize Batch Neural Thompson Sampling
    * Year: `2022`
    * Authors: Zhongxiang Dai, YAO SHU, Bryan Kian Hsiang Low, Patrick Jaillet
    * Abstract: Bayesian optimization (BO), which uses a Gaussian process (GP) as a surrogate to model its objective function, is popular for black-box optimization. However, due to the limitations of GPs, BO underperforms in some problems such as those with categorical, high-dimensional or image inputs. To this end, recent works have used the highly expressive neural networks (NNs) as the surrogate model and derived theoretical guarantees using the theory of neural tangent kernel (NTK). However, these works suffer from the limitations of the requirement to invert an extremely large parameter matrix and the restriction to the sequential (rather than batch) setting. To overcome these limitations, we introduce two algorithms based on the Thompson sampling (TS) policy named Sample-Then-Optimize Batch Neural TS (STO-BNTS) and STO-BNTS-Linear. To choose an input query, we only need to train an NN (resp. a linear model) and then choose the query by maximizing the trained NN (resp. linear model), which is equivalently sampled from the GP posterior with the NTK as the kernel function. As a result, our algorithms sidestep the need to invert the large parameter matrix yet still preserve the validity of the TS policy. Next, we derive regret upper bounds for our algorithms with batch evaluations, and use insights from batch BO and NTK to show that they are asymptotically no-regret under certain conditions. Finally, we verify their empirical effectiveness using practical AutoML and reinforcement learning experiments.
count=1
* Distinguishing discrete and continuous behavioral variability using warped autoregressive HMMs
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/96b3aa81a9e593ca5e9b184756034a43-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/96b3aa81a9e593ca5e9b184756034a43-Paper-Conference.pdf)]
    * Title: Distinguishing discrete and continuous behavioral variability using warped autoregressive HMMs
    * Year: `2022`
    * Authors: Julia Costacurta, Lea Duncker, Blue Sheffer, Winthrop Gillis, Caleb Weinreb, Jeffrey Markowitz, Sandeep R Datta, Alex Williams, Scott Linderman
    * Abstract: A core goal in systems neuroscience and neuroethology is to understand how neural circuits generate naturalistic behavior. One foundational idea is that complex naturalistic behavior may be composed of sequences of stereotyped behavioral syllables, which combine to generate rich sequences of actions. To investigate this, a common approach is to use autoregressive hidden Markov models (ARHMMs) to segment video into discrete behavioral syllables. While these approaches have been successful in extracting syllables that are interpretable, they fail to account for other forms of behavioral variability, such as differences in speed, which may be better described as continuous in nature. To overcome these limitations, we introduce a class of warped ARHMMs (WARHMM). As is the case in the ARHMM, behavior is modeled as a mixture of autoregressive dynamics. However, the dynamics under each discrete latent state (i.e. each behavioral syllable) are additionally modulated by a continuous latent ``warping variable.'' We present two versions of warped ARHMM in which the warping variable affects the dynamics of each syllable either linearly or nonlinearly. Using depth-camera recordings of freely moving mice, we demonstrate that the failure of ARHMMs to account for continuous behavioral variability results in duplicate cluster assignments. WARHMM achieves similar performance to the standard ARHMM while using fewer behavioral syllables. Further analysis of behavioral measurements in mice demonstrates that WARHMM identifies structure relating to response vigor.
count=1
* An Adaptive Kernel Approach to Federated Learning of Heterogeneous Causal Effects
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/9a9afa70eead1805f00e3a0df2a41157-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/9a9afa70eead1805f00e3a0df2a41157-Paper-Conference.pdf)]
    * Title: An Adaptive Kernel Approach to Federated Learning of Heterogeneous Causal Effects
    * Year: `2022`
    * Authors: Thanh Vinh Vo, Arnab Bhattacharyya, Young Lee, Tze-Yun Leong
    * Abstract: We propose a new causal inference framework to learn causal effects from multiple, decentralized data sources in a federated setting. We introduce an adaptive transfer algorithm that learns the similarities among the data sources by utilizing Random Fourier Features to disentangle the loss function into multiple components, each of which is associated with a data source. The data sources may have different distributions; the causal effects are independently and systematically incorporated. The proposed method estimates the similarities among the sources through transfer coefficients, and hence requiring no prior information about the similarity measures. The heterogeneous causal effects can be estimated with no sharing of the raw training data among the sources, thus minimizing the risk of privacy leak. We also provide minimax lower bounds to assess the quality of the parameters learned from the disparate sources. The proposed method is empirically shown to outperform the baselines on decentralized data sources with dissimilar distributions.
count=1
* ZIN: When and How to Learn Invariance Without Environment Partition?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/9b77f07301b1ef1fe810aae96c12cb7b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/9b77f07301b1ef1fe810aae96c12cb7b-Paper-Conference.pdf)]
    * Title: ZIN: When and How to Learn Invariance Without Environment Partition?
    * Year: `2022`
    * Authors: Yong Lin, Shengyu Zhu, Lu Tan, Peng Cui
    * Abstract: It is commonplace to encounter heterogeneous data, of which some aspects of the data distribution may vary but the underlying causal mechanisms remain constant. When data are divided into distinct environments according to the heterogeneity, recent invariant learning methods have proposed to learn robust and invariant models using this environment partition. It is hence tempting to utilize the inherent heterogeneity even when environment partition is not provided. Unfortunately, in this work, we show that learning invariant features under this circumstance is fundamentally impossible without further inductive biases or additional information. Then, we propose a framework to jointly learn environment partition and invariant representation, assisted by additional auxiliary information. We derive sufficient and necessary conditions for our framework to provably identify invariant features under a fairly general setting. Experimental results on both synthetic and real world datasets validate our analysis and demonstrate an improved performance of the proposed framework. Our findings also raise the need of making the role of inductive biases more explicit when learning invariant models without environment partition in future works. Codes are available at https://github.com/linyongver/ZIN_official .
count=1
* LAION-5B: An open large-scale dataset for training next generation image-text models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/a1859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/a1859debfb3b59d094f3504d5ebb6c25-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: LAION-5B: An open large-scale dataset for training next generation image-text models
    * Year: `2022`
    * Authors: Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, Jenia Jitsev
    * Abstract: Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection.
count=1
* Semi-supervised Vision Transformers at Scale
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/a4a1ee071ce0fe63b83bce507c9dc4d7-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/a4a1ee071ce0fe63b83bce507c9dc4d7-Paper-Conference.pdf)]
    * Title: Semi-supervised Vision Transformers at Scale
    * Year: `2022`
    * Authors: Zhaowei Cai, Avinash Ravichandran, Paolo Favaro, Manchen Wang, Davide Modolo, Rahul Bhotika, Zhuowen Tu, Stefano Soatto
    * Abstract: We study semi-supervised learning (SSL) for vision transformers (ViT), an under-explored topic despite the wide adoption of the ViT architectures to different tasks. To tackle this problem, we use a SSL pipeline, consisting of first un/self-supervised pre-training, followed by supervised fine-tuning, and finally semi-supervised fine-tuning. At the semi-supervised fine-tuning stage, we adopt an exponential moving average (EMA)-Teacher framework instead of the popular FixMatch, since the former is more stable and delivers higher accuracy for semi-supervised vision transformers. In addition, we propose a probabilistic pseudo mixup mechanism to interpolate unlabeled samples and their pseudo labels for improved regularization, which is important for training ViTs with weak inductive bias. Our proposed method, dubbed Semi-ViT, achieves comparable or better performance than the CNN counterparts in the semi-supervised classification setting. Semi-ViT also enjoys the scalability benefits of ViTs that can be readily scaled up to large-size models with increasing accuracy. For example, Semi-ViT-Huge achieves an impressive 80\% top-1 accuracy on ImageNet using only 1\% labels, which is comparable with Inception-v4 using 100\% ImageNet labels. The code is available at https://github.com/amazon-science/semi-vit.
count=1
* Branch &amp; Learn for Recursively and Iteratively Solvable Problems in Predict+Optimize
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/a59a11e8580a7ac850cb792f6179c7a0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/a59a11e8580a7ac850cb792f6179c7a0-Paper-Conference.pdf)]
    * Title: Branch &amp; Learn for Recursively and Iteratively Solvable Problems in Predict+Optimize
    * Year: `2022`
    * Authors: Xinyi Hu, Jasper Lee, Jimmy Lee, Allen Z. Zhong
    * Abstract: This paper proposes Branch & Learn, a framework for Predict+Optimize to tackle optimization problems containing parameters that are unknown at the time of solving. Given an optimization problem solvable by a recursive algorithm satisfying simple conditions, we show how a corresponding learning algorithm can be constructed directly and methodically from the recursive algorithm. Our framework applies also to iterative algorithms by viewing them as a degenerate form of recursion. Extensive experimentation shows better performance for our proposal over classical and state of the art approaches.
count=1
* Learning from Label Proportions by Learning with Label Noise
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/ac56fb3fab015124b541f6299016a21c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/ac56fb3fab015124b541f6299016a21c-Paper-Conference.pdf)]
    * Title: Learning from Label Proportions by Learning with Label Noise
    * Year: `2022`
    * Authors: Jianxin Zhang, Yutong Wang, Clay Scott
    * Abstract: Learning from label proportions (LLP) is a weakly supervised classification problem where data points are grouped into bags, and the label proportions within each bag are observed instead of the instance-level labels. The task is to learn a classifier to predict the labels of future individual instances. Prior work on LLP for multi-class data has yet to develop a theoretically grounded algorithm. In this work, we propose an approach to LLP based on a reduction to learning with label noise, using the forward correction (FC) loss of \textcite{Patrini2017MakingDN}. We establish an excess risk bound and generalization error analysis for our approach, while also extending the theory of the FC loss which may be of independent interest. Our approach demonstrates improved empirical performance in deep learning scenarios across multiple datasets and architectures, compared to the leading methods.
count=1
* DeepMed: Semiparametric Causal Mediation Analysis with Debiased Deep Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/b57939005a3cbe40f49b66a0efd6fc8c-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/b57939005a3cbe40f49b66a0efd6fc8c-Paper-Conference.pdf)]
    * Title: DeepMed: Semiparametric Causal Mediation Analysis with Debiased Deep Learning
    * Year: `2022`
    * Authors: Siqi Xu, Lin Liu, Zhonghua Liu
    * Abstract: Causal mediation analysis can unpack the black box of causality and is therefore a powerful tool for disentangling causal pathways in biomedical and social sciences, and also for evaluating machine learning fairness. To reduce bias for estimating Natural Direct and Indirect Effects in mediation analysis, we propose a new method called DeepMed that uses deep neural networks (DNNs) to cross-fit the infinite-dimensional nuisance functions in the efficient influence functions. We obtain novel theoretical results that our DeepMed method (1) can achieve semiparametric efficiency bound without imposing sparsity constraints on the DNN architecture and (2) can adapt to certain low dimensional structures of the nuisance functions, significantly advancing the existing literature on DNN-based semiparametric causal inference. Extensive synthetic experiments are conducted to support our findings and also expose the gap between theory and practice. As a proof of concept, we apply DeepMed to analyze two real datasets on machine learning fairness and reach conclusions consistent with previous findings.
count=1
* LTMD: Learning Improvement of Spiking Neural Networks with Learnable Thresholding Neurons and Moderate Dropout
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/b5fd95d6b16d3172e307103a97f19e1b-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/b5fd95d6b16d3172e307103a97f19e1b-Paper-Conference.pdf)]
    * Title: LTMD: Learning Improvement of Spiking Neural Networks with Learnable Thresholding Neurons and Moderate Dropout
    * Year: `2022`
    * Authors: SIQI WANG, Tee Hiang Cheng, Meng-Hiot Lim
    * Abstract: Spiking Neural Networks (SNNs) have shown substantial promise in processing spatio-temporal data, mimicking biological neuronal mechanisms, and saving computational power. However, most SNNs use fixed model regardless of their locations in the network. This limits SNNs’ capability of transmitting precise information in the network, which becomes worse for deeper SNNs. Some researchers try to use specified parametric models in different network layers or regions, but most still use preset or suboptimal parameters. Inspired by the neuroscience observation that different neuronal mechanisms exist in disparate brain regions, we propose a new spiking neuronal mechanism, named learnable thresholding, to address this issue. Utilizing learnable threshold values, learnable thresholding enables flexible neuronal mechanisms across layers, proper information flow within the network, and fast network convergence. In addition, we propose a moderate dropout method to serve as an enhancement technique to minimize inconsistencies between independent dropout runs. Finally, we evaluate the robustness of the proposed learnable thresholding and moderate dropout for image classification with different initial thresholds for various types of datasets. Our proposed methods produce superior results compared to other approaches for almost all datasets with fewer timesteps. Our codes are available at https://github.com/sq117/LTMD.git.
count=1
* TransBoost: Improving the Best ImageNet Performance using Deep Transduction
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/b60161e93f3e0e4207081a3b4ef5e8d8-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/b60161e93f3e0e4207081a3b4ef5e8d8-Paper-Conference.pdf)]
    * Title: TransBoost: Improving the Best ImageNet Performance using Deep Transduction
    * Year: `2022`
    * Authors: Omer Belhasin, Guy Bar-Shalom, Ran El-Yaniv
    * Abstract: This paper deals with deep transductive learning, and proposes TransBoost as a procedure for fine-tuning any deep neural model to improve its performance on any (unlabeled) test set provided at training time. TransBoost is inspired by a large margin principle and is efficient and simple to use. Our method significantly improves the ImageNet classification performance on a wide range of architectures, such as ResNets, MobileNetV3-L, EfficientNetB0, ViT-S, and ConvNext-T, leading to state-of-the-art transductive performance.Additionally we show that TransBoost is effective on a wide variety of image classification datasets. The implementation of TransBoost is provided at: https://github.com/omerb01/TransBoost .
count=1
* Sparse Probabilistic Circuits via Pruning and Growing
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/b6089408f4893289296ad0499783b3a6-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/b6089408f4893289296ad0499783b3a6-Paper-Conference.pdf)]
    * Title: Sparse Probabilistic Circuits via Pruning and Growing
    * Year: `2022`
    * Authors: Meihua Dang, Anji Liu, Guy Van den Broeck
    * Abstract: Probabilistic circuits (PCs) are a tractable representation of probability distributions allowing for exact and efficient computation of likelihoods and marginals. There has been significant recent progress on improving the scale and expressiveness of PCs. However, PC training performance plateaus as model size increases. We discover that most capacity in existing large PC structures is wasted: fully-connected parameter layers are only sparsely used. We propose two operations: pruning and growing, that exploit the sparsity of PC structures. Specifically, the pruning operation removes unimportant sub-networks of the PC for model compression and comes with theoretical guarantees. The growing operation increases model capacity by increasing the dimensions of latent states. By alternatingly applying pruning and growing, we increase the capacity that is meaningfully used, allowing us to significantly scale up PC learning. Empirically, our learner achieves state-of-the-art likelihoods on MNIST-family image datasets and an Penn Tree Bank language data compared to other PC learners and less tractable deep generative models such as flow-based models and variational autoencoders (VAEs).
count=1
* In Differential Privacy, There is Truth: on Vote-Histogram Leakage in Ensemble Private Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/ba8d1b46292c5e82cbfb3b3dc3b968af-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/ba8d1b46292c5e82cbfb3b3dc3b968af-Paper-Conference.pdf)]
    * Title: In Differential Privacy, There is Truth: on Vote-Histogram Leakage in Ensemble Private Learning
    * Year: `2022`
    * Authors: JIAQI WANG, Roei Schuster, I Shumailov, David Lie, Nicolas Papernot
    * Abstract: When learning from sensitive data, care must be taken to ensure that training algorithms address privacy concerns. The canonical Private Aggregation of Teacher Ensembles, or PATE, computes output labels by aggregating the predictions of a (possibly distributed) collection of teacher models via a voting mechanism. The mechanism adds noise to attain a differential privacy guarantee with respect to the teachers' training data. In this work, we observe that this use of noise, which makes PATE predictions stochastic, enables new forms of leakage of sensitive information. For a given input, our adversary exploits this stochasticity to extract high-fidelity histograms of the votes submitted by the underlying teachers. From these histograms, the adversary can learn sensitive attributes of the input such as race, gender, or age. Although this attack does not directly violate the differential privacy guarantee, it clearly violates privacy norms and expectations, and would not be possible $\textit{at all}$ without the noise inserted to obtain differential privacy. In fact, counter-intuitively, the attack $\textbf{becomes easier as we add more noise}$ to provide stronger differential privacy. We hope this encourages future work to consider privacy holistically rather than treat differential privacy as a panacea.
count=1
* Back Razor: Memory-Efficient Transfer Learning by Self-Sparsified Backpropagation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/bc6a1f968f8b1dae3e880f3f723d7d46-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/bc6a1f968f8b1dae3e880f3f723d7d46-Paper-Conference.pdf)]
    * Title: Back Razor: Memory-Efficient Transfer Learning by Self-Sparsified Backpropagation
    * Year: `2022`
    * Authors: Ziyu Jiang, Xuxi Chen, Xueqin Huang, Xianzhi Du, Denny Zhou, Zhangyang Wang
    * Abstract: Transfer learning from the model trained on large datasets to customized downstream tasks has been widely used as the pre-trained model can greatly boost the generalizability. However, the increasing sizes of pre-trained models also lead to a prohibitively large memory footprints for downstream transferring, making them unaffordable for personal devices. Previous work recognizes the bottleneck of the footprint to be the activation, and hence proposes various solutions such as injecting specific lite modules. In this work, we present a novel memory-efficient transfer framework called Back Razor, that can be plug-and-play applied to any pre-trained network without changing its architecture. The key idea of Back Razor is asymmetric sparsifying: pruning the activation stored for back-propagation, while keeping the forward activation dense. It is based on the observation that the stored activation, that dominates the memory footprint, is only needed for backpropagation. Such asymmetric pruning avoids affecting the precision of forward computation, thus making more aggressive pruning possible. Furthermore, we conduct the theoretical analysis for the convergence rate of Back Razor, showing that under mild conditions, our method retains the similar convergence rate as vanilla SGD. Extensive transfer learning experiments on both Convolutional Neural Networks and Vision Transformers with classification, dense prediction, and language modeling tasks show that Back Razor could yield up to 97% sparsity, saving 9.2x memory usage, without losing accuracy. The code is available at: https://github.com/VITA-Group/BackRazor_Neurips22.
count=1
* Patching open-vocabulary models by interpolating weights
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/bc6cddcd5d325e1c0f826066c1ad0215-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/bc6cddcd5d325e1c0f826066c1ad0215-Paper-Conference.pdf)]
    * Title: Patching open-vocabulary models by interpolating weights
    * Year: `2022`
    * Authors: Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, Ludwig Schmidt
    * Abstract: Open-vocabulary models like CLIP achieve high accuracy across many image classification tasks. However, there are still settings where their zero-shot performance is far from optimal. We study model patching, where the goal is to improve accuracy on specific tasks without degrading accuracy on tasks where performance is already adequate. Towards this goal, we introduce PAINT, a patching method that uses interpolations between the weights of a model before fine-tuning and the weights after fine-tuning on a task to be patched. On nine tasks where zero-shot CLIP performs poorly, PAINT increases accuracy by 15 to 60 percentage points while preserving accuracy on ImageNet within one percentage point of the zero-shot model. PAINT also allows a single model to be patched on multiple tasks and improves with model scale. Furthermore, we identify cases of broad transfer, where patching on one task increases accuracy on other tasks even when the tasks have disjoint classes. Finally, we investigate applications beyond common benchmarks such as counting or reducing the impact of typographic attacks on CLIP. Our findings demonstrate that it is possible to expand the set of tasks on which open-vocabulary models achieve high accuracy without re-training them from scratch.
count=1
* Unsupervised Visual Representation Learning via Mutual Information Regularized Assignment
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/bedc61a9936af18cb51b7c5e8f3b89a3-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/bedc61a9936af18cb51b7c5e8f3b89a3-Paper-Conference.pdf)]
    * Title: Unsupervised Visual Representation Learning via Mutual Information Regularized Assignment
    * Year: `2022`
    * Authors: Dong Hoon Lee, Sungik Choi, Hyunwoo J. Kim, Sae-Young Chung
    * Abstract: This paper proposes Mutual Information Regularized Assignment (MIRA), a pseudo-labeling algorithm for unsupervised representation learning inspired by information maximization. We formulate online pseudo-labeling as an optimization problem to find pseudo-labels that maximize the mutual information between the label and data while being close to a given model probability. We derive a fixed-point iteration method and prove its convergence to the optimal solution. In contrast to baselines, MIRA combined with pseudo-label prediction enables a simple yet effective clustering-based representation learning without incorporating extra training techniques or artificial constraints such as sampling strategy, equipartition constraints, etc. With relatively small training epochs, representation learned by MIRA achieves state-of-the-art performance on various downstream tasks, including the linear/${\it k}$-NN evaluation and transfer learning. Especially, with only 400 epochs, our method applied to ImageNet dataset with ResNet-50 architecture achieves 75.6% linear evaluation accuracy.
count=1
* Semantic Probabilistic Layers for Neuro-Symbolic Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/c182ec594f38926b7fcb827635b9a8f4-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/c182ec594f38926b7fcb827635b9a8f4-Paper-Conference.pdf)]
    * Title: Semantic Probabilistic Layers for Neuro-Symbolic Learning
    * Year: `2022`
    * Authors: Kareem Ahmed, Stefano Teso, Kai-Wei Chang, Guy Van den Broeck, Antonio Vergari
    * Abstract: We design a predictive layer for structured-output prediction (SOP) that can be plugged into any neural network guaranteeing its predictions are consistent with a set of predefined symbolic constraints. Our Semantic Probabilistic Layer (SPL) can model intricate correlations, and hard constraints, over a structured output space all while being amenable to end-to-end learning via maximum likelihood.SPLs combine exact probabilistic inference with logical reasoning in a clean and modular way, learning complex distributions and restricting their support to solutions of the constraint. As such, they can faithfully, and efficiently, model complex SOP tasks beyond the reach of alternative neuro-symbolic approaches. We empirically demonstrate that SPLs outperform these competitors in terms of accuracy on challenging SOP tasks such as hierarchical multi-label classification, pathfinding and preference learning, while retaining perfect constraint satisfaction.
count=1
* Tsetlin Machine for Solving Contextual Bandit Problems
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/c2d550cf3b2e177deb2d1720fb1e2710-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/c2d550cf3b2e177deb2d1720fb1e2710-Paper-Conference.pdf)]
    * Title: Tsetlin Machine for Solving Contextual Bandit Problems
    * Year: `2022`
    * Authors: Raihan Seraj, Jivitesh Sharma, Ole-Christoffer Granmo
    * Abstract: This paper introduces an interpretable contextual bandit algorithm using Tsetlin Machines, which solves complex pattern recognition tasks using propositional (Boolean) logic. The proposed bandit learning algorithm relies on straightforward bit manipulation, thus simplifying computation and interpretation. We then present a mechanism for performing Thompson sampling with Tsetlin Machine, given its non-parametric nature. Our empirical analysis shows that Tsetlin Machine as a base contextual bandit learner outperforms other popular base learners on eight out of nine datasets. We further analyze the interpretability of our learner, investigating how arms are selected based on propositional expressions that model the context.
count=1
* Text Classification with Born's Rule
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/c88d0c9bea6230b518ce71268c8e49e0-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/c88d0c9bea6230b518ce71268c8e49e0-Paper-Conference.pdf)]
    * Title: Text Classification with Born's Rule
    * Year: `2022`
    * Authors: Emanuele Guidotti, Alfio Ferrara
    * Abstract: This paper presents a text classification algorithm inspired by the notion of superposition of states in quantum physics. By regarding text as a superposition of words, we derive the wave function of a document and we compute the transition probability of the document to a target class according to Born's rule. Two complementary implementations are presented. In the first one, wave functions are calculated explicitly. The second implementation embeds the classifier in a neural network architecture. Through analysis of three benchmark datasets, we illustrate several aspects of the proposed method, such as classification performance, explainability, and computational efficiency. These ideas are also applicable to non-textual data.
count=1
* BagFlip: A Certified Defense Against Data Poisoning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/cc19e4ffde5540ac3fcda240e6d975cb-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/cc19e4ffde5540ac3fcda240e6d975cb-Paper-Conference.pdf)]
    * Title: BagFlip: A Certified Defense Against Data Poisoning
    * Year: `2022`
    * Authors: Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni
    * Abstract: Machine learning models are vulnerable to data-poisoning attacks, in which an attacker maliciously modifies the training set to change the prediction of a learned model. In a trigger-less attack, the attacker can modify the training set but not the test inputs, while in a backdoor attack the attacker can also modify test inputs. Existing model-agnostic defense approaches either cannot handle backdoor attacks or do not provide effective certificates (i.e., a proof of a defense). We present BagFlip, a model-agnostic certified approach that can effectively defend against both trigger-less and backdoor attacks. We evaluate BagFlip on image classification and malware detection datasets. BagFlip is equal to or more effective than the state-of-the-art approaches for trigger-less attacks and more effective than the state-of-the-art approaches for backdoor attacks.
count=1
* Debiased Self-Training for Semi-Supervised Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/d10d6b28d74c4f0fcab588feeb6fe7d6-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/d10d6b28d74c4f0fcab588feeb6fe7d6-Paper-Conference.pdf)]
    * Title: Debiased Self-Training for Semi-Supervised Learning
    * Year: `2022`
    * Authors: Baixu Chen, Junguang Jiang, Ximei Wang, Pengfei Wan, Jianmin Wang, Mingsheng Long
    * Abstract: Deep neural networks achieve remarkable performances on a wide range of tasks with the aid of large-scale labeled datasets. Yet these datasets are time-consuming and labor-exhaustive to obtain on realistic tasks. To mitigate the requirement for labeled data, self-training is widely used in semi-supervised learning by iteratively assigning pseudo labels to unlabeled samples. Despite its popularity, self-training is well-believed to be unreliable and often leads to training instability. Our experimental studies further reveal that the bias in semi-supervised learning arises from both the problem itself and the inappropriate training with potentially incorrect pseudo labels, which accumulates the error in the iterative self-training process. To reduce the above bias, we propose Debiased Self-Training (DST). First, the generation and utilization of pseudo labels are decoupled by two parameter-independent classifier heads to avoid direct error accumulation. Second, we estimate the worst case of self-training bias, where the pseudo labeling function is accurate on labeled samples, yet makes as many mistakes as possible on unlabeled samples. We then adversarially optimize the representations to improve the quality of pseudo labels by avoiding the worst case. Extensive experiments justify that DST achieves an average improvement of 6.3% against state-of-the-art methods on standard semi-supervised learning benchmark datasets and 18.9% against FixMatch on 13 diverse tasks. Furthermore, DST can be seamlessly adapted to other self-training methods and help stabilize their training and balance performance across classes in both cases of training from scratch and finetuning from pre-trained models.
count=1
* Transferring Fairness under Distribution Shifts via Fair Consistency Regularization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/d1dbaabf454a479ca86309e66592c7f6-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/d1dbaabf454a479ca86309e66592c7f6-Paper-Conference.pdf)]
    * Title: Transferring Fairness under Distribution Shifts via Fair Consistency Regularization
    * Year: `2022`
    * Authors: Bang An, Zora Che, Mucong Ding, Furong Huang
    * Abstract: The increasing reliance on ML models in high-stakes tasks has raised a major concern about fairness violations. Although there has been a surge of work that improves algorithmic fairness, most are under the assumption of an identical training and test distribution. In many real-world applications, however, such an assumption is often violated as previously trained fair models are often deployed in a different environment, and the fairness of such models has been observed to collapse. In this paper, we study how to transfer model fairness under distribution shifts, a widespread issue in practice. We conduct a fine-grained analysis of how the fair model is affected under different types of distribution shifts and find that domain shifts are more challenging than subpopulation shifts. Inspired by the success of self-training in transferring accuracy under domain shifts, we derive a sufficient condition for transferring group fairness. Guided by it, we propose a practical algorithm with fair consistency regularization as the key component. A synthetic dataset benchmark, which covers diverse types of distribution shifts, is deployed for experimental verification of the theoretical findings. Experiments on synthetic and real datasets, including image and tabular data, demonstrate that our approach effectively transfers fairness and accuracy under various types of distribution shifts.
count=1
* On the Discrimination Risk of Mean Aggregation Feature Imputation in Graphs
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/d4c2f25bf0c33065b7d4fb9be2a9add1-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/d4c2f25bf0c33065b7d4fb9be2a9add1-Paper-Conference.pdf)]
    * Title: On the Discrimination Risk of Mean Aggregation Feature Imputation in Graphs
    * Year: `2022`
    * Authors: Arjun Subramonian, Kai-Wei Chang, Yizhou Sun
    * Abstract: In human networks, nodes belonging to a marginalized group often have a disproportionate rate of unknown or missing features. This, in conjunction with graph structure and known feature biases, can cause graph feature imputation algorithms to predict values for unknown features that make the marginalized group's feature values more distinct from the the dominant group's feature values than they are in reality. We call this distinction the discrimination risk. We prove that a higher discrimination risk can amplify the unfairness of a machine learning model applied to the imputed data. We then formalize a general graph feature imputation framework called mean aggregation imputation and theoretically and empirically characterize graphs in which applying this framework can yield feature values with a high discrimination risk. We propose a simple algorithm to ensure mean aggregation-imputed features provably have a low discrimination risk, while minimally sacrificing reconstruction error (with respect to the imputation objective). We evaluate the fairness and accuracy of our solution on synthetic and real-world credit networks.
count=1
* FreGAN: Exploiting Frequency Components for Training GANs under Limited Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/d804cef41362be39d3972c1a71cfc4e9-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/d804cef41362be39d3972c1a71cfc4e9-Paper-Conference.pdf)]
    * Title: FreGAN: Exploiting Frequency Components for Training GANs under Limited Data
    * Year: `2022`
    * Authors: mengping yang, Zhe Wang, Ziqiu Chi, Yanbing Zhang
    * Abstract: Training GANs under limited data often leads to discriminator overfitting and memorization issues, causing divergent training. Existing approaches mitigate the overfitting by employing data augmentations, model regularization, or attention mechanisms. However, they ignore the frequency bias of GANs and take poor consideration towards frequency information, especially high-frequency signals that contain rich details. To fully utilize the frequency information of limited data, this paper proposes FreGAN, which raises the model's frequency awareness and draws more attention to synthesising high-frequency signals, facilitating high-quality generation. In addition to exploiting both real and generated images' frequency information, we also involve the frequency signals of real images as a self-supervised constraint, which alleviates the GAN disequilibrium and encourages the generator to synthesis adequate rather than arbitrary frequency signals. Extensive results demonstrate the superiority and effectiveness of our FreGAN in ameliorating generation quality in the low-data regime (especially when training data is less than 100). Besides, FreGAN can be seamlessly applied to existing regularization and attention mechanism models to further boost the performance.
count=1
* Turning the Tables: Biased, Imbalanced, Dynamic Tabular Datasets for ML Evaluation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/d9696563856bd350e4e7ac5e5812f23c-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/d9696563856bd350e4e7ac5e5812f23c-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: Turning the Tables: Biased, Imbalanced, Dynamic Tabular Datasets for ML Evaluation
    * Year: `2022`
    * Authors: Sérgio Jesus, José Pombal, Duarte Alves, André Cruz, Pedro Saleiro, Rita Ribeiro, João Gama, Pedro Bizarro
    * Abstract: Evaluating new techniques on realistic datasets plays a crucial role in the development of ML research and its broader adoption by practitioners. In recent years, there has been a significant increase of publicly available unstructured data resources for computer vision and NLP tasks. However, tabular data — which is prevalent in many high-stakes domains — has been lagging behind. To bridge this gap, we present Bank Account Fraud (BAF), the first publicly available 1 privacy-preserving, large-scale, realistic suite of tabular datasets. The suite was generated by applying state-of-the-art tabular data generation techniques on an anonymized,real-world bank account opening fraud detection dataset. This setting carries a set of challenges that are commonplace in real-world applications, including temporal dynamics and significant class imbalance. Additionally, to allow practitioners to stress test both performance and fairness of ML methods, each dataset variant of BAF contains specific types of data bias. With this resource, we aim to provide the research community with a more realistic, complete, and robust test bed to evaluate novel and existing methods.
count=1
* Delving into Out-of-Distribution Detection with Vision-Language Representations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/e43a33994a28f746dcfd53eb51ed3c2d-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/e43a33994a28f746dcfd53eb51ed3c2d-Paper-Conference.pdf)]
    * Title: Delving into Out-of-Distribution Detection with Vision-Language Representations
    * Year: `2022`
    * Authors: Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li, Yixuan Li
    * Abstract: Recognizing out-of-distribution (OOD) samples is critical for machine learning systems deployed in the open world. The vast majority of OOD detection methods are driven by a single modality (e.g., either vision or language), leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of OOD detection from a single-modal to a multi-modal regime. Particularly, we propose Maximum Concept Matching (MCM), a simple yet effective zero-shot OOD detection method based on aligning visual features with textual concepts. We contribute in-depth analysis and theoretical insights to understand the effectiveness of MCM. Extensive experiments demonstrate that MCM achieves superior performance on a wide variety of real-world tasks. MCM with vision-language features outperforms a common baseline with pure visual features on a hard OOD task with semantically similar classes by 13.1% (AUROC) Code is available at https://github.com/deeplearning-wisc/MCM.
count=1
* TwiBot-22: Towards Graph-Based Twitter Bot Detection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/e4fd610b1d77699a02df07ae97de992a-Abstract-Datasets_and_Benchmarks.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/e4fd610b1d77699a02df07ae97de992a-Paper-Datasets_and_Benchmarks.pdf)]
    * Title: TwiBot-22: Towards Graph-Based Twitter Bot Detection
    * Year: `2022`
    * Authors: Shangbin Feng, Zhaoxuan Tan, Herun Wan, Ningnan Wang, Zilong Chen, Binchi Zhang, Qinghua Zheng, Wenqian Zhang, Zhenyu Lei, Shujie Yang, Xinshun Feng, Qingyue Zhang, Hongrui Wang, Yuhan Liu, Yuyang Bai, Heng Wang, Zijian Cai, Yanbo Wang, Lijing Zheng, Zihan Ma, Jundong Li, Minnan Luo
    * Abstract: Twitter bot detection has become an increasingly important task to combat misinformation, facilitate social media moderation, and preserve the integrity of the online discourse. State-of-the-art bot detection methods generally leverage the graph structure of the Twitter network, and they exhibit promising performance when confronting novel Twitter bots that traditional methods fail to detect. However, very few of the existing Twitter bot detection datasets are graph-based, and even these few graph-based datasets suffer from limited dataset scale, incomplete graph structure, as well as low annotation quality. In fact, the lack of a large-scale graph-based Twitter bot detection benchmark that addresses these issues has seriously hindered the development and evaluation of novel graph-based bot detection approaches. In this paper, we propose TwiBot-22, a comprehensive graph-based Twitter bot detection benchmark that presents the largest dataset to date, provides diversified entities and relations on the Twitter network, and has considerably better annotation quality than existing datasets. In addition, we re-implement 35 representative Twitter bot detection baselines and evaluate them on 9 datasets, including TwiBot-22, to promote a fair comparison of model performance and a holistic understanding of research progress. To facilitate further research, we consolidate all implemented codes and datasets into the TwiBot-22 evaluation framework, where researchers could consistently evaluate new models and datasets. The TwiBot-22 Twitter bot detection benchmark and evaluation framework are publicly available at \url{https://twibot22.github.io/}.
count=1
* Triangulation candidates for Bayesian optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/e9750610639c3e7a849cff746bf60dbd-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/e9750610639c3e7a849cff746bf60dbd-Paper-Conference.pdf)]
    * Title: Triangulation candidates for Bayesian optimization
    * Year: `2022`
    * Authors: Robert B. Gramacy, Annie Sauer, Nathan Wycoff
    * Abstract: Bayesian optimization involves "inner optimization" over a new-data acquisition criterion which is non-convex/highly multi-modal, may be non-differentiable, or may otherwise thwart local numerical optimizers. In such cases it is common to replace continuous search with a discrete one over random candidates. Here we propose using candidates based on a Delaunay triangulation of the existing input design. We detail the construction of these "tricands" and demonstrate empirically how they outperform both numerically optimized acquisitions and random candidate-based alternatives, and are well-suited for hybrid schemes, on benchmark synthetic and real simulation experiments.
count=1
* Interpolation and Regularization for Causal Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/ed7b8e1312f6ba8af6e4316dcd28bb3d-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/ed7b8e1312f6ba8af6e4316dcd28bb3d-Paper-Conference.pdf)]
    * Title: Interpolation and Regularization for Causal Learning
    * Year: `2022`
    * Authors: Leena Chennuru Vankadara, Luca Rendsburg, Ulrike Luxburg, Debarghya Ghoshdastidar
    * Abstract: Recent work shows that in complex model classes, interpolators can achieve statistical generalization and even be optimal for statistical learning. However, despite increasing interest in learning models with good causal properties, there is no understanding of whether such interpolators can also achieve causal generalization. To address this gap, we study causal learning from observational data through the lens of interpolation and its counterpart---regularization. Under a simple linear causal model, we derive precise asymptotics for the causal risk of the min-norm interpolator and ridge regressors in the high-dimensional regime. We find a large range of behavior that can be precisely characterized by a new measure of confounding strength. When confounding strength is positive, which holds under independent causal mechanisms---a standard assumption in causal learning---we find that interpolators cannot be optimal. Indeed, causal learning requires stronger regularization than statistical learning. Beyond this assumption, when confounding is negative, we observe a phenomenon of self-induced regularization due to positive alignment between statistical and causal signals. Here, causal learning requires weaker regularization than statistical learning, interpolators can be optimal, and optimal regularization can even be negative.
count=1
* Transfer Learning on Heterogeneous Feature Spaces for Treatment Effects Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/f0e5cde3850e7dd0db125c0ebae16680-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/f0e5cde3850e7dd0db125c0ebae16680-Paper-Conference.pdf)]
    * Title: Transfer Learning on Heterogeneous Feature Spaces for Treatment Effects Estimation
    * Year: `2022`
    * Authors: Ioana Bica, Mihaela van der Schaar
    * Abstract: Consider the problem of improving the estimation of conditional average treatment effects (CATE) for a target domain of interest by leveraging related information from a source domain with a different feature space. This heterogeneous transfer learning problem for CATE estimation is ubiquitous in areas such as healthcare where we may wish to evaluate the effectiveness of a treatment for a new patient population for which different clinical covariates and limited data are available. In this paper, we address this problem by introducing several building blocks that use representation learning to handle the heterogeneous feature spaces and a flexible multi-task architecture with shared and private layers to transfer information between potential outcome functions across domains. Then, we show how these building blocks can be used to recover transfer learning equivalents of the standard CATE learners. On a new semi-synthetic data simulation benchmark for heterogeneous transfer learning, we not only demonstrate performance improvements of our heterogeneous transfer causal effect learners across datasets, but also provide insights into the differences between these learners from a transfer perspective.
count=1
* Theoretical analysis of deep neural networks for temporally dependent observations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/hash/f23653913d8390cd4fc1bee8a3238e17-Abstract-Conference.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2022/file/f23653913d8390cd4fc1bee8a3238e17-Paper-Conference.pdf)]
    * Title: Theoretical analysis of deep neural networks for temporally dependent observations
    * Year: `2022`
    * Authors: Mingliang Ma, Abolfazl Safikhani
    * Abstract: Deep neural networks are powerful tools to model observations over time with non-linear patterns. Despite the widespread useof neural networks in such settings, most theoretical developments of deep neural networks are under the assumption of independent observations, and theoretical results for temporally dependent observations are scarce. To bridge this gap, we study theoretical properties of deep neural networks on modeling non-linear time series data. Specifically, non-asymptotic bounds for prediction error of (sparse) feed-forward neural network with ReLU activation function is established under mixing-type assumptions. These assumptions are mild such that they include a wide range of time series models including auto-regressive models. Compared to independent observations, established convergence rates have additional logarithmic factors to compensate for additional complexity due to dependence among data points. The theoretical results are supported via various numerical simulation settings as well as an application to a macroeconomic data set.
count=1
* BAST: Bayesian Additive Regression Spanning Trees for Complex Constrained Domain
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/00b76fddeaaa7d8c2c43d504b2babd8a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/00b76fddeaaa7d8c2c43d504b2babd8a-Paper.pdf)]
    * Title: BAST: Bayesian Additive Regression Spanning Trees for Complex Constrained Domain
    * Year: `2021`
    * Authors: Zhao Tang Luo, Huiyan Sang, Bani Mallick
    * Abstract: Nonparametric regression on complex domains has been a challenging task as most existing methods, such as ensemble models based on binary decision trees, are not designed to account for intrinsic geometries and domain boundaries. This article proposes a Bayesian additive regression spanning trees (BAST) model for nonparametric regression on manifolds, with an emphasis on complex constrained domains or irregularly shaped spaces embedded in Euclidean spaces. Our model is built upon a random spanning tree manifold partition model as each weak learner, which is capable of capturing any irregularly shaped spatially contiguous partitions while respecting intrinsic geometries and domain boundary constraints. Utilizing many nice properties of spanning tree structures, we design an efficient Bayesian inference algorithm. Equipped with a soft prediction scheme, BAST is demonstrated to significantly outperform other competing methods in simulation experiments and in an application to the chlorophyll data in Aral Sea, due to its strong local adaptivity to different levels of smoothness.
count=1
* Learning Space Partitions for Path Planning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/03a3655fff3e9bdea48de9f49e938e32-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/03a3655fff3e9bdea48de9f49e938e32-Paper.pdf)]
    * Title: Learning Space Partitions for Path Planning
    * Year: `2021`
    * Authors: Kevin Yang, Tianjun Zhang, Chris Cummins, Brandon Cui, Benoit Steiner, Linnan Wang, Joseph E. Gonzalez, Dan Klein, Yuandong Tian
    * Abstract: Path planning, the problem of efficiently discovering high-reward trajectories, often requires optimizing a high-dimensional and multimodal reward function. Popular approaches like CEM and CMA-ES greedily focus on promising regions of the search space and may get trapped in local maxima. DOO and VOOT balance exploration and exploitation, but use space partitioning strategies independent of the reward function to be optimized. Recently, LaMCTS empirically learns to partition the search space in a reward-sensitive manner for black-box optimization. In this paper, we develop a novel formal regret analysis for when and why such an adaptive region partitioning scheme works. We also propose a new path planning method LaP3 which improves the function value estimation within each sub-region, and uses a latent representation of the search space. Empirically, LaP3 outperforms existing path planning methods in 2D navigation tasks, especially in the presence of difficult-to-escape local optima, and shows benefits when plugged into the planning components of model-based RL such as PETS. These gains transfer to highly multimodal real-world tasks, where we outperform strong baselines in compiler phase ordering by up to 39% on average across 9 tasks, and in molecular design by up to 0.4 on properties on a 0-1 scale. Code is available at https://github.com/yangkevin2/neurips2021-lap3.
count=1
* Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/05546b0e38ab9175cd905eebcc6ebb76-Paper.pdf)]
    * Title: Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers
    * Year: `2021`
    * Authors: Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, Christopher Ré
    * Abstract: Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence $u \mapsto y$ by simply simulating a linear continuous-time state-space representation $\dot{x} = Ax + Bu, y = Cx + Du$. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices $A$ that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences.
count=1
* Arbitrary Conditional Distributions with Energy
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/06c284d3f757b15c02f47f3ff06dc275-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/06c284d3f757b15c02f47f3ff06dc275-Paper.pdf)]
    * Title: Arbitrary Conditional Distributions with Energy
    * Year: `2021`
    * Authors: Ryan Strauss, Junier B. Oliva
    * Abstract: Modeling distributions of covariates, or density estimation, is a core challenge in unsupervised learning. However, the majority of work only considers the joint distribution, which has limited relevance to practical situations. A more general and useful problem is arbitrary conditional density estimation, which aims to model any possible conditional distribution over a set of covariates, reflecting the more realistic setting of inference based on prior knowledge. We propose a novel method, Arbitrary Conditioning with Energy (ACE), that can simultaneously estimate the distribution $p(\mathbf{x}_u \mid \mathbf{x}_o)$ for all possible subsets of unobserved features $\mathbf{x}_u$ and observed features $\mathbf{x}_o$. ACE is designed to avoid unnecessary bias and complexity --- we specify densities with a highly expressive energy function and reduce the problem to only learning one-dimensional conditionals (from which more complex distributions can be recovered during inference). This results in an approach that is both simpler and higher-performing than prior methods. We show that ACE achieves state-of-the-art for arbitrary conditional likelihood estimation and data imputation on standard benchmarks.
count=1
* Improving Conditional Coverage via Orthogonal Quantile Regression
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/1006ff12c465532f8c574aeaa4461b16-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf)]
    * Title: Improving Conditional Coverage via Orthogonal Quantile Regression
    * Year: `2021`
    * Authors: Shai Feldman, Stephen Bates, Yaniv Romano
    * Abstract: We develop a method to generate prediction intervals that have a user-specified coverage level across all regions of feature-space, a property called conditional coverage. A typical approach to this task is to estimate the conditional quantiles with quantile regression---it is well-known that this leads to correct coverage in the large-sample limit, although it may not be accurate in finite samples. We find in experiments that traditional quantile regression can have poor conditional coverage. To remedy this, we modify the loss function to promote independence between the size of the intervals and the indicator of a miscoverage event. For the true conditional quantiles, these two quantities are independent (orthogonal), so the modified loss function continues to be valid. Moreover, we empirically show that the modified loss function leads to improved conditional coverage, as evaluated by several metrics. We also introduce two new metrics that check conditional coverage by looking at the strength of the dependence between the interval size and the indicator of miscoverage.
count=1
* Inverse-Weighted Survival Games
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/10fb6cfa4c990d2bad5ddef4f70e8ba2-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/10fb6cfa4c990d2bad5ddef4f70e8ba2-Paper.pdf)]
    * Title: Inverse-Weighted Survival Games
    * Year: `2021`
    * Authors: Xintian Han, Mark Goldstein, Aahlad Puli, Thomas Wies, Adler Perotte, Rajesh Ranganath
    * Abstract: Deep models trained through maximum likelihood have achieved state-of-the-art results for survival analysis. Despite this training scheme, practitioners evaluate models under other criteria, such as binary classification losses at a chosen set of time horizons, e.g. Brier score (BS) and Bernoulli log likelihood (BLL). Models trained with maximum likelihood may have poor BS or BLL since maximum likelihood does not directly optimize these criteria. Directly optimizing criteria like BS requires inverse-weighting by the censoring distribution. However, estimating the censoring model under these metrics requires inverse-weighting by the failure distribution. The objective for each model requires the other, but neither are known. To resolve this dilemma, we introduce Inverse-Weighted Survival Games. In these games, objectives for each model are built from re-weighted estimates featuring the other model, where the latter is held fixed during training. When the loss is proper, we show that the games always have the true failure and censoring distributions as a stationary point. This means models in the game do not leave the correct distributions once reached. We construct one case where this stationary point is unique. We show that these games optimize BS on simulations and then apply these principles on real world cancer and critically-ill patient data.
count=1
* Self-Interpretable Model with Transformation Equivariant Interpretation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/1387a00f03b4b423e63127b08c261bdc-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/1387a00f03b4b423e63127b08c261bdc-Paper.pdf)]
    * Title: Self-Interpretable Model with Transformation Equivariant Interpretation
    * Year: `2021`
    * Authors: Yipei Wang, Xiaoqian Wang
    * Abstract: With the proliferation of machine learning applications in the real world, the demand for explaining machine learning predictions continues to grow especially in high-stakes fields. Recent studies have found that interpretation methods can be sensitive and unreliable, where the interpretations can be disturbed by perturbations or transformations of input data. To address this issue, we propose to learn robust interpretation through transformation equivariant regularization in a self-interpretable model. The resulting model is capable of capturing valid interpretation that is equivariant to geometric transformations. Moreover, since our model is self-interpretable, it enables faithful interpretations that reflect the true predictive mechanism. Unlike existing self-interpretable models, which usually sacrifice expressive power for the sake of interpretation quality, our model preserves the high expressive capability comparable to the state-of-the-art deep learning models in complex tasks, while providing visualizable and faithful high-quality interpretation. We compare with various related methods and validate the interpretation quality and consistency of our model.
count=1
* Automatic Unsupervised Outlier Model Selection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/23c894276a2c5a16470e6a31f4618d73-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/23c894276a2c5a16470e6a31f4618d73-Paper.pdf)]
    * Title: Automatic Unsupervised Outlier Model Selection
    * Year: `2021`
    * Authors: Yue Zhao, Ryan Rossi, Leman Akoglu
    * Abstract: Given an unsupervised outlier detection task on a new dataset, how can we automatically select a good outlier detection algorithm and its hyperparameter(s) (collectively called a model)? In this work, we tackle the unsupervised outlier model selection (UOMS) problem, and propose MetaOD, a principled, data-driven approach to UOMS based on meta-learning. The UOMS problem is notoriously challenging, as compared to model selection for classification and clustering, since (i) model evaluation is infeasible due to the lack of hold-out data with labels, and (ii) model comparison is infeasible due to the lack of a universal objective function. MetaOD capitalizes on the performances of a large body of detection models on historical outlier detection benchmark datasets, and carries over this prior experience to automatically select an effective model to be employed on a new dataset without any labels, model evaluations or model comparisons. To capture task similarity within our meta-learning framework, we introduce specialized meta-features that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by MetaOD significantly outperforms no model selection (e.g. always using the same popular model or the ensemble of many) as well as other meta-learning techniques that we tailored for UOMS. Moreover upon (meta-)training, MetaOD is extremely efficient at test time; selecting from a large pool of 300+ models takes less than 1 second for a new task. We open-source MetaOD and our meta-learning database for practical use and to foster further research on the UOMS problem.
count=1
* A$^2$-Net: Learning Attribute-Aware Hash Codes for Large-Scale Fine-Grained Image Retrieval
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/2d3acd3e240c61820625fff66a19938f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/2d3acd3e240c61820625fff66a19938f-Paper.pdf)]
    * Title: A$^2$-Net: Learning Attribute-Aware Hash Codes for Large-Scale Fine-Grained Image Retrieval
    * Year: `2021`
    * Authors: Xiu-Shen Wei, Yang Shen, Xuhao Sun, Han-Jia Ye, Jian Yang
    * Abstract: Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper, we propose an Attribute-Aware hashing Network (A$^2$-Net) for generating attribute-aware hash codes to not only make the retrieval process efficient, but also establish explicit correspondences between hash codes and visual attributes. Specifically, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. A$^2$-Net is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their representation abilities. Finally, the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on five benchmark fine-grained datasets show our superiority over competing methods. More importantly, quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of fine-grained objects.
count=1
* Provably efficient, succinct, and precise explanations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/30d4e6422cd65c7913bc9ce62e078b79-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/30d4e6422cd65c7913bc9ce62e078b79-Paper.pdf)]
    * Title: Provably efficient, succinct, and precise explanations
    * Year: `2021`
    * Authors: Guy Blanc, Jane Lange, Li-Yang Tan
    * Abstract: We consider the problem of explaining the predictions of an arbitrary blackbox model $f$: given query access to $f$ and an instance $x$, output a small set of $x$'s features that in conjunction essentially determines $f(x)$. We design an efficient algorithm with provable guarantees on the succinctness and precision of the explanations that it returns. Prior algorithms were either efficient but lacked such guarantees, or achieved such guarantees but were inefficient. We obtain our algorithm via a connection to the problem of {\sl implicitly} learning decision trees. The implicit nature of this learning task allows for efficient algorithms even when the complexity of~$f$ necessitates an intractably large surrogate decision tree. We solve the implicit learning problem by bringing together techniques from learning theory, local computation algorithms, and complexity theory. Our approach of “explaining by implicit learning” shares elements of two previously disparate methods for post-hoc explanations, global and local explanations, and we make the case that it enjoys advantages of both.
count=1
* Bootstrapping the Error of Oja's Algorithm 
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/3152e3b1e52e2cb123363787d5f76c95-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/3152e3b1e52e2cb123363787d5f76c95-Paper.pdf)]
    * Title: Bootstrapping the Error of Oja's Algorithm 
    * Year: `2021`
    * Authors: Robert Lunde, Purnamrita Sarkar, Rachel Ward
    * Abstract: We consider the problem of quantifying uncertainty for the estimation error of the leading eigenvector from Oja's algorithm for streaming principal component analysis, where the data are generated IID from some unknown distribution. By combining classical tools from the U-statistics literature with recent results on high-dimensional central limit theorems for quadratic forms of random vectors and concentration of matrix products, we establish a weighted $\chi^2$ approximation result for the $\sin^2$ error between the population eigenvector and the output of Oja’s algorithm. Since estimating the covariance matrix associated with the approximating distribution requires knowledge of unknown model parameters, we propose a multiplier bootstrap algorithm that may be updated in an online manner. We establish conditions under which the bootstrap distribution is close to the corresponding sampling distribution with high probability, thereby establishing the bootstrap as a consistent inferential method in an appropriate asymptotic regime.
count=1
* Contrastive Learning of Global and Local Video Representations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/38ef4b66cb25e92abe4d594acb841471-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/38ef4b66cb25e92abe4d594acb841471-Paper.pdf)]
    * Title: Contrastive Learning of Global and Local Video Representations
    * Year: `2021`
    * Authors: shuang ma, Zhaoyang Zeng, Daniel McDuff, Yale Song
    * Abstract: Contrastive learning has delivered impressive results for various tasks in the self-supervised regime. However, existing approaches optimize for learning representations specific to downstream scenarios, i.e., global representations suitable for tasks such as classification or local representations for tasks such as detection and localization. While they produce satisfactory results in the intended downstream scenarios, they often fail to generalize to tasks that they were not originally designed for. In this work, we propose to learn video representations that generalize to both the tasks which require global semantic information (e.g., classification) and the tasks that require local fine-grained spatio-temporal information (e.g., localization). We achieve this by optimizing two contrastive objectives that together encourage our model to learn global-local visual information given audio signals. We show that the two objectives mutually improve the generalizability of the learned global-local representations, significantly outperforming their disjointly learned counterparts. We demonstrate our approach on various tasks including action/sound classification, lipreading, deepfake detection, event and sound localization.
count=1
* Neural Ensemble Search for Uncertainty Estimation and Dataset Shift
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/41a6fd31aa2e75c3c6d427db3d17ea80-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/41a6fd31aa2e75c3c6d427db3d17ea80-Paper.pdf)]
    * Title: Neural Ensemble Search for Uncertainty Estimation and Dataset Shift
    * Year: `2021`
    * Authors: Sheheryar Zaidi, Arber Zela, Thomas Elsken, Chris C Holmes, Frank Hutter, Yee Teh
    * Abstract: Ensembles of neural networks achieve superior performance compared to standalone networks in terms of accuracy, uncertainty calibration and robustness to dataset shift. Deep ensembles, a state-of-the-art method for uncertainty estimation, only ensemble random initializations of a fixed architecture. Instead, we propose two methods for automatically constructing ensembles with varying architectures, which implicitly trade-off individual architectures’ strengths against the ensemble’s diversity and exploit architectural variation as a source of diversity. On a variety of classification tasks and modern architecture search spaces, we show that the resulting ensembles outperform deep ensembles not only in terms of accuracy but also uncertainty calibration and robustness to dataset shift. Our further analysis and ablation studies provide evidence of higher ensemble diversity due to architectural variation, resulting in ensembles that can outperform deep ensembles, even when having weaker average base learners. To foster reproducibility, our code is available: https://github.com/automl/nes
count=1
* Combining Latent Space and Structured Kernels for Bayesian Optimization over Combinatorial Spaces
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/44e76e99b5e194377e955b13fb12f630-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/44e76e99b5e194377e955b13fb12f630-Paper.pdf)]
    * Title: Combining Latent Space and Structured Kernels for Bayesian Optimization over Combinatorial Spaces
    * Year: `2021`
    * Authors: Aryan Deshwal, Jana Doppa
    * Abstract: We consider the problem of optimizing combinatorial spaces (e.g., sequences, trees, and graphs) using expensive black-box function evaluations. For example, optimizing molecules for drug design using physical lab experiments. Bayesian optimization (BO) is an efficient framework for solving such problems by intelligently selecting the inputs with high utility guided by a learned surrogate model. A recent BO approach for combinatorial spaces is through a reduction to BO over continuous spaces by learning a latent representation of structures using deep generative models (DGMs). The selected input from the continuous space is decoded into a discrete structure for performing function evaluation. However, the surrogate model over the latent space only uses the information learned by the DGM, which may not have the desired inductive bias to approximate the target black-box function. To overcome this drawback, this paper proposes a principled approach referred as LADDER. The key idea is to define a novel structure-coupled kernel that explicitly integrates the structural information from decoded structures with the learned latent space representation for better surrogate modeling. Our experiments on real-world benchmarks show that LADDER significantly improves over the BO over latent space method, and performs better or similar to state-of-the-art methods.
count=1
* Functionally Regionalized Knowledge Transfer for Low-resource Drug Discovery
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/459a4ddcb586f24efd9395aa7662bc7c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf)]
    * Title: Functionally Regionalized Knowledge Transfer for Low-resource Drug Discovery
    * Year: `2021`
    * Authors: Huaxiu Yao, Ying Wei, Long-Kai Huang, Ding Xue, Junzhou Huang, Zhenhui (Jessie) Li
    * Abstract: More recently, there has been a surge of interest in employing machine learning approaches to expedite the drug discovery process where virtual screening for hit discovery and ADMET prediction for lead optimization play essential roles. One of the main obstacles to the wide success of machine learning approaches in these two tasks is that the number of compounds labeled with activities or ADMET properties is too small to build an effective predictive model. This paper seeks to remedy the problem by transferring the knowledge from previous assays, namely in-vivo experiments, by different laboratories and against various target proteins. To accommodate these wildly different assays and capture the similarity between assays, we propose a functional rationalized meta-learning algorithm FRML for such knowledge transfer. FRML constructs the predictive model with layers of neural sub-networks or so-called functional regions. Building on this, FRML shares an initialization for the weights of the predictive model across all assays, while customizes it to each assay with a region localization network choosing the pertinent regions. The compositionality of the model improves the capacity of generalization to various and even out-of-distribution tasks. Empirical results on both virtual screening and ADMET prediction validate the superiority of FRML over state-of-the-art baselines powered with interpretability in assay relationship.
count=1
* Scalable Neural Data Server: A Data Recommender for Transfer Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/4b55df75e2e804bab559aa885be40310-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/4b55df75e2e804bab559aa885be40310-Paper.pdf)]
    * Title: Scalable Neural Data Server: A Data Recommender for Transfer Learning
    * Year: `2021`
    * Authors: Tianshi Cao, Sasha (Alexandre) Doubov, David Acuna, Sanja Fidler
    * Abstract: Absence of large-scale labeled data in the practitioner's target domain can be a bottleneck to applying machine learning algorithms in practice. Transfer learning is a popular strategy for leveraging additional data to improve the downstream performance, but finding the most relevant data to transfer from can be challenging. Neural Data Server (NDS), a search engine that recommends relevant data for a given downstream task, has been previously proposed to address this problem (Yan et al., 2020). NDS uses a mixture of experts trained on data sources to estimate similarity between each source and the downstream task. Thus, the computational cost to each user grows with the number of sources and requires an expensive training step for each data provider.To address these issues, we propose Scalable Neural Data Server (SNDS), a large-scale search engine that can theoretically index thousands of datasets to serve relevant ML data to end users. SNDS trains the mixture of experts on intermediary datasets during initialization, and represents both data sources and downstream tasks by their proximity to the intermediary datasets. As such, computational cost incurred by users of SNDS remains fixed as new datasets are added to the server, without pre-training for the data providers.We validate SNDS on a plethora of real world tasks and find that data recommended by SNDS improves downstream task performance over baselines. We also demonstrate the scalability of our system by demonstrating its ability to select relevant data for transfer outside of the natural image setting.
count=1
* Foundations of Symbolic Languages for Model Interpretability
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/60cb558c40e4f18479664069d9642d5a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/60cb558c40e4f18479664069d9642d5a-Paper.pdf)]
    * Title: Foundations of Symbolic Languages for Model Interpretability
    * Year: `2021`
    * Authors: Marcelo Arenas, Daniel Báez, Pablo Barceló, Jorge Pérez, Bernardo Subercaseaux
    * Abstract: Several queries and scores have recently been proposed to explain individual predictions over ML models. Examples include queries based on “anchors”, which are parts of an instance that are sufficient to justify its classification, and “feature-perturbation” scores such as SHAP. Given the need for flexible, reliable, and easy-to-apply interpretability methods for ML models, we foresee the need for developing declarative languages to naturally specify different explainability queries. We do this in a principled way by rooting such a language in a logic called FOIL, which allows for expressing many simple but important explainability queries, and might serve as a core for more expressive interpretability languages. We study the computational complexity of FOIL queries over two classes of ML models often deemed to be easily interpretable: decision trees and more general decision diagrams. Since the number of possible inputs for an ML model is exponential in its dimension, tractability of the FOIL evaluation problem is delicate but can be achieved by either restricting the structure of the models, or the fragment of FOIL being evaluated. We also present a prototype implementation of FOIL wrapped in a high-level declarative language and perform experiments showing that such a language can be used in practice.
count=1
* Contrastive Learning for Neural Topic Model
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/6467c327eaf8940b4dd07a08c63c5e85-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/6467c327eaf8940b4dd07a08c63c5e85-Paper.pdf)]
    * Title: Contrastive Learning for Neural Topic Model
    * Year: `2021`
    * Authors: Thong Nguyen, Anh Tuan Luu
    * Abstract: Recent empirical studies show that adversarial topic models (ATM) can successfully capture semantic patterns of the document by differentiating a document with another dissimilar sample. However, utilizing that discriminative-generative architecture has two important drawbacks: (1) the architecture does not relate similar documents, which has the same document-word distribution of salient words; (2) it restricts the ability to integrate external information, such as sentiments of the document, which has been shown to benefit the training of neural topic model. To address those issues, we revisit the adversarial topic architecture in the view point of mathematical analysis, propose a novel approach to re-formulate discriminative goal as an optimization problem, and design a novel sampling method which facilitates the integration of external variables. The reformulation encourages the model to incorporate the relations among similar samples and enforces the constraint on the similarity among dissimilar ones; while the sampling method, which is based on the internal input and reconstructed output, helps inform the model of salient words contributing to the main topic. Experimental results show that our framework outperforms other state-of-the-art neural topic models in three common benchmark datasets that belong to various domains, vocabulary sizes, and document lengths in terms of topic coherence.
count=1
* Object-aware Contrastive Learning for Debiased Scene Representation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/65d2ea03425887a717c435081cfc5dbb-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/65d2ea03425887a717c435081cfc5dbb-Paper.pdf)]
    * Title: Object-aware Contrastive Learning for Debiased Scene Representation
    * Year: `2021`
    * Authors: Sangwoo Mo, Hyunwoo Kang, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin
    * Abstract: Contrastive self-supervised learning has shown impressive results in learning visual representations from unlabeled images by enforcing invariance against different data augmentations. However, the learned representations are often contextually biased to the spurious scene correlations of different objects or object and background, which may harm their generalization on the downstream tasks. To tackle the issue, we develop a novel object-aware contrastive learning framework that first (a) localizes objects in a self-supervised manner and then (b) debias scene correlations via appropriate data augmentations considering the inferred object locations. For (a), we propose the contrastive class activation map (ContraCAM), which finds the most discriminative regions (e.g., objects) in the image compared to the other images using the contrastively trained models. We further improve the ContraCAM to detect multiple objects and entire shapes via an iterative refinement procedure. For (b), we introduce two data augmentations based on ContraCAM, object-aware random crop and background mixup, which reduce contextual and background biases during contrastive self-supervised learning, respectively. Our experiments demonstrate the effectiveness of our representation learning framework, particularly when trained under multi-object images or evaluated under the background (and distribution) shifted images. Code is available at https://github.com/alinlab/object-aware-contrastive.
count=1
* Answering Complex Causal Queries With the Maximum Causal Set Effect
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/670f0c94cc5271fe6017eeffa642b7d3-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/670f0c94cc5271fe6017eeffa642b7d3-Paper.pdf)]
    * Title: Answering Complex Causal Queries With the Maximum Causal Set Effect
    * Year: `2021`
    * Authors: Zachary Markovich
    * Abstract: The standard tools of causal inference have been developed to answer simple causal queries which can be easily formalized as a small number of statistical estimands in the context of a particular structural causal model (SCM); however, scientific theories often make diffuse predictions about a large number of causal variables. This article proposes a framework for parameterizing such complex causal queries as the maximum difference in causal effects associated with two sets of causal variables that have a researcher specified probability of occurring. We term this estimand the Maximum Causal Set Effect (MCSE) and develop an estimator for it that is asymptotically consistent and conservative in finite samples under assumptions that are standard in the causal inference literature. This estimator is also asymptotically normal and amenable to the non-parametric bootstrap, facilitating classical statistical inference about this novel estimand. We compare this estimator to more common latent variable approaches and find that it can uncover larger causal effects in both real world and simulated data.
count=1
* The Inductive Bias of Quantum Kernels
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/69adc1e107f7f7d035d7baf04342e1ca-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf)]
    * Title: The Inductive Bias of Quantum Kernels
    * Year: `2021`
    * Authors: Jonas Kübler, Simon Buchholz, Bernhard Schölkopf
    * Abstract: It has been hypothesized that quantum computers may lend themselves well to applications in machine learning. In the present work, we analyze function classes defined via quantum kernels. Quantum computers offer the possibility to efficiently compute inner products of exponentially large density operators that are classically hard to compute. However, having an exponentially large feature space renders the problem of generalization hard. Furthermore, being able to evaluate inner products in high dimensional spaces efficiently by itself does not guarantee a quantum advantage, as already classically tractable kernels can correspond to high- or infinite-dimensional reproducing kernel Hilbert spaces (RKHS). We analyze the spectral properties of quantum kernels and find that we can expect an advantage if their RKHS is low dimensional and contains functions that are hard to compute classically. If the target function is known to lie in this class, this implies a quantum advantage, as the quantum computer can encode this inductive bias, whereas there is no classically efficient way to constrain the function class in the same way. However, we show that finding suitable quantum kernels is not easy because the kernel evaluation might require exponentially many measurements. In conclusion, our message is a somewhat sobering one: we conjecture that quantum machine learning models can offer speed-ups only if we manage to encode knowledge about the problem at hand into quantum circuits, while encoding the same bias into a classical model would be hard. These situations may plausibly occur when learning on data generated by a quantum process, however, they appear to be harder to come by for classical datasets.
count=1
* Learning in Multi-Stage Decentralized Matching Markets
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/6a571fe98a2ba453e84923b447d79cff-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/6a571fe98a2ba453e84923b447d79cff-Paper.pdf)]
    * Title: Learning in Multi-Stage Decentralized Matching Markets
    * Year: `2021`
    * Authors: Xiaowu Dai, Michael Jordan
    * Abstract: Matching markets are often organized in a multi-stage and decentralized manner. Moreover, participants in real-world matching markets often have uncertain preferences. This article develops a framework for learning optimal strategies in such settings, based on a nonparametric statistical approach and variational analysis. We propose an efficient algorithm, built upon concepts of "lower uncertainty bound" and "calibrated decentralized matching," for maximizing the participants' expected payoff. We show that there exists a welfare-versus-fairness trade-off that is characterized by the uncertainty level of acceptance. Participants will strategically act in favor of a low uncertainty level to reduce competition and increase expected payoff. We prove that participants can be better off with multi-stage matching compared to single-stage matching. We demonstrate aspects of the theoretical predictions through simulations and an experiment using real data from college admissions.
count=1
* A Compositional Atlas of Tractable Circuit Operations for Probabilistic Inference
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/6e01383fd96a17ae51cc3e15447e7533-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/6e01383fd96a17ae51cc3e15447e7533-Paper.pdf)]
    * Title: A Compositional Atlas of Tractable Circuit Operations for Probabilistic Inference
    * Year: `2021`
    * Authors: Antonio Vergari, YooJung Choi, Anji Liu, Stefano Teso, Guy Van den Broeck
    * Abstract: Circuit representations are becoming the lingua franca to express and reason about tractable generative and discriminative models. In this paper, we show how complex inference scenarios for these models that commonly arise in machine learning---from computing the expectations of decision tree ensembles to information-theoretic divergences of sum-product networks---can be represented in terms of tractable modular operations over circuits. Specifically, we characterize the tractability of simple transformations---sums, products, quotients, powers, logarithms, and exponentials---in terms of sufficient structural constraints of the circuits they operate on, and present novel hardness results for the cases in which these properties are not satisfied. Building on these operations, we derive a unified framework for reasoning about tractable models that generalizes several results in the literature and opens up novel tractable inference scenarios.
count=1
* Learning Treatment Effects in Panels with General Intervention Patterns
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/7504adad8bb96320eb3afdd4df6e1f60-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/7504adad8bb96320eb3afdd4df6e1f60-Paper.pdf)]
    * Title: Learning Treatment Effects in Panels with General Intervention Patterns
    * Year: `2021`
    * Authors: Vivek Farias, Andrew Li, Tianyi Peng
    * Abstract: The problem of causal inference with panel data is a central econometric question. The following is a fundamental version of this problem: Let $M^*$ be a low rank matrix and $E$ be a zero-mean noise matrix. For a `treatment' matrix $Z$ with entries in $\{0,1\}$ we observe the matrix $O$ with entries $O_{ij} := M^*_{ij} + E_{ij} + \mathcal{T}_{ij} Z_{ij}$ where $\mathcal{T}_{ij} $ are unknown, heterogenous treatment effects. The problem requires we estimate the average treatment effect $\tau^* := \sum_{ij} \mathcal{T}_{ij} Z_{ij} / \sum_{ij} Z_{ij}$. The synthetic control paradigm provides an approach to estimating $\tau^*$ when $Z$ places support on a single row. This paper extends that framework to allow rate-optimal recovery of $\tau^*$ for general $Z$, thus broadly expanding its applicability. Our guarantees are the first of their type in this general setting. Computational experiments on synthetic and real-world data show a substantial advantage over competing estimators.
count=1
* A PAC-Bayes Analysis of Adversarial Robustness
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/78e8dffe65a2898eef68a33b8db35b78-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/78e8dffe65a2898eef68a33b8db35b78-Paper.pdf)]
    * Title: A PAC-Bayes Analysis of Adversarial Robustness
    * Year: `2021`
    * Authors: Paul Viallard, Eric Guillaume VIDOT, Amaury Habrard, Emilie Morvant
    * Abstract: We propose the first general PAC-Bayesian generalization bounds for adversarial robustness, that estimate, at test time, how much a model will be invariant to imperceptible perturbations in the input. Instead of deriving a worst-case analysis of the risk of a hypothesis over all the possible perturbations, we leverage the PAC-Bayesian framework to bound the averaged risk on the perturbations for majority votes (over the whole class of hypotheses). Our theoretically founded analysis has the advantage to provide general bounds (i) that are valid for any kind of attacks (i.e., the adversarial attacks), (ii) that are tight thanks to the PAC-Bayesian framework, (iii) that can be directly minimized during the learning phase to obtain a robust model on different attacks at test time.
count=1
* Adversarial Examples for k-Nearest Neighbor Classifiers Based on Higher-Order Voronoi Diagrams
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/82ca5dd156cc926b2992f73c2896f761-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/82ca5dd156cc926b2992f73c2896f761-Paper.pdf)]
    * Title: Adversarial Examples for k-Nearest Neighbor Classifiers Based on Higher-Order Voronoi Diagrams
    * Year: `2021`
    * Authors: Chawin Sitawarin, Evgenios Kornaropoulos, Dawn Song, David Wagner
    * Abstract: Adversarial examples are a widely studied phenomenon in machine learning models. While most of the attention has been focused on neural networks, other practical models also suffer from this issue. In this work, we propose an algorithm for evaluating the adversarial robustness of $k$-nearest neighbor classification, i.e., finding a minimum-norm adversarial example. Diverging from previous proposals, we propose the first geometric approach by performing a search that expands outwards from a given input point. On a high level, the search radius expands to the nearby higher-order Voronoi cells until we find a cell that classifies differently from the input point. To scale the algorithm to a large $k$, we introduce approximation steps that find perturbation with smaller norm, compared to the baselines, in a variety of datasets. Furthermore, we analyze the structural properties of a dataset where our approach outperforms the competition.
count=1
* Self-Supervised Learning with Kernel Dependence Maximization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/83004190b1793d7aa15f8d0d49a13eba-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/83004190b1793d7aa15f8d0d49a13eba-Paper.pdf)]
    * Title: Self-Supervised Learning with Kernel Dependence Maximization
    * Year: `2021`
    * Authors: Yazhe Li, Roman Pogodin, Danica J. Sutherland, Arthur Gretton
    * Abstract: We approach self-supervised learning of image representations from a statistical dependence perspective, proposing Self-Supervised Learning with the Hilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes dependence between representations of transformations of an image and the image identity, while minimizing the kernelized variance of those representations. This framework yields a new understanding of InfoNCE, a variational lower bound on the mutual information (MI) between different transformations. While the MI itself is known to have pathologies which can result in learning meaningless representations, its bound is much better behaved: we show that it implicitly approximates SSL-HSIC (with a slightly different regularizer).Our approach also gives us insight into BYOL, a negative-free SSL method, since SSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to directly optimize statistical dependence in time linear in the batch size, without restrictive data assumptions or indirect mutual information estimators. Trained with or without a target network, SSL-HSIC matches the current state-of-the-art for standard linear evaluation on ImageNet, semi-supervised learning and transfer to other classification and vision tasks such as semantic segmentation, depth estimation and object recognition. Code is available at https://github.com/deepmind/ssl_hsic.
count=1
* Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/8485ae387a981d783f8764e508151cd9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/8485ae387a981d783f8764e508151cd9-Paper.pdf)]
    * Title: Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables
    * Year: `2021`
    * Authors: Jakob Runge
    * Abstract: The problem of selecting optimal backdoor adjustment sets to estimate causal effects in graphical models with hidden and conditioned variables is addressed. Previous work has defined optimality as achieving the smallest asymptotic estimation variance and derived an optimal set for the case without hidden variables. For the case with hidden variables there can be settings where no optimal set exists and currently only a sufficient graphical optimality criterion of limited applicability has been derived. In the present work optimality is characterized as maximizing a certain adjustment information which allows to derive a necessary and sufficient graphical criterion for the existence of an optimal adjustment set and a definition and algorithm to construct it. Further, the optimal set is valid if and only if a valid adjustment set exists and has higher (or equal) adjustment information than the Adjust-set proposed in Perkovi{\'c} et~al. [Journal of Machine Learning Research, 18: 1--62, 2018] for any graph. The results translate to minimal asymptotic estimation variance for a class of estimators whose asymptotic variance follows a certain information-theoretic relation. Numerical experiments indicate that the asymptotic results also hold for relatively small sample sizes and that the optimal adjustment set or minimized variants thereof often yield better variance also beyond that estimator class. Surprisingly, among the randomly created setups more than 90\% fulfill the optimality conditions indicating that also in many real-world scenarios graphical optimality may hold.
count=1
* Motif-based Graph Self-Supervised Learning for Molecular Property Prediction
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/85267d349a5e647ff0a9edcb5ffd1e02-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/85267d349a5e647ff0a9edcb5ffd1e02-Paper.pdf)]
    * Title: Motif-based Graph Self-Supervised Learning for Molecular Property Prediction
    * Year: `2021`
    * Authors: ZAIXI ZHANG, Qi Liu, Hao Wang, Chengqiang Lu, Chee-Kong Lee
    * Abstract: Predicting molecular properties with data-driven methods has drawn much attention in recent years. Particularly, Graph Neural Networks (GNNs) have demonstrated remarkable success in various molecular generation and prediction tasks. In cases where labeled data is scarce, GNNs can be pre-trained on unlabeled molecular data to first learn the general semantic and structural information before being finetuned for specific tasks. However, most existing self-supervised pretraining frameworks for GNNs only focus on node-level or graph-level tasks. These approaches cannot capture the rich information in subgraphs or graph motifs. For example, functional groups (frequently-occurred subgraphs in molecular graphs) often carry indicative information about the molecular properties. To bridge this gap, we propose Motif-based Graph Self-supervised Learning (MGSSL) by introducing a novel self-supervised motif generation framework for GNNs. First, for motif extraction from molecular graphs, we design a molecule fragmentation method that leverages a retrosynthesis-based algorithm BRICS and additional rules for controlling the size of motif vocabulary. Second, we design a general motif-based generative pretraining framework in which GNNs are asked to make topological and label predictions. This generative framework can be implemented in two different ways, i.e., breadth-first or depth-first. Finally, to take the multi-scale information in molecular graphs into consideration, we introduce a multi-level self-supervised pre-training. Extensive experiments on various downstream benchmark tasks show that our methods outperform all state-of-the-art baselines.
count=1
* Improving Transferability of Representations via Augmentation-Aware Self-Supervision
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/94130ea17023c4837f0dcdda95034b65-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/94130ea17023c4837f0dcdda95034b65-Paper.pdf)]
    * Title: Improving Transferability of Representations via Augmentation-Aware Self-Supervision
    * Year: `2021`
    * Authors: Hankook Lee, Kibok Lee, Kimin Lee, Honglak Lee, Jinwoo Shin
    * Abstract: Recent unsupervised representation learning methods have shown to be effective in a range of vision tasks by learning representations invariant to data augmentations such as random cropping and color jittering. However, such invariance could be harmful to downstream tasks if they rely on the characteristics of the data augmentations, e.g., location- or color-sensitive. This is not an issue just for unsupervised learning; we found that this occurs even in supervised learning because it also learns to predict the same label for all augmented samples of an instance. To avoid such failures and obtain more generalizable representations, we suggest to optimize an auxiliary self-supervised loss, coined AugSelf, that learns the difference of augmentation parameters (e.g., cropping positions, color adjustment intensities) between two randomly augmented samples. Our intuition is that AugSelf encourages to preserve augmentation-aware information in learned representations, which could be beneficial for their transferability. Furthermore, AugSelf can easily be incorporated into recent state-of-the-art representation learning methods with a negligible additional training cost. Extensive experiments demonstrate that our simple idea consistently improves the transferability of representations learned by supervised and unsupervised methods in various transfer learning scenarios. The code is available at https://github.com/hankook/AugSelf.
count=1
* Self-Supervised Learning Disentangled Group Representation as Feature
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/97416ac0f58056947e2eb5d5d253d4f2-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/97416ac0f58056947e2eb5d5d253d4f2-Paper.pdf)]
    * Title: Self-Supervised Learning Disentangled Group Representation as Feature
    * Year: `2021`
    * Authors: Tan Wang, Zhongqi Yue, Jianqiang Huang, Qianru Sun, Hanwang Zhang
    * Abstract: A good visual representation is an inference map from observations (images) to features (vectors) that faithfully reflects the hidden modularized generative factors (semantics). In this paper, we formulate the notion of "good" representation from a group-theoretic view using Higgins' definition of disentangled representation, and show that existing Self-Supervised Learning (SSL) only disentangles simple augmentation features such as rotation and colorization, thus unable to modularize the remaining semantics. To break the limitation, we propose an iterative SSL algorithm: Iterative Partition-based Invariant Risk Minimization (IP-IRM), which successfully grounds the abstract semantics and the group acting on them into concrete contrastive learning. At each iteration, IP-IRM first partitions the training samples into two subsets that correspond to an entangled group element. Then, it minimizes a subset-invariant contrastive loss, where the invariance guarantees to disentangle the group element. We prove that IP-IRM converges to a fully disentangled representation and show its effectiveness on various benchmarks. Codes are available at https://github.com/Wangt-CN/IP-IRM.
count=1
* Understanding the Under-Coverage Bias in Uncertainty Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/9854d7afce413aa13cd0a1d39d0bcec5-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/9854d7afce413aa13cd0a1d39d0bcec5-Paper.pdf)]
    * Title: Understanding the Under-Coverage Bias in Uncertainty Estimation
    * Year: `2021`
    * Authors: Yu Bai, Song Mei, Huan Wang, Caiming Xiong
    * Abstract: Estimating the data uncertainty in regression tasks is often done by learning a quantile function or a prediction interval of the true label conditioned on the input. It is frequently observed that quantile regression---a vanilla algorithm for learning quantiles with asymptotic guarantees---tends to *under-cover* than the desired coverage level in reality. While various fixes have been proposed, a more fundamental understanding of why this under-coverage bias happens in the first place remains elusive.In this paper, we present a rigorous theoretical study on the coverage of uncertainty estimation algorithms in learning quantiles. We prove that quantile regression suffers from an inherent under-coverage bias, in a vanilla setting where we learn a realizable linear quantile function and there is more data than parameters. More quantitatively, for $\alpha>0.5$ and small $d/n$, the $\alpha$-quantile learned by quantile regression roughly achieves coverage $\alpha - (\alpha-1/2)\cdot d/n$ regardless of the noise distribution, where $d$ is the input dimension and $n$ is the number of training data. Our theory reveals that this under-coverage bias stems from a certain high-dimensional parameter estimation error that is not implied by existing theories on quantile regression. Experiments on simulated and real data verify our theory and further illustrate the effect of various factors such as sample size and model capacity on the under-coverage bias in more practical setups.
count=1
* Learning Models for Actionable Recourse
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/9b82909c30456ac902e14526e63081d4-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/9b82909c30456ac902e14526e63081d4-Paper.pdf)]
    * Title: Learning Models for Actionable Recourse
    * Year: `2021`
    * Authors: Alexis Ross, Himabindu Lakkaraju, Osbert Bastani
    * Abstract: As machine learning models are increasingly deployed in high-stakes domains such as legal and financial decision-making, there has been growing interest in post-hoc methods for generating counterfactual explanations. Such explanations provide individuals adversely impacted by predicted outcomes (e.g., an applicant denied a loan) with recourse---i.e., a description of how they can change their features to obtain a positive outcome. We propose a novel algorithm that leverages adversarial training and PAC confidence sets to learn models that theoretically guarantee recourse to affected individuals with high probability without sacrificing accuracy. We demonstrate the efficacy of our approach via extensive experiments on real data.
count=1
* Revisiting Deep Learning Models for Tabular Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/9d86d83f925f2149e9edb0ac3b49229c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/9d86d83f925f2149e9edb0ac3b49229c-Paper.pdf)]
    * Title: Revisiting Deep Learning Models for Tabular Data
    * Year: `2021`
    * Authors: Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko
    * Abstract: The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks. Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols. We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution. The source code is available at https://github.com/yandex-research/rtdl.
count=1
* Compressive Visual Representations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/a29a5ba2cb7bdeabba22de8c83321b46-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/a29a5ba2cb7bdeabba22de8c83321b46-Paper.pdf)]
    * Title: Compressive Visual Representations
    * Year: `2021`
    * Authors: Kuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, Ian Fischer
    * Abstract: Learning effective visual representations that generalize well without human supervision is a fundamental problem in order to apply Machine Learning to a wide variety of tasks. Recently, two families of self-supervised methods, contrastive learning and latent bootstrapping, exemplified by SimCLR and BYOL respectively, have made significant progress. In this work, we hypothesize that adding explicit information compression to these algorithms yields better and more robust representations. We verify this by developing SimCLR and BYOL formulations compatible with the Conditional Entropy Bottleneck (CEB) objective, allowing us to both measure and control the amount of compression in the learned representation, and observe their impact on downstream tasks. Furthermore, we explore the relationship between Lipschitz continuity and compression, showing a tractable lower bound on the Lipschitz constant of the encoders we learn. As Lipschitz continuity is closely related to robustness, this provides a new explanation for why compressed models are more robust. Our experiments confirm that adding compression to SimCLR and BYOL significantly improves linear evaluation accuracies and model robustness across a wide range of domain shifts. In particular, the compressed version of BYOL achieves 76.0% Top-1 linear evaluation accuracy on ImageNet with ResNet-50, and 78.8% with ResNet-50 2x.
count=1
* Knowledge-Adaptation Priors
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/a4380923dd651c195b1631af7c829187-Paper.pdf)]
    * Title: Knowledge-Adaptation Priors
    * Year: `2021`
    * Authors: Mohammad Emtiyaz Khan, Siddharth Swaroop
    * Abstract: Humans and animals have a natural ability to quickly adapt to their surroundings, but machine-learning models, when subjected to changes, often require a complete retraining from scratch. We present Knowledge-adaptation priors (K-priors) to reduce the cost of retraining by enabling quick and accurate adaptation for a wide-variety of tasks and models. This is made possible by a combination of weight and function-space priors to reconstruct the gradients of the past, which recovers and generalizes many existing, but seemingly-unrelated, adaptation strategies. Training with simple first-order gradient methods can often recover the exact retrained model to an arbitrary accuracy by choosing a sufficiently large memory of the past data. Empirical results show that adaptation with K-priors achieves performance similar to full retraining, but only requires training on a handful of past examples.
count=1
* Algorithmic stability and generalization of an unsupervised feature selection algorithm
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/a546203962b88771bb06faf8d6ec065e-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/a546203962b88771bb06faf8d6ec065e-Paper.pdf)]
    * Title: Algorithmic stability and generalization of an unsupervised feature selection algorithm
    * Year: `2021`
    * Authors: xinxing wu, Qiang Cheng
    * Abstract: Feature selection, as a vital dimension reduction technique, reduces data dimension by identifying an essential subset of input features, which can facilitate interpretable insights into learning and inference processes. Algorithmic stability is a key characteristic of an algorithm regarding its sensitivity to perturbations of input samples. In this paper, we propose an innovative unsupervised feature selection algorithm attaining this stability with provable guarantees. The architecture of our algorithm consists of a feature scorer and a feature selector. The scorer trains a neural network (NN) to globally score all the features, and the selector adopts a dependent sub-NN to locally evaluate the representation abilities for selecting features. Further, we present algorithmic stability analysis and show that our algorithm has a performance guarantee via a generalization error bound. Extensive experimental results on real-world datasets demonstrate superior generalization performance of our proposed algorithm to strong baseline methods. Also, the properties revealed by our theoretical analysis and the stability of our algorithm-selected features are empirically confirmed.
count=1
* Node Dependent Local Smoothing for Scalable Graph Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/a9eb812238f753132652ae09963a05e9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/a9eb812238f753132652ae09963a05e9-Paper.pdf)]
    * Title: Node Dependent Local Smoothing for Scalable Graph Learning
    * Year: `2021`
    * Authors: Wentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, Bin CUI
    * Abstract: Recent works reveal that feature or label smoothing lies at the core of Graph Neural Networks (GNNs). Concretely, they show feature smoothing combined with simple linear regression achieves comparable performance with the carefully designed GNNs, and a simple MLP model with label smoothing of its prediction can outperform the vanilla GCN. Though an interesting finding, smoothing has not been well understood, especially regarding how to control the extent of smoothness. Intuitively, too small or too large smoothing iterations may cause under-smoothing or over-smoothing and can lead to sub-optimal performance. Moreover, the extent of smoothness is node-specific, depending on its degree and local structure. To this end, we propose a novel algorithm called node-dependent local smoothing (NDLS), which aims to control the smoothness of every node by setting a node-specific smoothing iteration. Specifically, NDLS computes influence scores based on the adjacency matrix and selects the iteration number by setting a threshold on the scores. Once selected, the iteration number can be applied to both feature smoothing and label smoothing. Experimental results demonstrate that NDLS enjoys high accuracy -- state-of-the-art performance on node classifications tasks, flexibility -- can be incorporated with any models, scalability and efficiency -- can support large scale graphs with fast training.
count=1
* TOHAN: A One-step Approach towards Few-shot Hypothesis Adaptation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/af5d5ef24881f3c3049a7b9bfe74d58b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/af5d5ef24881f3c3049a7b9bfe74d58b-Paper.pdf)]
    * Title: TOHAN: A One-step Approach towards Few-shot Hypothesis Adaptation
    * Year: `2021`
    * Authors: Haoang Chi, Feng Liu, Wenjing Yang, Long Lan, Tongliang Liu, Bo Han, William Cheung, James Kwok
    * Abstract: In few-shot domain adaptation (FDA), classifiers for the target domain are trained with \emph{accessible} labeled data in the source domain (SD) and few labeled data in the target domain (TD). However, data usually contain private information in the current era, e.g., data distributed on personal phones. Thus, the private data will be leaked if we directly access data in SD to train a target-domain classifier (required by FDA methods). In this paper, to prevent privacy leakage in SD, we consider a very challenging problem setting, where the classifier for the TD has to be trained using few labeled target data and a well-trained SD classifier, named few-shot hypothesis adaptation (FHA). In FHA, we cannot access data in SD, as a result, the private information in SD will be protected well. To this end, we propose a target-oriented hypothesis adaptation network (TOHAN) to solve the FHA problem, where we generate highly-compatible unlabeled data (i.e., an intermediate domain) to help train a target-domain classifier. TOHAN maintains two deep networks simultaneously, in which one focuses on learning an intermediate domain and the other takes care of the intermediate-to-target distributional adaptation and the target-risk minimization. Experimental results show that TOHAN outperforms competitive baselines significantly.
count=1
* Closing the loop in medical decision support by understanding clinical decision-making: A case study on organ transplantation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/c344336196d5ec19bd54fd14befdde87-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/c344336196d5ec19bd54fd14befdde87-Paper.pdf)]
    * Title: Closing the loop in medical decision support by understanding clinical decision-making: A case study on organ transplantation
    * Year: `2021`
    * Authors: Yuchao Qin, Fergus Imrie, Alihan Hüyük, Daniel Jarrett, alexander gimson, Mihaela van der Schaar
    * Abstract: Significant effort has been placed on developing decision support tools to improve patient care. However, drivers of real-world clinical decisions in complex medical scenarios are not yet well-understood, resulting in substantial gaps between these tools and practical applications. In light of this, we highlight that more attention on understanding clinical decision-making is required both to elucidate current clinical practices and to enable effective human-machine interactions. This is imperative in high-stakes scenarios with scarce available resources. Using organ transplantation as a case study, we formalize the desiderata of methods for understanding clinical decision-making. We show that most existing machine learning methods are insufficient to meet these requirements and propose iTransplant, a novel data-driven framework to learn the factors affecting decisions on organ offers in an instance-wise fashion directly from clinical data, as a possible solution. Through experiments on real-world liver transplantation data from OPTN, we demonstrate the use of iTransplant to: (1) discover which criteria are most important to clinicians for organ offer acceptance; (2) identify patient-specific organ preferences of clinicians allowing automatic patient stratification; and (3) explore variations in transplantation practices between different transplant centers. Finally, we emphasize that the insights gained by iTransplant can be used to inform the development of future decision support tools.
count=1
* Learning Semantic Representations to Verify Hardware Designs
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/c5aa65949d20f6b20e1a922c13d974e7-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/c5aa65949d20f6b20e1a922c13d974e7-Paper.pdf)]
    * Title: Learning Semantic Representations to Verify Hardware Designs
    * Year: `2021`
    * Authors: Shobha Vasudevan, Wenjie (Joe) Jiang, David Bieber, Rishabh Singh, hamid shojaei, C. Richard Ho, Charles Sutton
    * Abstract: Verification is a serious bottleneck in the industrial hardware design cycle, routinely requiring person-years of effort. Practical verification relies on a "best effort" process that simulates the design on test inputs. This suggests a new research question: Can this simulation data be exploited to learn a continuous representation of a hardware design that allows us to predict its functionality? As a first approach to this new problem, we introduce Design2Vec, a deep architecture that learns semantic abstractions of hardware designs. The key idea is to work at a higher level of abstraction than the gate or the bit level, namely the Register Transfer Level (RTL), which is somewhat analogous to software source code, and can be represented by a graph that incorporates control and data flow. This allows us to learn representations of RTL syntax and semantics using a graph neural network. We apply these representations to several tasks within verification, including predicting what cover points of the design will be exercised by a test, and generating new tests that will exercise desired cover points. We evaluate Design2Vec on three real-world hardware designs, including an industrial chip used in commercial data centers. Our results demonstrate that Design2Vec dramatically outperforms baseline approaches that do not incorporate the RTL semantics, scales to industrial designs, and can generate tests that exercise design points that are currently hard to cover with manually written tests by design verification experts.
count=1
* MIRACLE: Causally-Aware Imputation via Learning Missing Data Mechanisms
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/c80bcf42c220b8f5c41f85344242f1b0-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/c80bcf42c220b8f5c41f85344242f1b0-Paper.pdf)]
    * Title: MIRACLE: Causally-Aware Imputation via Learning Missing Data Mechanisms
    * Year: `2021`
    * Authors: Trent Kyono, Yao Zhang, Alexis Bellot, Mihaela van der Schaar
    * Abstract: Missing data is an important problem in machine learning practice. Starting from the premise that imputation methods should preserve the causal structure of the data, we develop a regularization scheme that encourages any baseline imputation method to be causally consistent with the underlying data generating mechanism. Our proposal is a causally-aware imputation algorithm (MIRACLE). MIRACLE iteratively refines the imputation of a baseline by simultaneously modeling the missingness generating mechanism, encouraging imputation to be consistent with the causal structure of the data. We conduct extensive experiments on synthetic and a variety of publicly available datasets to show that MIRACLE is able to consistently improve imputation over a variety of benchmark methods across all three missingness scenarios: at random, completely at random, and not at random.
count=1
* Causal Bandits with Unknown Graph Structure
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/d010396ca8abf6ead8cacc2c2f2f26c7-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf)]
    * Title: Causal Bandits with Unknown Graph Structure
    * Year: `2021`
    * Authors: Yangyi Lu, Amirhossein Meisami, Ambuj Tewari
    * Abstract: In causal bandit problems the action set consists of interventions on variables of a causal graph. Several researchers have recently studied such bandit problems and pointed out their practical applications. However, all existing works rely on a restrictive and impractical assumption that the learner is given full knowledge of the causal graph structure upfront. In this paper, we develop novel causal bandit algorithms without knowing the causal graph. Our algorithms work well for causal trees, causal forests and a general class of causal graphs. The regret guarantees of our algorithms greatly improve upon those of standard multi-armed bandit (MAB) algorithms under mild conditions. Lastly, we prove our mild conditions are necessary: without them one cannot do better than standard MAB algorithms.
count=1
* Adversarial Training Helps Transfer Learning via Better Representations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/d3aeec875c479e55d1cdeea161842ec6-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/d3aeec875c479e55d1cdeea161842ec6-Paper.pdf)]
    * Title: Adversarial Training Helps Transfer Learning via Better Representations
    * Year: `2021`
    * Authors: Zhun Deng, Linjun Zhang, Kailas Vodrahalli, Kenji Kawaguchi, James Y. Zou
    * Abstract: Transfer learning aims to leverage models pre-trained on source data to efficiently adapt to target setting, where only limited data are available for model fine-tuning. Recent works empirically demonstrate that adversarial training in the source data can improve the ability of models to transfer to new domains. However, why this happens is not known. In this paper, we provide a theoretical model to rigorously analyze how adversarial training helps transfer learning. We show that adversarial training in the source data generates provably better representations, so fine-tuning on top of this representation leads to a more accurate predictor of the target data. We further demonstrate both theoretically and empirically that semi-supervised learning in the source data can also improve transfer learning by similarly improving the representation. Moreover, performing adversarial training on top of semi-supervised learning can further improve transferability, suggesting that the two approaches have complementary benefits on representations. We support our theories with experiments on popular data sets and deep learning architectures.
count=1
* Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf)]
    * Title: Batch Multi-Fidelity Bayesian Optimization with  Deep Auto-Regressive Networks
    * Year: `2021`
    * Authors: Shibo Li, Robert Kirby, Shandian Zhe
    * Abstract: Bayesian optimization (BO) is a powerful approach for optimizing black-box, expensive-to-evaluate functions. To enable a flexible trade-off between the cost and accuracy, many applications allow the function to be evaluated at different fidelities. In order to reduce the optimization cost while maximizing the benefit-cost ratio, in this paper we propose Batch Multi-fidelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of Bayesian neural networks to construct a fully auto-regressive model, which is expressive enough to capture strong yet complex relationships across all the fidelities, so as to improve the surrogate learning and optimization performance. Furthermore, to enhance the quality and diversity of queries, we develop a simple yet efficient batch querying method, without any combinatorial search over the fidelities. We propose a batch acquisition function based on Max-value Entropy Search (MES) principle, which penalizes highly correlated queries and encourages diversity. We use posterior samples and moment matching to fulfill efficient computation of the acquisition function, and conduct alternating optimization over every fidelity-input pair, which guarantees an improvement at each step. We demonstrate the advantage of our approach on four real-world hyperparameter optimization applications.
count=1
* Learning with Holographic Reduced Representations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/d71dd235287466052f1630f31bde7932-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/d71dd235287466052f1630f31bde7932-Paper.pdf)]
    * Title: Learning with Holographic Reduced Representations
    * Year: `2021`
    * Authors: Ashwinkumar Ganesan, Hang Gao, Sunil Gandhi, Edward Raff, Tim Oates, James Holt, Mark McLean
    * Abstract: Holographic Reduced Representations (HRR) are a method for performing symbolic AI on top of real-valued vectors by associating each vector with an abstract concept, and providing mathematical operations to manipulate vectors as if they were classic symbolic objects. This method has seen little use outside of older symbolic AI work and cognitive science. Our goal is to revisit this approach to understand if it is viable for enabling a hybrid neural-symbolic approach to learning as a differential component of a deep learning architecture. HRRs today are not effective in a differential solution due to numerical instability, a problem we solve by introducing a projection step that forces the vectors to exist in a well behaved point in space. In doing so we improve the concept retrieval efficacy of HRRs by over $100\times$. Using multi-label classification we demonstrate how to leverage the symbolic HRR properties to develop a output layer and loss function that is able to learn effectively, and allows us to investigate some of the pros and cons of an HRR neuro-symbolic learning approach.
count=1
* Why Do Better Loss Functions Lead to Less Transferable Features?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/f0bf4a2da952528910047c31b6c2e951-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/f0bf4a2da952528910047c31b6c2e951-Paper.pdf)]
    * Title: Why Do Better Loss Functions Lead to Less Transferable Features?
    * Year: `2021`
    * Authors: Simon Kornblith, Ting Chen, Honglak Lee, Mohammad Norouzi
    * Abstract: Previous work has proposed many new loss functions and regularizers that improve test accuracy on image classification tasks. However, it is not clear whether these loss functions learn better representations for downstream tasks. This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. We show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment to measure similarity between hidden representations of networks, we find that differences among loss functions are apparent only in the last few layers of the network. We delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks. Our results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks.
count=1
* Learning Robust Hierarchical Patterns of Human Brain across Many fMRI Studies
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/f33ba15effa5c10e873bf3842afb46a6-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/f33ba15effa5c10e873bf3842afb46a6-Paper.pdf)]
    * Title: Learning Robust Hierarchical Patterns of Human Brain across Many fMRI Studies
    * Year: `2021`
    * Authors: Dushyant Sahoo, Christos Davatzikos
    * Abstract: Multi-site fMRI studies face the challenge that the pooling introduces systematic non-biological site-specific variance due to hardware, software, and environment. In this paper, we propose to reduce site-specific variance in the estimation of hierarchical Sparsity Connectivity Patterns (hSCPs) in fMRI data via a simple yet effective matrix factorization while preserving biologically relevant variations. Our method leverages unsupervised adversarial learning to improve the reproducibility of the components. Experiments on simulated datasets display that the proposed method can estimate components with higher accuracy and reproducibility, while preserving age-related variation on a multi-center clinical data set.
count=1
* Language models enable zero-shot prediction of the effects of mutations on protein function
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/f51338d736f95dd42427296047067694-Paper.pdf)]
    * Title: Language models enable zero-shot prediction of the effects of mutations on protein function
    * Year: `2021`
    * Authors: Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, Alex Rives
    * Abstract: Modeling the effect of sequence variation on function is a fundamental problem for understanding and designing proteins. Since evolution encodes information about function into patterns in protein sequences, unsupervised models of variant effects can be learned from sequence data. The approach to date has been to fit a model to a family of related sequences. The conventional setting is limited, since a new model must be trained for each prediction task. We show that using only zero-shot inference, without any supervision from experimental data or additional training, protein language models capture the functional effects of sequence variation, performing at state-of-the-art.
count=1
* Are My Deep Learning Systems Fair? An Empirical Study of Fixed-Seed Training
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/hash/fdda6e957f1e5ee2f3b311fe4f145ae1-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2021/file/fdda6e957f1e5ee2f3b311fe4f145ae1-Paper.pdf)]
    * Title: Are My Deep Learning Systems Fair? An Empirical Study of Fixed-Seed Training
    * Year: `2021`
    * Authors: Shangshu Qian, Viet Hung Pham, Thibaud Lutellier, Zeou Hu, Jungwon Kim, Lin Tan, Yaoliang Yu, Jiahao Chen, Sameena Shah
    * Abstract: Deep learning (DL) systems have been gaining popularity in critical tasks such as credit evaluation and crime prediction. Such systems demand fairness. Recent work shows that DL software implementations introduce variance: identical DL training runs (i.e., identical network, data, configuration, software, and hardware) with a fixed seed produce different models. Such variance could make DL models and networks violate fairness compliance laws, resulting in negative social impact. In this paper, we conduct the first empirical study to quantify the impact of software implementation on the fairness and its variance of DL systems. Our study of 22 mitigation techniques and five baselines reveals up to 12.6% fairness variance across identical training runs with identical seeds. In addition, most debiasing algorithms have a negative impact on the model such as reducing model accuracy, increasing fairness variance, or increasing accuracy variance. Our literature survey shows that while fairness is gaining popularity in artificial intelligence (AI) related conferences, only 34.4% of the papers use multiple identical training runs to evaluate their approach, raising concerns about their results’ validity. We call for better fairness evaluation and testing protocols to improve fairness and fairness variance of DL systems as well as DL research validity and reproducibility at large.
count=1
* Off-Policy Evaluation and Learning for External Validity under a Covariate Shift
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/0084ae4bc24c0795d1e6a4f58444d39b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/0084ae4bc24c0795d1e6a4f58444d39b-Paper.pdf)]
    * Title: Off-Policy Evaluation and Learning for External Validity under a Covariate Shift
    * Year: `2020`
    * Authors: Masatoshi Uehara, Masahiro Kato, Shota Yasui
    * Abstract: We consider the evaluation and training of a new policy for the evaluation data by using the historical data obtained from a different policy. The goal of off-policy evaluation (OPE) is to estimate the expected reward of a new policy over the evaluation data, and that of off-policy learning (OPL) is to find a new policy that maximizes the expected reward over the evaluation data. Although the standard OPE and OPL assume the same distribution of covariate between the historical and evaluation data, there often exists a problem of a covariate shift,i.e., the distribution of the covariate of the historical data is different from that of the evaluation data. In this paper, we derive the efficiency bound of OPE under a covariate shift. Then, we propose doubly robust and efficient estimators for OPE and OPL under a covariate shift by using an estimator of the density ratio between the distributions of the historical and evaluation data. We also discuss other possible estimators and compare their theoretical properties. Finally, we confirm the effectiveness of the proposed estimators through experiments.
count=1
* Statistical Guarantees of Distributed Nearest Neighbor Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/022e0ee5162c13d9a7bb3bd00fb032ce-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/022e0ee5162c13d9a7bb3bd00fb032ce-Paper.pdf)]
    * Title: Statistical Guarantees of Distributed Nearest Neighbor Classification
    * Year: `2020`
    * Authors: Jiexin Duan, Xingye Qiao, Guang Cheng
    * Abstract: Nearest neighbor is a popular nonparametric method for classification and regression with many appealing properties. In the big data era, the sheer volume and spatial/temporal disparity of big data may prohibit centrally processing and storing the data. This has imposed considerable hurdle for nearest neighbor predictions since the entire training data must be memorized. One effective way to overcome this issue is the distributed learning framework. Through majority voting, the distributed nearest neighbor classifier achieves the same rate of convergence as its oracle version in terms of the regret, up to a multiplicative constant that depends solely on the data dimension. The multiplicative difference can be eliminated by replacing majority voting with the weighted voting scheme. In addition, we provide sharp theoretical upper bounds of the number of subsamples in order for the distributed nearest neighbor classifier to reach the optimal convergence rate. It is interesting to note that the weighted voting scheme allows a larger number of subsamples than the majority voting one. Our findings are supported by numerical studies.
count=1
* Minimax Classification with 0-1 Loss and Performance Guarantees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/02f657d55eaf1c4840ce8d66fcdaf90c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/02f657d55eaf1c4840ce8d66fcdaf90c-Paper.pdf)]
    * Title: Minimax Classification with 0-1 Loss and Performance Guarantees
    * Year: `2020`
    * Authors: Santiago Mazuelas, Andrea Zanoni, Aritz Pérez
    * Abstract: Supervised classification techniques use training samples to find classification rules with small expected 0-1 loss. Conventional methods achieve efficient learning and out-of-sample generalization by minimizing surrogate losses over specific families of rules. This paper presents minimax risk classifiers (MRCs) that do not rely on a choice of surrogate loss and family of rules. MRCs achieve efficient learning and out-of-sample generalization by minimizing worst-case expected 0-1 loss w.r.t. uncertainty sets that are defined by linear constraints and include the true underlying distribution. In addition, MRCs' learning stage provides performance guarantees as lower and upper tight bounds for expected 0-1 loss. We also present MRCs' finite-sample generalization bounds in terms of training size and smallest minimax risk, and show their competitive classification performance w.r.t. state-of-the-art techniques using benchmark datasets.
count=1
* Learning to search efficiently for causally near-optimal treatments
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/0e900ad84f63618452210ab8baae0218-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/0e900ad84f63618452210ab8baae0218-Paper.pdf)]
    * Title: Learning to search efficiently for causally near-optimal treatments
    * Year: `2020`
    * Authors: Samuel Håkansson, Viktor Lindblom, Omer Gottesman, Fredrik D. Johansson
    * Abstract: Finding an effective medical treatment often requires a search by trial and error. Making this search more efficient by minimizing the number of unnecessary trials could lower both costs and patient suffering. We formalize this problem as learning a policy for finding a near-optimal treatment in a minimum number of trials using a causal inference framework. We give a model-based dynamic programming algorithm which learns from observational data while being robust to unmeasured confounding. To reduce time complexity, we suggest a greedy algorithm which bounds the near-optimality constraint. The methods are evaluated on synthetic and real-world healthcare data and compared to model-free reinforcement learning. We find that our methods compare favorably to the model-free baseline while offering a more transparent trade-off between search time and treatment efficacy.
count=1
* On the Similarity between the Laplace and Neural Tangent Kernels
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/1006ff12c465532f8c574aeaa4461b16-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/1006ff12c465532f8c574aeaa4461b16-Paper.pdf)]
    * Title: On the Similarity between the Laplace and Neural Tangent Kernels
    * Year: `2020`
    * Authors: Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, Basri Ronen
    * Abstract: Recent theoretical work has shown that massively overparameterized neural networks are equivalent to kernel regressors that use Neural Tangent Kernels (NTKs). Experiments show that these kernel methods perform similarly to real neural networks. Here we show that NTK for fully connected networks with ReLU activation is closely related to the standard Laplace kernel. We show theoretically that for normalized data on the hypersphere both kernels have the same eigenfunctions and their eigenvalues decay polynomially at the same rate, implying that their Reproducing Kernel Hilbert Spaces (RKHS) include the same sets of functions. This means that both kernels give rise to classes of functions with the same smoothness properties. The two kernels differ for data off the hypersphere, but experiments indicate that when data is properly normalized these differences are not significant. Finally, we provide experiments on real data comparing NTK and the Laplace kernel, along with a larger class of $\gamma$-exponential kernels. We show that these perform almost identically. Our results suggest that much insight about neural networks can be obtained from analysis of the well-known Laplace kernel, which has a simple closed form.
count=1
* Re-Examining Linear Embeddings for High-Dimensional Bayesian Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/10fb6cfa4c990d2bad5ddef4f70e8ba2-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/10fb6cfa4c990d2bad5ddef4f70e8ba2-Paper.pdf)]
    * Title: Re-Examining Linear Embeddings for High-Dimensional Bayesian Optimization
    * Year: `2020`
    * Authors: Ben Letham, Roberto Calandra, Akshara Rai, Eytan Bakshy
    * Abstract: Bayesian optimization (BO) is a popular approach to optimize expensive-to-evaluate black-box functions. A significant challenge in BO is to scale to high-dimensional parameter spaces while retaining sample efficiency. A solution considered in existing literature is to embed the high-dimensional space in a lower-dimensional manifold, often via a random linear embedding. In this paper, we identify several crucial issues and misconceptions about the use of linear embeddings for BO. We study the properties of linear embeddings from the literature and show that some of the design choices in current approaches adversely impact their performance. We show empirically that properly addressing these issues significantly improves the efficacy of linear embeddings for BO on a range of problems, including learning a gait policy for robot locomotion.
count=1
* A Scalable MIP-based Method for Learning Optimal Multivariate Decision Trees
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/1373b284bc381890049e92d324f56de0-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/1373b284bc381890049e92d324f56de0-Paper.pdf)]
    * Title: A Scalable MIP-based Method for Learning Optimal Multivariate Decision Trees
    * Year: `2020`
    * Authors: Haoran Zhu, Pavankumar Murali, Dzung Phan, Lam Nguyen, Jayant Kalagnanam
    * Abstract: Several recent publications report advances in training optimal decision trees (ODTs) using mixed-integer programs (MIPs), due to algorithmic advances in integer programming and a growing interest in addressing the inherent suboptimality of heuristic approaches such as CART. In this paper, we propose a novel MIP formulation, based on 1-norm support vector machine model, to train a binary oblique ODT for classification problems. We further present techniques, such as cutting planes, to tighten its linear relaxation, to improve run times to reach optimality. Using 36 datasets from the University of California Irvine Machine Learning Repository, we demonstrate that our training approach outperforms its counterparts from literature in terms of out-of-sample performance (around 10% improvement in mean out-of-sample testing accuracy). Towards our goal of developing a scalable framework to train multivariate ODT on large datasets, we propose a new linear programming based data selection method to choose a subset of the data, and use it to train a decision tree through our proposed MIP model. We conclude this paper with extensive numerical testing results, that showcase the generalization performance of our new MIP formulation, and the improvement in mean out-of-sample accuracy on large datasets.
count=1
* What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/1e14bfe2714193e7af5abc64ecbd6b46-Paper.pdf)]
    * Title: What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation
    * Year: `2020`
    * Authors: Vitaly Feldman, Chiyuan Zhang
    * Abstract: Deep learning algorithms are well-known to have a propensity for fitting the training data very well and often fit even outliers and mislabeled data points. Such fitting requires memorization of training data labels, a phenomenon that has attracted significant research interest but has not been given a compelling explanation so far. A recent work of Feldman (2019) proposes a theoretical explanation for this phenomenon based on a combination of two insights. First, natural image and data distributions are (informally) known to be long-tailed, that is have a significant fraction of rare and atypical examples. Second, in a simple theoretical model such memorization is necessary for achieving close-to-optimal generalization error when the data distribution is long-tailed. However, no direct empirical evidence for this explanation or even an approach for obtaining such evidence were given. In this work we design experiments to test the key ideas in this theory. The experiments require estimation of the influence of each training example on the accuracy at each test example as well as memorization values of training examples. Estimating these quantities directly is computationally prohibitive but we show that closely-related subsampled influence and memorization values can be estimated much more efficiently. Our experiments demonstrate the significant benefits of memorization for generalization on several standard benchmarks. They also provide quantitative and visually compelling evidence for the theory put forth in Feldman (2019).
count=1
* Do Adversarially Robust ImageNet Models Transfer Better?
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/24357dd085d2c4b1a88a7e0692e60294-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/24357dd085d2c4b1a88a7e0692e60294-Paper.pdf)]
    * Title: Do Adversarially Robust ImageNet Models Transfer Better?
    * Year: `2020`
    * Authors: Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, Aleksander Madry
    * Abstract: Transfer learning is a widely-used paradigm in deep learning, where models pre-trained on standard datasets can be efficiently adapted to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Code and models is available in the supplementary material.
count=1
* Distribution-free binary classification: prediction sets, confidence intervals and calibration
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/26d88423fc6da243ffddf161ca712757-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/26d88423fc6da243ffddf161ca712757-Paper.pdf)]
    * Title: Distribution-free binary classification: prediction sets, confidence intervals and calibration
    * Year: `2020`
    * Authors: Chirag Gupta, Aleksandr Podkopaev, Aaditya Ramdas
    * Abstract: We study three notions of uncertainty quantification---calibration, confidence intervals and prediction sets---for binary classification in the distribution-free setting, that is without making any distributional assumptions on the data. With a focus towards calibration, we establish a 'tripod' of theorems that connect these three notions for score-based classifiers. A direct implication is that distribution-free calibration is only possible, even asymptotically, using a scoring function whose level sets partition the feature space into at most countably many sets. Parametric calibration schemes such as variants of Platt scaling do not satisfy this requirement, while nonparametric schemes based on binning do. To close the loop, we derive distribution-free confidence intervals for binned probabilities for both fixed-width and uniform-mass binning. As a consequence of our 'tripod' theorems, these confidence intervals for binned probabilities lead to distribution-free calibration. We also derive extensions to settings with streaming data and covariate shift.
count=1
* Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/32e54441e6382a7fbacbbbaf3c450059-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf)]
    * Title: Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models
    * Year: `2020`
    * Authors: Tom Heskes, Evi Sijben, Ioan Gabriel Bucur, Tom Claassen
    * Abstract: Shapley values underlie one of the most popular model-agnostic methods within explainable artificial intelligence. These values are designed to attribute the difference between a model's prediction and an average baseline to the different features used as input to the model. Being based on solid game-theoretic principles, Shapley values uniquely satisfy several desirable properties, which is why they are increasingly used to explain the predictions of possibly complex and highly non-linear machine learning models. Shapley values are well calibrated to a user’s intuition when features are independent, but may lead to undesirable, counterintuitive explanations when the independence assumption is violated. In this paper, we propose a novel framework for computing Shapley values that generalizes recent work that aims to circumvent the independence assumption. By employing Pearl's do-calculus, we show how these `causal' Shapley values can be derived for general causal graphs without sacrificing any of their desirable properties. Moreover, causal Shapley values enable us to separate the contribution of direct and indirect effects. We provide a practical implementation for computing causal Shapley values based on causal chain graphs when only partial information is available and illustrate their utility on a real-world example.
count=1
* AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/33a854e247155d590883b93bca53848a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/33a854e247155d590883b93bca53848a-Paper.pdf)]
    * Title: AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity
    * Year: `2020`
    * Authors: Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, Max Tegmark
    * Abstract: We present an improved method for symbolic regression that seeks to fit data to formulas that are Pareto-optimal, in the sense of having the best accuracy for a given complexity. It improves on the previous state-of-the-art by typically being orders of magnitude more robust toward noise and bad data, and also by discovering many formulas that stumped previous methods. We develop a method for discovering generalized symmetries (arbitrary modularity in the computational graph of a formula) from gradient properties of a neural network fit. We use normalizing flows to generalize our symbolic regression method to probability distributions from which we only have samples, and employ statistical hypothesis testing to accelerate robust brute-force search.
count=1
* Learning under Model Misspecification: Applications to Variational and Ensemble methods
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/3ac48664b7886cf4e4ab4aba7e6b6bc9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/3ac48664b7886cf4e4ab4aba7e6b6bc9-Paper.pdf)]
    * Title: Learning under Model Misspecification: Applications to Variational and Ensemble methods
    * Year: `2020`
    * Authors: Andres Masegosa
    * Abstract: Virtually any model we use in machine learning to make predictions does not perfectly represent reality. So, most of the learning happens under model misspecification. In this work, we present a novel analysis of the generalization performance of Bayesian model averaging under model misspecification and i.i.d. data using a new family of second-order PAC-Bayes bounds. This analysis shows, in simple and intuitive terms, that Bayesian model averaging provides suboptimal generalization performance when the model is misspecified. In consequence, we provide strong theoretical arguments showing that Bayesian methods are not optimal for learning predictive models, unless the model class is perfectly specified. Using novel second-order PAC-Bayes bounds, we derive a new family of Bayesian-like algorithms, which can be implemented as variational and ensemble methods. The output of these algorithms is a new posterior distribution, different from the Bayesian posterior, which induces a posterior predictive distribution with better generalization performance. Experiments with Bayesian neural networks illustrate these findings.
count=1
* Neuron Shapley: Discovering the Responsible Neurons
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/41c542dfe6e4fc3deb251d64cf6ed2e4-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/41c542dfe6e4fc3deb251d64cf6ed2e4-Paper.pdf)]
    * Title: Neuron Shapley: Discovering the Responsible Neurons
    * Year: `2020`
    * Authors: Amirata Ghorbani, James Y. Zou
    * Abstract: We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Computing exact Shapley values is computationally infeasible and therefore sampling-based approximations are used in practice. We introduce a new multi-armed bandit algorithm that is able to efficiently detect neurons with the largest Shapley value orders of magnitude faster than existing Shapley value approximation methods.
count=1
* Meta-learning from Tasks with Heterogeneous Attribute Spaces
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/438124b4c06f3a5caffab2c07863b617-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/438124b4c06f3a5caffab2c07863b617-Paper.pdf)]
    * Title: Meta-learning from Tasks with Heterogeneous Attribute Spaces
    * Year: `2020`
    * Authors: Tomoharu Iwata, Atsutoshi Kumagai
    * Abstract: We propose a heterogeneous meta-learning method that trains a model on tasks with various attribute spaces, such that it can solve unseen tasks whose attribute spaces are different from the training tasks given a few labeled instances. Although many meta-learning methods have been proposed, they assume that all training and target tasks share the same attribute space, and they are inapplicable when attribute sizes are different across tasks. Our model infers latent representations of each attribute and each response from a few labeled instances using an inference network. Then, responses of unlabeled instances are predicted with the inferred representations using a prediction network. The attribute and response representations enable us to make predictions based on the task-specific properties of attributes and responses even when attribute and response sizes are different across tasks. In our experiments with synthetic datasets and 59 datasets in OpenML, we demonstrate that our proposed method can predict the responses given a few labeled instances in new tasks after being trained with tasks with heterogeneous attribute spaces.
count=1
* Fair regression with Wasserstein barycenters
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/51cdbd2611e844ece5d80878eb770436-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/51cdbd2611e844ece5d80878eb770436-Paper.pdf)]
    * Title: Fair regression with Wasserstein barycenters
    * Year: `2020`
    * Authors: Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, Massimiliano Pontil
    * Abstract: We study the problem of learning a real-valued function that satisfies the Demographic Parity constraint. It demands the distribution of the predicted output to be independent of the sensitive attribute. We consider the case that the sensitive attribute is available for prediction. We establish a connection between fair regression and optimal transport theory, based on which we derive a close form expression for the optimal fair predictor. Specifically, we show that the distribution of this optimum is the Wasserstein barycenter of the distributions induced by the standard regression function on the sensitive groups. This result offers an intuitive interpretation of the optimal fair prediction and suggests a simple post-processing algorithm to achieve fairness. We establish risk and distribution-free fairness guarantees for this procedure. Numerical experiments indicate that our method is very effective in learning fair models, with a relative increase in error rate that is inferior to the relative gain in fairness.
count=1
* Learning from Aggregate Observations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/5b0fa0e4c041548bb6289e15d865a696-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/5b0fa0e4c041548bb6289e15d865a696-Paper.pdf)]
    * Title: Learning from Aggregate Observations
    * Year: `2020`
    * Authors: Yivan Zhang, Nontawat Charoenphakdee, Zhenguo Wu, Masashi Sugiyama
    * Abstract: We study the problem of learning from aggregate observations where supervision signals are given to sets of instances instead of individual instances, while the goal is still to predict labels of unseen individuals. A well-known example is multiple instance learning (MIL). In this paper, we extend MIL beyond binary classification to other problems such as multiclass classification and regression. We present a general probabilistic framework that accommodates a variety of aggregate observations, e.g., pairwise similarity/triplet comparison for classification and mean/difference/rank observation for regression. Simple maximum likelihood solutions can be applied to various differentiable models such as deep neural networks and gradient boosting machines. Moreover, we develop the concept of consistency up to an equivalence relation to characterize our estimator and show that it has nice convergence properties under mild assumptions. Experiments on three problem settings --- classification via triplet comparison and regression via mean/rank observation indicate the effectiveness of the proposed method.
count=1
* Novelty Search in Representational Space for Sample Efficient Exploration
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/5ca41a86596a5ed567d15af0be224952-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/5ca41a86596a5ed567d15af0be224952-Paper.pdf)]
    * Title: Novelty Search in Representational Space for Sample Efficient Exploration
    * Year: `2020`
    * Authors: Ruo Yu Tao, Vincent Francois-Lavet, Joelle Pineau
    * Abstract: We present a new approach for efficient exploration which leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. Our approach uses intrinsic rewards that are based on the distance of nearest neighbors in the low dimensional representational space to gauge novelty. We then leverage these intrinsic rewards for sample-efficient exploration with planning routines in representational space for hard exploration tasks with sparse rewards. One key element of our approach is the use of information theoretic principles to shape our representations in a way so that our novelty reward goes beyond pixel similarity. We test our approach on a number of maze tasks, as well as a control problem and show that our exploration approach is more sample-efficient compared to strong baselines.
count=1
* General Control Functions for Causal Effect Estimation from IVs
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/604f2c31e67034642b288d76a8df11d5-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/604f2c31e67034642b288d76a8df11d5-Paper.pdf)]
    * Title: General Control Functions for Causal Effect Estimation from IVs
    * Year: `2020`
    * Authors: Aahlad Puli, Rajesh Ranganath
    * Abstract: Causal effect estimation relies on separating the variation in the outcome into parts due to the treatment and due to the confounders. To achieve this separation, practitioners often use external sources of randomness that only influence the treatment called instrumental variables (IVs). We study variables constructed from treatment and IV that help estimate effects, called control functions. We characterize general control functions for effect estimation in a meta-identification result. Then, we show that structural assumptions on the treatment process allow the construction of general control functions, thereby guaranteeing identification. To construct general control functions and estimate effects, we develop the general control function method (GCFN). GCFN’s first stage called variational decoupling (VDE) constructs general control functions by recovering the residual variation in the treatment given the IV. Using VDE’s control function, GCFN’s second stage estimates effects via regression. Further, we develop semi-supervised GCFN to construct general control functions using subsets of data that have both IV and confounders observed as supervision; this needs no structural treatment process assumptions. We evaluate GCFN on low and high dimensional simulated data and on recovering the causal effect of slave export on modern community trust [30]
count=1
* Curriculum Learning by Dynamic Instance Hardness
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/62000dee5a05a6a71de3a6127a68778a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/62000dee5a05a6a71de3a6127a68778a-Paper.pdf)]
    * Title: Curriculum Learning by Dynamic Instance Hardness
    * Year: `2020`
    * Authors: Tianyi Zhou, Shengjie Wang, Jeffrey Bilmes
    * Abstract: A good teacher can adjust the curriculum based on students' learning history. By analogy, in this paper, we study the dynamics of a deep neural network's (DNN) performance on individual samples during its learning process. The observed properties allow us to develop an adaptive curriculum that leads to faster learning of more accurate models. We introduce dynamic instance hardness (DIH), the exponential moving average of a sample's instantaneous hardness (e.g., a loss, or a change in outputs) over the training history. A low DIH indicates that a model retains knowledge about a sample over time, and implies a flat loss landscape for that sample. Moreover, for DNNs, we find that a sample's DIH early in training predicts its DIH in later stages. Hence, we can train a model using samples with higher DIH and safely ignore those with lower DIH. This motivates a DIH guided curriculum learning (DIHCL). Compared to existing CL methods: (1) DIH is more stable over time than using only instantaneous hardness, which is noisy due to stochastic training and DNN's non-smoothness; (2) DIHCL is computationally inexpensive since it uses only a byproduct of back-propagation and thus does not require extra inference. On 11 datasets, DIHCL significantly outperforms random mini-batch SGD and recent CL methods in terms of efficiency and final performance.
count=1
* Baxter Permutation Process
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/6271faadeedd7626d661856b7a004e27-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/6271faadeedd7626d661856b7a004e27-Paper.pdf)]
    * Title: Baxter Permutation Process
    * Year: `2020`
    * Authors: Masahiro Nakano, Akisato Kimura, Takeshi Yamada, Naonori Ueda
    * Abstract: In this paper, a Bayesian nonparametric (BNP) model for Baxter permutations (BPs), termed BP process (BPP) is proposed and applied to relational data analysis. The BPs are a well-studied class of permutations, and it has been demonstrated that there is one-to-one correspondence between BPs and several interesting objects including floorplan partitioning (FP), which constitutes a subset of rectangular partitioning (RP). Accordingly, the BPP can be used as an FP model. We combine the BPP with a multi-dimensional extension of the stick-breaking process called the {\it block-breaking process} to fill the gap between FP and RP, and obtain a stochastic process on arbitrary RPs. Compared with conventional BNP models for arbitrary RPs, the proposed model is simpler and has a high affinity with Bayesian inference.
count=1
* Self-Supervised Learning by Cross-Modal Audio-Video Clustering
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/6f2268bd1d3d3ebaabb04d6b5d099425-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/6f2268bd1d3d3ebaabb04d6b5d099425-Paper.pdf)]
    * Title: Self-Supervised Learning by Cross-Modal Audio-Video Clustering
    * Year: `2020`
    * Authors: Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, Du Tran
    * Abstract: Visual and audio modalities are highly correlated, yet they contain different information. Their strong correlation makes it possible to predict the semantics of one from the other with good accuracy. Their intrinsic differences make cross-modal prediction a potentially more rewarding pretext task for self-supervised learning of video and audio representations compared to within-modality learning. Based on this intuition, we propose Cross-Modal Deep Clustering (XDC), a novel self-supervised method that leverages unsupervised clustering in one modality (e.g., audio) as a supervisory signal for the other modality (e.g., video). This cross-modal supervision helps XDC utilize the semantic correlation and the differences between the two modalities. Our experiments show that XDC outperforms single-modality clustering and other multi-modal variants. XDC achieves state-of-the-art accuracy among self-supervised methods on multiple video and audio benchmarks. Most importantly, our video model pretrained on large-scale unlabeled data significantly outperforms the same model pretrained with full-supervision on ImageNet and Kinetics for action recognition on HMDB51 and UCF101. To the best of our knowledge, XDC is the first self-supervised learning method that outperforms large-scale fully-supervised pretraining for action recognition on the same architecture.
count=1
* Regularizing Black-box Models for Improved Interpretability
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/770f8e448d07586afbf77bb59f698587-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/770f8e448d07586afbf77bb59f698587-Paper.pdf)]
    * Title: Regularizing Black-box Models for Improved Interpretability
    * Year: `2020`
    * Authors: Gregory Plumb, Maruan Al-Shedivat, Ángel Alexander Cabrera, Adam Perer, Eric Xing, Ameet Talwalkar
    * Abstract: Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for interpretability, or post-hoc explanation systems, whose explanation quality can be unpredictable. Our method, ExpO, is a hybridization of these approaches that regularizes a model for explanation quality at training time. Importantly, these regularizers are differentiable, model agnostic, and require no domain knowledge to define. We demonstrate that post-hoc explanations for ExpO-regularized models have better explanation quality, as measured by the common fidelity and stability metrics. We verify that improving these metrics leads to significantly more useful explanations with a user study on a realistic task.
count=1
* SnapBoost: A Heterogeneous Boosting Machine
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/7fd3b80fb1884e2927df46a7139bb8bf-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/7fd3b80fb1884e2927df46a7139bb8bf-Paper.pdf)]
    * Title: SnapBoost: A Heterogeneous Boosting Machine
    * Year: `2020`
    * Authors: Thomas Parnell, Andreea Anghel, Małgorzata Łazuka, Nikolas Ioannou, Sebastian Kurella, Peshal Agarwal, Nikolaos Papandreou, Haralampos Pozidis
    * Abstract: Modern gradient boosting software frameworks, such as XGBoost and LightGBM, implement Newton descent in a functional space. At each boosting iteration, their goal is to find the base hypothesis, selected from some base hypothesis class, that is closest to the Newton descent direction in a Euclidean sense. Typically, the base hypothesis class is fixed to be all binary decision trees up to a given depth. In this work, we study a Heterogeneous Newton Boosting Machine (HNBM) in which the base hypothesis class may vary across boosting iterations. Specifically, at each boosting iteration, the base hypothesis class is chosen, from a fixed set of subclasses, by sampling from a probability distribution. We derive a global linear convergence rate for the HNBM under certain assumptions, and show that it agrees with existing rates for Newton's method when the Newton direction can be perfectly fitted by the base hypothesis at each boosting iteration. We then describe a particular realization of a HNBM, SnapBoost, that, at each boosting iteration, randomly selects between either a decision tree of variable depth or a linear regressor with random Fourier features. We describe how SnapBoost is implemented, with a focus on the training complexity. Finally, we present experimental results, using OpenML and Kaggle datasets, that show that SnapBoost is able to achieve better generalization loss than competing boosting frameworks, without taking significantly longer to tune.
count=1
* On Adaptive Distance Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/803ef56843860e4a48fc4cdb3065e8ce-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/803ef56843860e4a48fc4cdb3065e8ce-Paper.pdf)]
    * Title: On Adaptive Distance Estimation
    * Year: `2020`
    * Authors: Yeshwanth Cherapanamjeri, Jelani Nelson
    * Abstract: We provide a static data structure for distance estimation which supports {\it adaptive} queries. Concretely, given a dataset $X = \{x_i\}_{i = 1}^n$ of $n$ points in $\mathbb{R}^d$ and $0 < p \leq 2$, we construct a randomized data structure with low memory consumption and query time which, when later given any query point $q \in \mathbb{R}^d$, outputs a $(1+\varepsilon)$-approximation of $\|q - x_i\|_p$ with high probability for all $i\in[n]$. The main novelty is our data structure's correctness guarantee holds even when the sequence of queries can be chosen adaptively: an adversary is allowed to choose the $j$th query point $q_j$ in a way that depends on the answers reported by the data structure for $q_1,\ldots,q_{j-1}$. Previous randomized Monte Carlo methods do not provide error guarantees in the setting of adaptively chosen queries. Our memory consumption is $\tilde O(nd/\varepsilon^2)$, slightly more than the $O(nd)$ required to store $X$ in memory explicitly, but with the benefit that our time to answer queries is only $\tilde O(\varepsilon^{-2}(n + d))$, much faster than the naive $\Theta(nd)$ time obtained from a linear scan in the case of $n$ and $d$ very large. Here $\tilde O$ hides $\log(nd/\varepsilon)$ factors. We discuss applications to nearest neighbor search and nonparametric estimation. Our method is simple and likely to applicable to other domains: we describe a generic approach for transforming randomized Monte Carlo data structures which do not support adaptive queries to ones that do, and show that for the problem at hand it can be applied to standard nonadaptive solutions to $\ell_p$ norm estimation with negligible overhead in query time and a factor $d$ overhead in memory.
count=1
* TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/81f7acabd411274fcf65ce2070ed568a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/81f7acabd411274fcf65ce2070ed568a-Paper.pdf)]
    * Title: TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning
    * Year: `2020`
    * Authors: Han Cai, Chuang Gan, Ligeng Zhu, Song Han
    * Abstract: Efficient on-device learning requires a small memory footprint at training time to fit the tight memory constraint. Existing work solves this problem by reducing the number of trainable parameters. However, this doesn't directly translate to memory saving since the major bottleneck is the activations, not parameters. In this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient on-device learning. TinyTL freezes the weights while only learns the memory-efficient bias modules, thus no need to store the intermediate activations. To maintain the adaptation capacity, we introduce a new memory-efficient bias module, the lite residual module, to refine the feature extractor by learning small residual feature maps adding only 3.8% memory overhead. Extensive experiments show that TinyTL significantly saves the memory (up to 6.5x) with little accuracy loss compared to fine-tuning the full network. Compared to fine-tuning the last layer, TinyTL provides significant accuracy improvements (up to 33.8%) with little memory overhead. Furthermore, combined with feature extractor adaptation, TinyTL provides 7.5-12.9x memory saving without sacrificing accuracy compared to fine-tuning the full Inception-V3. Code is released at https://github.com/mit-han-lab/tinyML/tree/master/tinyTL.
count=1
* Identifying Causal-Effect Inference Failure with Uncertainty-Aware Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/860b37e28ec7ba614f00f9246949561d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/860b37e28ec7ba614f00f9246949561d-Paper.pdf)]
    * Title: Identifying Causal-Effect Inference Failure with Uncertainty-Aware Models
    * Year: `2020`
    * Authors: Andrew Jesson, Sören Mindermann, Uri Shalit, Yarin Gal
    * Abstract: Recommending the best course of action for an individual is a major application of individual-level causal effect estimation. This application is often needed in safety-critical domains such as healthcare, where estimating and communicating uncertainty to decision-makers is crucial. We introduce a practical approach for integrating uncertainty estimation into a class of state-of-the-art neural network methods used for individual-level causal estimates. We show that our methods enable us to deal gracefully with situations of "no-overlap", common in high-dimensional data, where standard applications of causal effect approaches fail. Further, our methods allow us to handle covariate shift, where the train and test distributions differ, common when systems are deployed in practice. We show that when such a covariate shift occurs, correctly modeling uncertainty can keep us from giving overconfident and potentially harmful recommendations. We demonstrate our methodology with a range of state-of-the-art models. Under both covariate shift and lack of overlap, our uncertainty-equipped methods can alert decision makers when predictions are not to be trusted while outperforming standard methods that use the propensity score to identify lack of overlap.
count=1
* CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/8965f76632d7672e7d3cf29c87ecaa0c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/8965f76632d7672e7d3cf29c87ecaa0c-Paper.pdf)]
    * Title: CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances
    * Year: `2020`
    * Authors: Jihoon Tack, Sangwoo Mo, Jongheon Jeong, Jinwoo Shin
    * Abstract: Novelty detection, i.e., identifying whether a given sample is drawn from outside the training distribution, is essential for reliable machine learning. To this end, there have been many attempts at learning a representation well-suited for novelty detection and designing a score based on such representation. In this paper, we propose a simple, yet effective method named contrasting shifted instances (CSI), inspired by the recent success on contrastive learning of visual representations. Specifically, in addition to contrasting a given sample with other instances as in conventional contrastive learning methods, our training scheme contrasts the sample with distributionally-shifted augmentations of itself. Based on this, we propose a new detection score that is specific to the proposed training scheme. Our experiments demonstrate the superiority of our method under various novelty detection scenarios, including unlabeled one-class, unlabeled multi-class and labeled multi-class settings, with various image benchmark datasets. Code and pre-trained models are available at https://github.com/alinlab/CSI.
count=1
* GS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/9547ad6b651e2087bac67651aa92cd0d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/9547ad6b651e2087bac67651aa92cd0d-Paper.pdf)]
    * Title: GS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators
    * Year: `2020`
    * Authors: Dingfan Chen, Tribhuvanesh Orekondy, Mario Fritz
    * Abstract: The wide-spread availability of rich data has fueled the growth of machine learning applications in numerous domains. However, growth in domains with highly-sensitive data (e.g., medical) is largely hindered as the private nature of data prohibits it from being shared. To this end, we propose Gradient-sanitized Wasserstein Generative Adversarial Networks (GS-WGAN), which allows releasing a sanitized form of the sensitive data with rigorous privacy guarantees. In contrast to prior work, our approach is able to distort gradient information more precisely, and thereby enabling training deeper models which generate more informative samples. Moreover, our formulation naturally allows for training GANs in both centralized and federated (i.e., decentralized) data scenarios. Through extensive experiments, we find our approach consistently outperforms state-of-the-art approaches across multiple metrics (e.g., sample quality) and datasets.
count=1
* Learning from Positive and Unlabeled Data with Arbitrary Positive Shift
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/98b297950041a42470269d56260243a1-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/98b297950041a42470269d56260243a1-Paper.pdf)]
    * Title: Learning from Positive and Unlabeled Data with Arbitrary Positive Shift
    * Year: `2020`
    * Authors: Zayd Hammoudeh, Daniel Lowd
    * Abstract: Positive-unlabeled (PU) learning trains a binary classifier using only positive and unlabeled data. A common simplifying assumption is that the positive data is representative of the target positive class. This assumption rarely holds in practice due to temporal drift, domain shift, and/or adversarial manipulation. This paper shows that PU learning is possible even with arbitrarily non-representative positive data given unlabeled data from the source and target distributions. Our key insight is that only the negative class's distribution need be fixed. We integrate this into two statistically consistent methods to address arbitrary positive bias - one approach combines negative-unlabeled learning with unlabeled-unlabeled learning while the other uses a novel, recursive risk estimator. Experimental results demonstrate our methods' effectiveness across numerous real-world datasets and forms of positive bias, including disjoint positive class-conditional supports. Additionally, we propose a general, simplified approach to address PU risk estimation overfitting.
count=1
* Finite Versus Infinite Neural Networks: an Empirical Study
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/ad086f59924fffe0773f8d0ca22ea712-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/ad086f59924fffe0773f8d0ca22ea712-Paper.pdf)]
    * Title: Finite Versus Infinite Neural Networks: an Empirical Study
    * Year: `2020`
    * Authors: Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, Jascha Sohl-Dickstein
    * Abstract: We perform a careful, thorough, and large scale empirical study of the correspondence between wide neural networks and kernel methods. By doing so, we resolve a variety of open questions related to the study of infinitely wide neural networks. Our experimental results include: kernel methods outperform fully-connected finite-width networks, but underperform convolutional finite width networks; neural network Gaussian process (NNGP) kernels frequently outperform neural tangent (NT) kernels; centered and ensembled finite networks have reduced posterior variance and behave more similarly to infinite networks; weight decay and the use of a large learning rate break the correspondence between finite and infinite networks; the NTK parameterization outperforms the standard parameterization for finite width networks; diagonal regularization of kernels acts similarly to early stopping; floating point precision limits kernel performance beyond a critical dataset size; regularized ZCA whitening improves accuracy; finite network performance depends non-monotonically on width in ways not captured by double descent phenomena; equivariance of CNNs is only beneficial for narrow networks far from the kernel regime. Our experiments additionally motivate an improved layer-wise scaling for weight decay which improves generalization in finite-width networks. Finally, we develop improved best practices for using NNGP and NT kernels for prediction, including a novel ensembling technique. Using these best practices we achieve state-of-the-art results on CIFAR-10 classification for kernels corresponding to each architecture class we consider.
count=1
* Disentangling Human Error from Ground Truth in Segmentation of Medical Images
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/b5d17ed2b502da15aa727af0d51508d6-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/b5d17ed2b502da15aa727af0d51508d6-Paper.pdf)]
    * Title: Disentangling Human Error from Ground Truth in Segmentation of Medical Images
    * Year: `2020`
    * Authors: Le Zhang, Ryutaro Tanno, Mou-Cheng Xu, Chen Jin, Joseph Jacob, Olga Cicarrelli, Frederik Barkhof, Daniel Alexander
    * Abstract: Recent years have seen increasing use of supervised learning methods for segmentation tasks. However, the predictive performance of these algorithms depends on the quality of labels. This problem is particularly pertinent in the medical image domain, where both the annotation cost and inter-observer variability are high. In a typical label acquisition process, different human experts provide their estimates of the ``true'' segmentation labels under the influence of their own biases and competence levels. Treating these noisy labels blindly as the ground truth limits the performance that automatic segmentation algorithms can achieve. In this work, we present a method for jointly learning, from purely noisy observations alone, the reliability of individual annotators and the true segmentation label distributions, using two coupled CNNs. The separation of the two is achieved by encouraging the estimated annotators to be maximally unreliable while achieving high fidelity with the noisy training data. We first define a toy segmentation dataset based on MNIST and study the properties of the proposed algorithm. We then demonstrate the utility of the method on three public medical imaging segmentation datasets with simulated (when necessary) and real diverse annotations: 1) MSLSC (multiple-sclerosis lesions); 2) BraTS (brain tumours); 3) LIDC-IDRI (lung abnormalities). In all cases, our method outperforms competing methods and relevant baselines particularly in cases where the number of annotations is small and the amount of disagreement is large. The experiments also show strong ability to capture the complex spatial characteristics of annotators' mistakes. Our code is available at \url{https://github.com/moucheng2017/LearnNoisyLabelsMedicalImages}.
count=1
* Estimating the Effects of Continuous-valued Interventions using Generative Adversarial Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/bea5955b308361a1b07bc55042e25e54-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/bea5955b308361a1b07bc55042e25e54-Paper.pdf)]
    * Title: Estimating the Effects of Continuous-valued Interventions using Generative Adversarial Networks
    * Year: `2020`
    * Authors: Ioana Bica, James Jordon, Mihaela van der Schaar
    * Abstract: While much attention has been given to the problem of estimating the effect of discrete interventions from observational data, relatively little work has been done in the setting of continuous-valued interventions, such as treatments associated with a dosage parameter. In this paper, we tackle this problem by building on a modification of the generative adversarial networks (GANs) framework. Our model, SCIGAN, is flexible and capable of simultaneously estimating counterfactual outcomes for several different continuous interventions. The key idea is to use a significantly modified GAN model to learn to generate counterfactual outcomes, which can then be used to learn an inference model, using standard supervised methods, capable of estimating these counterfactuals for a new sample. To address the challenges presented by shifting to continuous interventions, we propose a novel architecture for our discriminator - we build a hierarchical discriminator that leverages the structure of the continuous intervention setting. Moreover, we provide theoretical results to support our use of the GAN framework and of the hierarchical discriminator. In the experiments section, we introduce a new semi-synthetic data simulation for use in the continuous intervention setting and demonstrate improvements over the existing benchmark models.
count=1
* Gaussian Gated Linear Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/c0356641f421b381e475776b602a5da8-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/c0356641f421b381e475776b602a5da8-Paper.pdf)]
    * Title: Gaussian Gated Linear Networks
    * Year: `2020`
    * Authors: David Budden, Adam Marblestone, Eren Sezener, Tor Lattimore, Gregory Wayne, Joel Veness
    * Abstract: We propose the Gaussian Gated Linear Network (G-GLN), an extension to the recently proposed GLN family of deep neural networks. Instead of using backpropagation to learn features, GLNs have a distributed and local credit assignment mechanism based on optimizing a convex objective. This gives rise to many desirable properties including universality, data-efficient online learning, trivial interpretability and robustness to catastrophic forgetting. We extend the GLN framework from classification to multiple regression and density modelling by generalizing geometric mixing to a product of Gaussian densities. The G-GLN achieves competitive or state-of-the-art performance on several univariate and multivariate regression benchmarks, and we demonstrate its applicability to practical tasks including online contextual bandits and density estimation via denoising.
count=1
* Self-Supervised Visual Representation Learning from Hierarchical Grouping
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/c1502ae5a4d514baec129f72948c266e-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/c1502ae5a4d514baec129f72948c266e-Paper.pdf)]
    * Title: Self-Supervised Visual Representation Learning from Hierarchical Grouping
    * Year: `2020`
    * Authors: Xiao Zhang, Michael Maire
    * Abstract: We create a framework for bootstrapping visual representation learning from a primitive visual grouping capability. We operationalize grouping via a contour detector that partitions an image into regions, followed by merging of those regions into a tree hierarchy. A small supervised dataset suffices for training this grouping primitive. Across a large unlabeled dataset, we apply this learned primitive to automatically predict hierarchical region structure. These predictions serve as guidance for self-supervised contrastive feature learning: we task a deep network with producing per-pixel embeddings whose pairwise distances respect the region hierarchy. Experiments demonstrate that our approach can serve as state-of-the-art generic pre-training, benefiting downstream tasks. We additionally explore applications to semantic region search and video-based object instance tracking.
count=1
* PAC-Bayes Analysis Beyond the Usual Bounds
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/c3992e9a68c5ae12bd18488bc579b30d-Paper.pdf)]
    * Title: PAC-Bayes Analysis Beyond the Usual Bounds
    * Year: `2020`
    * Authors: Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvari, John Shawe-Taylor
    * Abstract: We focus on a stochastic learning model where the learner observes a finite set of training examples and the output of the learning process is a data-dependent distribution over a space of hypotheses. The learned data-dependent distribution is then used to make randomized predictions, and the high-level theme addressed here is guaranteeing the quality of predictions on examples that were not seen during training, i.e. generalization. In this setting the unknown quantity of interest is the expected risk of the data-dependent randomized predictor, for which upper bounds can be derived via a PAC-Bayes analysis, leading to PAC-Bayes bounds. Specifically, we present a basic PAC-Bayes inequality for stochastic kernels, from which one may derive extensions of various known PAC-Bayes bounds as well as novel bounds. We clarify the role of the requirements of fixed ‘data-free’ priors, bounded losses, and i.i.d. data. We highlight that those requirements were used to upper-bound an exponential moment term, while the basic PAC-Bayes theorem remains valid without those restrictions. We present three bounds that illustrate the use of data-dependent priors, including one for the unbounded square loss.
count=1
* ImpatientCapsAndRuns: Approximately Optimal Algorithm Configuration from an Infinite Pool
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/ca5520b5672ea120b23bde75c46e76c6-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/ca5520b5672ea120b23bde75c46e76c6-Paper.pdf)]
    * Title: ImpatientCapsAndRuns: Approximately Optimal Algorithm Configuration from an Infinite Pool
    * Year: `2020`
    * Authors: Gellert Weisz, András György, Wei-I Lin, Devon Graham, Kevin Leyton-Brown, Csaba Szepesvari, Brendan Lucier
    * Abstract: Algorithm configuration procedures optimize parameters of a given algorithm to perform well over a distribution of inputs. Recent theoretical work focused on the case of selecting between a small number of alternatives. In practice, parameter spaces are often very large or infinite, and so successful heuristic procedures discard parameters ``impatiently'', based on very few observations. Inspired by this idea, we introduce ImpatientCapsAndRuns, which quickly discards less promising configurations, significantly speeding up the search procedure compared to previous algorithms with theoretical guarantees, while still achieving optimal runtime up to logarithmic factors under mild assumptions. Experimental results demonstrate a practical improvement.
count=1
* Learning outside the Black-Box: The pursuit of interpretable models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/ce758408f6ef98d7c7a7b786eca7b3a8-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/ce758408f6ef98d7c7a7b786eca7b3a8-Paper.pdf)]
    * Title: Learning outside the Black-Box: The pursuit of interpretable models
    * Year: `2020`
    * Authors: Jonathan Crabbe, Yao Zhang, William Zame, Mihaela van der Schaar
    * Abstract: Machine learning has proved its ability to produce accurate models -- but the deployment of these models outside the machine learning community has been hindered by the difficulties of interpreting these models. This paper proposes an algorithm that produces a continuous global interpretation of any given continuous black-box function. Our algorithm employs a variation of projection pursuit in which the ridge functions are chosen to be Meijer G-functions, rather than the usual polynomial splines. Because Meijer G-functions are differentiable in their parameters, we can "tune" the parameters of the representation by gradient descent; as a consequence, our algorithm is efficient. Using five familiar data sets from the UCI repository and two familiar machine learning algorithms, we demonstrate that our algorithm produces global interpretations that are both faithful (highly accurate) and parsimonious (involve a small number of terms). Our interpretations permit easy understanding of the relative importance of features and feature interactions. Our interpretation algorithm represents a leap forward from the previous state of the art.
count=1
* Representation Learning for Integrating Multi-domain Outcomes to Optimize Individualized Treatment
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/d0bb8259d8fe3c7df4554dab9d7da3c9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/d0bb8259d8fe3c7df4554dab9d7da3c9-Paper.pdf)]
    * Title: Representation Learning for Integrating Multi-domain Outcomes to Optimize Individualized Treatment
    * Year: `2020`
    * Authors: Yuan Chen, Donglin Zeng, Tianchen Xu, Yuanjia Wang
    * Abstract: For mental disorders, patients' underlying mental states are non-observed latent constructs which have to be inferred from observed multi-domain measurements such as diagnostic symptoms and patient functioning scores. Additionally, substantial heterogeneity in the disease diagnosis between patients needs to be addressed for optimizing individualized treatment policy in order to achieve precision medicine. To address these challenges, we propose an integrated learning framework that can simultaneously learn patients' underlying mental states and recommend optimal treatments for each individual. This learning framework is based on the measurement theory in psychiatry for modeling multiple disease diagnostic measures as arising from the underlying causes (true mental states). It allows incorporation of the multivariate pre- and post-treatment outcomes as well as biological measures while preserving the invariant structure for representing patients' latent mental states. A multi-layer neural network is used to allow complex treatment effect heterogeneity. Optimal treatment policy can be inferred for future patients by comparing their potential mental states under different treatments given the observed multi-domain pre-treatment measurements. Experiments on simulated data and a real-world clinical trial data show that the learned treatment polices compare favorably to alternative methods on heterogeneous treatment effects, and have broad utilities which lead to better patient outcomes on multiple domains.
count=1
* X-CAL: Explicit Calibration for Survival Analysis
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/d4a93297083a23cc099f7bd6a8621131-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/d4a93297083a23cc099f7bd6a8621131-Paper.pdf)]
    * Title: X-CAL: Explicit Calibration for Survival Analysis
    * Year: `2020`
    * Authors: Mark Goldstein, Xintian Han, Aahlad Puli, Adler Perotte, Rajesh Ranganath
    * Abstract: Survival analysis models the distribution of time until an event of interest, such as discharge from the hospital or admission to the ICU. When a model’s predicted number of events within any time interval is similar to the observed number, it is called well-calibrated. A survival model’s calibration can be measured using, for instance, distributional calibration (D-CALIBRATION) [Haider et al., 2020] which computes the squared difference between the observed and predicted number of events within different time intervals. Classically, calibration is addressed in post-training analysis. We develop explicit calibration (X-CAL), which turns D-CALIBRATION into a differentiable objective that can be used in survival modeling alongside maximum likelihood estimation and other objectives. X-CAL allows us to directly optimize calibration and strike a desired trade-off between predictive power and calibration. In our experiments, we fit a variety of shallow and deep models on simulated data, a survival dataset based on MNIST, on length-of-stay prediction using MIMIC-III data, and on brain cancer data from The Cancer Genome Atlas. We show that the models we study can be miscalibrated. We give experimental evidence on these datasets that X-CAL improves D-CALIBRATION without a large decrease in concordance or likelihood.
count=1
* Near-Optimal Comparison Based Clustering
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/e11943a6031a0e6114ae69c257617980-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/e11943a6031a0e6114ae69c257617980-Paper.pdf)]
    * Title: Near-Optimal Comparison Based Clustering
    * Year: `2020`
    * Authors: Michaël Perrot, Pascal Esser, Debarghya Ghoshdastidar
    * Abstract: The goal of clustering is to group similar objects into meaningful partitions. This process is well understood when an explicit similarity measure between the objects is given. However, far less is known when this information is not readily available and, instead, one only observes ordinal comparisons such as ``object i is more similar to j than to k.'' In this paper, we tackle this problem using a two-step procedure: we estimate a pairwise similarity matrix from the comparisons before using a clustering method based on semi-definite programming (SDP). We theoretically show that our approach can exactly recover a planted clustering using a near-optimal number of passive comparisons. We empirically validate our theoretical findings and demonstrate the good behaviour of our method on real data.
count=1
* Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/e2ce14e81dba66dbff9cbc35ecfdb704-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/e2ce14e81dba66dbff9cbc35ecfdb704-Paper.pdf)]
    * Title: Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search
    * Year: `2020`
    * Authors: Linnan Wang, Rodrigo Fonseca, Yuandong Tian
    * Abstract: High dimensional black-box optimization has broad applications but remains a challenging problem to solve. Given a set of samples xi, yi, building a global model (like Bayesian Optimization (BO)) suffers from the curse of dimensionality in the high-dimensional search space, while a greedy search may lead to sub-optimality. By recursively splitting the search space into regions with high/low function values, recent works like LaNAS shows good performance in Neural Architecture Search (NAS), reducing the sample complexity empirically. In this paper, we coin LA-MCTS that extends LaNAS to other domains. Unlike previous approaches, LA-MCTS learns the partition of the search space using a few samples and their function values in an online fashion. While LaNAS uses linear partition and performs uniform sampling in each region, our LA-MCTS adopts a nonlinear decision boundary and learns a local model to pick good candidates. If the nonlinear partition function and the local model fits well with ground-truth black-box function, then good partitions and candidates can be reached with much fewer samples. LA-MCTS serves as a meta-algorithm by using existing black-box optimizers (e.g., BO, TuRBO as its local models, achieving strong performance in general black-box optimization and reinforcement learning benchmarks, in particular for high-dimensional problems.
count=1
* A Limitation of the PAC-Bayes Framework
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/ec79d4bed810ed64267d169b0d37373e-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/ec79d4bed810ed64267d169b0d37373e-Paper.pdf)]
    * Title: A Limitation of the PAC-Bayes Framework
    * Year: `2020`
    * Authors: Roi Livni, Shay Moran
    * Abstract: PAC-Bayes is a useful framework for deriving generalization bounds which was introduced by McAllester ('98). This framework has the flexibility of deriving distribution- and algorithm-dependent bounds, which are often tighter than VC-related uniform convergence bounds. In this manuscript we present a limitation for the PAC-Bayes framework. We demonstrate an easy learning task which is not amenable to a PAC-Bayes analysis. Specifically, we consider the task of linear classification in 1D; it is well-known that this task is learnable using just $O(\log(1/\delta)/\epsilon)$ examples. 	On the other hand, we show that this fact can not be proved using a PAC-Bayes analysis: for any algorithm that learns 1-dimensional linear classifiers there exists a (realizable) distribution for which the PAC-Bayes bound is arbitrarily large.
count=1
* Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf)]
    * Title: Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning
    * Year: `2020`
    * Authors: Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, Michal Valko
    * Abstract: We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a standard ResNet-50 architecture and 79.6% with a larger ResNet. We also show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.
count=1
* Minimax Optimal Nonparametric Estimation of Heterogeneous Treatment Effects
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/f75b757d3459c3e93e98ddab7b903938-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/f75b757d3459c3e93e98ddab7b903938-Paper.pdf)]
    * Title: Minimax Optimal Nonparametric Estimation of Heterogeneous Treatment Effects
    * Year: `2020`
    * Authors: Zijun Gao, Yanjun Han
    * Abstract: A central goal of causal inference is to detect and estimate the treatment effects of a given treatment or intervention on an outcome variable of interest, where a member known as the heterogeneous treatment effect (HTE) is of growing popularity in recent practical applications such as the personalized medicine. In this paper, we model the HTE as a smooth nonparametric difference between two less smooth baseline functions, and determine the tight statistical limits of the nonparametric HTE estimation as a function of the covariate geometry. In particular, a two-stage nearest-neighbor-based estimator throwing away observations with poor matching quality is near minimax optimal. We also establish the tight dependence on the density ratio without the usual assumption that the covariate densities are bounded away from zero, where a key step is to employ a novel maximal inequality which could be of independent interest.
count=1
* Learning from Label Proportions: A Mutual Contamination Framework
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/fcde14913c766cf307c75059e0e89af5-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/fcde14913c766cf307c75059e0e89af5-Paper.pdf)]
    * Title: Learning from Label Proportions: A Mutual Contamination Framework
    * Year: `2020`
    * Authors: Clayton Scott, Jianxin Zhang
    * Abstract: Learning from label proportions (LLP) is a weakly supervised setting for classification in which unlabeled training instances are grouped into bags, and each bag is annotated with the proportion of each class occurring in that bag. Prior work on LLP has yet to establish a consistent learning procedure, nor does there exist a theoretically justified, general purpose training criterion. In this work we address these two issues by posing LLP in terms of mutual contamination models (MCMs), which have recently been applied successfully to study various other weak supervision settings. In the process, we establish several novel technical results for MCMs, including unbiased losses and generalization error bounds under non-iid sampling plans. We also point out the limitations of a common experimental setting for LLP, and propose a new one based on our MCM framework.
count=1
* Multilabel Classification by Hierarchical Partitioning and Data-dependent Grouping
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/hash/fea16e782bc1b1240e4b3c797012e289-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2020/file/fea16e782bc1b1240e4b3c797012e289-Paper.pdf)]
    * Title: Multilabel Classification by Hierarchical Partitioning and Data-dependent Grouping
    * Year: `2020`
    * Authors: Shashanka Ubaru, Sanjeeb Dash, Arya Mazumdar, Oktay Gunluk
    * Abstract: In modern multilabel classification problems, each data instance belongs to a small number of classes among a large set of classes. In other words, these problems involve learning very sparse binary label vectors. Moreover, in the large-scale problems, the labels typically have certain (unknown) hierarchy. In this paper we exploit the sparsity of label vectors and the hierarchical structure to embed them in low-dimensional space using label groupings. Consequently, we solve the classification problem in a much lower dimensional space and then obtain labels in the original space using an appropriately defined lifting. Our method builds on the work of (Ubaru & Mazumdar, 2017), where the idea of group testing was also explored for multilabel classification. We first present a novel data-dependent grouping approach, where we use a group construction based on a low-rank Nonnegative Matrix Factorization (NMF) of the label matrix of training instances. The construction also allows us, using recent results, to develop a fast prediction algorithm that has a \emph{logarithmic runtime in the number of labels}. We then present a hierarchical partitioning approach that exploits the label hierarchy in large-scale problems to divide the large label space into smaller sub-problems, which can then be solved independently via the grouping approach. Numerical results on many benchmark datasets illustrate that, compared to other popular methods, our proposed methods achieve comparable accuracy with significantly lower computational costs.
count=1
* Learning Macroscopic Brain Connectomes via Group-Sparse Factorization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/0bfce127947574733b19da0f30739fcd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/0bfce127947574733b19da0f30739fcd-Paper.pdf)]
    * Title: Learning Macroscopic Brain Connectomes via Group-Sparse Factorization
    * Year: `2019`
    * Authors: Farzane Aminmansour, Andrew Patterson, Lei Le, Yisu Peng, Daniel Mitchell, Franco Pestilli, Cesar F. Caiafa, Russell Greiner, Martha White
    * Abstract: Mapping structural brain connectomes for living human brains typically requires expert analysis and rule-based models on diffusion-weighted magnetic resonance imaging. A data-driven approach, however, could overcome limitations in such rule-based approaches and improve precision mappings for individuals. In this work, we explore a framework that facilitates applying learning algorithms to automatically extract brain connectomes. Using a tensor encoding, we design an objective with a group-regularizer that prefers biologically plausible fascicle structure. We show that the objective is convex and has unique solutions, ensuring identifiable connectomes for an individual. We develop an efficient optimization strategy for this extremely high-dimensional sparse problem, by reducing the number of parameters using a greedy algorithm designed specifically for the problem. We show that this greedy algorithm significantly improves on a standard greedy algorithm, called Orthogonal Matching Pursuit. We conclude with an analysis of the solutions found by our method, showing we can accurately reconstruct the diffusion information while maintaining contiguous fascicles with smooth direction changes.
count=1
* On the Fairness of Disentangled Representations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/1b486d7a5189ebe8d8c46afc64b0d1b4-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/1b486d7a5189ebe8d8c46afc64b0d1b4-Paper.pdf)]
    * Title: On the Fairness of Disentangled Representations
    * Year: `2019`
    * Authors: Francesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard Schölkopf, Olivier Bachem
    * Abstract: Recently there has been a significant interest in learning disentangled representations, as they promise increased interpretability, generalization to unseen scenarios and faster learning on downstream tasks. In this paper, we investigate the usefulness of different notions of disentanglement for improving the fairness of downstream prediction tasks based on representations. We consider the setting where the goal is to predict a target variable based on the learned representation of high-dimensional observations (such as images) that depend on both the target variable and an unobserved sensitive variable. We show that in this setting both the optimal and empirical predictions can be unfair, even if the target variable and the sensitive variable are independent. Analyzing the representations of more than 12600 trained state-of-the-art disentangled models, we observe that several disentanglement scores are consistently correlated with increased fairness, suggesting that disentanglement may be a useful property to encourage fairness when sensitive variables are not observed.
count=1
* Correlation Priors for Reinforcement Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/1bd2caf96a17d892c2c7e9959549cfc7-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/1bd2caf96a17d892c2c7e9959549cfc7-Paper.pdf)]
    * Title: Correlation Priors for Reinforcement Learning
    * Year: `2019`
    * Authors: Bastian Alt, Adrian Šošić, Heinz Koeppl
    * Abstract: Many decision-making problems naturally exhibit pronounced structures inherited from the characteristics of the underlying environment. In a Markov decision process model, for example, two distinct states can have inherently related semantics or encode resembling physical state configurations. This often implies locally correlated transition dynamics among the states. In order to complete a certain task in such environments, the operating agent usually needs to execute a series of temporally and spatially correlated actions. Though there exists a variety of approaches to capture these correlations in continuous state-action domains, a principled solution for discrete environments is missing. In this work, we present a Bayesian learning framework based on Pólya-Gamma augmentation that enables an analogous reasoning in such cases. We demonstrate the framework on a number of common decision-making related problems, such as imitation learning, subgoal extraction, system identification and Bayesian reinforcement learning. By explicitly modeling the underlying correlation structures of these problems, the proposed approach yields superior predictive performance compared to correlation-agnostic models, even when trained on data sets that are an order of magnitude smaller in size.
count=1
* Debiased Bayesian inference for average treatment effects
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/342285bb2a8cadef22f667eeb6a63732-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/342285bb2a8cadef22f667eeb6a63732-Paper.pdf)]
    * Title: Debiased Bayesian inference for average treatment effects
    * Year: `2019`
    * Authors: Kolyan Ray, Botond Szabo
    * Abstract: Bayesian approaches have become increasingly popular in causal inference problems due to their conceptual simplicity, excellent performance and in-built uncertainty quantification ('posterior credible sets'). We investigate Bayesian inference for average treatment effects from observational data, which is a challenging problem due to the missing counterfactuals and selection bias. Working in the standard potential outcomes framework, we propose a data-driven modification to an arbitrary (nonparametric) prior based on the propensity score that corrects for the first-order posterior bias, thereby improving performance. We illustrate our method for Gaussian process (GP) priors using (semi-)synthetic data. Our experiments demonstrate significant improvement in both estimation accuracy and uncertainty quantification compared to the unmodified GP, rendering our approach highly competitive with the state-of-the-art.
count=1
* CXPlain: Causal Explanations for Model Interpretation under Uncertainty
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/3ab6be46e1d6b21d59a3c3a0b9d0f6ef-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/3ab6be46e1d6b21d59a3c3a0b9d0f6ef-Paper.pdf)]
    * Title: CXPlain: Causal Explanations for Model Interpretation under Uncertainty
    * Year: `2019`
    * Authors: Patrick Schwab, Walter Karlen
    * Abstract: Feature importance estimates that inform users about the degree to which given inputs influence the output of a predictive model are crucial for understanding, validating, and interpreting machine-learning models. However, providing fast and accurate estimates of feature importance for high-dimensional data, and quantifying the uncertainty of such estimates remain open challenges. Here, we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task, and train causal explanation (CXPlain) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. CXPlain can, once trained, be used to explain the target model in little time, and enables the quantification of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that CXPlain is significantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition, we confirm that the uncertainty estimates provided by CXPlain ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.
count=1
* Demystifying Black-box Models with Symbolic Metamodels
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/567b8f5f423af15818a068235807edc0-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/567b8f5f423af15818a068235807edc0-Paper.pdf)]
    * Title: Demystifying Black-box Models with Symbolic Metamodels
    * Year: `2019`
    * Authors: Ahmed M. Alaa, Mihaela van der Schaar
    * Abstract: Understanding the predictions of a machine learning model can be as crucial as the model's accuracy in many application domains. However, the black-box nature of most highly-accurate (complex) models is a major hindrance to their interpretability. To address this issue, we introduce the symbolic metamodeling framework — a general methodology for interpreting predictions by converting "black-box" models into "white-box" functions that are understandable to human subjects. A symbolic metamodel is a model of a model, i.e., a surrogate model of a trained (machine learning) model expressed through a succinct symbolic expression that comprises familiar mathematical functions and can be subjected to symbolic manipulation. We parameterize symbolic metamodels using Meijer G-functions — a class of complex-valued contour integrals that depend on scalar parameters, and whose solutions reduce to familiar elementary, algebraic, analytic and closed-form functions for different parameter settings. This parameterization enables efficient optimization of metamodels via gradient descent, and allows discovering the functional forms learned by a machine learning model with minimal a priori assumptions. We show that symbolic metamodeling provides an all-encompassing framework for model interpretation — all common forms of global and local explanations of a model can be analytically derived from its symbolic metamodel.
count=1
* Robustness to Adversarial Perturbations in Learning from Incomplete Data
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/60ad83801910ec976590f69f638e0d6d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/60ad83801910ec976590f69f638e0d6d-Paper.pdf)]
    * Title: Robustness to Adversarial Perturbations in Learning from Incomplete Data
    * Year: `2019`
    * Authors: Amir Najafi, Shin-ichi Maeda, Masanori Koyama, Takeru Miyato
    * Abstract: What is the role of unlabeled data in an inference problem, when the presumed underlying distribution is adversarially perturbed? To provide a concrete answer to this question, this paper unifies two major learning frameworks: Semi-Supervised Learning (SSL) and Distributionally Robust Learning (DRL). We develop a generalization theory for our framework based on a number of novel complexity measures, such as an adversarial extension of Rademacher complexity and its semi-supervised analogue. Moreover, our analysis is able to quantify the role of unlabeled data in the generalization under a more general condition compared to the existing theoretical works in SSL. Based on our framework, we also present a hybrid of DRL and EM algorithms that has a guaranteed convergence rate. When implemented with deep neural networks, our method shows a comparable performance to those of the state-of-the-art on a number of real-world benchmark datasets.
count=1
* Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/6c4bb406b3e7cd5447f7a76fd7008806-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/6c4bb406b3e7cd5447f7a76fd7008806-Paper.pdf)]
    * Title: Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network
    * Year: `2019`
    * Authors: Siqi Wang, Yijie Zeng, Xinwang Liu, En Zhu, Jianping Yin, Chuanfu Xu, Marius Kloft
    * Abstract: Despite the wide success of deep neural networks (DNN), little progress has been made on end-to-end unsupervised outlier detection (UOD) from high dimensional data like raw images. In this paper, we propose a framework named E^3Outlier, which can perform UOD in a both effective and end-to-end manner: First, instead of the commonly-used autoencoders in previous end-to-end UOD methods, E^3Outlier for the first time leverages a discriminative DNN for better representation learning, by using surrogate supervision to create multiple pseudo classes from original unlabelled data. Next, unlike classic UOD that utilizes data characteristics like density or proximity, we exploit a novel property named inlier priority to enable end-to-end UOD by discriminative DNN. We demonstrate theoretically and empirically that the intrinsic class imbalance of inliers/outliers will make the network prioritize minimizing inliers' loss when inliers/outliers are indiscriminately fed into the network for training, which enables us to differentiate outliers directly from DNN's outputs. Finally, based on inlier priority, we propose the negative entropy based score as a simple and effective outlierness measure. Extensive evaluations show that E^3Outlier significantly advances UOD performance by up to 30% AUROC against state-of-the-art counterparts, especially on relatively difficult benchmarks.
count=1
* Scalable Global Optimization via Local Bayesian Optimization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/6c990b7aca7bc7058f5e98ea909e924b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf)]
    * Title: Scalable Global Optimization via Local Bayesian Optimization
    * Year: `2019`
    * Authors: David Eriksson, Michael Pearce, Jacob Gardner, Ryan D. Turner, Matthias Poloczek
    * Abstract: Bayesian optimization has recently emerged as a popular method for the sample-efficient optimization of expensive black-box functions. However, the application to high-dimensional problems with several thousand observations remains challenging, and on difficult problems Bayesian optimization is often not competitive with other paradigms. In this paper we take the view that this is due to the implicit homogeneity of the global probabilistic models and an overemphasized exploration that results from global acquisition. This motivates the design of a local probabilistic approach for global optimization of large-scale high-dimensional problems. We propose the TuRBO algorithm that fits a collection of local models and performs a principled global allocation of samples across these models via an implicit bandit approach. A comprehensive evaluation demonstrates that TuRBO outperforms state-of-the-art methods from machine learning and operations research on problems spanning reinforcement learning, robotics, and the natural sciences.
count=1
* Single-Model Uncertainties for Deep Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/73c03186765e199c116224b68adc5fa0-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/73c03186765e199c116224b68adc5fa0-Paper.pdf)]
    * Title: Single-Model Uncertainties for Deep Learning
    * Year: `2019`
    * Authors: Natasa Tagasovska, David Lopez-Paz
    * Abstract: We provide single-model estimates of aleatoric and epistemic uncertainty for deep neural networks. To estimate aleatoric uncertainty, we propose Simultaneous Quantile Regression (SQR), a loss function to learn all the conditional quantiles of a given target variable. These quantiles can be used to compute well-calibrated prediction intervals. To estimate epistemic uncertainty, we propose Orthonormal Certificates (OCs), a collection of diverse non-constant functions that map all training samples to zero. These certificates map out-of-distribution examples to non-zero values, signaling epistemic uncertainty. Our uncertainty estimators are computationally attractive, as they do not require ensembling or retraining deep models, and achieve state-of-the-art performance.
count=1
* Towards Automatic Concept-based Explanations
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/77d2afcb31f6493e350fca61764efb9a-Paper.pdf)]
    * Title: Towards Automatic Concept-based Explanations
    * Year: `2019`
    * Authors: Amirata Ghorbani, James Wexler, James Y. Zou, Been Kim
    * Abstract: Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \emph{concept} based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that \alg discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.
count=1
* Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/8ca01ea920679a0fe3728441494041b9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/8ca01ea920679a0fe3728441494041b9-Paper.pdf)]
    * Title: Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with Dirichlet calibration
    * Year: `2019`
    * Authors: Meelis Kull, Miquel Perello Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, Peter Flach
    * Abstract: Class probabilities predicted by most multiclass classifiers are uncalibrated, often tending towards over-confidence. With neural networks, calibration can be improved by temperature scaling, a method to learn a single corrective multiplicative factor for inputs to the last softmax layer. On non-neural models the existing methods apply binary calibration in a pairwise or one-vs-rest fashion. We propose a natively multiclass calibration method applicable to classifiers from any model class, derived from Dirichlet distributions and generalising the beta calibration method from binary classification. It is easily implemented with neural nets since it is equivalent to log-transforming the uncalibrated probabilities, followed by one linear layer and softmax. Experiments demonstrate improved probabilistic predictions according to multiple measures (confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of datasets and classifiers. Parameters of the learned Dirichlet calibration map provide insights to the biases in the uncalibrated model.
count=1
* Accurate Layerwise Interpretable Competence Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/a11da6bd58b95b334f8cd49f00918f16-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/a11da6bd58b95b334f8cd49f00918f16-Paper.pdf)]
    * Title: Accurate Layerwise Interpretable Competence Estimation
    * Year: `2019`
    * Authors: Vickram Rajendran, William LeVine
    * Abstract: Estimating machine learning performance “in the wild” is both an important and unsolved problem. In this paper, we seek to examine, understand, and predict the pointwise competence of classification models. Our contributions are twofold: First, we establish a statistically rigorous definition of competence that generalizes the common notion of classifier confidence; second, we present the ALICE (Accurate Layerwise Interpretable Competence Estimation) Score, a pointwise competence estimator for any classifier. By considering distributional, data, and model uncertainty, ALICE empirically shows accurate competence estimation in common failure situations such as class-imbalanced datasets, out-of-distribution datasets, and poorly trained models. Our contributions allow us to accurately predict the competence of any classification model given any input and error function. We compare our score with state-of-the-art confidence estimators such as model confidence and Trust Score, and show significant improvements in competence prediction over these methods on datasets such as DIGITS, CIFAR10, and CIFAR100.
count=1
* Foundations of Comparison-Based Hierarchical Clustering
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/a76da37101dffabe00e5d636c01719b6-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/a76da37101dffabe00e5d636c01719b6-Paper.pdf)]
    * Title: Foundations of Comparison-Based Hierarchical Clustering
    * Year: `2019`
    * Authors: Debarghya Ghoshdastidar, Michaël Perrot, Ulrike von Luxburg
    * Abstract: We address the classical problem of hierarchical clustering, but in a framework where one does not have access to a representation of the objects or their pairwise similarities. Instead, we assume that only a set of comparisons between objects is available, that is, statements of the form ``objects i and j are more similar than objects k and l.'' Such a scenario is commonly encountered in crowdsourcing applications. The focus of this work is to develop comparison-based hierarchical clustering algorithms that do not rely on the principles of ordinal embedding. We show that single and complete linkage are inherently comparison-based and we develop variants of average linkage. We provide statistical guarantees for the different methods under a planted hierarchical partition model. We also empirically demonstrate the performance of the proposed approaches on several datasets.
count=1
* Making the Cut: A Bandit-based Approach to Tiered Interviewing
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/d3fad7d3634dbfb61018813546edbccb-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/d3fad7d3634dbfb61018813546edbccb-Paper.pdf)]
    * Title: Making the Cut: A Bandit-based Approach to Tiered Interviewing
    * Year: `2019`
    * Authors: Candice Schumann, Zhi Lang, Jeffrey Foster, John Dickerson
    * Abstract: Given a huge set of applicants, how should a firm allocate sequential resume screenings, phone interviews, and in-person site visits? In a tiered interview process, later stages (e.g., in-person visits) are more informative, but also more expensive than earlier stages (e.g., resume screenings). Using accepted hiring models and the concept of structured interviews, a best practice in human resources, we cast tiered hiring as a combinatorial pure exploration (CPE) problem in the stochastic multi-armed bandit setting. The goal is to select a subset of arms (in our case, applicants) with some combinatorial structure. We present new algorithms in both the probably approximately correct (PAC) and fixed-budget settings that select a near-optimal cohort with provable guarantees. We show via simulations on real data from one of the largest US-based computer science graduate programs that our algorithms make better hiring decisions or use less budget than the status quo.
count=1
* Convergent Policy Optimization for Safe Reinforcement Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/db29450c3f5e97f97846693611f98c15-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/db29450c3f5e97f97846693611f98c15-Paper.pdf)]
    * Title: Convergent Policy Optimization for Safe Reinforcement Learning
    * Year: `2019`
    * Authors: Ming Yu, Zhuoran Yang, Mladen Kolar, Zhaoran Wang
    * Abstract: We study the safe reinforcement learning problem with nonlinear function approximation, where policy optimization is formulated as a constrained optimization problem with both the objective and the constraint being nonconvex functions. For such a problem, we construct a sequence of surrogate convex constrained optimization problems by replacing the nonconvex functions locally with convex quadratic functions obtained from policy gradient estimators. We prove that the solutions to these surrogate problems converge to a stationary point of the original nonconvex problem. Furthermore, to extend our theoretical results, we apply our algorithm to examples of optimal control and multi-agent reinforcement learning with safety constraints.
count=1
* Rates of Convergence for Large-scale Nearest Neighbor Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/e347c51419ffb23ca3fd5050202f9c3d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/e347c51419ffb23ca3fd5050202f9c3d-Paper.pdf)]
    * Title: Rates of Convergence for Large-scale Nearest Neighbor Classification
    * Year: `2019`
    * Authors: Xingye Qiao, Jiexin Duan, Guang Cheng
    * Abstract: Nearest neighbor is a popular class of classification methods with many desirable properties. For a large data set which cannot be loaded into the memory of a single machine due to computation, communication, privacy, or ownership limitations, we consider the divide and conquer scheme: the entire data set is divided into small subsamples, on which nearest neighbor predictions are made, and then a final decision is reached by aggregating the predictions on subsamples by majority voting. We name this method the big Nearest Neighbor (bigNN) classifier, and provide its rates of convergence under minimal assumptions, in terms of both the excess risk and the classification instability, which are proven to be the same rates as the oracle nearest neighbor classifier and cannot be improved. To significantly reduce the prediction time that is required for achieving the optimal rate, we also consider the pre-training acceleration technique applied to the bigNN method, with proven convergence rate. We find that in the distributed setting, the optimal choice of the neighbor k should scale with both the total sample size and the number of partitions, and there is a theoretical upper limit for the latter. Numerical studies have verified the theoretical findings.
count=1
* STREETS: A Novel Camera Network Dataset for Traffic Flow
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/ee389847678a3a9d1ce9e4ca69200d06-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/ee389847678a3a9d1ce9e4ca69200d06-Paper.pdf)]
    * Title: STREETS: A Novel Camera Network Dataset for Traffic Flow
    * Year: `2019`
    * Authors: Corey Snyder, Minh Do
    * Abstract: In this paper, we introduce STREETS, a novel traffic flow dataset from publicly available web cameras in the suburbs of Chicago, IL. We seek to address the limitations of existing datasets in this area. Many such datasets lack a coherent traffic network graph to describe the relationship between sensors. The datasets that do provide a graph depict traffic flow in urban population centers or highway systems and use costly sensors like induction loops. These contexts differ from that of a suburban traffic body. Our dataset provides over 4 million still images across 2.5 months and one hundred web cameras in suburban Lake County, IL. We divide the cameras into two distinct communities described by directed graphs and count vehicles to track traffic statistics. Our goal is to give researchers a benchmark dataset for exploring the capabilities of inexpensive and non-invasive sensors like web cameras to understand complex traffic bodies in communities of any size. We present benchmarking tasks and baseline results for one such task to guide how future work may use our dataset.
count=1
* Constrained deep neural network architecture search for IoT devices accounting for hardware calibration
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/f8037f94e53f17a2cc301033ca86d278-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/f8037f94e53f17a2cc301033ca86d278-Paper.pdf)]
    * Title: Constrained deep neural network architecture search for IoT devices accounting for hardware calibration
    * Year: `2019`
    * Authors: Florian Scheidegger, Luca Benini, Costas Bekas, A. Cristiano I. Malossi
    * Abstract: Deep neural networks achieve outstanding results for challenging image classification tasks. However, the design of network topologies is a complex task, and the research community is conducting ongoing efforts to discover top-accuracy topologies, either manually or by employing expensive architecture searches. We propose a unique narrow-space architecture search that focuses on delivering low-cost and rapidly executing networks that respect strict memory and time requirements typical of Internet-of-Things (IoT) near-sensor computing platforms. Our approach provides solutions with classification latencies below 10~ms running on a low-cost device with 1~GB RAM and a peak performance of 5.6~GFLOPS. The narrow-space search of floating-point models improves the accuracy on CIFAR10 of an established IoT model from 70.64% to 74.87% within the same memory constraints. We further improve the accuracy to 82.07% by including 16-bit half types and obtain the highest accuracy of 83.45% by extending the search with model-optimized IEEE 754 reduced types. To the best of our knowledge, this is the first empirical demonstration of more than 3000 trained models that run with reduced precision and push the Pareto optimal front by a wide margin. Within a given memory constraint, accuracy is improved by more than 7% points for half and more than 1% points for the best individual model format.
count=1
* A Benchmark for Interpretability Methods in Deep Neural Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf)]
    * Title: A Benchmark for Interpretability Methods in Deep Neural Networks
    * Year: `2019`
    * Authors: Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim
    * Abstract: We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches---VarGrad and SmoothGrad-Squared---outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.
count=1
* Nonparametric Bayesian Lomax delegate racing for survival analysis with competing risks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/0fe473396242072e84af286632d3f0ff-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/0fe473396242072e84af286632d3f0ff-Paper.pdf)]
    * Title: Nonparametric Bayesian Lomax delegate racing for survival analysis with competing risks
    * Year: `2018`
    * Authors: Quan Zhang, Mingyuan Zhou
    * Abstract: We propose Lomax delegate racing (LDR) to explicitly model the mechanism of survival under competing risks and to interpret how the covariates accelerate or decelerate the time to event. LDR explains non-monotonic covariate effects by racing a potentially infinite number of sub-risks, and consequently relaxes the ubiquitous proportional-hazards assumption which may be too restrictive. Moreover, LDR is naturally able to model not only censoring, but also missing event times or event types. For inference, we develop a Gibbs sampler under data augmentation for moderately sized data, along with a stochastic gradient descent maximum a posteriori inference algorithm for big data applications. Illustrative experiments are provided on both synthetic and real datasets, and comparison with various benchmark algorithms for survival analysis with competing risks demonstrates distinguished performance of LDR.
count=1
* Learning Deep Disentangled Embeddings With the F-Statistic Loss
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/2b24d495052a8ce66358eb576b8912c8-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf)]
    * Title: Learning Deep Disentangled Embeddings With the F-Statistic Loss
    * Year: `2018`
    * Authors: Karl Ridgeway, Michael C. Mozer
    * Abstract: Deep-embedding methods aim to discover representations of a domain that make explicit the domain's class structure and thereby support few-shot learning. Disentangling methods aim to make explicit compositional or factorial structure. We combine these two active but independent lines of research and propose a new paradigm suitable for both goals. We propose and evaluate a novel loss function based on the $F$ statistic, which describes the separation of two or more distributions. By ensuring that distinct classes are well separated on a subset of embedding dimensions, we obtain embeddings that are useful for few-shot learning. By not requiring separation on all dimensions, we encourage the discovery of disentangled representations. Our embedding method matches or beats state-of-the-art, as evaluated by performance on recall@$k$ and few-shot learning tasks. Our method also obtains performance superior to a variety of alternatives on disentangling, as evaluated by two key properties of a disentangled representation: modularity and explicitness. The goal of our work is to obtain more interpretable, manipulable, and generalizable deep representations of concepts and categories.
count=1
* Confounding-Robust Policy Improvement
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/3a09a524440d44d7f19870070a5ad42f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/3a09a524440d44d7f19870070a5ad42f-Paper.pdf)]
    * Title: Confounding-Robust Policy Improvement
    * Year: `2018`
    * Authors: Nathan Kallus, Angela Zhou
    * Abstract: We study the problem of learning personalized decision policies from observational data while accounting for possible unobserved confounding in the data-generating process. Unlike previous approaches that assume unconfoundedness, i.e., no unobserved confounders affected both treatment assignment and outcomes, we calibrate policy learning for realistic violations of this unverifiable assumption with uncertainty sets motivated by sensitivity analysis in causal inference. Our framework for confounding-robust policy improvement optimizes the minimax regret of a candidate policy against a baseline or reference "status quo" policy, over an uncertainty set around nominal propensity weights. We prove that if the uncertainty set is well-specified, robust policy learning can do no worse than the baseline, and only improve if the data supports it. We characterize the adversarial subproblem and use efficient algorithmic solutions to optimize over parametrized spaces of decision policies such as logistic treatment assignment. We assess our methods on synthetic data and a large clinical trial, demonstrating that confounded selection can hinder policy learning and lead to unwarranted harm, while our robust approach guarantees safety and focuses on well-evidenced improvement.
count=1
* Forecasting Treatment Responses Over Time Using Recurrent Marginal Structural Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/56e6a93212e4482d99c84a639d254b67-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/56e6a93212e4482d99c84a639d254b67-Paper.pdf)]
    * Title: Forecasting Treatment Responses Over Time Using Recurrent Marginal Structural Networks
    * Year: `2018`
    * Authors: Bryan Lim
    * Abstract: Electronic health records provide a rich source of data for machine learning methods to learn dynamic treatment responses over time. However, any direct estimation is hampered by the presence of time-dependent confounding, where actions taken are dependent on time-varying variables related to the outcome of interest. Drawing inspiration from marginal structural models, a class of methods in epidemiology which use propensity weighting to adjust for time-dependent confounders, we introduce the Recurrent Marginal Structural Network - a sequence-to-sequence architecture for forecasting a patient's expected response to a series of planned treatments. Using simulations of a state-of-the-art pharmacokinetic-pharmacodynamic (PK-PD) model of tumor growth, we demonstrate the ability of our network to accurately learn unbiased treatment responses from observational data – even under changes in the policy of treatment assignments – and performance gains over benchmarks.
count=1
* Posterior Concentration for Sparse Deep Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/59b90e1005a220e2ebc542eb9d950b1e-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf)]
    * Title: Posterior Concentration for Sparse Deep Learning
    * Year: `2018`
    * Authors: Nicholas G. Polson, Veronika Ročková
    * Abstract: We introduce Spike-and-Slab Deep Learning (SS-DL), a fully Bayesian alternative to dropout for improving generalizability of deep ReLU networks. This new type of regularization enables provable recovery of smooth input-output maps with {\sl unknown} levels of smoothness. Indeed, we show that the posterior distribution concentrates at the near minimax rate for alpha-Holder smooth maps, performing as well as if we knew the smoothness level alpha ahead of time. Our result sheds light on architecture design for deep neural networks, namely the choice of depth, width and sparsity level. These network attributes typically depend on unknown smoothness in order to be optimal. We obviate this constraint with the fully Bayes construction. As an aside, we show that SS-DL does not overfit in the sense that the posterior concentrates on smaller networks with fewer (up to the optimal number of) nodes and links. Our results provide new theoretical justifications for deep ReLU networks from a Bayesian point of view.
count=1
* Causal Inference via Kernel Deviance Measures
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/73fed7fd472e502d8908794430511f4d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/73fed7fd472e502d8908794430511f4d-Paper.pdf)]
    * Title: Causal Inference via Kernel Deviance Measures
    * Year: `2018`
    * Authors: Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh
    * Abstract: Discovering the causal structure among a set of variables is a fundamental problem in many areas of science. In this paper, we propose Kernel Conditional Deviance for Causal Inference (KCDC) a fully nonparametric causal discovery method based on purely observational data. From a novel interpretation of the notion of asymmetry between cause and effect, we derive a corresponding asymmetry measure using the framework of reproducing kernel Hilbert spaces. Based on this, we propose three decision rules for causal discovery. We demonstrate the wide applicability and robustness of our method across a range of diverse synthetic datasets. Furthermore, we test our method on real-world time series data and the real-world benchmark dataset Tübingen Cause-Effect Pairs where we outperform state-of-the-art approaches.
count=1
* Neural Interaction Transparency (NIT): Disentangling Learned Interactions for Improved Interpretability
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/74378afe5e8b20910cf1f939e57f0480-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/74378afe5e8b20910cf1f939e57f0480-Paper.pdf)]
    * Title: Neural Interaction Transparency (NIT): Disentangling Learned Interactions for Improved Interpretability
    * Year: `2018`
    * Authors: Michael Tsang, Hanpeng Liu, Sanjay Purushotham, Pavankumar Murali, Yan Liu
    * Abstract: Neural networks are known to model statistical interactions, but they entangle the interactions at intermediate hidden layers for shared representation learning. We propose a framework, Neural Interaction Transparency (NIT), that disentangles the shared learning across different interactions to obtain their intrinsic lower-order and interpretable structure. This is done through a novel regularizer that directly penalizes interaction order. We show that disentangling interactions reduces a feedforward neural network to a generalized additive model with interactions, which can lead to transparent models that perform comparably to the state-of-the-art models. NIT is also flexible and efficient; it can learn generalized additive models with maximum $K$-order interactions by training only $O(1)$ models.
count=1
* SplineNets: Continuous Neural Decision Graphs
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/9246444d94f081e3549803b928260f56-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/9246444d94f081e3549803b928260f56-Paper.pdf)]
    * Title: SplineNets: Continuous Neural Decision Graphs
    * Year: `2018`
    * Authors: Cem Keskin, Shahram Izadi
    * Abstract: We present SplineNets, a practical and novel approach for using conditioning in convolutional neural networks (CNNs). SplineNets are continuous generalizations of neural decision graphs, and they can dramatically reduce runtime complexity and computation costs of CNNs, while maintaining or even increasing accuracy. Functions of SplineNets are both dynamic (i.e., conditioned on the input) and hierarchical (i.e.,conditioned on the computational path). SplineNets employ a unified loss function with a desired level of smoothness over both the network and decision parameters, while allowing for sparse activation of a subset of nodes for individual samples. In particular, we embed infinitely many function weights (e.g. filters) on smooth, low dimensional manifolds parameterized by compact B-splines, which are indexed by a position parameter. Instead of sampling from a categorical distribution to pick a branch, samples choose a continuous position to pick a function weight. We further show that by maximizing the mutual information between spline positions and class labels, the network can be optimally utilized and specialized for classification tasks. Experiments show that our approach can significantly increase the accuracy of ResNets with negligible cost in speed, matching the precision of a 110 level ResNet with a 32 level SplineNet.
count=1
* Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/c4616f5a24a66668f11ca4fa80525dc4-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/c4616f5a24a66668f11ca4fa80525dc4-Paper.pdf)]
    * Title: Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization
    * Year: `2018`
    * Authors: Bruno Korbar, Du Tran, Lorenzo Torresani
    * Abstract: There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective models for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further fine-tuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +19.9% in action recognition accuracy on UCF101 and a boost of +17.7% on HMDB51.
count=1
* Improved Expressivity Through Dendritic Neural Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/e32c51ad39723ee92b285b362c916ca7-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/e32c51ad39723ee92b285b362c916ca7-Paper.pdf)]
    * Title: Improved Expressivity Through Dendritic Neural Networks
    * Year: `2018`
    * Authors: Xundong Wu, Xiangwen Liu, Wei Li, Qing Wu
    * Abstract: A typical biological neuron, such as a pyramidal neuron of the neocortex, receives thousands of afferent synaptic inputs on its dendrite tree and sends the efferent axonal output downstream. In typical artificial neural networks, dendrite trees are modeled as linear structures that funnel weighted synaptic inputs to the cell bodies. However, numerous experimental and theoretical studies have shown that dendritic arbors are far more than simple linear accumulators. That is, synaptic inputs can actively modulate their neighboring synaptic activities; therefore, the dendritic structures are highly nonlinear. In this study, we model such local nonlinearity of dendritic trees with our dendritic neural network (DENN) structure and apply this structure to typical machine learning tasks. Equipped with localized nonlinearities, DENNs can attain greater model expressivity than regular neural networks while maintaining efficient network inference. Such strength is evidenced by the increased fitting power when we train DENNs with supervised machine learning tasks. We also empirically show that the locality structure can improve the generalization performance of DENNs, as exemplified by DENNs outranking naive deep neural network architectures when tested on 121 classification tasks from the UCI machine learning repository.
count=1
* Proximal Graphical Event Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/hash/f1ababf130ee6a25f12da7478af8f1ac-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2018/file/f1ababf130ee6a25f12da7478af8f1ac-Paper.pdf)]
    * Title: Proximal Graphical Event Models
    * Year: `2018`
    * Authors: Debarun Bhattacharjya, Dharmashankar Subramanian, Tian Gao
    * Abstract: Event datasets include events that occur irregularly over the timeline and are prevalent in numerous domains. We introduce proximal graphical event models (PGEM) as a representation of such datasets. PGEMs belong to a broader family of models that characterize relationships between various types of events, where the rate of occurrence of an event type depends only on whether or not its parents have occurred in the most recent history. The main advantage over the state of the art models is that they are entirely data driven and do not require additional inputs from the user, which can require knowledge of the domain such as choice of basis functions or hyperparameters in graphical event models. We theoretically justify our learning of optimal windows for parental history and the choices of parental sets, and the algorithm are sound and complete in terms of parent structure learning. We present additional efficient heuristics for learning PGEMs from data, demonstrating their effectiveness on synthetic and real datasets.
count=1
* Causal Effect Inference with Deep Latent-Variable Models
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/94b5bde6de888ddf9cde6748ad2523d1-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/94b5bde6de888ddf9cde6748ad2523d1-Paper.pdf)]
    * Title: Causal Effect Inference with Deep Latent-Variable Models
    * Year: `2017`
    * Authors: Christos Louizos, Uri Shalit, Joris M. Mooij, David Sontag, Richard Zemel, Max Welling
    * Abstract: Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.
count=1
* Deep Recurrent Neural Network-Based Identification of Precursor microRNAs
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/b2531e7bb29bf22e1daae486fae3417a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/b2531e7bb29bf22e1daae486fae3417a-Paper.pdf)]
    * Title: Deep Recurrent Neural Network-Based Identification of Precursor microRNAs
    * Year: `2017`
    * Authors: Seunghyun Park, Seonwoo Min, Hyun-Soo Choi, Sungroh Yoon
    * Abstract: MicroRNAs (miRNAs) are small non-coding ribonucleic acids (RNAs) which play key roles in post-transcriptional gene regulation. Direct identification of mature miRNAs is infeasible due to their short lengths, and researchers instead aim at identifying precursor miRNAs (pre-miRNAs). Many of the known pre-miRNAs have distinctive stem-loop secondary structure, and structure-based filtering is usually the first step to predict the possibility of a given sequence being a pre-miRNA. To identify new pre-miRNAs that often have non-canonical structure, however, we need to consider additional features other than structure. To obtain such additional characteristics, existing computational methods rely on manual feature extraction, which inevitably limits the efficiency, robustness, and generalization of computational identification. To address the limitations of existing approaches, we propose a pre-miRNA identification method that incorporates (1) a deep recurrent neural network (RNN) for automated feature learning and classification, (2) multimodal architecture for seamless integration of prior knowledge (secondary structure), (3) an attention mechanism for improving long-term dependence modeling, and (4) an RNN-based class activation mapping for highlighting the learned representations that can contrast pre-miRNAs and non-pre-miRNAs. In our experiments with recent benchmarks, the proposed approach outperformed the compared state-of-the-art alternatives in terms of various performance metrics.
count=1
* Matching on Balanced Nonlinear Representations for Treatment Effects Estimation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/b2eeb7362ef83deff5c7813a67e14f0a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf)]
    * Title: Matching on Balanced Nonlinear Representations for Treatment Effects Estimation
    * Year: `2017`
    * Authors: Sheng Li, Yun Fu
    * Abstract: Estimating treatment effects from observational data is challenging due to the missing counterfactuals. Matching is an effective strategy to tackle this problem. The widely used matching estimators such as nearest neighbor matching (NNM) pair the treated units with the most similar control units in terms of covariates, and then estimate treatment effects accordingly. However, the existing matching estimators have poor performance when the distributions of control and treatment groups are unbalanced. Moreover, theoretical analysis suggests that the bias of causal effect estimation would increase with the dimension of covariates. In this paper, we aim to address these problems by learning low-dimensional balanced and nonlinear representations (BNR) for observational data. In particular, we convert counterfactual prediction as a classification problem, develop a kernel learning model with domain adaptation constraint, and design a novel matching estimator. The dimension of covariates will be significantly reduced after projecting data to a low-dimensional subspace. Experiments on several synthetic and real-world datasets demonstrate the effectiveness of our approach.
count=1
* PRUNE: Preserving Proximity and Global Ranking for Network Embedding
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/cdd96eedd7f695f4d61802f8105ba2b0-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/cdd96eedd7f695f4d61802f8105ba2b0-Paper.pdf)]
    * Title: PRUNE: Preserving Proximity and Global Ranking for Network Embedding
    * Year: `2017`
    * Authors: Yi-An Lai, Chin-Chi Hsu, Wen Hao Chen, Mi-Yen Yeh, Shou-De Lin
    * Abstract: We investigate an unsupervised generative approach for network embedding. A multi-task Siamese neural network structure is formulated to connect embedding vectors and our objective to preserve the global node ranking and local proximity of nodes. We provide deeper analysis to connect the proposed proximity objective to link prediction and community detection in the network. We show our model can satisfy the following design properties: scalability, asymmetry, unity and simplicity. Experiment results not only verify the above design properties but also demonstrate the superior performance in learning-to-rank, classification, regression, and link prediction tasks.
count=1
* Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/df0aab058ce179e4f7ab135ed4e641a9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/df0aab058ce179e4f7ab135ed4e641a9-Paper.pdf)]
    * Title: Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search
    * Year: `2017`
    * Authors: Luigi Acerbi, Wei Ji Ma
    * Abstract: Computational models in fields such as computational neuroscience are often evaluated via stochastic simulation or numerical approximation. Fitting these models implies a difficult optimization problem over complex, possibly noisy parameter landscapes. Bayesian optimization (BO) has been successfully applied to solving expensive black-box problems in engineering and machine learning. Here we explore whether BO can be applied as a general tool for model fitting. First, we present a novel hybrid BO algorithm, Bayesian adaptive direct search (BADS), that achieves competitive performance with an affordable computational overhead for the running time of typical models. We then perform an extensive benchmark of BADS vs. many common and state-of-the-art nonconvex, derivative-free optimizers, on a set of model-fitting problems with real data and models from six studies in behavioral, cognitive, and computational neuroscience. With default settings, BADS consistently finds comparable or better solutions than other methods, including `vanilla' BO, showing great promise for advanced BO techniques, and BADS in particular, as a general model-fitting tool.
count=1
* Bayesian Dyadic Trees and Histograms for  Regression
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/e00406144c1e7e35240afed70f34166a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/e00406144c1e7e35240afed70f34166a-Paper.pdf)]
    * Title: Bayesian Dyadic Trees and Histograms for  Regression
    * Year: `2017`
    * Authors: Stéphanie van der Pas, Veronika Ročková
    * Abstract: Many machine learning tools for regression are based on recursive partitioning of the covariate space into smaller regions, where the regression function can be estimated locally. Among these, regression trees and their ensembles have demonstrated impressive empirical performance. In this work, we shed light on the machinery behind Bayesian variants of these methods. In particular, we study Bayesian regression histograms, such as Bayesian dyadic trees, in the simple regression case with just one predictor. We focus on the reconstruction of regression surfaces that are piecewise constant, where the number of jumps is unknown. We show that with suitably designed priors, posterior distributions concentrate around the true step regression function at a near-minimax rate. These results {\sl do not} require the knowledge of the true number of steps, nor the width of the true partitioning cells. Thus, Bayesian dyadic regression trees are fully adaptive and can recover the true piecewise regression function nearly as well as if we knew the exact number and location of jumps. Our results constitute the first step towards understanding why Bayesian trees and their ensembles have worked so well in practice. As an aside, we discuss prior distributions on balanced interval partitions and how they relate to an old problem in geometric probability. Namely, we relate the probability of covering the circumference of a circle with random arcs whose endpoints are confined to a grid, a new variant of the original problem.
count=1
* Incorporating Side Information by Adaptive Convolution
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/hash/e7e23670481ac78b3c4122a99ba60573-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2017/file/e7e23670481ac78b3c4122a99ba60573-Paper.pdf)]
    * Title: Incorporating Side Information by Adaptive Convolution
    * Year: `2017`
    * Authors: Di Kang, Debarun Dhar, Antoni Chan
    * Abstract: Computer vision tasks often have side information available that is helpful to solve the task. For example, for crowd counting, the camera perspective (e.g., camera angle and height) gives a clue about the appearance and scale of people in the scene. While side information has been shown to be useful for counting systems using traditional hand-crafted features, it has not been fully utilized in counting systems based on deep learning. In order to incorporate the available side information, we propose an adaptive convolutional neural network (ACNN), where the convolution filter weights adapt to the current scene context via the side information. In particular, we model the filter weights as a low-dimensional manifold within the high-dimensional space of filter weights. The filter weights are generated using a learned ``filter manifold'' sub-network, whose input is the side information. With the help of side information and adaptive weights, the ACNN can disentangle the variations related to the side information, and extract discriminative features related to the current context (e.g. camera perspective, noise level, blur kernel parameters). We demonstrate the effectiveness of ACNN incorporating side information on 3 tasks: crowd counting, corrupted digit recognition, and image deblurring. Our experiments show that ACNN improves the performance compared to a plain CNN with a similar number of parameters. Since existing crowd counting datasets do not contain ground-truth side information, we collect a new dataset with the ground-truth camera angle and height as the side information.
count=1
* A Non-parametric Learning Method for Confidently Estimating Patient's Clinical State and Dynamics
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/062ddb6c727310e76b6200b7c71f63b5-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/062ddb6c727310e76b6200b7c71f63b5-Paper.pdf)]
    * Title: A Non-parametric Learning Method for Confidently Estimating Patient's Clinical State and Dynamics
    * Year: `2016`
    * Authors: William Hoiles, Mihaela van der Schaar
    * Abstract: Estimating patient's clinical state from multiple concurrent physiological streams plays an important role in determining if a therapeutic intervention is necessary and for triaging patients in the hospital. In this paper we construct a non-parametric learning algorithm to estimate the clinical state of a patient. The algorithm addresses several known challenges with clinical state estimation such as eliminating bias introduced by therapeutic intervention censoring, increasing the timeliness of state estimation while ensuring a sufficient accuracy, and the ability to detect anomalous clinical states. These benefits are obtained by combining the tools of non-parametric Bayesian inference, permutation testing, and generalizations of the empirical Bernstein inequality. The algorithm is validated using real-world data from a cancer ward in a large academic hospital.
count=1
* Bayesian Optimization for Probabilistic Programs
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/31fefc0e570cb3860f2a6d4b38c6490d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf)]
    * Title: Bayesian Optimization for Probabilistic Programs
    * Year: `2016`
    * Authors: Tom Rainforth, Tuan Anh Le, Jan-Willem van de Meent, Michael A. Osborne, Frank Wood
    * Abstract: We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages. We present applications of our method to a number of tasks including engineering design and parameter optimization.
count=1
* Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/3fab5890d8113d0b5a4178201dc842ad-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/3fab5890d8113d0b5a4178201dc842ad-Paper.pdf)]
    * Title: Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes
    * Year: `2016`
    * Authors: Jack Rae, Jonathan J. Hunt, Ivo Danihelka, Timothy Harley, Andrew W. Senior, Gregory Wayne, Alex Graves, Timothy Lillicrap
    * Abstract: Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs $1,\!000\times$ faster and with $3,\!000\times$ less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring $100,\!000$s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.
count=1
* A Probabilistic Programming Approach To Probabilistic Data Analysis
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/46072631582fc240dd2674a7d063b040-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/46072631582fc240dd2674a7d063b040-Paper.pdf)]
    * Title: A Probabilistic Programming Approach To Probabilistic Data Analysis
    * Year: `2016`
    * Authors: Feras Saad, Vikash K. Mansinghka
    * Abstract: Probabilistic techniques are central to data analysis, but different approaches can be challenging to apply, combine, and compare. This paper introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include discriminative machine learning, hierarchical Bayesian models, multivariate kernel methods, clustering algorithms, and arbitrary probabilistic programs. We demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling definition language and structured query language. The practical value is illustrated in two ways. First, the paper describes an analysis on a database of Earth satellites, which identifies records that probably violate Kepler’s Third Law by composing causal probabilistic programs with non-parametric Bayes in 50 lines of probabilistic code. Second, it reports the lines of code and accuracy of CGPMs compared with baseline solutions from standard machine learning libraries.
count=1
* SoundNet: Learning Sound Representations from Unlabeled Video
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/7dcd340d84f762eba80aa538b0c527f7-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/7dcd340d84f762eba80aa538b0c527f7-Paper.pdf)]
    * Title: SoundNet: Learning Sound Representations from Unlabeled Video
    * Year: `2016`
    * Authors: Yusuf Aytar, Carl Vondrick, Antonio Torralba
    * Abstract: We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.
count=1
* Bayesian Optimization with Robust Bayesian Neural Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/a96d3afec184766bfeca7a9f989fc7e7-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/a96d3afec184766bfeca7a9f989fc7e7-Paper.pdf)]
    * Title: Bayesian Optimization with Robust Bayesian Neural Networks
    * Year: `2016`
    * Authors: Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, Frank Hutter
    * Abstract: Bayesian optimization is a prominent method for optimizing expensive to evaluate black-box functions that is prominently applied to tuning the hyperparameters of machine learning algorithms. Despite its successes, the prototypical Bayesian optimization approach - using Gaussian process models - does not scale well to either many hyperparameters or many function evaluations. Attacking this lack of scalability and flexibility is thus one of the key challenges of the field. We present a general approach for using flexible parametric models (neural networks) for Bayesian optimization, staying as close to a truly Bayesian treatment as possible. We obtain scalability through stochastic gradient Hamiltonian Monte Carlo, whose robustness we improve via a scale adaptation. Experiments including multi-task Bayesian optimization with 21 tasks, parallel optimization of deep neural networks and deep reinforcement learning show the power and flexibility of this approach.
count=1
* A Probabilistic Framework for Deep Learning
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/hash/c70daf247944fe3add32218f914c75a6-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2016/file/c70daf247944fe3add32218f914c75a6-Paper.pdf)]
    * Title: A Probabilistic Framework for Deep Learning
    * Year: `2016`
    * Authors: Ankit B. Patel, Minh Tan Nguyen, Richard Baraniuk
    * Abstract: We develop a probabilistic framework for deep learning based on the Deep Rendering Mixture Model (DRMM), a new generative probabilistic model that explicitly capture variations in data due to latent task nuisance variables. We demonstrate that max-sum inference in the DRMM yields an algorithm that exactly reproduces the operations in deep convolutional neural networks (DCNs), providing a first principles derivation. Our framework provides new insights into the successes and shortcomings of DCNs as well as a principled route to their improvement. DRMM training via the Expectation-Maximization (EM) algorithm is a powerful alternative to DCN back-propagation, and initial training results are promising. Classification based on the DRMM and other variants outperforms DCNs in supervised digit classification, training 2-3x faster while achieving similar accuracy. Moreover, the DRMM is applicable to semi-supervised and unsupervised learning tasks, achieving results that are state-of-the-art in several categories on the MNIST benchmark and comparable to state of the art on the CIFAR10 benchmark.
count=1
* Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/26657d5ff9020d2abefe558796b99584-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/26657d5ff9020d2abefe558796b99584-Paper.pdf)]
    * Title: Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
    * Year: `2015`
    * Authors: Armand Joulin, Tomas Mikolov
    * Abstract: Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.
count=1
* The Return of the Gating Network: Combining Generative Models and Discriminative Training in Natural Image Priors
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/596f713f9a7376fe90a62abaaedecc2d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/596f713f9a7376fe90a62abaaedecc2d-Paper.pdf)]
    * Title: The Return of the Gating Network: Combining Generative Models and Discriminative Training in Natural Image Priors
    * Year: `2015`
    * Authors: Dan Rosenbaum, Yair Weiss
    * Abstract: In recent years, approaches based on machine learning have achieved state-of-the-art performance on image restoration problems. Successful approaches include both generative models of natural images as well as discriminative training of deep neural networks. Discriminative training of feed forward architectures allows explicit control over the computational cost of performing restoration and therefore often leads to better performance at the same cost at run time. In contrast, generative models have the advantage that they can be trained once and then adapted to any image restoration task by a simple use of Bayes' rule. In this paper we show how to combine the strengths of both approaches by training a discriminative, feed-forward architecture to predict the state of latent variables in a generative model of natural images. We apply this idea to the very successful Gaussian Mixture Model (GMM) of natural images. We show that it is possible to achieve comparable performance as the original GMM but with two orders of magnitude improvement in run time while maintaining the advantage of generative models.
count=1
* 3D Object Proposals for Accurate Object Class Detection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/6da37dd3139aa4d9aa55b8d237ec5d4a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/6da37dd3139aa4d9aa55b8d237ec5d4a-Paper.pdf)]
    * Title: 3D Object Proposals for Accurate Object Class Detection
    * Year: `2015`
    * Authors: Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Andrew G. Berneshawi, Huimin Ma, Sanja Fidler, Raquel Urtasun
    * Abstract: The goal of this paper is to generate high-quality 3D object proposals in the context of autonomous driving. Our method exploits stereo imagery to place proposals in the form of 3D bounding boxes. We formulate the problem as minimizing an energy function encoding object size priors, ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. Combined with convolutional neural net (CNN) scoring, our approach outperforms all existing results on all three KITTI object classes.
count=1
* Deep Knowledge Tracing
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/bac9162b47c56fc8a4d2a519803d51b3-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/bac9162b47c56fc8a4d2a519803d51b3-Paper.pdf)]
    * Title: Deep Knowledge Tracing
    * Year: `2015`
    * Authors: Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J. Guibas, Jascha Sohl-Dickstein
    * Abstract: Knowledge tracing, where a machine models the knowledge of a student as they interact with coursework, is an established and significantly unsolved problem in computer supported education.In this paper we explore the benefit of using recurrent neural networks to model student learning.This family of models have important advantages over current state of the art methods in that they do not require the explicit encoding of human domain knowledge,and have a far more flexible functional form which can capture substantially more complex student interactions.We show that these neural networks outperform the current state of the art in prediction on real student data,while allowing straightforward interpretation and discovery of structure in the curriculum.These results suggest a promising new line of research for knowledge tracing.
count=1
* Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/d43ab110ab2489d6b9b2caa394bf920f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/d43ab110ab2489d6b9b2caa394bf920f-Paper.pdf)]
    * Title: Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation
    * Year: `2015`
    * Authors: Marijn F. Stollenga, Wonmin Byeon, Marcus Liwicki, Jürgen Schmidhuber
    * Abstract: Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelise on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelise, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).
count=1
* Logarithmic Time Online Multiclass prediction
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/e369853df766fa44e1ed0ff613f563bd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/e369853df766fa44e1ed0ff613f563bd-Paper.pdf)]
    * Title: Logarithmic Time Online Multiclass prediction
    * Year: `2015`
    * Authors: Anna E. Choromanska, John Langford
    * Abstract: We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.
count=1
* Convolutional Networks on Graphs for Learning Molecular Fingerprints
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2015/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf)]
    * Title: Convolutional Networks on Graphs for Learning Molecular Fingerprints
    * Year: `2015`
    * Authors: David K. Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, Ryan P. Adams
    * Abstract: We introduce a convolutional neural network that operates directly on graphs.These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape.The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints.We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.
count=1
* Ranking via Robust Binary Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/3d779cae2d46cf6a8a99a35ba4167977-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/3d779cae2d46cf6a8a99a35ba4167977-Paper.pdf)]
    * Title: Ranking via Robust Binary Classification
    * Year: `2014`
    * Authors: Hyokun Yun, Parameswaran Raman, S. Vishwanathan
    * Abstract: We propose RoBiRank, a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification. The algorithm shows a very competitive performance on standard benchmark datasets against other representative algorithms in the literature. Further, in large scale problems where explicit feature vectors and scores are not given, our algorithm can be efficiently parallelized across a large number of machines; for a task that requires 386,133 x 49,824,519 pairwise interactions between items to be ranked, our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation.
count=1
* Learning with Pseudo-Ensembles
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/66be31e4c40d676991f2405aaecc6934-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/66be31e4c40d676991f2405aaecc6934-Paper.pdf)]
    * Title: Learning with Pseudo-Ensembles
    * Year: `2014`
    * Authors: Philip Bachman, Ouais Alsharif, Doina Precup
    * Abstract: We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.
count=1
* Improved Multimodal Deep Learning with Variation of Information
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/801c14f07f9724229175b8ef8b4585a8-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/801c14f07f9724229175b8ef8b4585a8-Paper.pdf)]
    * Title: Improved Multimodal Deep Learning with Variation of Information
    * Year: `2014`
    * Authors: Kihyuk Sohn, Wenling Shang, Honglak Lee
    * Abstract: Deep learning has been successfully applied to multimodal representation learning problems, with a common strategy to learning joint representations that are shared across multiple modalities on top of layers of modality-specific networks. Nonetheless, there still remains a question how to learn a good association between data modalities; in particular, a good generative model of multimodal data should be able to reason about missing data modality given the rest of data modalities. In this paper, we propose a novel multimodal representation learning framework that explicitly aims this goal. Rather than learning with maximum likelihood, we train the model to minimize the variation of information. We provide a theoretical insight why the proposed learning objective is sufficient to estimate the data-generating joint distribution of multimodal data. We apply our method to restricted Boltzmann machines and introduce learning methods based on contrastive divergence and multi-prediction training. In addition, we extend to deep networks with recurrent encoding structure to finetune the whole network. In experiments, we demonstrate the state-of-the-art visual recognition performance on MIR-Flickr database and PASCAL VOC 2007 database with and without text features.
count=1
* Self-Paced Learning with Diversity
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/c60d060b946d6dd6145dcbad5c4ccf6f-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/c60d060b946d6dd6145dcbad5c4ccf6f-Paper.pdf)]
    * Title: Self-Paced Learning with Diversity
    * Year: `2014`
    * Authors: Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, Alexander Hauptmann
    * Abstract: Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which formalizes the preference for both easy and diverse samples into a general regularizer. This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method significantly outperforms the conventional SPL on three real-world datasets. Specifically, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.
count=1
* A Representation Theory for Ranking Functions
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/hash/d96409bf894217686ba124d7356686c9-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2014/file/d96409bf894217686ba124d7356686c9-Paper.pdf)]
    * Title: A Representation Theory for Ranking Functions
    * Year: `2014`
    * Authors: Harsh H. Pareek, Pradeep K. Ravikumar
    * Abstract: This paper presents a representation theory for permutation-valued functions, which in their general form can also be called listwise ranking functions. Pointwise ranking functions assign a score to each object independently, without taking into account the other objects under consideration; whereas listwise loss functions evaluate the set of scores assigned to all objects as a whole. In many supervised learning to rank tasks, it might be of interest to use listwise ranking functions instead; in particular, the Bayes Optimal ranking functions might themselves be listwise, especially if the loss function is listwise. A key caveat to using listwise ranking functions has been the lack of an appropriate representation theory for such functions. We show that a natural symmetricity assumption that we call exchangeability allows us to explicitly characterize the set of such exchangeable listwise ranking functions. Our analysis draws from the theories of tensor analysis, functional analysis and De Finetti theorems. We also present experiments using a novel reranking method motivated by our representation theory.
count=1
* Learning to Pass Expectation Propagation Messages
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/hash/1714726c817af50457d810aae9d27a2e-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/file/1714726c817af50457d810aae9d27a2e-Paper.pdf)]
    * Title: Learning to Pass Expectation Propagation Messages
    * Year: `2013`
    * Authors: Nicolas Heess, Daniel Tarlow, John Winn
    * Abstract: Expectation Propagation (EP) is a popular approximate posterior inference algorithm that often provides a fast and accurate alternative to sampling-based methods. However, while the EP framework in theory allows for complex non-Gaussian factors, there is still a significant practical barrier to using them within EP, because doing so requires the implementation of message update operators, which can be difficult and require hand-crafted approximations. In this work, we study the question of whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model e.g., a neural network or random forest) to map EP message inputs to EP message outputs. We address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising.
count=1
* Decision Jungles: Compact and Rich Models for Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/hash/69adc1e107f7f7d035d7baf04342e1ca-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/file/69adc1e107f7f7d035d7baf04342e1ca-Paper.pdf)]
    * Title: Decision Jungles: Compact and Rich Models for Classification
    * Year: `2013`
    * Authors: Jamie Shotton, Toby Sharp, Pushmeet Kohli, Sebastian Nowozin, John Winn, Antonio Criminisi
    * Abstract: Randomized decision trees and forests have a rich history in machine learning and have seen considerable success in application, perhaps particularly so for computer vision. However, they face a fundamental limitation: given enough data, the number of nodes in decision trees will grow exponentially with depth. For certain applications, for example on mobile or embedded processors, memory is a limited resource, and so the exponential growth of trees limits their depth, and thus their potential accuracy. This paper proposes decision jungles, revisiting the idea of ensembles of rooted decision directed acyclic graphs (DAGs), and shows these to be compact and powerful discriminative models for classification. Unlike conventional decision trees that only allow one path to every node, a DAG in a decision jungle allows multiple paths from the root to each leaf. We present and compare two new node merging algorithms that jointly optimize both the features and the structure of the DAGs efficiently. During training, node splitting and node merging are driven by the minimization of exactly the same objective function, here the weighted sum of entropies at the leaves. Results on varied datasets show that, compared to decision forests and several other baselines, decision jungles require dramatically less memory while considerably improving generalization.
count=1
* On Flat versus Hierarchical Classification in Large-Scale Taxonomies
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/hash/cbb6a3b884f4f88b3a8e3d44c636cbd8-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf)]
    * Title: On Flat versus Hierarchical Classification in Large-Scale Taxonomies
    * Year: `2013`
    * Authors: Rohit Babbar, Ioannis Partalas, Eric Gaussier, Massih R. Amini
    * Abstract: We study in this paper flat and hierarchical classification strategies in the context of large-scale taxonomies. To this end, we first propose a multiclass, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. We then introduce another type of bounds targeting the approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune (or flatten) in a large-scale taxonomy. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.
count=1
* Rapid Distance-Based Outlier Detection via Sampling
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/hash/d296c101daa88a51f6ca8cfc1ac79b50-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/file/d296c101daa88a51f6ca8cfc1ac79b50-Paper.pdf)]
    * Title: Rapid Distance-Based Outlier Detection via Sampling
    * Year: `2013`
    * Authors: Mahito Sugiyama, Karsten Borgwardt
    * Abstract: Distance-based approaches to outlier detection are popular in data mining, as they do not require to model the underlying probability distribution, which is particularly challenging for high-dimensional data. We present an empirical comparison of various approaches to distance-based outlier detection across a large number of datasets. We report the surprising observation that a simple, sampling-based scheme outperforms state-of-the-art techniques in terms of both efficiency and effectiveness. To better understand this phenomenon, we provide a theoretical analysis why the sampling-based approach outperforms alternative methods based on k-nearest neighbor search.
count=1
* Structured Learning via Logistic Regression
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/hash/d9fc5b73a8d78fad3d6dffe419384e70-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2013/file/d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf)]
    * Title: Structured Learning via Logistic Regression
    * Year: `2013`
    * Authors: Justin Domke
    * Abstract: A successful approach to structured learning is to write the learning objective as a joint function of linear parameters and inference messages, and iterate between updates to each. This paper observes that if the inference problem is “smoothed” through the addition of entropy terms, for fixed messages, the learning objective reduces to a traditional (non-structured) logistic regression problem with respect to parameters. In these logistic regression problems, each training example has a bias term determined by the current set of messages. Based on this insight, the structured energy function can be extended from linear factors to any function class where an “oracle” exists to minimize a logistic loss.
count=1
* Practical Bayesian Optimization of Machine Learning Algorithms
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf)]
    * Title: Practical Bayesian Optimization of Machine Learning Algorithms
    * Year: `2012`
    * Authors: Jasper Snoek, Hugo Larochelle, Ryan P. Adams
    * Abstract: The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including Latent Dirichlet Allocation, Structured SVMs and convolutional neural networks.
count=1
* Deep Learning of Invariant Features via Simulated Fixations in Video
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/13d7dc096493e1f77fb4ccf3eaf79df1-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/13d7dc096493e1f77fb4ccf3eaf79df1-Paper.pdf)]
    * Title: Deep Learning of Invariant Features via Simulated Fixations in Video
    * Year: `2012`
    * Authors: Will Zou, Shenghuo Zhu, Kai Yu, Andrew Ng
    * Abstract: We apply salient feature detection and tracking in videos to simulate ﬁxations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classiﬁcation accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset.
count=1
* Discriminatively Trained Sparse Code Gradients for Contour Detection
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/16a5cdae362b8d27a1d8f8c7b78b4330-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/16a5cdae362b8d27a1d8f8c7b78b4330-Paper.pdf)]
    * Title: Discriminatively Trained Sparse Code Gradients for Contour Detection
    * Year: `2012`
    * Authors: Ren Xiaofeng, Liefeng Bo
    * Abstract: Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most existing approaches including the state-of-the-art Global Pb (gPb) operator. In this work, we show that contour detection accuracy can be significantly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding. We use K-SVD and Orthogonal Matching Pursuit for efficient dictionary learning and encoding, and use multi-scale pooling and power transforms to code oriented local neighborhoods before computing gradients and applying linear SVM. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and find contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours). Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth images and surface normals lead to promising contour detection using depth and depth+color, as verified on the NYU Depth Dataset. Our work combines the concept of oriented gradients with sparse representation and opens up future possibilities for learning contour detection and segmentation.
count=1
* Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/459a4ddcb586f24efd9395aa7662bc7c-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/459a4ddcb586f24efd9395aa7662bc7c-Paper.pdf)]
    * Title: Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images
    * Year: `2012`
    * Authors: Dan Ciresan, Alessandro Giusti, Luca Gambardella, Jürgen Schmidhuber
    * Abstract: We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment {\em biological} neuron membranes, we use a special type of deep {\em artificial} neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a $512 \times 512 \times 30$ stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. \emph{rand error}, \emph{warping error} and \emph{pixel error}. For pixel error, our approach is the only one outperforming a second human observer.
count=1
* Multimodal Learning with Deep Boltzmann Machines
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf)]
    * Title: Multimodal Learning with Deep Boltzmann Machines
    * Year: `2012`
    * Authors: Nitish Srivastava, Russ R. Salakhutdinov
    * Abstract: We propose a Deep Boltzmann Machine for learning a generative model of multimodal data. We show how to use the model to extract a meaningful representation of multimodal data. We find that the learned representation is useful for classification and information retreival tasks, and hence conforms to some notion of semantic similarity. The model defines a probability density over the space of multimodal inputs. By sampling from the conditional distributions over each data modality, it possible to create the representation even when some data modalities are missing. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that our model can significantly outperform SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves significant gains.
count=1
* Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/c0c7c76d30bd3dcaefc96f40275bdc0a-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Paper.pdf)]
    * Title: Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction
    * Year: `2012`
    * Authors: Christoph H. Lampert
    * Abstract: We study the problem of maximum marginal prediction (MMP) in probabilistic graphical models, a task that occurs, for example, as the Bayes optimal decision rule under a Hamming loss. MMP is typically performed as a two-stage procedure: one estimates each variable's marginal probability and then forms a prediction from the states of maximal probability. In this work we propose a simple yet effective technique for accelerating MMP when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable. This allows us to identify the point of time when we are sufficiently certain about any individual decision. Whenever this is the case, we dynamically prune the variable we are confident about from the underlying factor graph. Consequently, at any time only samples of variable whose decision is still uncertain need to be created. Experiments in two prototypical scenarios, multi-label classification and image inpainting, shows that adaptive sampling can drastically accelerate MMP without sacrificing prediction accuracy.
count=1
* Locally Uniform Comparison Image Descriptor
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/c20ad4d76fe97759aa27a0c99bff6710-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/c20ad4d76fe97759aa27a0c99bff6710-Paper.pdf)]
    * Title: Locally Uniform Comparison Image Descriptor
    * Year: `2012`
    * Authors: Andrew Ziegler, Eric Christiansen, David Kriegman, Serge Belongie
    * Abstract: Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. For real-time mobile applications, very fast but less accurate descriptors like BRIEF and related methods use a random sampling of pairwise comparisons of pixel intensities in an image patch. Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a simple description method based on permutation distances between the ordering of intensities of RGB values between two patches. LUCID is computable in linear time with respect to patch size and does not require floating point computation. An analysis reveals an underlying issue that limits the potential of BRIEF and related approaches compared to LUCID. Experiments demonstrate that LUCID is faster than BRIEF, and its accuracy is directly comparable to SURF while being more than an order of magnitude faster.
count=1
* ImageNet Classification with Deep Convolutional Neural Networks
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)]
    * Title: ImageNet Classification with Deep Convolutional Neural Networks
    * Year: `2012`
    * Authors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton
    * Abstract: We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\% and 18.9\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.
count=1
* Learning with Target Prior
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/c9f95a0a5af052bffce5c89917335f67-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/c9f95a0a5af052bffce5c89917335f67-Paper.pdf)]
    * Title: Learning with Target Prior
    * Year: `2012`
    * Authors: Zuoguan Wang, Siwei Lyu, Gerwin Schalk, Qiang Ji
    * Abstract: In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables $\y$ can be modeled with a prior model $p(\y)$ and the relations between data and target variables are estimated through $p(\y)$ and a set of uncorresponded data $\x$ in training. We term this method as learning with target priors (LTP). Specifically, LTP learning seeks parameter $\t$ that maximizes the log likelihood of $f_\t(\x)$ on a uncorresponded training set with regards to $p(\y)$. Compared to the conventional (semi)supervised learning approach, LTP can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efficiently implemented and deployed in tasks where running efficiency is critical, such as on-line BCI signal decoding. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video.
count=1
* Projection Retrieval for Classification
    [[abs-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html)]
    [[pdf-NeurIPS](https://papers.nips.cc/paper_files/paper/2012/file/ea8fcd92d59581717e06eb187f10666d-Paper.pdf)]
    * Title: Projection Retrieval for Classification
    * Year: `2012`
    * Authors: Madalina Fiterau, Artur Dubrawski
    * Abstract: In many applications classification systems often require in the loop human intervention. In such cases the decision process must be transparent and comprehensible simultaneously requiring minimal assumptions on the underlying data distribution. To tackle this problem, we formulate it as an axis-alligned subspacefinding task under the assumption that query specific information dictates the complementary use of the subspaces. We develop a regression-based approach called RECIP that efficiently solves this problem by finding projections that minimize a nonparametric conditional entropy estimator. Experiments show that the method is accurate in identifying the informative projections of the dataset, picking the correct ones to classify query points and facilitates visual evaluation by users.
